<DOC>
<DOCNO>WT11-B28-1</DOCNO>
<DOCOLDNO>IA012-000123-B020-234</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.15.html 128.240.150.127 19970217003533 text/html 12033
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:34:04 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 15</TITLE>
<LINK REL="Prev" HREF="/Risks/3.14.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.16.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.14.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.16.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 15</H1>
<H2> Sunday, 29 June 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
A Personal View on SDI from Harlan Mills 
</A>
<DD>
<A HREF="#subj1.1">
Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Having an influence from "within the system" 
</A>
<DD>
<A HREF="#subj2.1">
Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: Research programs that pay for themselves 
</A>
<DD>
<A HREF="#subj3.1">
Rich Cowan
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Text Scanners 
</A>
<DD>
<A HREF="#subj4.1">
Fred Hapgood
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
A Personal View on SDI from Harlan Mills
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Sun, 29 Jun 1986  06:12 EDT
</i><PRE>

On the whole, I am touched by Harlan Mills' remarks.  But I am bothered by
two things.  He says that 

    I [Mills] regard SDI as a political question that will be ultimately
    settled in our political system by the 525 members of our Congress.
    I trust them to make the *WISEST* disposition possible of this question.

    I depend on the Congress
    to make the final, collective, decisions, in how to *BEST* reflect that
    strength for peace in political, diplomatic, and military matters.
    [Emphasis added by me]

These comments reflect a trust in a rational process of government
that I wish I could share; it almost sounds as though he believes that
whatever decision the Congress makes will be right *by definition*.  I
have seen too many instances in which Congress manifestly did NOT do
the right thing to believe in their collective wisdom.  The nature of
a democratic system forces me to *abide* by their decisions, but that
is not the same thing as approving of them or believing in their
wisdom. (On the other hand, I would not trade democracy for anything
else.)

At a somewhat more fundamental level, he states that 

    .. it is somewhat misleading to convert
    the problem of SDI feasibility into the question of software perfection.
    ... The best man can do in
    any physical system is to reduce the probability of failure to low levels,
    not to zero.

The latter statement is a position with which all TECHNICAL analysts
agree: a perfect system is impossible.  But the POLITICAL debate has
been cast in terms of "Do you want to defend yourself or not?",
"eliminating (NOT reducing) the threat of nuclear ballistic missiles"
and "the immorality of threats to kill innocent civilians".  

The technical analysis of the political questions posed above is
absolutely clear, and is that it is impossible to develop technology
that will allow us to get rid of offensive nuclear weapons and shrug
off nuclear missiles should they happen to be launched our way).
Technical analysts then debate the technically more interesting
question of what CAN be done, in which case Mills' comment that

    ... the intent
    of most scientists and engineers working on SDI is to explore the technical
    side intelligently enough to provide the widest range of options possible
    for the political and diplomatic side.

makes a great deal of sense.

But SDI supporters in the political arena find THIS question much less
interesting.  The support that SDI garners from the population at
large, and indeed from those that push it arises from the fact that
defense against ballistic missiles is a truly revolutionary
possibility, that will result in a military posture that is
qualitatively different from that which exists at present.  It won't,
as SDI supporters admit when pushed; they say defenses will enhance
deterrence, and that we will still have to accept societal
vulnerability and to rely on the threat of retaliation to deter Soviet
attack.

Looking at the question from another side, all technical analysts
agree that it is possible to build SOMETHING that sometimes does some
fraction of what you want it to do, and the interesting technical
questions are what is the nature of this something, what will it be
able to do, and how often can it do it.  But the political debate is
cast against the backdrop of technology that is capable of meeting a
certain absolute level of performance, and a rather high one at that.
The technology to do THAT is much more demanding -- if the level of
performance is societal perfection, then it's not reachable at all.
The political proponents try to have it both ways; they want the
political support that comes from belief in the feasibility of this
very demanding technology, and they try to deflect technical criticism
of this political position by saying the question is one of
discovering what technology can do.

Thus, until the broader political debate can be recast in terms of the
desirability of IMPERFECT defenses, and SDI supporters concede
POLITICALLY that defenses will not do what is being claimed for it,
technical analysts, in my view, are fully justified in pointing out
that perfection is not possible.  When SDI supporters make this
concession, the perfect defense issue will become a dead horse
politically as well as technically, and we can all go on to talk about
more interesting things.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Having an influence from "within the system"
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Sat, 28 Jun 1986  17:52 EDT
</i><PRE>

    From: Richard A. Cowan &lt;COWAN&gt;

    You have here touched upon what I believe is -- more often than not -- a
    delusion:  that it is more effective to work within the system to change
    it than to protest it from without.  

Without addressing the specific merits of doing SDI work at this time,
I think this statement needs qualification.  

There is a role for people outside the system.  There is also one for
people inside the system.  Activists are necessary to bring political
pressure.  But they have to have some technical credibility.  As bad
as things are in government now (with people believing in the Tooth
Fairy,.. excuse me, I meant perfect ballistic missile defense), there
is only minimal support for other things that other people would also
like to have -- teaching creationism in the schools for one.  The
reason is that there is NO serious scientific opinion that creationism
has any literal validity at all.  I can assure you that if there were,
the battle to keep creationism out of the textbooks would be a lot
more difficult to fight.

Technical credibility is not the same thing as being "inside the
system".  But "the system" does many things, some of which are
probably right, and others wrong.  But should that mean that people
should give up on the whole thing?  Some of the most effective critics
of the system are those who have extensive experience in it -- Richard
Garwin comes to mind as a prime example.  His effectiveness comes
about because he knows what he is talking about, and it is hard to
imagine that he could have developed his expertise had he remained
forever outside the system.  By contrast, Kosta Tsipis -- while he has
made a rather significant name for himself in the public domain -- has
been identified in most of the public debate that I have heard as a
flake who instinctively knee-jerks against US defense; Tsipis has
never been part of "the system".  (This is not to make a judgement
about the quality of Tsipis' work.)

Then why doesn't the system stop doing silly things?  I guess the
answer has to take the form -- if you think things are bad now, just
imagine how much worse they would be without the likes of Garwin.
While being technically right doesn't necessarily mean that your
position will win, being technically wrong is often the kiss of death.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: Research programs that pay for themselves
</A>
</H3>
<address>
Matthew P. Wiener
&lt;<A HREF="mailto:weemba@brahms.berkeley.edu ">
weemba@brahms.berkeley.edu 
</A>&gt;
</address>
<i>
Sun, 29 Jun 86 02:47:49 pdt
</i><PRE>

I'd like to add a small comment to Richard Cowan's remarks.

One concern about SDI spinoffs is that DoD gets to choose some of
them.  I wonder if, for example, we are going to see more incidents
like the ASATing of Solar Max--a fully working scientific satellite
whose routine operating grant renewal was turned down last summer
to provide a suitable test target.

ucbvax!brahms!weemba	Matthew P Wiener/UCB Math Dept/Berkeley CA 94720

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Text Scanners 
</A>
</H3>
<address>
"Fred Hapgood" 
&lt;<A HREF="mailto:SIDNEY.G.HAPGOOD%OZ.AI.MIT.EDU@XX.LCS.MIT.EDU">
SIDNEY.G.HAPGOOD%OZ.AI.MIT.EDU@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Sat 28 Jun 86 06:33:34-EDT
</i><PRE>
To: risks%OZ.AI.MIT.EDU@XX.LCS.MIT.EDU

The archetypal computer risk is of course unemployment. With regard to this
issue, does anyone know what sort of inroads page and form scanners are or
are not making into the data entry industry, and what features are pacing or
retarding penetration into that market?  Or would anyone have any
suggestions of whom I might call to find out more?
            [Please respond privately to Fred unless your 
             response has RISKS-related implications.  PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.14.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.16.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-2</DOCNO>
<DOCOLDNO>IA012-000123-B020-257</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.16.html 128.240.150.127 19970217003545 text/html 11213
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:34:14 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 16</TITLE>
<LINK REL="Prev" HREF="/Risks/3.15.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.17.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.15.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.17.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 16</H1>
<H2> Monday, 30 June 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Chernobyl (a suprise to the Soviets) 
</A>
<DD>
<A HREF="#subj1.1">
Martin Minow
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Airwaves &amp; Security (2 Subjects) 
</A>
<DD>
<A HREF="#subj2.1">
Richard S. D'Ippolito via dhm
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Interesting Technical Questions (originally SDI) 
</A>
<DD>
<A HREF="#subj3.1">
Martin Moore
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Chernobyl (a suprise to the Soviets)
</A>
</H3>
<address>
Martin Minow, DECtalk Engineering ML3-1/U47 223-9922
&lt;<A HREF="mailto:minow%pauper.DEC@decwrl.DEC.COM ">
minow%pauper.DEC@decwrl.DEC.COM 
</A>&gt;
</address>
<i>
30-Jun-1986 1510
</i><PRE>

From the Danish newspaper Information, May 31, 1986.

Soviet Union
	Ove Nathan: Chernobyl Totally Choked the Leaders

The Danish atomic physicist and rector for Copenhagen University, Ove
Nathan, who is currently attending a conference on atomic weapons in Moscow,
said Friday [May 30] in an interview with Swedish Broadcasting that an
intensive discussion is going on behind the scenes in the Soviet Academy of
Sciences.

According to Ove Nathan, the accident at Chernobyl totally choked the
politicians in charge of the Soviet Union.  They had never imagined that
something similar could have occurred.

Ove Nathan has spoken with several members of the Soviet Academy of Sciences
who said that the mathematical calculations they used in their probability
computations were completely incorrect.  These must be revised, and possibly
also the decision to locate nuclear reactors in or near densely populated
areas.

"The new thing is that they openly admit that they do not know how they will
handle the situation after the accident.  They say that is extremely
complicated, nothing can be taken for granted, and there are no sure factors
one can rely on.  Every day brings a new surprise."

Professor Nathan suggests that this is a situation that is completely
un-Sovietic.  This is the first time in the Soviet history that the elite in
the Soviet Academy of Sciences admit that they don't have firm ground under
their feet.

Ove Nathan believes, that the most serious consequence of the Chernobyl
catastrophe will be an increased demand in the Soviet society for
open information from the government.

Translated by Martin Minow

[The Danish original of the text that I translated as "the mathematical
calculations they used in their probability computations were completely
incorrect" is "den matematiske kalkyle, man har anvendt i sine
sandsynlighedsberegninger, var helt fejlagtige" -- I don't have a dictionary
so I'm not quite certain my translation was completely correct.]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Airwaves &amp; Security (2 Subjects)
</A>
</H3>
<address>
&lt;<A HREF="mailto:dhm@sei.cmu.edu">
dhm@sei.cmu.edu
</A>&gt;
</address>
<i>
30 Jun 1986 15:20-EDT
</i><PRE>

[This message is being forwarded for Richard S. D'Ippolito (rsd@sei.cmu.edu)
whose machine does not yet have ARPAnet access; replies temporarily to
dhm@sei.cmu.edu]

AIRWAVES

It seems to me that what's been missing in the debate on Airwaves/Privacy is
that 'public' ownership is being erroneously equated with 'free access'. We
certainly pay camping fees at public parks and tolls on some public roads.
Public ownership of the airwaves (essentially nothing real) means simply
equal access under the same set of government (public) rules and regulations
so that no group is denied access for discriminatory (in the constitutional
sense) reasons. Now then, why should a business expect to have its product
stolen, which is essentially what is happening? And why can't they protect
their normal interests, i.e., proprietary information, with whatever
security deemed necessary and have the government back them up (with laws
and penalties) just as they do with communications through the mails --
another 'publically owned' and equally accessible enterprise? And by the
way, your rights in this state (PA) in public parks are considerably
restricted from what they are on your own property -- no firearms, alcohol,
pets, or explosives. I can't feel sorry for those who want to steal a
service.

SECURITY

Mr. Richard Cowan has presented what I think to be a commonly held but
misconceived argument on security, locks, and crime. It is not the proper
duty or function of business to reduce the causes of crime by paying
unrealistic wages or creating unnecessary jobs. Some people are thieves,
period, not because they are poor or unemployed. And, as long as there is
one left, all prudent people will want locks. Please, let's skip the
sociological arguments in the discussions of SDI. [Disclaimer: For those who
do not know (most of Pittsburgh doesn't yet) the SEI is not involved with
SDI, nor do we write war (or any) software here -- no flames, please.]

The SDI should be evaluated on several, I believe, criteria. Please let me
try to be brief and state several assumptions (which not all of us may hold):

() We have a defense need (implicit function of the government).
() The perfect defense is one that is never tried.
() The Soviet Union is our strongest enemy.

Given these, we can view the SDI in several ways (sorry to condense):

() If the Soviets are against it, it must be good for us, i.e., it's a
political diversion and keeps them from spending more time on sorry ventures
like Afghanistan.
() It doesn't have to work -- it's successful if no enemy tests it.
() If it causes our enemies to spend a lot of time and resources to match
it, then the diversion of their resources from their people can de-stabilize
the government through the rise of dissent and unrest.

Now, don't we need to include issues like that in the evaluation of any
defense? I'm certainly as unhappy as anybody about wasted tax dollars, as I
pay to many of them now. Also, I would like to live in a peaceful world
(read risk-free), too, but it just isn't going to happen. I would like all
engineers (I'm one) and scientists to take the high side of the debate to
the public -- that we work our butts off to make things as risk-free as
possible and that we are willing to discuss and quantify (where possible)
the magnitude and probabilities of the risks.

In Great Britain, they talk about these things to the public all the time.
Here, only the insurance companies know. For example, in building a chemical
plant, the calculations of the magnitudes and probabilities of a life-
injuring or -destroying accident and the resulting cost (yes, they put cold
numbers on them -- your medical insurance company already has the value of
your arm listed) is factored in along with all the other costs to determine
the proper design and location of the plant in economic terms.

It is totally unrealistic for us to put infinite values on human lives (I
didn't say life) because that's when we conclude that everything must be
perfect and risk free. A perfect example of this kind of reasoning can be
seen in the FDA's treatment of hazardous substances. Have you notice that
the allowable limits of these substances always decreases to the limits of
measurability as new measuring instruments are devised, even in the absence
of direct risk at those levels which are now orders of magnitude below the
levels accepted as harmful? Where do we stop? In more concrete terms, I was
unable to attend a lecture on this subject: Is a program with a known and 
predictable error rate of one wrong answer in 10,000 executions useless?,
but the subject did intrigue me.

				--- Richard S. D'Ippolito (rsd@sei.cmu.edu)
				    Software Engineering Institute
				    Carnegie-Mellon University

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Interesting Technical Questions (originally SDI)
</A>
</H3>
<address>

&lt;<A HREF="mailto:mooremj@eglin-vax">
mooremj@eglin-vax
</A>&gt;
</address>
<i>
0  0 00:00:00 CDT
</i><PRE>
To: "risks" &lt;risks@sri-csl&gt;

&gt; Looking at the question from another side, all technical analysts
&gt; agree that it is possible to build SOMETHING that sometimes does some
&gt; fraction of what you want it to do, and the interesting technical
&gt; questions are what is the nature of this something, what will it be
&gt; able to do, and how often can it do it.

...and how much will it COST?  Not only in money, but in people, raw 
materials, other resources, etc.  This is a fundamental question in
ANY engineering effort.

			Martin Moore (mooremj@eglin-vax.arpa)

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.15.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.17.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-3</DOCNO>
<DOCOLDNO>IA012-000123-B020-279</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.17.html 128.240.150.127 19970217003600 text/html 21612
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:34:29 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 17</TITLE>
<LINK REL="Prev" HREF="/Risks/3.16.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.18.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.16.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.18.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 17</H1>
<H2> Thursday, 3 July 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
How Much Computer Literacy Is Enough? 
</A>
<DD>
<A HREF="#subj1.1">
JAN Lee
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Working within the system 
</A>
<DD>
<A HREF="#subj2.1">
Rich Cowan
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: [Airwaves &amp;] Security -- SDI 
</A>
<DD>
<A HREF="#subj3.1">
Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Complex issues, complex answers 
</A>
<DD>
<A HREF="#subj4.1">
Bob Estell
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Politics and Engineering Practice 
</A>
<DD>
<A HREF="#subj5.1">
Seifert
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Multiple copies of <A HREF="/Risks/3.16.html">RISKS-3.16</A> 
</A>
<DD>
<A HREF="#subj6.1">
Kenneth Sloan
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  GTE Sprint billing problems 
</A>
<DD>
<A HREF="#subj7.1">
Chuck Weinstock/Lee Breisacher
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
 How Much Computer Literacy Is Enough?
</A>
</H3>
<address>
     
&lt;<A HREF="mailto:JANLEE%VTCS1.BITNET@WISCVM.ARPA">
JANLEE%VTCS1.BITNET@WISCVM.ARPA
</A>&gt;
</address>
<i>
Wed,  2-JUL-1986 11:46 EDT
</i><PRE>
ReSent-To: RISKS@CSL.SRI.COM

I would like to open a new area for discussion, that I hope can involve
three elements of the audience: educators, workers, and philosophers, but
hitting at what I believe to be a fundamental element of the "Risks to the
Public" concept.  It is the area of teaching programming.

Over the past several years there has been a salutory movement in the
presentation of first course material away from a course in the syntax of
BASIC (etc.) to a course which is now entitled "Computer Literacy".  There
are numerous textbooks available (25 as of my count published since last
Fall alone!) and the topics seems to fall into four basic areas: (1) An
overview of what a computer is -- including hardware and software, (2) An
excursion into the applications of computers in various fields (which can be
tailored to specific student's interests), (3) The social impacts of
computers on the world (hopefully including something about risks), and (4)
Exposure to some elementary activities such as Word Processing,
Spreadsheets, Graphics and/or Data Bases.  This organization I support
strongly for those for whom this is likely to be the only course they will
ever take in this area, and it's not bad also for those who might go on and
take a programming course later -- at least they get the background needed
for a better understanding of the issues.

NOW FOR MY PROBLEM:  We have taught such a course for about four years
(since the advent of the PC) and have been pleased with the results, one of
which is to strip these students who merely need an exposure to the field
out of the later programming courses.  HOWEVER, in the normal review for a
new course, we were refused approval to continue offering this course unless
we included "real" programming.  Many departments on campus want to have
their students only take one CS course and to be able to program (mostly in
BASIC) problems in application areas afterwards.

To do a plausible job of teaching programming (to my way of thinking)
requires preparation in the methods of problem solving first and a good
grounding in the development process afterwards.  Without cutting out the
guts of a literacy course, I estimate we have 3-4 weeks (9-12 class periods)
to do all this.  These students are going to go out and write programs which
put people at risk -- dieticians, agriculturalists, etc.

I am refusing to offer this course since I do not believe that I can cast
out into the field a group of students whose grasp of the problems of
programming are insufficient to protect themselves (and others) against
errors. So someone else will teach it!

NOW FOR MY QUESTION:  How little can we get away with in preparing students
to use the computer for problem solving and not put their eventual clients
at risk?

JAN

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Re: Working within the system
</A>
</H3>
<address>
Richard A. Cowan 
&lt;<A HREF="mailto:COWAN@XX.LCS.MIT.EDU">
COWAN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Thu 3 Jul 86 21:07:12-EDT
</i><PRE>
To: risks@CSL.SRI.COM

As Herb Lin pointed out, my statement about working within vs.
working outside the system had problems.  First of all, I
unfortunately implied (but did not mean) that "people should give up
on the whole thing &lt;lin@xx&gt;"; in fact, I believe that it is almost
always possible to work within the system to change it!  I think most
people can have a significant, visible effect!

The problem is that many people define "working within the system" in
a narrow, technical or traditional sense which may blunt or negate the
impact they COULD have.  Since the nature of our work and the
prevailing modes of communication are set up in a compartmentalized
fashion to reinforce "the system," one must sometimes circumvent those
normal channels to produce change.  People are deluded only if they
think change will occur through "business as usual."

Although "working outside the system" (and I did not mean violence, as
Mr. Jong of Honeywell assumed) sometimes is necessary, organizing a
peaceful, but active protest towards a goal may divide people over the
goal, alienate those who disagree, produce an institutionally funded
backlash, and discourage supporters if it is unsuccessful.  Instead of
demonstrating, individuals can try to change the CLIMATE in which
group positions are formed FROM WITHIN THE SYSTEM, just by banding
together in small groups to develop arguments that challenge the
standard corporate line.

STRATEGY:
One possible strategy for changing the climate from within is to try
to MAKE IT ACCEPTABLE for the head of your company/institution to
publicly air your concerns.  Although some business leaders may
already have strong contrary views, and be impossible to convince, a
surprising number may already agree with you -- but remain silent for
they lack a support group to give them evidence and confidence.

EXAMPLE:
The president of MIT recently criticized federal research priorities
-- 75% military funding of R&amp;D -- in a public speech (Science June 13,
1986, p. 1333).  Two things had to happen for him to do this: a)
students gave him information documenting these trends and b) people
within the upper eschelons of MIT began talking about the issue after
it was raised by faculty and students.

This may not seem very significant, but such criticisms are rarely
voiced by the heads of US institutions highly dependent on military
funding.  This sends a signal to all kinds of observers, including
policymakers, that the "establishment" is changing course.  It also
sends a signal to management/professors and workers/students (when the
position is reported in the company paper, for example) that makes
it easier for them to discuss the same issues.

If 100 additional university and corporate executives were to each be
persuaded by the actions of a few people in each institution to make
statements on topics generally excluded from public debate, I believe
a significant portion of the "consensus" for US domestic and foreign
policy would erode.  (i.e. imagine what would happen if several
corporate executives felt free to voice opinions such as "a foreign
policy which makes friends of thousands and enemies of millions does
not seem to make good long-term sense" or "certain fields get more
research funding than can be efficiently spent.")

WHERE YOU CAN DO IT:
Certainly professional societies and conferences provide a perfect
medium for high tech people to raise such issues, thereby making it
"acceptable" for others in the profession to have the same concerns.
Even a lowly 23-year-old student like myself can have an enormous
impact merely by clipping articles for professors or administrators
whom I know are concerned but lack the time to get in touch with
activist groups or track down references.  Given a few good references,
these people won't hesitate to incorporate such ideas into their
conversations or speeches, or to express them to people higher in the
chain of command.  When leaders are concerned, the mainstream press
will be more inclined to investigate the issue.  When they do, the
non-activist public follows.

Since economics necessitates that most people must remain within the
system, those people may as well try to make people within existing
institutions more open to change.  The political role of institutions
(especially the leaders) in setting the tone for debate must be held
accountable to someone -- why not the employees?  Think globally, act
locally.  People must insist that the meaning of "service to one's
institution" be redefined so that duties besides "maximizing its
profit in the short term" are included.  Otherwise solutions embodying
these concerns (i.e. economic conversion) will always appear radical
and be immediately dismissed before they reach the public eye.

-rich

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: [Airwaves &amp;] Security -- SDI
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Thu, 3 Jul 1986  10:39 EDT
</i><PRE>

    From: dhm at sei.cmu.edu
    The SDI should be evaluated on several, I believe, criteria. Please let me
    try to be brief and state several assumptions.

    () We have a defense need (implicit function of the government).
    () The perfect defense is one that is never tried.
    () The Soviet Union is our strongest enemy.

These assumptions follow from another, and in my mind, more basic premise:
we want to maintain our way of life free from external coercion.  This more
basic premise can lead to your set of assumptions, or to different sets of
assumptions.  For example, it could lead to the assumption that a reduction
in tensions is a sensible thing to do, which is not mentioned in your set.
Of course, I don't think you intended your list to be complete, so I am just
adding to it.

    Given these, we can view the SDI in several ways:            (condensed)

    () If the Soviets are against it, it must be good for us, i.e., it's a
    political diversion and keeps them from spending more time on
    sorry ventures like Afghanistan.

Maybe true and maybe false.  If you are my enemy, and you start
drilling a hole in your side of the boat, I'm sure going to start
complaining.  I'd think you'd be well advised to listen to me under
those circumstances.

    () It doesn't have to work -- it's successful if no enemy tests it.

But what keeps them from testing it?  The threat of retaliation.
That's what we have now!  That means you have to make an evaluation of
why SDI is a better thing to do given all of the other options if you
say SDI is the way to go.

    () If it causes our enemies to spend a lot of time and resources to match
    it, then the diversion of their resources from their people can 
    de-stabilize the government through the rise of dissent and unrest.

Maybe this is good, and maybe this isn't.  A time-honored way of
rallying the people behind you in time of internal crisis is to
provoke a war.  Do you really want to push the Soviets into that kind
of corner?

    ...Is a program with a known and 
    predictable error rate of one wrong answer in 10,000 executions useless?

It depends on what you use the program for and how often you run it.  For
some things, a 1/10,000 chance of failure is quite acceptable.  For others,
it is quite intolerable.  It depends on what depends on that wrong answer.

Herb Lin

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Complex issues, complex answers
</A>
</H3>
<address>
"143C::ESTELL" 
&lt;<A HREF="mailto:estell%143c.decnet@nwc-143b.ARPA">
estell%143c.decnet@nwc-143b.ARPA
</A>&gt;
</address>
<i>
3 Jul 86 11:14:00 PST
</i><PRE>
To: "risks" &lt;risks@sri-csl.arpa&gt;

There is a risk - however small - that we, like the machines we use, can
begin thinking in "ones and zeros" so that everything is either "true" or
"false."  I believe that much of the power of computers comes from the 
aggregation of those "on" and "off" states to represent complex variables, 
text files, program logic, etc.  Further, it helps to recognize sometimes 
that a third value of even a "logical" variable is "not initialized."

I greatly appreciate Harlan Mills' words that a good decision will come of
the collective wisdom of our 535 Congressmen; they will of course be influ-
enced by literally thousands of citizens(*), hopefully including many with
expert technical qualifications.  Moreover, I see the "official" policy at
any moment as being only one "delta" of a long vector, subject to "mid 
course correction." 
                    [* Note: On the other hand, congress seems heavily 
                       influenced by one citizen in particular.  PGN]

Thus I assert MY OPINION that SDI should not equate to ICBM defense,
even while acknowledging The President's original definition.  Mr. Reagan
also promised to balance the budget, in his 1980 campaign speeches.  That 
goal has proved elusive - if not "illusive."  The nation pursues updated 
versions of it.  Similarly, President Kennedy chartered the "man on the 
moon" project; but that did not later deter the "grand tour of the planets" 
which is still going on.

It follows that I agree that working "within" the system is NOT the only
way; it just happens to be my way, since I am inside.  I applaud efforts
of others to work outside the system, but not against it destructively.
As for "opportunity lost" costs, they are always hard to measure; but we
must attempt that, because it's vital.  What else can we do with the SDI
billions?  Find the cure to the common cold? explore Mars? cut crime in
half? teach Johnny to read? reduce the deficit?  ALL good options.  But
I think we can't expect those alternatives until after '89.  In the interim
if we can begin a DEFENSIVE system that can be shared with allies and others
as well, maybe after '90 we can re-direct many more billions towards these
other worthwhile causes.

Finally, my "epsilon" in the SDI vector is to argue that the billions that
DOD probably WILL spend in this decade be dedicated to concepts and objects
that are feasible, and do have at least potentially useful side effects.
If a major policy shift overtakes that viewpoint, I'll be very grateful.
But meantime, I'd like my professional time, and my tax dollars, to go for
something that I can be proud of - even after the Millennium.

Bob
            [The last paragraph was a little vague and ambiguous, but if you
             read between the lines in this and Bob's previous messages, the
             intended meaning is presumably clear.  However, let's all try to
             sharpen our thoughts and our prose on this issue in the future.
             And keep an eye on the computer relevancy.  PGN]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Politics and Engineering Practice
</A>
</H3>
<address>
Snoopy 
&lt;<A HREF="mailto:seifert%hammer.tek.csnet@CSNET-RELAY.ARPA">
seifert%hammer.tek.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Wed, 2 Jul 86 08:51:56 PDT
</i><PRE>
Reply-To: Snoopy &lt;doghouse.GWD.TEK%hammer.tek.csnet@CSNET-RELAY.ARPA&gt;
Apparently-To: risks@sri-csl.arpa

In <A HREF="/Risks/3.13.html">RISKS-3.13</A>, the sad fact that politics overrules sound engineering
practices is pointed out once more.  Later, our fearless moderator comments
on e-mail bouncing.  Well, guess what?  Part of the e-mail bouncing problem
is political! Here at Tektronix, the mail system was suddenly changed
without notice, thus either bouncing or dropping mail for days or weeks
until every machine changes software, and the "new improved" addresses can
be distributed throughout the world.  The old addresses do not work.  (Real
good design there, guys!) Advance notice would have helped substantially,
but politics dictated otherwise. -sigh-

Snoopy
tektronix!doghouse.GWD.TEK!snoopy	(address du jour)

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Multiple copies of <A HREF="/Risks/3.16.html">RISKS-3.16</A>
</A>
</H3>
<address>
Kenneth Sloan 
&lt;<A HREF="mailto:sloan@uw-tanga.arpa">
sloan@uw-tanga.arpa
</A>&gt;
</address>
<i>
1 Jul 1986 10:16-PDT
</i><PRE>
To: RISKS@csl.sri.com

I received (at least) two copies of <A HREF="/Risks/3.16.html">RISKS-3.16</A>.  Ken Sloan
             ++++++++++++++++++++++++++++++++++++++++
  &gt;From NEUMANN@SRI-CSL.arpa Tue Jul  1 01:09:38 1986
  &gt;Date: Mon 30 Jun 86 23:23:56-PDT
  &gt;From: RISKS FORUM    (Peter G. Neumann, Coordinator) &lt;RISKS@SRI-CSL.arpa&gt;
  &gt;Subject: <A HREF="/Risks/3.16.html">RISKS-3.16</A>
             ++++++++++++++++++++++++++++++++++++++++
  &gt;From NEUMANN@CSL.SRI.COM Tue Jul  1 03:05:47 1986
  &gt;Date: Mon 30 Jun 86 23:23:56-PDT
  &gt;From: RISKS FORUM    (Peter G. Neumann, Coordinator) &lt;RISKS@CSL.SRI.COM&gt;
  &gt;Subject: <A HREF="/Risks/3.16.html">RISKS-3.16</A>
             ++++++++++++++++++++++++++++++++++++++++

      [The clue of course is the different FROM Fields.  SRI-CSL went down
       during the wee hours of the morning in order to be reborn under its
       new name of CSL.SRI.COM.  The mailer did its usual trick when the
       system bombs in the middle of a mailing -- it retries certain addresses
       to which it had already sent successfully.  Sorry.  But PLEASE NOTE THE
       NEW HOST NAME for RISKS and RISKS-Request: @CSL.SRI.COM.  Thanks.  PGN]

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
GTE Sprint billing problems
</A>
</H3>
<address>
&lt;<A HREF="mailto:Chuck.Weinstock@sei.cmu.edu [and From: Breisacher.OsbuSouth@Xerox.COM]">
Chuck.Weinstock@sei.cmu.edu [and From: Breisacher.OsbuSouth@Xerox.COM]
</A>&gt;
</address>
<i>
2 Jul 1986 11:31-EDT
</i><PRE>

Sprint just enclosed the following notice in its latest billing:

  We have recently discovered an error in our billing system related to
  the changeover to daylight savings time.  The error may have caused
  some calls made in the period April 27, 1986 - May 1, 1986 to be
  billed incorrectly.  The error has been corrected, and we are in the
  process of determining whether your bill was affected.  If so, an
  appropriate adjustment, including applicable taxes and interest, will
  appear on a future bill...

   [...although this one does not appear to have been too costly...  PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.16.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.18.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-4</DOCNO>
<DOCOLDNO>IA012-000123-B020-301</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.18.html 128.240.150.127 19970217003612 text/html 14234
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:34:42 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 18</TITLE>
<LINK REL="Prev" HREF="/Risks/3.17.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.19.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.17.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.19.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 18</H1>
<H2> Tuesday, 8 July 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Computer Crime in Scandinavia 
</A>
<DD>
<A HREF="#subj1.1">
Martin Minow
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Re: Risks from inappropriate scale of energy technologies 
</A>
<DD>
<A HREF="#subj2.1">
Henry Spencer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Sensor technology and disinformation 
</A>
<DD>
<A HREF="#subj3.1">
Eugene Miya
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Educating to prevent RISKS 
</A>
<DD>
<A HREF="#subj4.1">
Steven Gutfreund
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Rash of 'Undeliverable mail' 
</A>
<DD>
<A HREF="#subj5.1">
Chuck Price
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Computer Crime in Scandinavia
</A>
</H3>
<address>
Martin Minow, DECtalk Engineering ML3-1/U47 223-9922
&lt;<A HREF="mailto:minow%pauper.DEC@decwrl.DEC.COM  ">
minow%pauper.DEC@decwrl.DEC.COM  
</A>&gt;
</address>
<i>
04-Jul-1986 0922
</i><PRE>

From the Danish newspaper, Information, (I think on 31-May-1986):

Datatapping -- The Oslo [Norway] firm, Finn Solvang A/S, has reported a
Danish engineer to the police in Denmark for an attempt to get a woman
employed by the firm to tap the company's computer system for valuable
information on customer lists and design.  The woman was offered money and
instruction on how she could do the work during a weekend.  The engineer is
employed by a Danish firm which had collaborated with the Norwegian, but
which became a competitor at the beginning of the year.

Martin Minow

(In my note on Chernobyl, I accidentally translated the Danish
word "chokerade" as "choked" when it should be "shocked" -- that's
what comes from writing with my fingers and not my mind.  Funny
that my spelling checker didn't catch it...

A few native speakers of Danish confirmed that the sentence I wasn't
too certain of was reasonably translated.  One said that a better
translation might have been "the mathematical models used were
completely wrong," making it more of a design failure than a
programming bug.

Martin.)

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Re: Risks from inappropriate scale of energy technologies
</A>
</H3>
<address>
&lt;<A HREF="mailto:decwrl!decvax!utzoo!henry@ucbvax.Berkeley.EDU">
decwrl!decvax!utzoo!henry@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Fri, 4 Jul 86 21:18:30 edt &lt;RETRY OF MUCH EARLIER FAILED TRANSMISSION&gt;
</i><PRE>

&gt;   I think that we should be pursuing a policy course which develops
&gt; technology that can be put safely in the hands of non-technical people.
&gt; This might take the form of small burners which use the methanol from
&gt; organic wastes, windmills, or non-electrical solar collectors, to name a few
&gt; possibilities.  Localized, distributed technologies have many advantages,
&gt; including ease of repair, localization of risk from outage, and major
&gt; reductions in distribution losses and cost of distribution equipment and
&gt; labor...

Let us not forget that distributed technologies create their own new
categories of risks.  The advantage of centralized resources is that much
more attention can be given to keeping them safe, and they do not have to
be designed to be utterly idiot-proof.  (Although it helps...)

Automatic collision avoidance for airliners is imminent, while for cars it
is far away.  Why?  Because such a system for cars would have to be cheap,
easy to install and maintain, and 99.999999% reliable in a wide range of
conditions despite being maintained at long, irregular intervals by
largely unskilled people.  Although all these characteristics certainly
would be desirable for airliner systems, they are not *necessary*.  Airlines
can afford relatively expensive systems needing frequent attention, and can
ensure that they are given regular checkouts by skilled personnel.  An
airliner system can also assume that a qualified pilot, prepared for the
possibility of mechanical failure, is minding the store at all times.
(Such assumptions are not invariably true even for airliners; the point
is that they are seldom or never true for cars.)

Even disregarding this specific example, a quick look at accident rates for
car travel and air travel yields interesting results for the "distributed
is better" theory.  Does anyone seriously believe that the level of safety
attention routinely given to aircraft could possibly be given to cars?

Don't forget to compute the accident potential of distributed technologies.
Methane is an explosion hazard, as witness the safety considerations for
virtually any appliance using natural gas (natural gas is essentially
straight methane).  Windmills and solar-heat collectors don't have that
problem, at least, but they do require maintenance and they are generally
far enough off the ground to present a risk of accidental falls.  (Last
I heard, falls were the #2 [after auto accidents] cause of accidental
death.)  One can argue about whether lots of little accidents are preferable
to a few big ones, but dead is dead either way if you're one of the victims.
And it's not clear that the overall death rates are lower for distributed
systems.

There is also the question of voluntarily-assumed risks versus ones one
cannot avoid, but it seems to me that this case doesn't really present much
of a dichotomy.  If nobody builds central power plants, I really have little
choice about whether I assume the risks of generating my own power.  Yes,
I can avoid them at the cost of major inconvenience (doing without), but I
could also avoid most of the risks of centralized power at the cost of
major inconvenience (move to Fiji).

				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Sensor technology and disinformation
</A>
</H3>
<address>
Eugene miya 
&lt;<A HREF="mailto:eugene@ames-aurora.arpa">
eugene@ames-aurora.arpa
</A>&gt;
</address>
<i>
7 Jul 1986 1519-PDT (Monday)
</i><PRE>

As the person who started the SDI sensor technology question which
has had a couple of follow ons to Arms-d, permit me to make one comment
and raise one question which Charlie Crummer@aerospace only alludes.

First, IR technology despite advances in sensor technology cannot get around
the "3-body, hidden object" problem.  Given a sensor and a target, if an
intervening "warmer object" passes in between, the target disappears.  This
is an azimuth ambiguity.  It sound trivial, but it is not, especially when
the intervening object might be air (which does have temperature), or a
mist, or other non-massive-solid.  My intent is only to point this out, not
some IR remote sensing.

Second, the Administration has stated a policy of disinformation with regard
to SDI and letters denouncing such have appeared in Aviation Week.  My
question is: if we as scientists announce something as "disinformation" as
one of Charlie's comments, what are all of the consequences?  I can think of
several including counter-announcements, the usual financial thumbscrews to
funding agencies, Ellsberg type operations, and so forth.  Problem is this
is not a leak of information, and it's not clear to me that the SDIO can
persecute this like espionage cases.  Is Charlie irresponsible for revealing
disinformation?  Are we as scientists expected to maintain disinformation?
Also, disinformation in the past has been known to backfire (another risk?).

Again the usual disclaimer that these are the opinions of the individual
and not my employer, and STAR WARS is a trademark of Lucasfilm, Ltd.
despite what courts say.

--eugene miya
  NASA Ames Research Center
  eugene@ames-aurora.ARPA

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
 Educating to prevent RISKS
</A>
</H3>
<address>
    "Steven H. Gutfreund" 
&lt;<A HREF="mailto:GUTFREUND%cs.umass.edu@CSNET-RELAY.ARPA">
GUTFREUND%cs.umass.edu@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Mon, 7 Jul 86 12:32 EST
</i><PRE>

RE: Jan Lee (RISKS V3 N17) on the risks of not educating casual programmers.

Your problem (in a nutshell) seem to be with the administration which needs
to be made aware (educated) about the risks of under-educated programmers,
than with the students themselves.

To phrase this question in full generality:

	How do I make a person aware that his course of action 
	contains risks which he is underplaying or not cognizant of?

Classic examples of this are:

a) Try teaching a child not to touch the hot stove.
b) Teach your young and eager disciple that you have learned (via years
   of painful pratical experience) that he needs to take a more cautious
   approach (e.g. to design of large programming problems)
c) Teach your manager (who lacks modern engineering skills) that the project
   plan is too risky.


Approaches to attack this include:

1) Let the kid touch the stove (or the project go down the tubes)
2) Turn the issue into a confrontation (boycott the project meetings,
   threaten the child with loss of priviledges, etc.)
3) Try and instill the Fear of G-D in the person (long careful explanations,
   dissertations, memos, etc.)

There seems to be a fundamental problem in any form of directly trying to
educate the unaware individual. Since what you are basically trying to do
is increase the persons level of anxiety, fear, or distrust of his own
thought processes. Since these emotions are not normally identified with
more "rational" attitudes, there is bound to be distrust of your motives.
As long as you proceed with any of the above mentioned "direct" approaches,
he is bound to be AWARE of your efforts, and draw the negative conclusions.

It seems to me then that only indirect and subtle approaches will succeed.

This conclusion should be seen as especially relevent to RISKS contributors
since most of them seem to be involved in publicizing fears and anxieties.

			- Steven Gutfreund

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Rash of 'Undeliverable mail' 
</A>
</H3>
<address>
Chuck Price
&lt;<A HREF="mailto:price@src.DEC.COM ">
price@src.DEC.COM 
</A>&gt;
</address>
<i>
Tue, 8 Jul 86 11:20:05 pdt
</i><PRE>
Subtitle: Risks of undigestification

Help! Ever since you published "License Plate Risks" in the Risks Forum,
I have been receiving a number of 'undeliverable mail' messages. A sample
is attached.

Is there any way we can stop this? I'm starting to feel like Robert Barbour.

-chuck
 
  ------- Forwarded Message  [...]

  Date: 8 Jul 1986 12:30:26-EDT
  From: netmailer%MIT-CCC@mit-mc
  Subject: Undeliverable mail
  Apparently-To: &lt;price@SRC.DEC.COM&gt;

-- Your letter to `ghuber@MIT-MARIE' is being returned because: --

	Mail undeliverable for too long

-- Returned letter follows: --

Date: 30 Jun 1986 12:32:31-EDT
From: price@SRC.DEC.COM@MIT-CCC
Date: Monday, 23 June 1986  12:56-EDT
To: RISKS-LIST:@XX.LCS.MIT.EDU, RISKS@SRI-CSL.ARPA
Subject:   License Plate Risks
ReSent-From: LENOIL@XX.LCS.MIT.EDU
ReSent-To: info-cobol@ccc
ReSent-Date: Mon 30 Jun 1986 01:50-EDT

    [Chuck's original message followed.  This could be another risk
     of undigestification.  If I simply remailed individually all of the
     messages in each issue of RISKS, then EACH contributor would have to
     put up with the enormous number of BARF message that your moderator
     otherwise puts up with!  PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.17.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.19.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-5</DOCNO>
<DOCOLDNO>IA012-000123-B020-320</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.19.html 128.240.150.127 19970217003623 text/html 12826
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:34:53 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 19</TITLE>
<LINK REL="Prev" HREF="/Risks/3.18.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.20.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.18.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.20.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 19</H1>
<H2> Thursday, 10 July 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Computer Literacy 
</A>
<DD>
<A HREF="#subj1.1">
Rick Smith
</A><br>
<A HREF="#subj1.2">
 Bob Estell
</A><br>
<A HREF="#subj1.3">
 Col. G. L. Sicherman
</A><br>
<A HREF="#subj1.4">
 PGN
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Computer Literacy and BASIC
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: 09 Jul 86 10:38:27 CDT (Wed)
From: smith%umn.csnet@CSNET-RELAY.ARPA

  No doubt JAN Lee's colleagues in other departments think that the literacy
course is simply propaganda to improve the image of computer science and
programming as serious (and difficult) work.  It's a pity that it can be so
easy to get a program to APPEAR to work and that most people are satisfied
with apparent success.  After all, a screwdriver almost looks like a chisel,
and it does almost as good of a job, at least for a while.
  I think Weinberg had an anecdote in "Psychology of Computer Programming"
about how some DP types tried to show their managers how hard programming was
by making them do some trivial BASIC programs. The managers had little
trouble with their programs and went away convinced that programming was
even easier than they thought.
  There's a story that circulates around here about a BASIC program written
several years ago. The program simulates household heating plants as part of
a model of resource usage.  It started as a Fortran program written at the
research center of a large, local computer company.  The company hires students
for part-time work, one of which helped write the original Fortran program.
Another student was hired later to re-code the program in BASIC. Since then
the program has been sold to one of the gas industry associations and a copy
was eventually sold to the Department of Energy. The students who worked on
the program describe the style as a form of 'advanced spaghetti' and don't
know whether to laugh or cry at the thought of it being used to plan national
energy policy.

Rick Smith.
U. Minnesota

</PRE>
<HR><H3><A NAME="subj1.2">
recognizing that one programming course is NOT enough
</A>
</H3>
<address>
"143C::ESTELL" 
&lt;<A HREF="mailto:estell%143c.decnet@nwc-143b.ARPA">
estell%143c.decnet@nwc-143b.ARPA
</A>&gt;
</address>
<i>
10 Jul 86 09:04:00 PST
</i><PRE>
To: "risks" &lt;risks@sri-csl.arpa&gt;

Who should bear the responsibility for damage done by programming errors?
Everyone involved; e.g.

 Colleges screen students for admission, give exams and require term
 projects for course credits, and charge tuition and fees for all that;
 thus colleges ultimately bear some responsibility for the credentials
 of their graduates.  Those who over a period of time produce shoddy 
 workers should lose their reputation, if not their accreditation.

 Employers hire workers, give them tasks to do, and pay them for the work; 
 and then make profits from the sale of those products or services;
 thus employers ultimately bear some responsibility for the products and
 services of their employees.  Those who over a period of time produce 
 shoddy products or services should lose money, or even go bankrupt.

 Buyers seek products and services, and pay for them, so they ultimately
 bear some responsibility for their choices.  Let the buyer beware.

 Last but certainly not least, individuals who study, produce, and sell must
 certainly bear some responsibility for the products &amp; services they offer.
 Recently, Nader has lobbied through laws making individual corporate exec-
 utives criminally liable for obviously defective products; e.g., when it
 can be proved that an auto maker produced and sold cars known to contain
 safety faults that led to accidental failures, injury, etc., then the man
 who gave the order to proceed can not hide behind a corporate mask ; a 
 corporate fine is not enough; the man may end up in jail.

I would suggest then that when John Doe, a graduate of College of Somewhere, 
working for the Acme Corp., writes code that causes damage, his Alma Mater,
the Acme Corp, John himself, and the "buyer" are jointly responsible.

Because buyers can't know enough to intellengly "beware" it will be often
necessary to "buy insurance" in some form; that's why most of us go to MD's
that are licensed by the state, and colleges that are accredited by peer
groups; and why so many computing consultants "recommend IBM."

When unschooled folks set themselves up as private consultants, and hard-
sell their products or services, they bear 99% of the total responsibility
for the results.  That might have the effect of reducing the number of
freelance consultants, who charge lots of money for buzz-wordy reports.
I would view that as a step forward in our industry.  The good ones would
not only survive, they would prosper - and be easier to find.

Finally, how can a professor convince the dean that one programming course
is not enough?  We can start by telling folks that since "IBM can teach
you to program in FORTRAN in three days" it does NOT follow that one so
trained can DO real problems in any language.  By analogy, the Acme Driving
School may teach one to drive in three days; that does not entitle him to
a special license as a chauffeur, or to drive a 5-axle rig; and certainly
does not qualify him to race a Le Mans, or Indy.  Maybe if we [computer
folks] turn the problem around, the others can see it better; e.g., we
might suggest that our computer graduates need to appreicate physics or
economics, so that they can write code that will darn near dominate the
future work of physicists and economists; thus we suggest that those other
departments devise one 3-hour [4-hour?] course to teach them all they need
to know.  After the initial [angry] retort, maybe we can enter a dialogue.

Bob

P.s. The foregoing are personal opinions, not those of my employer.

</PRE>
<HR><H3><A NAME="subj1.3">
Re: <A HREF="/Risks/3.17.html">RISKS-3.17</A> (JAN Lee on Computer Literacy)
</A>
</H3>
<address>
"Col. G. L. Sicherman" 
&lt;<A HREF="mailto:colonel%buffalo.csnet@CSNET-RELAY.ARPA">
colonel%buffalo.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Wed, 9 Jul 86 12:57:17 EDT
</i><PRE>
To: risks &lt;@CSNET-RELAY.ARPA:risks@CSL.SRI.COM&gt;
Summary: more on risks of teaching "just" programming
References: &lt;8607040434.AA11090@ucbvax.Berkeley.EDU&gt;

&gt; NOW FOR MY QUESTION:  How little can we get away with in preparing students
&gt; to use the computer for problem solving and not put their eventual clients
&gt; at risk?

JAN Lee's concern is misplaced.  The "top-down" approach to teaching _about_
computers is overemphasized, perhaps because the phrase "computer literacy"
sounds meaningful to educators.  But the one absolute requisite for becoming
a good programmer is to write programs, programs, and more programs--in any
language, on any equipment available, in any environment.

I've taught hundreds of C.S. students here.  By the time we graduate them,
I know which students are likely to succeed:  it's those who are self-
motivated.  The students who are just "getting an education" write no
more programs than they need to, develop very slowly, and go on to write
some very bad code for their employers.  The students who _like_ to program
write plenty of programs, learn from experience what the others try to
learn by attending lectures, find alternative computers to work on or buy
P.C.s if the school computer is unusable, and tend to excel in all kinds
of C.S. courses.

In short, while BASIC is obviously "riskier" than Pascal, I regard the
language issue as a minor one.  The earlier a student starts turning
problems into programs, the safer her eventual clients will be.  It's
futile and counterproductive to refuse to teach "just programming" on
the grounds that computers are dangerous when they go wrong.  Cars are
dangerous, but we don't require auto mechanics to know about the thermo-
dynamics of combustion engines or the social consequences of motor travel.
We ask only that they be competent mechanics.

</PRE>
<HR><H3><A NAME="subj1.4">
Computer Literacy (Programming versus software engineering)
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Thu 10 Jul 86 14:56:07-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

With regard to the previous message, I am in Washington this week for a
conference on ensuring that a system really does what it is supposed to
(COMPASS 86) and a workshop on testing, formal verification, and software
engineering.  This prompts me to make all sorts of comments on this issue,
although they may have to wait until later.

THERE IS AN ENORMOUS DIFFERENCE BETWEEN WRITING CODE AND WRITING GOOD
SOFTWARE.  Any damned fool can write code.  It takes a particularly perverse
damn fool to write software that can be trusted to live up to rigorous
requirements (which might include rugged and forgiving interfaces,
reliability, maintainability, understandability, reusability, security,
human safety, and so on). It also takes a lot of discipline, good taste, an
instinct for elegance, training, and experience.  An appropriate programming
language might also help (but does not substitute for the above), as might a
software development methodology -- if large and complex software is to be
developed.  The grave danger of computer literacy courses is indeed that
they tend to endow BASIC or LISP or FORTRAN or C (or even Ada!) with magic
properties.  BEWARE OF SIMPLISTIC SOLUTIONS.

Writing hundreds of BASIC programs won't teach you very much about good
programming style.  In fact, if you did write hundreds of BASIC programs,
one might suspect you hadn't learned the most important things at all --
which might even include the lesson of learning to look for a better
programming language!

Allegedly "competent mechanics" have cost me hours of anguish, many dollars,
and a few grave personal risks.  I prefer really good, experienced mechanics
who work well because they know what they are doing.  If you give one an
engine he has never seen before, he has to go through a learning curve --
although he will undoubtably learn much faster than the mere competent.
But, the analogy is awkward -- you are asking your mechanic to keep your car
working safely, not to design it from scratch in the first place.

PGN

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.18.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.20.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-6</DOCNO>
<DOCOLDNO>IA012-000123-B020-339</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.20.html 128.240.150.127 19970217003637 text/html 19283
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:35:05 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 20</TITLE>
<LINK REL="Prev" HREF="/Risks/3.19.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.19.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 20</H1>
<H2> Tuesday, 15 July 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Risks of computer incompetence 
</A>
<DD>
<A HREF="#subj1.1">
Dave Benson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  RE: educating about RISKS 
</A>
<DD>
<A HREF="#subj2.1">
Don Lindsay
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Computer Literacy (<A HREF="/Risks/3.19.html">RISKS-3.19</A>) 
</A>
<DD>
<A HREF="#subj3.1">
Ron Morgan
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Computer Literacy and Basic 
</A>
<DD>
<A HREF="#subj4.1">
Martin Minow
</A><br>
<A HREF="#subj4.2">
 Andrew Klossner
</A><br>
<A HREF="#subj4.3">
 PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Dial-up computing 
</A>
<DD>
<A HREF="#subj5.1">
Sterling Bjorndahl
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Research programs that pay for themselves  
</A>
<DD>
<A HREF="#subj6.1">
Clayton Cramer
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Risks of computer incompetence
</A>
</H3>
<address>
Dave Benson 
&lt;<A HREF="mailto:benson%wsu.csnet@CSNET-RELAY.ARPA">
benson%wsu.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Fri, 11 Jul 86 12:43:42 pdt
</i><PRE>

Regarding who is responsible for computer mistakes:  The individual or
organization who sold/licensed the software is (or should be) responsible
in the eyes of the law.

In civil engineering, and a few other engineering disciplines, this
responsiblity is dealt with explicitly by professional licensing by the
state governments.  Unfortunately, there is no professional registration for
software engineers in any state. (If you know of one, please do let me know.)

The registration as a professional engineer has the same sort of effect as
the licensing to practice medicine--a public statement of at least a minimal
competence and a certain small amount of protection in case mistakes are made.

Not much is going to improve until the citizens agree that such a licensing
procedure is necessary and software purchasers are willing to pay the extra
cost this will cause--in essense, that they are willing to pay extra for
lower risk.

JAN Lee and other educators might take the tack that the first course in
computing is the beginning of a professional degree program.  One course
does not establish competence in any other field.

However, just as I need not have a professional registration in civil
engineering to design and build a shed in my backyard, so I need only a
little "computer literacy" to write a large range of truly useful,
small-scale software.  Since the results are not that remote from immediate
experience, there is little risk.  For example, a small program which makes
pie charts can have the output quickly checked for accuracy.

A far greater concern is that most of the B-school BASIC hackers do not
understand the mathematics underlying the calculations made in their small
economic prediction models.  Now there is a far greater risk that the model
will produce wrong results, either from software misdesign or from a failure
to understand the limitations of the mathematical model.

A BA or BS from a modern American university should never be taken as a
license to practice.  It is a minimal certification that the graduate
learned something, but no guarantee of competence.  Indeed, to obtain a
professional registration as an engineer requires several years of practice
as "engineering associate" under the direct guidance of an experienced,
registered engineer.  Surely the same effect holds in Business
Administration, Software Engineering.  It certainly does in Medicine as the
new MD is required to intern before licensure for private practice.

None of this social mechanism applies to the truly large-scale software
systems used in commercial and military practice today.  The means of
establishing low risk for any large project (software, nuclear power
reactor, SDI, etc.) are imperfectly understood.  I believe that it requires
the right sort of organization, a particular commitment to quality which
used to be exemplified by NASA.  But I certainly couldn't tell you just what
the characteristics of such organizations might be beyond high morale and
lots of $$.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
RE: educating about RISKS
</A>
</H3>
<address>
&lt;<A HREF="mailto:LINDSAY@TL-20B.ARPA">
LINDSAY@TL-20B.ARPA
</A>&gt;
</address>
<i>
Thu 10 Jul 86 12:09:30-EDT
</i><PRE>
To: risks@SRI-CSL.ARPA

Steven H. Gutfreund stated a problem:

	How do I make a person aware that his course of action 
	contains risks which he is underplaying or not cognizant of?

Speaking as a parent, I believe in letting the kid touch the hot stove.
(Yes, I really did.)  Speaking as a software engineer, I believe that
humor is the only effective way to communicate anxieties to students.

There are several reasons why storytelling works. For one, it sugar-coats the
lesson. It makes the point more memorable. It creates the (lesser)
anxiety of becoming the butt of peer amusement. And, for some students,
it seems to be the only way to give them any appreciation of why they
they should change their ways.

Don Lindsay

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Computer Literacy (<A HREF="/Risks/3.19.html">RISKS-3.19</A>)
</A>
</H3>
<address>
Ron Morgan
&lt;<A HREF="mailto:osmigo1@ngp.UTEXAS.EDU ">
osmigo1@ngp.UTEXAS.EDU 
</A>&gt;
</address>
<i>
Mon, 14 Jul 86 23:46:14 cdt
</i><PRE>
Organization: Speech Communication UT Austin
Apparently-To: risks@sri-csl.arpa

As a certified all-level teacher, I'd like to say a word or two about the
current "computer literacy" craze. First of all, there seems to be this
constant desire to equate "computer literacy" with "programming," which
ignores the fact that probably 90% of the people who use computers are *NOT*
programmers. Programming is a profession, just like welding or accounting or
dentistry. Courses in programming are by their very nature pre-vocational
courses, regardless of whether or not they are intended as such. 

Don't get me wrong; I'm not against courses in programming. A semester of it
should be required of all secondary students, to give them an idea of what
makes a computer tick, as well as giving them an awareness of what a proper
program (stylewise) is; hopefully, they will become good software critics,
at least. Students that feel an interest in becoming professional
programmers should be all means have access to advanced courses that teach
good style, preferably in a structure-sensitive language like Pascal. It
would be a waste not to do so, in light of some of the young geniuses we are
seeing more and more often these days. I know of more than one "high school
hacker" that has written his or her own "bulletin board" program in
*self-taught* assembly language, on such machines as the TI 99/4A and Atari
800. Recently, I talked with a 16-year-old boy that wrote a program linking
two IIe's for use in running a bulletin board as a *dual-CPU* system. Sure,
give these kids what they want. I'm all for it.

However, for the average Jack and Jill student, the emphasis, in my opinion,
should be on developing a wide range of solid skills in USING computers.
That's basically what "computer literacy" is supposed to be preparing them
for, right?  A society that USES computers, not a "society of programmers."
I say give them courses in *real* word-processing, setting up spreadsheets,
integrated applications, graphics design, telecommunications, music
synthesis, database management, printer codes, statistics programs, and so
on. Such knowledge, for the average student, would be far more useful, both
vocationally and personally, than ten tons of required programming courses.

Ron Morgan

osmigo1, UTexas Computation Center, Austin, Texas 78712
ARPA:  osmigo1@ngp.UTEXAS.EDU
UUCP:  ihnp4!ut-ngp!osmigo1  allegra!ut-ngp!osmigo1  gatech!ut-ngp!osmigo1
       seismo!ut-sally!ut-ngp!osmigo1  harvard!ut-sally!ut-ngp!osmigo1

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Basic (a flame)
</A>
</H3>
<address>
Martin Minow, DECtalk Engineering ML3-1/U47 223-9922
&lt;<A HREF="mailto:minow%pauper.DEC@decwrl.DEC.COM  ">
minow%pauper.DEC@decwrl.DEC.COM  
</A>&gt;
</address>
<i>
11-Jul-1986 2112
</i><PRE>

I -- and a number of my friends and collegues -- have written large
numbers of high quality Basic programs.  These programs have been
reliable, suitable to their tasks, maintainable, and efficient.

Thirteen years ago, I published a paper on writing "professional"
programs in Basic (Decus European Symposium, London 1973).  Very little
of what I said there was particularly original: it is the sort of stuff
I was taught when I learned to program way back when.

Basic has the great advantage of being easy to learn.  The concepts
of arithmetic and control flow seem quite natural, in many ways simpler
than "structured" languages such as the descendents of Algol 58.

More importantly, Basic (Dec's RSTS/E Basic-Plus) was the first language
I worked with to offer immediate feedback for syntax errors and easy
incremental development.  I dearly wish the people who demean Basic
would invent a tool which suits their tastes, but retains the
simplicity and user-friendliness of Basic.

[...] Come to think of it, it might be interesting for the Risks subscribers
to compare the relative risks-to-society of a simple, intuitive langauge
such as Basic against the more elegant, but harder to use, language such as
ADA (or even Pascal).

Martin Minow.

</PRE>
<HR><H3><A NAME="subj4.2">
Re: <A HREF="/Risks/3.19.html">RISKS-3.19</A>
</A>
</H3>
<address>
Andrew Klossner 
&lt;<A HREF="mailto:andrew%lemming.gwd.tek.csnet@CSNET-RELAY.ARPA">
andrew%lemming.gwd.tek.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Fri, 11 Jul 86 08:00:00 PDT
</i><PRE>
ReSent-To: RISKS@CSL.SRI.COM

	"Writing hundreds of BASIC programs won't teach you very much
	about good programming style.  In fact, if you did write
	hundreds of BASIC programs, one might suspect you hadn't
	learned the most important things at all -- which might even
	include the lesson of learning to look for a better programming
	language!"

This sort of chauvinism has no place in the RISKS forum.  BASIC, like
any tool, has excellent utility in its domain.  For example, a
complicated graphics display can be programmed easily in ANSI BASIC-86,
which has a standardized statement level binding to an appropriate
subset of the GKS Graphical Kernel Standard.

By now we should be beyond the point where we laugh at any language
other than our favorite as being unsuitable for any serious programming
endeavor.

  -=- Andrew Klossner   (decvax!tektronix!tekecs!andrew)       [UUCP]
                        (tekecs!andrew.tektronix@csnet-relay)  [ARPA]

</PRE>
<HR><H3><A NAME="subj4.3">
Re: <A HREF="/Risks/3.19.html">RISKS-3.19</A>
</A>
</H3>
<address>
Andrew Klossner 
&lt;<A HREF="mailto:andrew%lemming.gwd.tek.CSNET@CSNET-RELAY.ARPA">
andrew%lemming.gwd.tek.CSNET@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Tue, 15 Jul 86 07:33:18 PDT
</i><PRE>
ReSent-To: RISKS@CSL.SRI.COM

   [PGN responsed to AK:
   However, the intrinsic pitfalls of BASIC are such that you might be very 
   foolish to use it in a critical application.  I have used several popular
   BASIC programs that can't even give reproducible results!]

You'd be hard put to come up with a commonly-used language for which
this isn't true.                              [Nonreproducible?  Yuk.  PGN]

But your original statement didn't concern itself with critical
applications.  You spoke of any situation in which someone had written
hundreds of programs.[*]  In a RSTS DP shop, popular a few years ago on
PDP-11s, BASIC was the only reasonable language available, and it was quite
suited to the task.  In educational software development, where target
systems are characterized by inexpensiveness and availability of BASIC, that
language must be used if code is to be portable.
                        [* from the RISKS point of view, of course...  PGN]

The point is that a knee jerk reaction that BASIC, or any single language,
is inherently unsuited for any field of application smacks of elitism.

  -=- Andrew Klossner   (decvax!tektronix!tekecs!andrew)       [UUCP]
                        (tekecs!andrew.tektronix@csnet-relay)  [ARPA]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Basic and critical systems
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Tue 15 Jul 86 21:34:28-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

This topic generated quite a few replies.  The intent of my original comment
was of course RISKS related.  Certainly, a skilled and careful programmer
can write excellent Basic programs, and a sloppy programmer can write bad
programs in any language.  But Basic has many intrinsic pitfalls that could
make it harder to use in developing critical systems -- lack of modularity,
abstraction and type safety, the presence of GOTOs (PLEASE let us not start
that controversy again -- GOTOs are not impossible to use safely, just
easier to misuse), etc.  PGN

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Dial-up computing
</A>
</H3>
<address>
&lt;<A HREF="mailto: BJORNDAS%CLARGRAD.BITNET@WISCVM.ARPA">
 BJORNDAS%CLARGRAD.BITNET@WISCVM.ARPA
</A>&gt;
</address>
<i>
15 JUL 86 12:53-PST
</i><PRE>

Saturday night I dialed up to our academic computing center's VAX,  as
usual.  Later, as I sometimes do, I absent-mindedly hit the disconnect
button on my modem before logging off.  "Bad form," I said, "I really
shouldn't do that."  But I didn't worry, because I *knew* that the
network computer would log me off.  It always had in the past.
(I am a student at Claremont Graduate School.)

Sunday morning I dialed up again and found myself in the middle of the
process I had left the night before.  No login.  No password.  Just a
'$' prompt on my screen.  I had been "connected" for 13 hours.
Luckily for me, no one else had tried to dial in during that time.
Not even some youthful hacker with a machine to try out all the phone
numbers in sequence....

Checking it out on Monday with our consultants, I found that new
changes to the networking software had introduced this bug.  What
happened to me had also happened to several people with privileged
accounts a few days earlier.

Risks to the public?  My risk was basically personal.  But if someone
had gotten into high security accounts this way, the whole
installation might have been at risk.  The results of important
academic research might have been lost as well.  Or would that have
been a benefit to the public? :-)

Sterling Bjorndahl

   [We have noted previously the long-standing TENEX flaw with the similar 
    effect -- TENEX fails to detect line loss or hangup without logout, and
    leaves your job logged in with its port waiting for the next person to
    stumble upon it.  PGN]

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Research programs that pay for themselves 
</A>
</H3>
<address>
Clayton Cramer
&lt;<A HREF="mailto:voder!kontron!cramer@ucbvax.Berkeley.EDU ">
voder!kontron!cramer@ucbvax.Berkeley.EDU 
</A>&gt;
</address>
<i>
Wed, 9 Jul 86 17:30:54 pdt
</i><PRE>
To: voder!sri-csl.arpa!risks

&gt; RISKS-LIST: RISKS-FORUM Digest,  Thursday, 26 June 1986  Volume 3 : Issue 13
&gt; Date: Thu 26 Jun 86 00:08:21-EDT
&gt; From: Richard A. Cowan &lt;COWAN@XX.LCS.MIT.EDU&gt;
&gt; Subject: Research programs that pay for themselves

  [I have deleted the quote of Cowan's original message.  The response from
   Clayton Cramer is probably not relevant, but if have erred by including 
   something that subsequently deserves a rebuttal, then it seems that I 
   should let the flavor of the rebuttal through.  PGN]

It would be awfully good if people didn't feel they could throw any old
nonsense (or even off-topic sense) into a moderated group.  Mr. Cowan
assertions are at least arguable, and many people would even consider
false.  

  Assumption One: Crime is a result of unemployment, poor housing, and
  lack of facilities to keep young people entertained.

  Assumption Two: Unemployment can be reduced by reducing the work week.

  Assumption Three: Unemployment is a major problem.

[...]  Clayton E. Cramer

    [Clayton's message went on to counter each assertion, at some length.
     However, that seemed wholly inappropriate for RISKS readers, and thus I
     have deviated from my usual policy and truncated.  PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.19.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-7</DOCNO>
<DOCOLDNO>IA012-000123-B020-356</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.21.html 128.240.150.127 19970217003646 text/html 379
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:35:19 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML>
<HEAD>
<TITLE>Risks Digest</TITLE>
</HEAD>
<BODY>
<H1>Bad request</H1>
"/RISKS/3.21.html" is not a valid issue of Risks.
<HR>
<ADDRESS>
<A HREF="http://catless.ncl.ac.uk/Lindsay.html">Lindsay.Marshall@newcastle.ac.uk</A>
</ADDRESS>
</BODY>
</HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-8</DOCNO>
<DOCOLDNO>IA012-000123-B020-375</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.22.html 128.240.150.127 19970217003704 text/html 14400
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:35:27 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 22</TITLE>
<LINK REL="Prev" HREF="/Risks/3.21.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.23.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.21.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.23.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 22</H1>
<H2> Saturday, 19 July 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Nostalgia 
</A>
<DD>
<A HREF="#subj1.1">
Mike Williams
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Flames about BASIC 
</A>
<DD>
<A HREF="#subj2.1">
Jim Anderson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  More on risks of teaching "just" programming 
</A>
<DD>
<A HREF="#subj3.1">
Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Responsibility for Computer Actions 
</A>
<DD>
<A HREF="#subj4.1">
George S. Cole
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  CDP and Certification 
</A>
<DD>
<A HREF="#subj5.1">
Andy Glew
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  The undetected hang-up risk (more) 
</A>
<DD>
<A HREF="#subj6.1">
Ted Lee
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
 Nostalgia
</A>
</H3>
<address>
 "John Michael (Mike) Williams" 
&lt;<A HREF="mailto:JWilliams@DOCKMASTER.ARPA">
JWilliams@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Thu, 17 Jul 86 17:34 EDT
</i><PRE>
To:  RISKS@CSL.SRI.COM

Willis' comment in connection with computer literacy in the old days:

    &gt;Remember when you read this:  I'm talking of the period when
    &gt;it was all mainframes and centralized computing shops, and the programming
    &gt;fraternity argued persuasively for and held sway in the closed-shop!

triggered all sorts of memories I'm sure Willis shares.

Surely we both remember the Bendix G-15?  the Monrobot?  the CDC 160A (that
motherless PPU)?  Yes, there were big IBM 704 and UNIVAC II shops, but there
were also IBM 604 Punched Card Electronic Calculators, and UNIVAC 40/60/120s; 
the latter I remember being used for critical airframe and weapons system
calculations by then-Douglas Aircraft in 1956, when I joined the industry.

I don't remember, if I ever knew, what computers were used to support the
Comet and Electra I designs, but perhaps there may be a connection between
their sorry record and RISKS.  In any case, the problem of distributed small
computing environments has always been with us, if on a smaller scale.

Mike Williams, System Development Corp.  McLean VA

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
 Flames about BASIC
</A>
</H3>
<address>
&lt;<A HREF="mailto: JPAnderson@DOCKMASTER.ARPA">
 JPAnderson@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Thu, 17 Jul 86 22:45 EDT
</i><PRE>
To:  Neumann@CSL.SRI.COM
ReSent-To: RISKS@CSL.SRI.COM

Those of your readership who bristle when one programming language or
another is put down for any reason might like to read what has to rank as
the ultimate rebuttal.  I refer of course to Howard E. Tompkins paper "In
Defense of Teaching Structured COBOL as Computer Science (or Notes on being
Sage Struck).  It appeared in SIGPLAN notices, V18,4 of April 1983.  A real
hoot!
                                        Jim

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
More on risks of teaching "just" programming
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Sat, 19 Jul 1986  02:23 EDT
</i><PRE>

My own feeling is that for for "computer literacy" in the general populace
(rather than say for engineers or economists who will have to write
programs), programming is mostly irrelevant.  The most important notions for
everyone to have (that is after all the meaning of "literacy") are those
related to procedures: what procedures are, what input is, what output is,
how input can be related to output and so on.  Being able to ask the
question "But how can the computer know to do X?" in a meaningful way, and
puzzling out the answer to that question is in my view a whole lot more
important than knowing the syntax of PASCAL or BASIC.

The problem is ultimately related to clear thinking, and how to teach
people to do THAT.

    [We have included various somewhat redundant responses on this topic
     in recent RISKS, because the points being made are IMPORTANT but
     OFTEN IGNORED.  There is no substitute for style, elegance, care,
     and -- above all -- understanding what you are doing.   PGN]

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Responsibility for Computer Actions
</A>
</H3>
<address>
&lt;<A HREF="mailto:cole.pa@Xerox.COM">
cole.pa@Xerox.COM
</A>&gt;
</address>
<i>
31 Dec 00 16:30 PST
</i><PRE>
To: RISKS@csl.sri.com

	Responsibility for a computer foul-up can realistically be laid
anywhere from the individual operator's feet (for placing the wrong hard
disk in or plugging in the wrong power supply) to the hardware
designer's feet (for allowing ungrounded power plug-ins) to the system
programmer's feet to the compiler design team's feet (group shot) to the
application's designers' feet (another group shot?) ... all depending on
what is the "source" of the failure.
	Presuming for the nonce that the fault lies in the application
software (not in its implementation, via the transition from high-level to
machine code or transition from electronic state to electronic state), there
still remains a problem of determining who is responsible. Who provided the
algorithm? The implementation? The specification? Did anybody perform a
mathematical theorem validation? Could such realistically be done for the
entire program? (Hah.) Hindsight allows a (relatively) easy post-mortem that
shows "this step" could have been validated (and thus had the error shown
up), often enough. But the program is a SYSTEM, and the safeguards are at
this point far from perfect.
	Ought they be perfect? Think how much that would cost.
	Rather than tacking terms like "responsibility" to the entire
spectrum of computer programs, it would make more sense (legally and
ethically) to designate the principles and requirements for liability to be
attached for an injury, and let the moralists be concerned with the
responsibility. (Responsibility can NEVER be attached, no matter how hard it
is thrown; it is only accepted. But I would far rather have people
programming with or for me who voluntarily accept responsibility, since they
then provide the best protection.)
 
	Professional licensing, which requires the establishment of minimal
standards, allows actions based on malpractice to be brought. As long as
this licensing is voluntary and not mandatory the market can help
establish responsibility -- for then the product seller who hires an
unlicensed programmer to produce the core program will have to consider
whether they might be charged with negligence.

	Standard applications, however, should only be subject to strict
"products" liability where there is a standard operating environment. If a
program specifies that it is designed to operate on an Apple II-E with an
Epson MX-80 or FX-80 printer, (or some set of CPU chips, terminals, and
printers with a set of standard operating systems), any user who goes to a
different environment (even if somebody else promised it would be identical,
or just compatible) has no one but himself to curse. The difference between
a hammer and a consumer computer application is (realistically) indifferent
in terms of consumer law -- if you use a hammer as a wedge or a support for
some scaffolding, you can hardly cry foul when it fails at a task for which
it is not designed.
	(Of course, the above is complicated by some rulings that
"foreseeable misuses" allow liability. The consumer applications computer
company will want to restrict the range by specifying where it guarantees
its product , and will want to extend the probable hardiness to a penumbra
of likely modifications beyond that to prevent mishaps.)
	
					George S. Cole, Esq.

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
CDP and Certification
</A>
</H3>
<address>
Andy "Krazy" Glew
&lt;<A HREF="mailto:aglew%ccvaxa@gswd-vms.ARPA ">
aglew%ccvaxa@gswd-vms.ARPA 
</A>&gt;
</address>
<i>
Thu, 17 Jul 86 09:11:19 cdt
</i><PRE>

Eugene Miya asks whether the CDP is a level of professional certification.  I
do not have a CDP, but I passed the Certified Computer Programmer (CCP) exam in
Systems Programming which is also given by the Institute for the Certification
of Computer Professionals (ICCP).

Does passing the exam itself indicate any level of competence? No - I would
expect first year engineering students to be able to pass it with no
difficulty. However, the fact that someone is serious enough about
`professionalism' to go out and get certified probably indicates something
about his character, if not his abilities. Obviously, the certification process
must become more stringent - the new requirement for periodic recertification
is a step in the right direction.

A secondary effect of `professional' certification is that you are expected to
subscribe to a code of ethics. Many people deride these, but I know that I, at
least, have them in the back of my mind when I consider systems whose failure
can harm people. `Empty symbology' has a powerful psychological effect: wearing
an Iron Ring reminds me about an oath I took with much rattling of chains that
I would never "pass bad workmanship". The ancient Greeks used to pour libations
to gods they knew weren't there.

Why take something like the CCP? For frankly mercenary reasons - I took it to
increase my chances of getting a job. But also because I am familiar with the
history of engineering as a profession in Canada and Great Britain (engineering
isn't a profession in the United States yet, is it?) and though that the ICCP
might be the beginning of something similar for software engineering / computer
science / programming. 

What would distinguish such a profession from the present situation? Purely and
simply, liability. A professional is liable for his actions, not just to the
best of his ability, but to the limits of knowledge in his field.

Liability is a great incentive for taking proper care of your work. To the
extent that care, the highest reasonable level of care that we can expect
humans to provide, can reduce the chance of failure in software systems,
professionalism is a good thing.

Andy "Krazy" Glew. Gould CSD-Urbana.    USEnet:  ihnp4!uiucdcs!ccvaxa!aglew
1101 E. University, Urbana, IL 61801    ARPAnet: aglew@gswd-vms

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
 The undetected hang-up risk (more)
</A>
</H3>
<address>
&lt;<A HREF="mailto: TMPLee@DOCKMASTER.ARPA">
 TMPLee@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Fri, 18 Jul 86 03:08 EDT
</i><PRE>
To:  risks@CSL.SRI.COM

When our local GTE Telenet office finally installed its 2400 baud service I
discovered the same problem referred to in the penultimate Risks:  if the
line dropped, there was a very good chance the local Telenet machine did not
detect it and one could later dial back in.  Several times i dialed in and
found myself in the middle of someone else's connection; I also, of course,
after several hours (almost a day one time, I seem to remember) was able to
dial back in and find myself connected to my original host system.  It took
several weeks of trouble reports, as well as calls from "high government
officials" (the computer I was using was this one:  the folks at the
National Computer Security Center were not, as one would hope and expect,
pleased) before Telenet acknowledged there was a problem and did something
about it.  I seem to remember that it was simply an ill modem, but the
experience was enlightening.
                                               Ted

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.21.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.23.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-9</DOCNO>
<DOCOLDNO>IA012-000123-B021-12</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.23.html 128.240.150.127 19970217003723 text/html 8232
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:35:52 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 23</TITLE>
<LINK REL="Prev" HREF="/Risks/3.22.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.24.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.22.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.24.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 23</H1>
<H2> Tuesday, 22 July 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Re: Comet and Electra 
</A>
<DD>
<A HREF="#subj1.1">
Jim Horning
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  100,000 Late Phone Bills 
</A>
<DD>
<A HREF="#subj2.1">
Mike McLaughlin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Types of "Programming" 
</A>
<DD>
<A HREF="#subj3.1">
Henry Schaffer
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Re: Comet and Electra
</A>
</H3>
<address>
Jim Horning
&lt;<A HREF="mailto:horning@src.DEC.COM ">
horning@src.DEC.COM 
</A>&gt;
</address>
<i>
Mon, 21 Jul 86 14:19:04 pdt
</i><PRE>
ReSent-To: RISKS@CSL.SRI.COM

The message from Mike Williams (<A HREF="/Risks/3.22.html">RISKS-3.22</A>) reminded me of two stories that
have been passed down through the oral tradition.  I have no direct evidence
concerning either.  Perhaps some readers of Risks have evidence that could
help set the record straight?

- A numerical analyst once explained to me why all modern airliner windows
have rounded corners: Anyone concerned with solving partial differential
equations knows that square corners lead to singularities.  He said that the
Comet crashes were traced to metal fatigue at the (square) corners of its
windows.  (He concluded that airplane designers should study Numerical
Analysis.)  Does anyone know whether computers were used in the design of
the Comet?

- I also heard that the structural defect in the Electra I wing design had
not been caught by the stress analysis program because of an undetected
overflow on a critical run.  Can anyone provide documentation for this?  (I
think this story was on the grapevine at the NATO Software Engineering
Conferences in 68-69.)

These pieces of our mythology are worth documenting or debunking.  There may
be valuable lessons to be learned from them, and we ought not to insist on
learning them the hard way.

Jim H.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
100,000 Late Phone Bills
</A>
</H3>
<address>
Mike McLaughlin
&lt;<A HREF="mailto:mikemcl@nrl-csr ">
mikemcl@nrl-csr 
</A>&gt;
</address>
<i>
Mon, 21 Jul 86 16:03:50 edt
</i><PRE>
ReSent-To: RISKS@CSL.SRI.COM

Excerpted from the Washington Post, Saturday, 9 July 1986, page D1.
[Omissions... (bridges) and [comments] as shown.]

    More Than 100,000 Getting Months-Late (telephone) Bills
        By Nell Henderson, Washington Post Staff Writer

More than 100,000 Chesapeake &amp; Potomac Telephone Co. customers might think
they've had a summer vacation from telephone bills.
But yesterday the company said the vacation is over: The bills are on the
way after a two-month delay.
The customers... have not received bills for local or long-distance service 
or both - since a computer tape failure in mid-May.

The high-tech roots of the problem were "flaws" in computer tapes that were
programmed for preparing the bills, (a spokesman said).  "The problem erases
itself," he added.
The low-tech solution was to use people to put the billing information into the
system, using separate records of the calls, he said.
The result was that many of the customers did not receive phone bills for
several months....

(A) customer... was told to call... if he has any trouble paying the entire
bill at once. 
"We would be lenient on payment, and would be glad to speak to customers on 
an individual basis . . . We're sorry for any inconvenience,"... 
The problem also affected an unknown number of bills for long-distance service
provided by MCI Communications Service...

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Types of "Programming"
</A>
</H3>
<address>
Henry Schaffer 
&lt;<A HREF="mailto:ecsvax!hes%mcnc.csnet@CSNET-RELAY.ARPA">
ecsvax!hes%mcnc.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Fri, 18 Jul 86 23:37:38 edt
</i><PRE>

  "Programming" encompasses much more than the use of the traditional
languages (Basic, Ada, or whatever.)  Entering formulas in a spreadsheet
or specifying record and report structures in a database are also
programming - in higher-level, albeit specialized, languages.  Thus
JAN Lee *is* teaching his students to program, and in the most
appropriate and productive manner.  They can learn something quite
important and useful in this part of the class.  It is the other
faculty/administrative objectors (the ones who want to have 4 weeks
of traditional language put in) who are asking for something both
unproductive (most of the students will neither learn new concepts
nor something useful) and risky.

  There is an implicit understanding about a terminal course - that
you've been carried along far enough so that you can use what you've
been taught.  A student who finishes one semester of a CSC sequence
knows that he/she is not through learning,
and should not presume (one hopes) to take on responsibilty for a
critical application program.  However, a student who is taught
that programming is 4 weeks of a survey course in computing might
not be so timid!  (I assume that these students will not usually
take any more programming - if they generally did then there wouldn't
have been the pressure to push programming into JAN Lee's course.)

  Our university (NCSU) has recognized that the details of the
type of "programming" needed are dependent on the discipline, and
can variously include spreadsheets, statistical packages (I can
argue that one can "program" in SAS), etc., and also the more
traditional languages.

--henry schaffer  n c state univ   ...mcnc!ecsvax!hes  (uucp)
                                      tsches@ecsvax.bitnet

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.22.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.24.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-10</DOCNO>
<DOCOLDNO>IA012-000123-B021-31</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.24.html 128.240.150.127 19970217003734 text/html 14524
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:36:04 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 24</TITLE>
<LINK REL="Prev" HREF="/Risks/3.23.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.25.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.23.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.25.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 24</H1>
<H2> Thursday, 24 July 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Comet and Electra 
</A>
<DD>
<A HREF="#subj1.1">
Jerry Saltzer
</A><br>
<A HREF="#subj1.2">
 Marv Zelkowitz
</A><br>
<A HREF="#subj1.3">
 Don Chiasson
</A><br>
<A HREF="#subj1.4">
 Bard Bloom
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  No gasoline because the computer is down? 
</A>
<DD>
<A HREF="#subj2.1">
Jim Barnes
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  HBO Hacker Captain Midnight Caught 
</A>
<DD>
<A HREF="#subj3.1">
via Geoff Goodfellow
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Re: Comet and Electra
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
Tue, 22 Jul 86 23:26:59 EDT
</i><PRE>
From: Jerome H. Saltzer &lt;Saltzer@ATHENA.MIT.EDU&gt;

&gt; - I also heard that the structural defect in the Electra I wing design had
&gt; not been caught by the stress analysis program because of an undetected
&gt; overflow on a critical run.  Can anyone provide documentation for this?  (I
&gt; think this story was on the grapevine at the NATO Software Engineering
&gt; Conferences in 68-69.)

In case it helps anyone recall where that one might be documented:  the
version of the story that came through here had it that some piece of
simulation input data was typed with the wrong minus sign.  (The commonly
available version of the 026 key punch had a minus sign and a hyphen as
distinct characters.  And the input format conversion routines in those days
were both unforgiving and silent about errors.)
                                                       Jerry

</PRE>
<HR><H3><A NAME="subj1.2">
Re: Comet and Electra
</A>
</H3>
<address>
Marvin Zelkowitz
&lt;<A HREF="mailto:mvz@aaron.cs.umd.edu ">
mvz@aaron.cs.umd.edu 
</A>&gt;
</address>
<i>
Wed, 23 Jul 86 09:57:25 edt
</i><PRE>

Horning's recent comment reminds me of two related items:

- On the Electra I wing design defect: My version of the story goes
that the undetected overflow error was finally detected when these
"correct" programs were used as benchmarks for a new computer (a
Burroughs I think), which gave radically different answers. I do not have 
any proof of this, but it might give some additional help in tracking it
down.

- On overflow detection: In the late 60s, a certain vendor's FORTRAN
did not detect overflow. At a users' group meeting, the vendor offered
to add overflow detection at an execution penalty of one instruction
per arithmetic operation (e.g., branch-on-overflow). This was voted down.
The only conclusion is that users would rather be fast than right. 
The issue for RISKS is "Are these people the ones 'still in control'?"

--Marv Zelkowitz

</PRE>
<HR><H3><A NAME="subj1.3">
Re: Comet and Electra
</A>
</H3>
<address>
 Don Chiasson 
&lt;<A HREF="mailto:CHIASSON@DREA-XX.ARPA">
CHIASSON@DREA-XX.ARPA
</A>&gt;
</address>
<i>
Wed 23 Jul 86 09:17:42-ADT
</i><PRE>
To: risks@CSL.SRI.COM

&gt; From: horning@src.DEC.COM (Jim Horning)

&gt; - A numerical analyst once explained to me why all modern airliner windows
&gt; have rounded corners: Anyone concerned with solving partial differential
&gt; equations knows that square corners lead to singularities.  He said that the
&gt; Comet crashes were traced to metal fatigue at the (square) corners of its
&gt; windows.  (He concluded that airplane designers should study Numerical
&gt; Analysis.)  

Most engineers know that any sharp corner on a stressed member will cause
an increase of actual stress over the nominal calculated stress, and the
ratio of these is called the stress concentration factor, K.  The value of
K is sort of inversely proportional to the radius of curvature of the
discontinuity.  High K is the reason cracks propagate so well. The
temporary fix for a crack is to drill a hole at the end of the crack which
increases the radius of the "corner" and decreases K.  It is standard
design practice to avoid sharp corners.  Stress concentration is usually
discussed in design textbooks without going into the differential
equations: there are lots of tables.

This brings up a problem encountered in computer applications: the
difficulty of a programmer learning the standard practices of a field in
which he is working.  Engineers know about stress concentration, but
programmers and mathematicians may not.

&gt; - I also heard that the structural defect in the Electra I wing design had
&gt; not been caught [...].  Can anyone provide documentation for this? 

I can't give a direct answer to this, but I know that a mid 60's computer
which was heavily used in scientific and engineering applications had very
poor accuracy in its trig package.  Is this perhaps the same topic?  (Or was
the Electra designed in the 50's??)  Note: I can identify the manufacturer
and machine, but feel that if I did so, I would be potentially libelous.
                Don Chiasson

</PRE>
<HR><H3><A NAME="subj1.4">
Re: Comet and Electra
</A>
</H3>
<address>
Bard Bloom 
&lt;<A HREF="mailto:BARD@XX.LCS.MIT.EDU">
BARD@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Wed 23 Jul 86 11:44:00-EDT
</i><PRE>
To: RISKS@CSL.SRI.COM

  [Structural defect in the Electra I wing design, again.  See Jerry, above.]

I don't know about this, but I was trying to move some software in Fortran
from an IBM to VAX for McDonnell-Douglas one summer.  The program on the VAX
kept dying, with a message to the effect of "I can't take a sine of a number
this large".  The program was trying to take sines of large (order of 10^20)
numbers in 16-digit arithmetic.  The first thing that the sine routine does
is reduce its argument modulo pi, which loses *all* of the precision of the
20-digit number.  The VAX's software generated an error about this.  The IBM
did not; and the programmers hadn't realized that it might be a problem (I
guess).  They had been using that program, gleefully taking sines of random
numbers and using them to build planes, for a decade or two.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
No gasoline because the computer is down?
</A>
</H3>
<address>
Jim Barnes
&lt;<A HREF="mailto:decvax!wanginst!infinet!barnes@seismo.CSS.GOV ">
decvax!wanginst!infinet!barnes@seismo.CSS.GOV 
</A>&gt;
</address>
<i>
Wed, 23 Jul 86 13:56:44 edt
</i><PRE>

Last Friday, on my way home, I stopped at the local gasoline station to
"fill 'er up".  However, they could not pump any gas because the "computer
was down".  It seems that the pumps at the station were the new kind (with
the digital displays for price per gallon, total, etc.) and were linked
through to some computer somewhere.  Who would have thought that a computer
failure could prevent us from being able to purchase gasoline?  But now that
I think of it, all those new point of sale terminals linked to a computer
could be in trouble if the computer fails.

It used to be that this kind of problem would occur only if there was an
electrical power outage, but now just having the computer down can cause the
same problem.

decvax!wanginst!infinet!barnes 		Jim Barnes

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
HBO Hacker Captain Midnight Caught
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
23 Jul 1986 17:08-PDT
</i><PRE>
Cc: neumann@CSL.SRI.COM
ReSent-To: risks@CSL.SRI.COM
	
    JACKSONVILLE, Fla. (AP) - Investigators using a complicated process of
elimination have unmasked ''Captain Midnight,'' who admitted in court he
overrode HBO's satellite delivery system to transmit a message.
    John R. MacDougall, owner of a home satellite dish business in Ocala
that officials said was hurt by cable companies' decisions to scramble their
signals, agreed to plead guilty to illegal transmission of a satellite
signal in exchange for a $5,000 fine and one year probation.
    He could have faced a maximum $10,000 fine and a year imprisonment.
    MacDougall, who was released on a $5,000 bond, and his attorney,
John M. Green Jr., refused to comment as they left the federal court
building Tuesday after entering the plea before a U.S. magistrate.
    Sentencing is set for Aug. 26 and MacDougall can retract his plea if
the judge will not accept the arrangement.
    Early on April 27, MacDougall was the only one working at a satellite
transmission center called Central Florida Teleport with the kind of
equipment needed to disrupt the HBO signal, officials said.
    Although the video sneak attack was only a minor annoyance to HBO and
its viewers, the Federal Communications Commission launched a massive
investigation because of the potential problems a less selective video
hacker might cause.
    ''The potential for damage to critical satellite frequencies cannot be
underestimated,'' said Richard M. Smith, chief of the FCC's field operations
bureau. He noted that critical telephone calls, air traffic control,
military data and medical information are sent by satellite and that even an
accidental interruption of one of these messages could cause dire
consequences.
    On April 27, HBO viewers saw a message replace the movie ''The
Falcon and the Snowman.'' The message said:
    ''Good Evening HBO
    ''From Captain Midnight
    ''$12.95 month
    ''No way!
    ''(Showtime Movie Channel beware.)''
    The wording was an apparent reference to HBO's decision to scramble
its satellite-delivered signal so it could not be watched by those
not paying for HBO, officials said.
    ''His company was sustaining substantial losses because of the
scrambing of HBO and threats of other scrambling,'' said Assistant
U.S. Attorney Lawrence Gentile III.
    MacDougall also interrupted HBO video signals on April 20, when he
transmitted a color bar pattern, officials said.
    On Jan. 15, HBO became the first cable TV network to scramble its
signal full time. Showtime and The Movie Channel scrambled their
programming full time on May 27.
    The scrambling makes pictures unwatchable without a descrambler and
slowed sales of satellite dishes.
    Of 580 satellite facilities with a transmitting dish large enough to
overpower HBO's signal, less than a dozen had sufficient power and
the right kind of electronic typewriter to write the protest message
Captain Midnight transmitted, investigators said.
    The investigation focused on Ocala after a tipster vacationing in
Florida reported to the FCC an overheard telephone call about Captain
Midnight. The tipster provided the caller's description and license
plate number.
    The caller who was overheard was not the suspect, but the FCC said
the information provided proved extremely beneficial.

   [The L.A. Times refined this a little, after noting that there were only
    580 appropriate candidate facilities:

        "By studying tapes of the illegal video signal, the FCC's field staff
      concluded that the message had been generated using a specific make
      and model of character-generator device to transmit symbols, such as
      letters and numbers, onto a television screen.
        "After visiting those plants, investigators had three prime suspects,
      including MacDougall.  When he was notified he was a suspect, MacDougall
      turned himself in."

   This seems like a nice bit of detective work, and certainly presents an
   interesting risk for would-be perpetrators -- somewhat like radioactive 
   traces in dyes, watermarks in paper, imperfections in certain characters
   on a typewriter or printer, and voiceprints (all of which have been used
   successfully to identify or subset culprits).  On the other hand, the
   smart perpetrator, aware of such tell-tale signatures, might figure out a
   way to spoof someone else's tell-tale, similar to changing the answer-back 
   drum on a teletype or hacking your cellular telephone identifier (as noted
   in a previous RISKS by Geoff).  Will this case escalate the sophistication 
   of satellite attacks?  PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.23.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.25.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-11</DOCNO>
<DOCOLDNO>IA012-000123-B021-49</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.25.html 128.240.150.127 19970217003748 text/html 20027
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:36:16 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 25</TITLE>
<LINK REL="Prev" HREF="/Risks/3.24.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.26.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.24.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.26.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 25</H1>
<H2> Thursday, 21 July 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Petroski on the Comet failures 
</A>
<DD>
<A HREF="#subj1.1">
Alan Wexelblat
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Re: Comet and Electra 
</A>
<DD>
<A HREF="#subj2.1">
Douglas Adams
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  On the dangers of human error 
</A>
<DD>
<A HREF="#subj3.1">
Brian Randell via Lindsay Marshall
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Software Paranoia 
</A>
<DD>
<A HREF="#subj4.1">
Ken Laws
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Royal Wedding Risks 
</A>
<DD>
<A HREF="#subj5.1">
Lindsay Marshall
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  How to Think Creatively 
</A>
<DD>
<A HREF="#subj6.1">
John Mackin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Dangers of improperly protected equipment 
</A>
<DD>
<A HREF="#subj7.1">
Kevin Belles
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Petroski on the Comet failures
</A>
</H3>
<address>
Alan Wexelblat 
&lt;<A HREF="mailto:wex@mcc.com">
wex@mcc.com
</A>&gt;
</address>
<i>
Thu, 24 Jul 86 12:02:41 CDT
</i><PRE>

Henry Petroski's book _To Engineer is Human_ has a segment discussing
the Comet crashes and the detective work done to figure out why they
occurred (pages 176-184).  The story he tells makes no mention of curved
or rounded window corners.  The highlights:

 - On May 2, 1953, a de Havilland Comet was destroyed on takeoff
   from Dum-Dum Airport in Calcutta, India.  The Indian Government
   Board of Inquiry concluded officially that the accident was caused
   by some sort of structural failure either due to a tropical storm
   or to pilot overreaction to storm conditions.

 - The Comet was flown "off the drawing board"; no prototypes were ever
   built or tested.

 - On January 10, 1954, a Comet exploded after takeoff from Rome under
   mild weather conditions.  The plane was at 27,000 feet so the debris
   fell into a large area of the Mediterranean.  Not enough was recovered
   to allow any conclusion on why the crash had occurred.

 - On April 8, 1954, another flight leaving Rome exploded.  The pieces from
   this one fell into water too deep to allow recovery, so more pieces from
   the previous crash were sought and found.

 - Investigators eventually found the tail section which provided conclusive
   evidence that the forward section had exploded backward.  The print from
   a newspaper page was seared into the tail so strongly that it was still
   legible after months in the Mediterranean.

 - The question now was WHY did the cabin explode?  The reason was found only
   by taking an actual Comet, submerging it in a tank of water and simulating
   flight conditions (by pressurizing and depressurizing the cabin and by
   hydraulicly simulating flight stresses on the wings).

 - After about 3000 simulated flights, a crack appeared at a corner of one
   cabin window which rapidly spead (when the cabin was pressurized) and the
   cabin blew apart.

 - Analysis finally showed that rivet holes near the window openings in the
   fuselage caused excessive stress.  The whole length of the window panel
   was replaced in the later Comet 4 with a new panel that contained special
   reinforcement around the window openings.

Although Petroski doesn't give his sources directly, much of his material
appears to be drawn from the autobiography of Sir Geoffrey de Havilland
(called _Sky Fever: The Autobiography_, published in London in 1961) and from
a book called _The Tale of the Comet_ written by Derek Dempster in 1958.

In general, I recommend Petroski's book; it's quite readable and has lots of
material that would be interesting to we RISKS readers.  Of particular
interest is the chapter called "From Slide Rule to Computer: Forgetting How
it Used to be Done."  It's an interesting (if superficial) treatment of
some of the risks of CAD.

Alan Wexelblat
ARPA: WEX@MCC.ARPA
UUCP: {ihnp4, seismo, harvard, gatech, pyramid}!ut-sally!im4u!milano!wex

Currently recruiting for the `sod squad.'

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Re: Comet and Electra
</A>
</H3>
<address>
Adams Douglas
&lt;<A HREF="mailto:crash!pnet01!adamsd@nosc.ARPA ">
crash!pnet01!adamsd@nosc.ARPA 
</A>&gt;
</address>
<i>
Thu, 24 Jul 86 07:43:49 PDT
</i><PRE>

It was my understanding that the problem with the early Electras was whirl-mode
flexing of the outboard half of the wing. I had heard that Lockheed reassigned
its few then-existing computers to full-time research on the problem. But it
was also my understand that the original design cycle for the Electra did not
involve computer assistance at all--they weren't being used for aircraft
"simulation" that early (1948?).

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
On the dangers of human error [contributed on behalf of Brian Randell]
</A>
</H3>
<address>
"Lindsay F. Marshall" 
&lt;<A HREF="mailto:lindsay%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
lindsay%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Thu, 24 Jul 86 11:28:28 bst
</i><PRE>

[From brian Fri Jul 18 17:30 GMT 1986]
The following article appeared in the Guardian newspaper (published in
London and Manchester) for Wed. July 16. The author, Mary Midgely is,
incidentally, a former lecturer of Philosophy at the University of Newcastle
upon Tyne. Brian R. was pleased to see such a sensible discussion in a daily
newspaper of the dangers of human error that he thought it worth passing on
to the RISKS readership, so here it is.....

  IDIOT PROOF
  
  Little did I know, when I wrote my last article about human error, that the
  matter was about to receive so much expensive and high-powered attention. 
  Since Chernobyl, it has been hard to turn on television without receiving 
  more official reassurance that accidents do not happen here.  Leading the 
  chorus, the chairman of the Central Electricity Generating Board came on 
  the air to explain that, in British nuclear reactors, human error has been 
  programmed out entirely.  Other equally impressive testimonies followed.  
  Even on these soothing occasions, however, disturbing noises were sometimes 
  heard.  During one soporific film, an expert on such accidents observed that 
  human error is indeed rather hard to anticipate, and told the following 
  story.
  
  A surprising series of faults occurred at a newly-built nuclear power 
  station, and were finally traced to failure in the cables.  On
  investigation, some of these proved to have corroded at an extraordinary
  rate, and the corroding substance turned out to be a rather unusual one,
  namely human urine.  Evidently the workmen putting up the power-station
  had needed relief, and had found the convenient concrete channels in the
  concrete walls they were building irresistibly inviting.  Telling the
  tale, the chap reasonably remarked that you cannot hope to anticipate this
  kind of thing - infinitely variable human idiocy is a fact of life, and
  you can only do your best to provide against the forms of it that happen
  already to have occurred to you.
  
  This honest position, which excluded all possible talk of programming it
  out, is the one commonly held by working engineers.
  
  They know by hard experience that if a thing can go wrong it will, and that
  there are always more of these things in store than anybody can possibly have
  thought of.  (Typically, two or three small things go wrong at once, which is
  all that is needed).  But the important thing which does not seem to have
  been widely realised is that hi-tech makes this situation worse, not better.
  
  Hi-tech concentrates power.  This means that a single fault, if it does
  occur, can be much more disastrous.  This gloomy truth goes for human as well
  as mechanical ones.  Dropping a hammer at home does not much matter; dropping
  it into the core of a reactor does.  People have not been eliminated.  They 
  still figure everywhere - perhaps most obviously as the maintenance-crews who
  seem to have done the job at Chernobyl, but also as designers, sellers and 
  buyers, repairers, operators of whatever processes are still human-handled, 
  suppliers of materials, and administrators responsible for ordering and 
  supervising the grand machines.
  
  What follows?  Not, of course, that we have to stop using machines, but that 
  we have to stop deceiving ourselves about them.  This self-deception is 
  always grossest over relatively new technology.  The romanticism typical of 
  our century is altogether at its most uncontrolled over novelties.  We are as
  besotted with new things as some civilisations are with old ones.
  
  This is specially unfortunate about machines, because with them the gap
  between theory and practice is particularly stark.  Only long and painful
  experience of actual disasters - such as we have for instance in the case
  of the railways - can ever begin to bridge it.  Until that day, all
  estimates of the probability of particular failures are arbitrary guesses.
  
  What this means is that those who put forward new technology always
  underestimate its costs, because they leave out this unpredictable extra
  load.  Over nuclear power, this is bad enough, first, because its single
  disasters can be so vast - far vaster than Chernobyl - and second, because
  human carelessness has launched it before solving the problem of nuclear
  waste.
  
  Nuclear weapons, however, differ from power in being things with no actual
  use at all.  They exist, we are assured, merely as gestures.  But if they
  went off, they would go off for real.  And there have been plenty of
  accidents involving them.  Since Chernobyl and Libya, people seem to be
  noticing these things. Collecting votes lately for my local poll on the
  Nuclear Freezen project, I was surprised how many householders said at
  once: "My God, yes, let's get rid of the things."  This seems like sense.
  Could it happen here?  Couldn't it? People are only people.  Ooops - sorry...

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Software Paranoia
</A>
</H3>
<address>
Ken Laws 
&lt;<A HREF="mailto:Laws@SRI-STRIPE.ARPA">
Laws@SRI-STRIPE.ARPA
</A>&gt;
</address>
<i>
Thu 24 Jul 86 17:40:04-PDT
</i><PRE>
To: Risks@CSL.SRI.COM

  From: Bard Bloom &lt;BARD@XX.LCS.MIT.EDU&gt;
  The VAX's software generated an error about this.  The IBM
  did not; and the programmers hadn't realized that it might be a problem (I
  guess).  They had been using that program, gleefully taking sines of random
  numbers and using them to build planes, for a decade or two.

Let's not jump to conclusions.  Taking the sine of 10^20 is obviously bogus,
but numbers of that magnitude usually come from (or produce) other bogus
conditions.  The program may well have included a test for an associated
condition &gt;&gt;after&lt;&lt; taking the sine, instead of recognizing the situation
&gt;&gt;before&lt;&lt; taking the sine.  Poor programming practice, but not serious.

A major failing of current programming languages is that they do not force
the programmer to test the validity of all input data (including returned
function values) and the success of all subroutine calls.  Debugging would
be much easier if errors were always caught as soon as they occur.  The
overhead of such error checking has been unacceptable, but the new hardware
is so much faster that we should consider building validity tests into the
silicon.  The required conditions on a return value (or the error-handling
subroutine) would be specified as a parameter of every function call.

I tend to write object-oriented subroutines (in C) that return complex
structures derived from user interaction or other "knowledge-based"
transactions.  Nearly every subroutine call must be followed by a test
to make sure that the structure was indeed returned.  (Testing for valid
substructure is impractical, so I use NULL returns whenever a subroutine
cannot construct an object that is at least minimally valid.)  All these
tests are a pain, and I sometimes wish I had PL/I ON conditions to hide
them.  Unfortunately, that's a bad solution: an intelligent program must
handle error returns intelligently, and that means the programmer should
be forced to consider every possible return condition and specify what
to do with it.

Errors that arise within the error handlers are similarly important, but
beyond my ability to even contemplate in the context of current languages.

Expert systems (e.g., production systems) often aid rapid prototyping by
ignoring unexpected situations -- the rules trigger only on conditions
that the programmer anticipated and knew how to handle.  New rules are
added whenever significant misbehavior is noticed, but there may be
no attempt to handle even the full range of legal conditions intelligently
-- let alone all the illegal conditions that can arise from user, database,
algorithm, or hardware errors.  I like expert systems, but from a Risks
standpoint I have to consider them at least an order of magnitude more
dangerous than Ada software.
					-- Ken Laws

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Royal Wedding Risks 
</A>
</H3>
<address>
"Lindsay F. Marshall" 
&lt;<A HREF="mailto:lindsay%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK">
lindsay%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Thu, 24 Jul 86 13:46:31 gmt
</i><PRE>

Yesterday (23rd) we lost all power to our machine room when a circuit
breaker blew.  The cause of this was a glitch which hit us at about
13:50 P.M.  This was approximately the time that the main Royal Wedding
television coverage stopped............ 

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
How to Think Creatively
</A>
</H3>
<address>
&lt;<A HREF="mailto:munnari!basser.oz!john@seismo.CSS.GOV">
munnari!basser.oz!john@seismo.CSS.GOV
</A>&gt;
</address>
<i>
Thu, 24 Jul 86 18:21:08 EST
</i><PRE>

Recent comments in Risks about ``computer literacy'' lead Herb Lin
to comment that:

&gt; The problem is ultimately related to clear thinking, and how to teach
&gt; people to do THAT.

This reminded me of some mail I received last year, from a staff member
here who was teaching a first-year course on data structures.  His mail,
which was sent to a number of us here, was a plea for assistance as to
the right way to respond to some mail he had received from one of his
students.  The student's mail said:

&gt; Dear Jason,... You have really done a great job on IDS. It really helped to
&gt; clear a lot of lingering doubts Lent term left behind.  Thanks a lot
&gt; again.  Could you advise on how to think creatively. I can't "see" a
&gt; program naturally and think deep enough to make the required alterations...

None of us really knew how to answer that.

John Mackin, Basser Department of Computer Science,
	     University of Sydney, Sydney, Australia

john%basser.oz@SEISMO.CSS.GOV
{seismo,hplabs,mcvax,ukc,nttlab}!munnari!basser.oz!john

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Dangers of improperly protected equipment
</A>
</H3>
<address>
Kevin Belles
&lt;<A HREF="mailto:crash!pnet01!kevinb@nosc.ARPA ">
crash!pnet01!kevinb@nosc.ARPA 
</A>&gt;
</address>
<i>
Thu, 24 Jul 86 01:08:50 PDT
</i><PRE>

  Is there any device or devices that protect not only the active lines 
but the ground lines as well from surge, spike, and EMI-type disturbance? 
My system appears to have been victimized, thanks to our local electric
utility, by the ground for my apartment complex being raised, which caused
damage to all the damage to all the grounded equipment on my home computer
system, save some cards apparently protected by my boat-anchor power supply,
and the fact that each card in my cage is independently regulated.  In my
case, the surge entered the ground and apparently corrupted my main floppy
drive supply to the point where it propagated along the 8" and 5 1/4"
cables, destroying the logic boards on all drives and the dynamic memory,
which was being accessed at that time. It also managed to get my printer, on
another leg entirely, while miraculously missing my terminal and modem. This
completely bypassed the fuses and only a trace on the controller board being
opened saved the rest of my system being damaged. Result: 3 dead DSDD 8"
drives, 1 dead SSDD 5 1/4" drive, 3 drive power supplies, 1 dot-matrix
printer, 1 64K DRAM board, and a floppy controller board. Dollar cost:
estimated minimum of over $2000.00 if equipment is replaced by new, with no
cost for loss of access being figured in.

Let this be a warning: Protect your equipment! Any investment in anti-surge
equipment, anti-spike equipment, and UPSs are investments in your computing
future.

Kevin J. Belles - UUCP {sdcsvax,noscvax,ihnp4,akgua}!crash!pnet01!kevinb

(Disclaimer: Anything I may say is my opinion, and does not reflect
            the company I keep. KjB)

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.24.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.26.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-12</DOCNO>
<DOCOLDNO>IA012-000123-B021-77</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.26.html 128.240.150.127 19970217003803 text/html 16290
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:36:29 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 26</TITLE>
<LINK REL="Prev" HREF="/Risks/3.25.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.27.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.25.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.27.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 26</H1>
<H2> Saturday, 26 July 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
DIVAD 
</A>
<DD>
<A HREF="#subj1.1">
Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Royal wedding risks -- common change modes 
</A>
<DD>
<A HREF="#subj2.1">
Don Chiasson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Security and dialbacks 
</A>
<DD>
<A HREF="#subj3.1">
David I. Emery via Herb Lin
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
DIVAD
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Sat, 26 Jul 1986  00:39 EDT
</i><PRE>

Some time ago there was a flap about whether or not DIVAD did or did not
shoot at a latrine fan.  [See Doug Schuler in RISKS-3.1, with subsequent
discussion in RISKS-3.3, 4, 5.]  I have documentation now from a person who
should know: Richard DeLauer, former Undersecretaty of Defense for Research
and Engineering in the first Reagan term.  He says it did, and that it was
supposed to do that.  See [MIT] Technology Review, July 1986, page 64.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Royal wedding risks -- common change modes
</A>
</H3>
<address>
 Don Chiasson 
&lt;<A HREF="mailto:CHIASSON@DREA-XX.ARPA">
CHIASSON@DREA-XX.ARPA
</A>&gt;
</address>
<i>
Fri 25 Jul 86 10:25:41-ADT
</i><PRE>
To: lindsay%cheviot.newcastle.ac.uk@CS.UCL.AC.UK
cc: Neumann@CSL.SRI.COM
ReSent-To: RISKS@CSL.SRI.COM

	Phenomena like this are well known by the CEGB (Central Electricity
Generating Board) engineers.  Operation of a power grid assumes that the
load does not change suddenly, indeed sudden changes can cause instability.
Anyway, it is well known in the U.K. (I'm not sure about the U.S. and
Canada) that the largest power surge is at the end of Coronation Street, or
one of the other soaps, when everyone gets up from the Telly and plugs in
the kettle to make tea.  I assume that's what happened at the end of the
wedding telecast.

	A similar thing happened in the U.S. a couple of years ago.  I
think it was somewhere in New Mexico or Arizona that there was a pause in
the super bowl game so a lot of people got up, went to the bathroom (all
that beer) and flushed at nearly the same time which caused some sewer
backups.
		Don Chiasson

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Security and dialbacks
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Fri, 25 Jul 1986  09:46 EDT
</i><PRE>
 MSG:  *MSG   5759  
 Date: 24 Jul 86 12:22:30 GMT
 From: frog!die at EDDIE.MIT.EDU (Dave Emery, Software)
 Re:   Security and dialbacks
 DISTRIB: *BBOARD

 Summary: Dialbacks aren't very secure (repost of old article)
 Apparently-To: codebreakers
  
 In article &lt;906@hoptoad.uucp&gt; gnu@hoptoad.UUCP writes:
 &gt;Here are the two messages I have archived on the subject...
 
 &gt;[I believe the definitive article in that discussion was by Lauren Weinstein,
 &gt;vortex!lauren; perhaps he has a copy.
  
 	What follows is the original article that started the discussion.
 I do not know whether it qualifies as the "definitive article"  as I
 think I remember Lauren and I both posted further comments.
  							    - Dave
  		** ARTICLE FOLLOWS **
  
      ----------------------------------------------------------------------
  
  	An increasingly popular technique for protecting dial-in ports from
 the ravages of hackers and other more sinister system penetrators is dial
 back operation wherein a legitimate user initiates a call to the system
 he desires to connect with, types in his user ID and perhaps a password,
 disconnects and waits for the system to call him back at a prearranged number.
 It is assumed that a penetrator will not be able to specify the dial back
 number (which is carefully protected), and so even if he is able to guess
 a user-name/password pair he cannot penetrate the system because he cannot
 do anything meaningful except type in a user-name and password when he is
 connected to the system. If he has a correct pair it is assumed the worst that
 could happen is a spurious call to some legitimate user which will do no harm
 and might even result in a security investigation.
  
 	Many installations depend on dial-back operation of modems for
 their principle protection against penetration via their dial up ports
 on the incorrect presumption that there is no way a penetrator could
 get connected to the modem on the call back call unless he was able to
 tap directly into the line being called back.  Alas, this assumption
 is not always true - compromises in the design of modems and the
 telephone network unfortunately make it all too possible for a clever
 penetrator to get connected to the call back call and fool the modem 
 into thinking that it had in fact dialed the legitimate user.
  
  	The problem areas are as follows:
  
  		Caller control central offices
  
 	Many older telephone central office switches implement caller
 control in which the release of the connection from a calling telephone
 to a called telephone is exclusively controlled by the originating
 telephone.  This means that if the penetrator simply failed to hang up
 a call to a modem on such a central office after he typed the legitimate
 user's user-name and password, the modem would be unable to hang up the
 connection.
  
  	Almost all modems would simply go on-hook in this situation
 and not notice that the connection had not been broken.  If the same line
 was used to dial out on as the call came in on,  when the modem
 went to dial out to call the legitimate user back the it might not
 notice (there is no standard way of doing so electrically) that the
 penetrator was still connected on the line.  This means that the modem
 might attempt to dial and then wait for an answerback tone from the far 
 end modem. If the penetrator was kind enough to supply the answerback tone
 from his modem after he heard the system modem dial, he could make a 
 connection and penetrate the system. Of course aome modems incorporate dial
 tone detectors and ringback detectors and in fact wait for dial tone before
 dialing, and ringback after dialing but fooling those with a recording of
 dial tone (or a dial tone generator chip) should pose little problem.
  
  	
  		Trying to call out on a ringing line
  
  	Some modems are dumb enough to pick up a ringing line and
 attempt to make a call out on it.   This fact could be used by a
 system penetrator to break dial back security even on joint control or
 called party control central offices.  A penetrator would merely have to
 dial in on the dial-out line (which would work even if it was a separate
 line as long as the penetrator was able to obtain it's number), just as
 the modem was about to dial out.  The same technique of waiting for
 dialing to complete and then supplying answerback tone could be used - and
 of course the same technique of supplying dial tone to a modem which waited
 for it would work here too.
  
  	Calling the dial-out line would work especially well in cases where the
 software controlling the modem either disabled auto-answer during the period
 between dial-in and dial-back (and thus allowed the line to ring with no
 action being taken) or allowed the modem to answer the line (auto-answer
 enabled) and paid no attention to whether the line was already connected
 when it tried to dial out on it.
  
  
  		The ring window
  
  	However, even carefully written software can be
 fooled by the ring window problem.  Many central offices actually will connect
 an incoming call to a line if the line goes off hook just as the call comes
 in without first having put the 20 hz. ringing voltage on the line to make it
 ring.  The ring voltage in many telephone central offices is supplied
 asynchronously every 6 seconds to every line on which there is an incoming
 call that has not been answered, so if an incoming call reaches
 a line just an instant after the end of the ring period and the line
 clairvointly responds by going off hook it may never see any ring voltage.
  
 	This means that a modem that picks up the line to dial out just as our
 penetrator dials in may not see any ring voltage and may therefore have no
 way of knowing that it is connected to an incoming call rather than
 the call originating circuitry of the switch.  And even if the switch
 always rings before connecting an incoming call, most modems have a
 window just as they are going off hook to originate a call when they
 will ignore transients (such as ringing voltage) on the assumption that
 they originate from the going-off-hook process. [The author is aware
 that some central offices reverse battery (the polarity of the voltage
 on the line) in the answer condition to distinguish it from the
 originate condition, but as this is by no means universal few if any
 modems take advantage of the information supplied] 
  
  
  		In Summary
  
  	It is thus impossible to say with any certainty that when a modem
 goes off hook and tries to dial out on a line which can accept incoming calls
 it really is connected to the switch and actually making an outgoing call.
 And because it is relatively easy for a system penetrator to fool the
 tone detecting circuitry in a modem into believing that it is seeing dial
 tone, ringback and so forth until he supplies answerback tone and connects
 and penetrates system security should not depend on this sort of dial-back.
  
  
  		Some Recommendations
  
  	Dial back using the same line used to dial in is not very secure
 and cannot be made completely secure with conventional modems.  Use of
 dithered (random) time delays between dial in and dial back combined with
 allowing the modem to answer during the wait period (with provisions made for
 recognizing the fact that this wasn't the originated call - perhaps by
 checking to see if the modem is in originate or answer mode) will
 substantially reduce this window of vulnerability but nothing can completely
 eliminate it.
  
  	Obviously if one happens to be connected to an older caller control
 switch, using the same line for dial in and dial out isn't secure at 
 all.  It is easy to experimentally determine this, so it ought to be possible
 to avoid such situations.
  
  	Dial back using a separate line (or line and modem) for dialing
 out is much better, provided that either the dial out line is sterile
 (not readily traceable by a penetrator to the target system) or that it is
 a one way line that cannot accept incoming calls at all.  Unfortunately the
 later technique is far superior to the former in most organizations as
 concealing the telephone number of dial out lines for long periods involves
 considerable risk.  The author has not tried to order a dial out only
 telephone line, so he is unaware of what special charges might be made for
 this service or even if it is available.
  
  		A final word of warning
 
 	In years past it was possible to access telephone company test
 and verification trunks in some areas of the country by using mf tones from so
 called "blue boxes". These test trunks connect to special ports on telephone
 switches that allow a test connection to be made to a line that doesn't
 disconnect when the line hangs up.   These test connections could
 be used to fool a dial out modem, even one on a dial out only line (since
 the telephone company needs a way to test it, they usually supply test
 connections to it even if the customer can't receive calls).
  
  	Access to verification and test ports and trunks has been tightened
 (they are a kind of dial-a-wiretap so it ought to be pretty difficult)
 but in any as in any system there is always the danger that someone, through
 stupidity or ignorance if not mendacity will allow a system penetrator
 access to one.
  
  		**  Some more recent comments **
  
  	Since posting this I have had several people suggest use
 of PBX lines that can dial out but not be dialed into or outward WATS
 lines that also cannot be dialed.  Several people have also suggested
 use of call forwarding to forward incoming calls on the dial out
 line to the security office.  [This may not work too well in areas
 served by certain ESS's which ring the number from which calls are
 being forwarded once anyway in case someone forgot to cancel forwarding.
 Forwarding is also subject to being cancelled at random times by central
 office software reboots.]
  
 	And since posting this I actually tried making some measurements
 of how wide the incoming call window is for the modems we use for dial
 in at CRDS.  It appears to be at least 2-3 seconds for US Robotics
 Courier 2400 baud modems.  I found I could defeat same-line-for-dial-out  
 dialback quite handily in a few dozen tries no matter what tricks I
 played with timing and watching modem status in the dial back login software.
 I eventually concluded that short of reprogramming the micro in the modem
 to be smarter about monitoring line state, there was little I could do at
 the login (getty) level to provide much security for same line dialback.
  
  	Since it usually took a few tries to break in, it is possible to
 provide some slight security improvement by sharply limiting the number of
 unsucessful callbacks per user per day so that a hacker with only
 a couple of passwords would have to try over a significant period of time.
 
  	Note that dialback on a dedicated dial-out only line is 
 somewhat secure.
  
        David I. Emery    Charles River Data Systems   617-626-1102
        983 Concord St., Framingham, MA 01701.
        uucp: decvax!frog!die
  
</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.25.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.27.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-13</DOCNO>
<DOCOLDNO>IA012-000123-B021-92</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.27.html 128.240.150.127 19970217003818 text/html 9526
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:36:45 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 27</TITLE>
<LINK REL="Prev" HREF="/Risks/3.26.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.28.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.26.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.28.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 27</H1>
<H2> Tuesday, 29 July 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Whoops!  Lost an Area Code! 
</A>
<DD>
<A HREF="#subj1.1">
Clayton Cramer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Comet-Electra (<A HREF="/Risks/3.25.html">RISKS-3.25</A>) 
</A>
<DD>
<A HREF="#subj2.1">
Stephen Little
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Comparing computer security with human security 
</A>
<DD>
<A HREF="#subj3.1">
Bob Estell
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Whoops!  Lost an Area Code!
</A>
</H3>
<address>
Clayton Cramer
&lt;<A HREF="mailto:voder!kontron!cramer@ucbvax.Berkeley.EDU ">
voder!kontron!cramer@ucbvax.Berkeley.EDU 
</A>&gt;
</address>
<i>
Mon, 28 Jul 86 11:29:10 pdt
</i><PRE>
To: voder!sri-csl.arpa!risks

I had an interesting and aggravating experience this last Saturday.  The 707
area code billing system failed.  Completely.  For over five hours.

During that time, you could not dial into the 707 area code, dial out
of it, make local calls billed to a credit card, or get an operator.  
The ENTIRE area code.  Fortunately, the 911 emergency number doesn't 
go through the billing system, so I doubt any lives were lost or
threatened by this failure, but I shudder to think of how this could
happen.  My guess is someone cut over to a new release of software
and it just failed.

No great philosophical comments, but one of those discouraging examples
of the fragility of highly centralized systems.

Clayton E. Cramer

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Comet-Electra (<A HREF="/Risks/3.25.html">RISKS-3.25</A>)
</A>
</H3>
<address>
S Little
&lt;<A HREF="mailto:munnari!gucis.oz!edsel@seismo.CSS.GOV ">
munnari!gucis.oz!edsel@seismo.CSS.GOV 
</A>&gt;
</address>
<i>
Tue, 29 Jul 86 15:14:30 est
</i><PRE>

Initial design studies for a trans-atlantic turbo-jet powered mail plane
were begun during World War II by de Havilland.  Eventually a much larger
airliner, the DH-106 Comet prototype flew in 1949, so that computer
involvement in the design is not an issue.  The test program involved may
have been adequate for forties technologies, but the jet-based mileages and
altitudes obviously revealed a new range of problems which have resulted in
the more stringent certification procedures now applied.

Whatever the source of the disastrous crack propagation (said in one case to
be possibly a radio antenna fixing), the design change to rounded windows
was in response to this danger.  The only square window Comets remained in
RAF service without pressurization for many years (Air International vol.12
no.4, 1977).

Given that computer representation is limited by our understanding of a
design situation, is there a general concern with the performance of, inter
alia, flight simulators, which may accurately represent an inadequate
understanding of the behaviour of the system modelled.  I have been told of
one major accident in which the pilot followed the drill for a specific
failure, as practiced on the simulator, only to crash because a critical
common-mode feature of the system was neither understood, or incorporated in
the simulation.  I highly reccommend Charles Perrow's "Normal Accidents" for
an analysis of the components of complexity in such situations.

I understand that the Shuttle auto-pilot is the source of re-appraisal
including expert systems derivation of responses to the large number
of relevant variables.  What are people's feelings about the induction
of knowledge in such areas, is it felt to increase or decrease risk
via computer ?

Stephen Little, Computing &amp; Information Studies,
                Griffith Uni, Qld, Australia.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Comparing computer security with human security
</A>
</H3>
<address>
"143B::ESTELL" 
&lt;<A HREF="mailto:estell%143b.decnet@nwc-143b.ARPA">
estell%143b.decnet@nwc-143b.ARPA
</A>&gt;
</address>
<i>
29 Jul 86 08:29:00 PST
</i><PRE>
To: "risks" &lt;risks@csl.sri.com&gt;

The question has been raised: Are there significant differences in the
quality of security in computer system, based on elaborate software models
[passwords, access lists, et al], versus having human guards at the door; 
e.g., humans can be bribed, computers can't; but computers can fail.

Hmmmmm... First let me admit a bias: I think the "MIT rule" applies: 
 No system can be better than the PEOPLE who design, build, and operate it.
[I call it that because that's where I first heard in in '68.]

Aside from that bias, there seems to be some assumptions:
(1) People don't "fail" [at least not like computers do]; and
(2) Computer can't be "diverted" in the manner of a bribe.

Seems to me that people DO FAIL, somewhat like computers; i.e., we have
memory lapses [similar perhaps to incorrect data fetches?]; and we make
perception errors [similar perhaps to routing replies to the wrong CRT?]

And computers can be diverted.  Examples:

(1) A malicious agent, only wanting to deny others service on a computer,
    rather than gain access himself, can often find ways to exploit the
    priority structure of the system; e.g., some timesharing systems give 
    high priority to "login" sequences; attacking these with a "faulty 
    modem" can drain CPU resources phenominally.

(2) There are some operating systems/security packages that fail in a com-
    bination of circumstances; I'm going to be deliberatly vague here, in
    part because the details were shared with me with the understanding
    that I not broadcast them, and in part because I've forgotten them,
    and in part because the exact info is not key to the discussion;
    but to continue:

	If the terminal input buffer is overrun [e.g., if the user-id or
	password is VERY long], and if the "next" dozen [or so] bytes
	matches a "key string" then the intruder is allowed on; not only
	that, but at a privileged level.

    In other words, the code gets confused.  But isn't that what a person
    suffers when he trades his freedom, his honor, and all his future earn-
    ings [hundreds of thousands of dollars?] for a few "easy" tens of thous-
    ands of dollars now for one false act?  I'm saying that most "bribes"
    aren't nearly large enough to let the "criminal" relocate somewhere
    safe from extradition, and live a life of luxury ever after; instead,
    most bribes are only big enough to "buy a new car" or pay a overdue
    mortgage or medical bill.

------

OR is the real risk in both cases [human and computer] that the most potent
penetrations are those that never come to light; e.g., the computer "bug"
that is so subtle that it leaves no traces; and the "human bribe" that is
so tempting that authorities [and victims] don't talk about it - precisely
because they don't want folks to know how much it can be worth?

Discussion and comments, please.             Bob

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.26.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.28.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-14</DOCNO>
<DOCOLDNO>IA012-000123-B021-115</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.28.html 128.240.150.127 19970217003840 text/html 11361
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:37:01 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 28</TITLE>
<LINK REL="Prev" HREF="/Risks/3.27.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.29.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.27.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.29.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 28</H1>
<H2> Thursday, 31 July 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Laserprinter dangers 
</A>
<DD>
<A HREF="#subj1.1">
Mansfiel
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Errors in error-handlers 
</A>
<DD>
<A HREF="#subj2.1">
Mansfiel
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Military testing errors 
</A>
<DD>
<A HREF="#subj3.1">
Alan Wexelblat
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: Comet-Electra (<A HREF="/Risks/3.25.html">RISKS-3.25</A>) 
</A>
<DD>
<A HREF="#subj4.1">
Bill Fisher
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Computer and Human Security 
</A>
<DD>
<A HREF="#subj5.1">
Lindsay Marshall
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Laserprinter dangers
</A>
</H3>
<address>
   
&lt;<A HREF="mailto:MANSFIEL%DHDEMBL5.BITNET@WISCVM.ARPA">
MANSFIEL%DHDEMBL5.BITNET@WISCVM.ARPA
</A>&gt;
</address>
<i>
Mon 31 Jul 86 17:38:10 N
</i><PRE>

Increasingly, large and "official" organisations such as motor vehicle tax
offices, insurance companies, etc. are using laser printers to print the
bills and other requests for money which are sent to customers. Whereas
previously pre-printed letterheads (often with several and or coloured inks)
were used, now the laser printer is relied on to print the letterhead
itself, so that plain paper can be used.

It is probably only a matter of time before some clever person prints off a
batch that looks fine but that have the c.d.'s own account number (or some
other slightly safer one) on them, sends them out, and gets lots of money.

There must be lots of other forgery and swindling possibilities with laser
printers.  Have any frauds of this type have actually been committed?

   [Most banks no longer make blank deposit slips routinely available, after
    various episodes of people magnetically coding account numbers onto the
    blanks and leaving these slips in the stack of blanks.  Spoofing of
    letterheads is of course relatively easy with laser printers, but also
    with many of the electronic mailers around the net.  PGN]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Errors in error-handlers
</A>
</H3>
<address>
   
&lt;<A HREF="mailto:MANSFIEL%DHDEMBL5.BITNET@WISCVM.ARPA">
MANSFIEL%DHDEMBL5.BITNET@WISCVM.ARPA
</A>&gt;
</address>
<i>
Mon 31 Jul 86 15:47:17 N
</i><PRE>

Ken Laws, in <A HREF="/Risks/3.25.html">RISKS-3.25</A> said

        &gt; Errors that arise within the error handlers are similarly
        &gt; important, but beyond my ability to even contemplate in
        &gt; the context of current languages.

A related problem, but much simpler and much more common in my experience,
is that the user-written error handling code contains lots of errors.
Reasons for this include

        (a) This code is not considered "important", because we don't
        really expect it ever to be used, and even if it is,
        it will be used so rarely that normal criteria for
        neatness, etc., are not relevant.

        (b) To exercise the code, the errors have to be caused
        or simulated. This is just too much work, especially
        as the program works "satisfactorily" as it is anyway.

The usual result is that when a rare error occurs, the error handler blows
up, or worse, gives a wrong report. Then, having found the problem after
many fevered days, you realise that the one time you need all the help you
can get, including accurate error reports, is when you are under pressure to
repair a crashed system, and you vow that in future ...

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Military testing errors
</A>
</H3>
<address>
Alan Wexelblat 
&lt;<A HREF="mailto:wex@mcc.com">
wex@mcc.com
</A>&gt;
</address>
<i>
Wed, 30 Jul 86 14:49:03 CDT
</i><PRE>

The following second-hand item appeared in the local Austin rag:

"SANTA ANA, Calif (AP) - A Pentagon error that knocked off two points on
aptitude test taken by military recruits caused thousands of servicemen to
lose training and benefits, according to a newspaper report.

 The scoring error on nearly 2 million aptitude tests since 1984 could have
been crucial for some recruits, because a single point can mean the
difference between college-level training and a less-desirable assignment.

 The _Orange County Register_ said Saturday that the military did not
announce the errors but acknowledged them when queried by the newspaper.  [...]

 Rep. Robert Badham, R-Calif., said the House Armed Services Subcommittee
on Military Personnel is investigating the testing problem and its effects.

 It was unclear what caused the problem.  The newspaper said that the error
was apparently due to either to a miscalculation of the scoring curve
incorporated into the Chicago testing computer or an actual misprint in the
test booklets."

Does anyone have any better information than this?
Alan Wexelblat
ARPA: WEX@MCC.ARPA
UUCP: {ihnp4, seismo, harvard, gatech, pyramid}!ut-sally!im4u!milano!wex

"It is quite impossible for any design to be `the logical outcome of the
requirements' simply because, the requirements being in conflict, their
logical outcome is an impossibility."

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: Comet-Electra (<A HREF="/Risks/3.25.html">RISKS-3.25</A>)
</A>
</H3>
<address>
&lt;<A HREF="mailto:bfisher.ES@Xerox.COM">
bfisher.ES@Xerox.COM
</A>&gt;
</address>
<i>
30 Jul 86 07:33:41 PDT (Wednesday)
</i><PRE>
To: RISKS FORUM (Peter G. Neumann -- Coordinator) &lt;RISKS@CSL.SRI.COM&gt;

Some years back (&gt;10) there was a book out, "The Tail of the Comet,"
analyzing the design process for the Comet and then the investigations and
procedures which pinpointed the design errors.  I can't remember the author,
but a comment of his is carved in memory, viz., "Extrapolation and
interpolation are the fertile parents of error."

Bill Fisher 

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Computer and Human Security
</A>
</H3>
<address>
"Lindsay F. Marshall" 
&lt;<A HREF="mailto:lindsay%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK">
lindsay%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Wed, 30 Jul 86 13:30:00 gmt
</i><PRE>

I feel that there are significant differences between the quality of the two
sorts of security.  I appreciate the similarities that Bob has described and
agree with his "MIT" rule, but there are many instances where computer
security seems very much more superficial than human security.  Passwords are
the most obvious example - there is no simple way to determine whether or
not the person typing the password is in fact the person expected, whereas
there are other clues available to a trained human (NOT that I am saying
that these are always correct or are always used!).  In simplistic terms, it
is much easier (for the average person) to impersonate someone "anonymously"
by using their password, than it is for someone to actual pretend to be that
person to other people. Of course, someone with enough confidence can get
away with a phenomenal amount of pretence, because most people aren't really
supicious (e.g., men in white coats in hospitals/labs, cleaners, postmen
(cf. Father Brown story, "The Invisible Man")) or because people don't
follow the rules (e.g. people with photos of apes/Einstein stuck to their
identity cards).  An example from my own experience when working in Industry:

	I had received a tape written at 1600bpi on an IBM machine and
needed a copy made at 800bpi for our PDP-11, so I went to the computer
centre of our parent organisation, stopped an operator and asked him to make
the copy and if possible to run the job that was on the tape.  (It was an
ENORMOUS Fortran H compilation...)  The op said OK and I hung around a bit,
looked over peoples shoulders and chatted with some people whom I knew, but
that wasn't obvious).  An hour later the op returned with my tapes and
listing and said "By the way, who are you?".  The day after that they
installed electronic card locks on all the doors to the computing centre and
stationed someone on the door....

	I got away with this a) because I had never thought that there would
be a problem, and so was the reverse of furtive (I may add that I had a lot
of hair at time as well) and b) because the management hadn't actually
considered the security risks (they did MOD work on the machine). On these
lines has anybody more information about the Lockheed document scandal or is
that too hush-hush???
                                        Lindsay

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.27.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.29.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-15</DOCNO>
<DOCOLDNO>IA012-000123-B021-140</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.29.html 128.240.150.127 19970217003855 text/html 13084
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:37:22 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 29</TITLE>
<LINK REL="Prev" HREF="/Risks/3.28.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.30.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.28.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.30.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 29</H1>
<H2> Friday, 1 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Ozone hole undetected for years due to programming error 
</A>
<DD>
<A HREF="#subj1.1">
Bill McGarry
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Aircraft simulators and risks 
</A>
<DD>
<A HREF="#subj2.1">
Art Evans
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Military testing errors 
</A>
<DD>
<A HREF="#subj3.1">
Scott E. Preece
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Risks: computers in the electoral process 
</A>
<DD>
<A HREF="#subj4.1">
Kurt Hyde via Pete Kaiser
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Risks of CAD 
</A>
<DD>
<A HREF="#subj5.1">
Alan Wexelblat
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Ozone hole undetected for years due to programming error
</A>
</H3>
<address>
Bill McGarry
&lt;<A HREF="mailto:sdcsvax!dcdwest!ittatc!bunker!wtm@ucbvax.Berkeley.EDU ">
sdcsvax!dcdwest!ittatc!bunker!wtm@ucbvax.Berkeley.EDU 
</A>&gt;
</address>
<i>
Fri, 1 Aug 86 0:48:48 EDT
</i><PRE>

(I read the following in a magazine but when I went to write this
 article, I could not remember which magazine and some of the exact
 details.  My apologies for any inaccuracies.)  

Recently, it was disclosed that a large hole in the ozone layer appears once
a year over the South Pole.  The researchers had first detected this hole
approximately 8 years ago by tests done at the South Pole itself.

Why did they wait 8 years to disclose this disturbing fact?  Because the
satellite that normally gives ozone levels had not reported any such hole
and the researchers could not believe that the satellite's figures could be
incorrect.  It took 8 years of testing before they felt confident enough to
dispute the satellite's figures.

And why did the satellite fail to report this hole?  Because it had been
programmed to reject values that fell outside the "normal" range!

I do not know which is more disturbing -- that the researchers had so much
faith in the satellite that it took 8 years of testing before they would
dispute the satellite or that the satellite would observe this huge drop in
the ozone level year after year and just throw the results away?

			Bill McGarry
			Bunker Ramo, Trumbull, CT
			{decvax, philabs, ittatc, fortune}!bunker!wtm

          [A truly remarkable saga.  I read it too, and was going to report
           on it -- but could not find the source.  HELP, PLEASE!  PGN]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Aircraft simulators and risks
</A>
</H3>
<address>
"Art Evans" 
&lt;<A HREF="mailto:Evans@TL-20B.ARPA">
Evans@TL-20B.ARPA
</A>&gt;
</address>
<i>
Thu 31 Jul 86 13:26:48-EDT
</i><PRE>
To: Risks@CSL.SRI.COM

In <A HREF="/Risks/3.27.html">RISKS-3.27</A>, Stephen Little comments on the risks in using an aircraft
simulator which inadequately represents the aircraft being simulated:

    I have been told of one major accident in which the pilot followed
    the drill for a specific failure, as practiced on the simulator,
    only to crash because a critical common-mode feature of the system
    was neither understood, or incorporated in the simulation.

The implication is that use of such a simulator is risky, which is
surely true.  However, as is so often the case, we must also examine the
risk of not using the simulator.  Pilots flying simulators frequently
practice maneuvers which are quite risky in a real aircraft.  A common
example is loss of power in one engine at a critical moment on takeoff.
This is just too risky to practice for real (since sometimes the "right"
answer is to crash straight ahead on the softest and least expensive
piece of real estate in sight), but practice in the simulator is quite
valuable.  All we can do is make the simulator as good as state of the
art permits, and improve it whenever we are subjected to one of the
expensive lessons Little refers to.

Little also comments on the shuttle simulator.  There, I would guess,
the critical issue is the cost of using the real thing as opposed to
cost of the simulator.  Again, the simulator is as good as practical,
and is improved as more data are gathered.

Art Evans

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Military testing errors
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%ccvaxa@GSWD-VMS.ARPA">
preece%ccvaxa@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Fri, 1 Aug 86 16:53:13 cdt
</i><PRE>

The New York Times report indicated that some of the tests were printed with
a major section set in six-point type instead of ten-point, making it very
hard to read.  The section consisted of math word problems and the object
was to do as many as possible in a set time.  People with the small-type
tests did significantly worse than those with the large-type tests.
Although this MIGHT be a computer-related problem (if the error was, for
instance, lack of a font change in a machine-readable source file), I don't
think the article specifically said that.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Risks: computers in the electoral process
</A>
</H3>
<address>
Systems Consultant
&lt;<A HREF="mailto:kaiser%furilo.DEC@decwrl.DEC.COM  ">
kaiser%furilo.DEC@decwrl.DEC.COM  
</A>&gt;
</address>
<i>
01-Aug-1986 1529
</i><PRE>
Really-From: [Forwarded by Pete K. on behalf of] Kurt Hyde

There will be a symposium on security and reliability of computers in
the electoral process at Boston University this August 14th &amp; 15.

Computers are relatively new in the electoral process and most decision
makers in this process have little, if any, experience.  One of the
speakers found evidence of a Trojan Horse in ballot counting software.
He will be speaking about that in the symposium.

PLACE: Boston University   Engineering Building, Room B33
DATE:  August 14th &amp; 15th
TIME:  9:00 AM thru 4:00 PM

I would like to thank the many RISKS readers who contributed last semester
to my students' request for ideas on how to make the computerized voting
booth safe from computer fraud.  I'll be presenting many of the findings of
our study.
                                Kurt Hyde

             [Recall Ron Newman's detailed summary in <A HREF="/Risks/2.42.html">RISKS-2.42</A> of 
              Eva Waskell's talk on this subject.  Perhaps we will get
              an update on any new information presented at BU.  We
              look forward to Kurt's findings as well.  PGN]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Risks of CAD
</A>
</H3>
<address>
Alan Wexelblat 
&lt;<A HREF="mailto:wex@mcc.com">
wex@mcc.com
</A>&gt;
</address>
<i>
Fri, 1 Aug 86 15:45:54 CDT
</i><PRE>

Henry Petroski's book, _To Engineer is Human_ contains a chapter called
"From Slide Rule to Computer," in which he talks about some risks of
computers and specifically of computer-aided design (CAD).  I will try to
summarize his main points below.

Petroski points out that the transition away from slide rules has, in
itself, some risks.  First of all, there is the problem of precision.
Everyone knows that computers can produce very precise results, but this
tends to blind us to the fact that the results are really no more precise
than the inputs that were combined to produce them.  A twelve-digit answer
is no good if one of your inputs is accurate to only three digits.

A side effect of this is that we have tended to lose a `feel' for the
proper magnitudes for our numbers.  When arithmetic was done on a slide
rule, students had to supply the decimal place and thus needed to know
approximately how big the answer should be.  This lack of feel seems to
have been (at least part of) the problem with the x-ray machine that burned
a patient by applying too large a dose.

In "the old days" calculating stresses and the like was expensive and so
engineers didn't have time to do too much of it.  So they tended to design
things that were close to their experience and where they knew approximately
what the stresses, etc.  should be.  With optimization (and other CAD)
packages, engineers can do much more calculating and can therefore design
structures that are more novel and that they are less familiar with.  This
increases the risk that the engineer will not be able to spot errors in the
CAD programs' output.  Again, he has no `feel' for what the output should be.

Petroski also fears that inadequate computer simulation is replacing crucial
real testing.  Engineers who are not programmers may not realize that
certain stress calculations have not been done by the program; thus he may
be inclined to forgo simple things (like physically stretching or bending a
pipe to see where it breaks).  An example of this oversimplification is the
collapse of the roof of the Hartford Civic Center (under a weight of ice
and snow).  Post mortem analysis revealed that the interconnection of the
rods and girders in the ceiling had been modeled too simplistically in the
computer programs that were used during the design.

In general, Petroski fears that the CAD programs' optimization of things is
leading to structures that are "least-safe."  That is, there's no room for
error in the optimized structure.

There is also a risk that with a software crutch a less-than-qualified
engineer can put together a design that looks better than it is.  Even an
engineer who is qualified in one area may be encouraged by the ease of CAD
to venture outside his area of expertise.

There is also one other item of interest to RISKS readers.  In the chapter
called "The Limits of Design," Petroski quotes from the proceedings of the
"Proceedings of the First International Conference on Computing in Civil
Engineering."  Apparently, there was a session on `Computer Disasters' at
that conference, but NO PAPERS WERE PUBLISHED.  Supposedly, this encouraged
candor.  The conference was held in New York, in 1981.  Were any RISKS
readers there?  Do you know someone who was?  It would be interesting to
see if we can construct a list of our own.

In any event, Petroski's book (ISBN 0-312-80680-9) is a good read and can
be bought at a discount by members of LCIS.  I recommend it highly.

Alan Wexelblat
UUCP: {ihnp4, seismo, harvard, gatech, pyramid}!ut-sally!im4u!milano!wex

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.28.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.30.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-16</DOCNO>
<DOCOLDNO>IA012-000123-B021-157</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.30.html 128.240.150.127 19970217003913 text/html 8532
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:37:36 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 30</TITLE>
<LINK REL="Prev" HREF="/Risks/3.29.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.31.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.29.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.31.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 30</H1>
<H2> Monday, 4 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Ozone hole undetected 
</A>
<DD>
<A HREF="#subj1.1">
Jeffrey Mogul
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Re: Risks of CAD 
</A>
<DD>
<A HREF="#subj2.1">
Henry Spencer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Comment on Hartford Civic Roof Design 
</A>
<DD>
<A HREF="#subj3.1">
Richard S D'Ippolito
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Expert system to catch spies 
</A>
<DD>
<A HREF="#subj4.1">
Larry Van Sickle
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Ozone hole undetected
</A>
</H3>
<address>
Jeffrey Mogul
&lt;<A HREF="mailto:mogul@decwrl.DEC.COM ">
mogul@decwrl.DEC.COM 
</A>&gt;
</address>
<i>
4 Aug 1986 1058-PDT (Monday)
</i><PRE>

Although I, too, am relying on memory, I'm pretty sure that the article Bill
McGarry mentioned was published in The New Yorker sometime during the past
two or three months.                   [Also something in Science a few issues
                                        ago on the phenonenon itself...  PGN]

My understanding is that it was not so much a case of the researchers
believing the satellite instead of other evidence, but rather that the
researchers who ran the satellite must not have been too terribly interested
in what was going on over the poles.  After all, if they were interested, I
would think they might have been bothered by large empty spots in their data.

As to Bill's being disturbed that "the satellite would observe this huge
drop in the ozone level year after year and just throw the results away", I
think this imputes a certain level of intelligence to the computer system
that probably isn't there.  I'd bet that their computer spits out maps of
the ozone layer, but probably doesn't have any facility to spot trends.

Still, it's obvious that a little more care in the decision to discard
anomalous data would have gone a long way.  When humans through away
anomalous results, at least they realize that they are doing so [although
not always consciously; see Stephen Jay Gould's "The Mismeasure of Man".]
When a computer throws away anomalous data, the user might not be aware that
anything unusual is going on.  A good program would at least remark that it
has thrown away some fraction of the input data, to alert the user that
something might be amiss.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Re: Risks of CAD
</A>
</H3>
<address>
&lt;<A HREF="mailto:decwrl!decvax!utzoo!henry@ucbvax.Berkeley.EDU">
decwrl!decvax!utzoo!henry@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Sun, 3 Aug 86 03:17:32 edt
</i><PRE>

Alan Wexelblat comments:

&gt; Petroski also fears that inadequate computer simulation is replacing crucial
&gt; real testing...

One can see examples of the sort of engineering this produces in many pieces
of high-tech US military equipment.  In the recent times, the criteria used
to evaluate a new military system have increasingly drifted away from straight
field-test results and toward complex and arbitrary scoring schemes only
vaguely related to real use.  Consider how many official reports on the
Sergeant York air-defence gun concluded, essentially, "no serious problems",
when people participating in actual trials clearly knew better.  Some of this
was probably deliberate obfuscation -- juggling the scoring scheme to make
the results look good -- but this was possible only because the evaluation
process was well divorced from the field trials.  Another infamous example
is the study a decade or so ago which seriously contended that the F-15 would
have a kill ratio of several hundred to one against typical opposition.
These are conspicuous cases because the evaluation results are so grossly
unrealistic, but a lot of this goes on, and the result is unreliable equipment
with poor performance.

It should be noted, however, that there is "real testing" and real testing.
Even the most realistic testing is usually no better than a fair facsimile
of worst-case real conditions.  The shuttle boosters superficially looked
all right because conditions had never been bad enough to produce major
failure.  The Copperhead laser-guided antitank shell looks good until you
note that most testing has been in places like Arizona, not in the cloud and
drizzle more typical of a land war in Europe.  Trustworthy test results
come from real efforts to produce realistic conditions and vary them as much
as possible; witness the lengthy and elaborate tests a new aircraft gets.
Even if the results of CAD do get real-world testing, one has to wonder
whether those tests will be scattered data points to "validate" the output
of simulations, as opposed to thorough efforts to uncover subtle flaws that
may be hiding between the data points.

				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Comment on Hartford Civic Roof Design
</A>
</H3>
<address>
&lt;<A HREF="mailto:Richard.S.D'Ippolito@sei.cmu.edu">
Richard.S.D'Ippolito@sei.cmu.edu
</A>&gt;
</address>
<i>
4 Aug 1986 00:33:41-EDT
</i><PRE>
Apparently-To: Risks@SRI-CSL.ARPA

I would like to point out that Alan Wexelblat's comment on inadequate use of
computers for CAD might be somewhat misleading regarding the roof modelling
for the Hartford Civic Center. The problem was that the program user
selected the wrong model for the beam connection to be used. When the 
program was re-run with the correct model, it predicted the collapse in 
precisely the mode that it happened. I'm not sure that that was clear from 
the wording in Mr. Wexelblat's comment, i.e., that the modelling was 
improperly done by the operator (GIGO again!).

Richard D'Ippolito, P.E.
Carnegie-Mellon University
Software Engineering Institute
(412)268-6752
rsd@SEI.CMU.EDU

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.29.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.31.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-17</DOCNO>
<DOCOLDNO>IA012-000123-B021-179</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.31.html 128.240.150.127 19970217003941 text/html 18844
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:38:01 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 31</TITLE>
<LINK REL="Prev" HREF="/Risks/3.30.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.32.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.30.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.32.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 31</H1>
<H2> Tuesday, 5 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Another cruise missile lands outside Eglin test range 
</A>
<DD>
<A HREF="#subj1.1">Martin J. Moore</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Aircraft simulators and risks 
</A>
<DD>
<A HREF="#subj2.1">Gary Wemmerus</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: Comment on Hartford Civic Roof Design 
</A>
<DD>
<A HREF="#subj3.1">Brad Davis</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Expert system to catch spies (<A HREF="/Risks/3.30.html">RISKS-3.30</A>) 
</A>
<DD>
<A HREF="#subj4.1">Chris McDonald</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Computer and Human Security 
</A>
<DD>
<A HREF="#subj5.1">Henry Spencer</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Ozone Reference 
</A>
<DD>
<A HREF="#subj6.1">Eugene Miya</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Financial risks 
</A>
<DD>
<A HREF="#subj7.1">Robert Stroud</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Mail Load Light(e)ning? 
</A>
<DD>
<A HREF="#subj8.1">
SRI-CSL Mail Daemon
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Another cruise missile lands outside Eglin test range
</A>
</H3>
<address>

&lt;<A HREF="mailto:mooremj@eglin-vax">
mooremj@eglin-vax
</A>&gt;
</address>
<i>
0  0 00:00:00 CDT
</i><PRE>
To: "risks" &lt;risks@sri-csl&gt;

An unarmed Tomahawk cruise missile malfunctioned and landed unexpectedly
during a test launch at Eglin AFB last Saturday (8/2/86).  The missile,
launched from the battleship Iowa at 10:15 am CDT, flew successfully for 69
minutes before deploying its recovery parachute for reasons not yet
determined.  The missile made a soft landing in an uninhabited area 16 miles
west of Monroeville, Alabama.  No injuries or property damage were reported. 

The cause of the failure is not yet known.  The missile, which suffered no
apparent external damage, was recovered and returned to the General Dynamics
works in San Diego for investigation.  The missile was the second in four
launches to land outside the 800-square-mile Eglin reservation.  Last December 
8, the first Tomahawk launched at Eglin landed near Freeport, Florida.  The 
cause of that failure was a procedural problem which caused portions of the 
missile's flight control program to be erased during loading.

Saturday's failure followed a successful Tomahawk launch on the previous day.
A missile launched from the destroyer Conolly successfully flew a 500-mile
zigzag course over southern Alabama and the Florida Panhandle before landing
at the designated recovery point on the Eglin range.

				-- Martin J. Moore

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Re: Aircraft simulators and risks
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Tue, 05 Aug 86 09:45:51 -0800
From: Gary Wemmerus &lt;gfw@ICSE.UCI.EDU&gt;

	I heard a story about the DC-10 crash at O'Hare in 1979 that might
be the one you mentioned.
	After the crash, they programmed that sequence of events into the
simulator and tried out pilots on it.  Every one of the pilots that followed
the correct procedures as listed in the MANUAL for that sequence of events
CRASHED.  The problem was that the sequence of events did not include loss
of an engine, just loss of engine power, and did not take into account total
loss of hydraulic power.  I have heard that there are no instruments on the
DC-10 that would tell a pilot that the engine was gone, just that there was
no power from it.
	When pilots tried a different way or responding to the sequence of
events, I believe that a successful landing was achieved 80% of the time.  I
think that there was no problem with the simulator, but there were two sets
of events that led to one set of indicators to the pilot, and the manual
listed the correct procedure for the other set of events.  My guess is that
they never expected the sequence that occurred and have now come up with a
way to distinguish between the two events.  
                                                    -gfw 

PS. A lot of this is from second-hand sources, so I cannot totally vouch for
its accuracy.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: Comment on Hartford Civic Roof Design
</A>
</H3>
<address>
Brad Davis
&lt;<A HREF="mailto:b-davis@utah-cs.arpa ">
b-davis@utah-cs.arpa 
</A>&gt;
</address>
<i>
Tue, 5 Aug 86 13:18:08 MDT
</i><PRE>

Along with the problems of wrong model is the problems with not
testing at proper extremes or making bad assumptions.  About 15 years
ago a new shopping mall was being built in Salt Lake City.  The
engineers (and architects?)  from California consulted their data
books (or ran their CAD systems) and determined the amount of weight
the building needed to support to make it through a desert winter.
Even though Utah is a desert, we get 1 foot snowfalls in twelve-hour
periods.  The roof caved in at the first big snowfall of the season.
Luckily the mall hadn't opened yet.  They did fix it and the mall
hasn't had any problems since.
					Brad Davis

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Expert system to catch spies (<A HREF="/Risks/3.30.html">RISKS-3.30</A>)
</A>
</H3>
<address>
Chris McDonald  SD 
&lt;<A HREF="mailto:cmcdonal@wsmr06.arpa">
cmcdonal@wsmr06.arpa
</A>&gt;
</address>
<i>
Tue, 5 Aug 86  7:31:33 MDT
</i><PRE>
To: RISKS FORUM    (Peter G. Neumann -- Coordinator) &lt;RISKS@CSL.SRI.COM&gt;

 Larry Van Sickle asks the question "Is it doable?" regarding the use of an
"expert system" to screen out or to identify potential espionage agents.  From
my sixteen years of experience in positions which require a security clearance
and actually access to classified defense information, I conclude "NO!"  The
reason is that potentially millions of government as well as contractor
employees have clearances with access to national defense information.  I find
it incredible to belive that any "expert system" could realistically factor in
all the variables which might cause an individual to be recruited for espionage
or to recruit him or herself for such activity.  

Second, while the news media has reported the apparent "greed" of the most
recent batch of US citizens involved in espionage against their country, I
would surmise that there were probably equally compelling personnel and
philosophical reasons for their actions.  Whenever there is an in-depth damage
assessment of espionage cases "after the fact," it seems historically that
there are many motivations at work.  

Third, if "disaffection" might be one of the causes of a successful espionage
recruitment, then the problem is magnified by the very bureaucracy that
employs individuals with security clearances.  For example, there has not been
a President or Executive Branch since 1970 which has not proposed that the
Federal workforce is a collection of lazy, misfits who could not be employed
anywhere else.  There has never been a sustained call for "excellence" in the
government on the assumption that this is a contradiction in terms.  How could
any "expert system" factor in cuts in salary, retirement and benefits without--
perhaps with some exaggeration-- potentially disqualifying the entire 
workforce.  The defense contractor side of the house experiences the same sort
of problems as it goes through one cycle after another in which today we build
the B-1 bomber and the next day we shut down the line.

Finally, although I do not have the benefit of reading the actually article
which Larry mentions, it does appear that the so-called "former intelligence
analyst" has confused the issues of "suitability" and "loyalty".  Just because
an individual has financial problems does not necessarily mean that he will spy
against the US.  While "suitability" factors may appear in actual espionage
cases to have had some influence on "loyalty," they are usually never the sole
reason.  Indeed, if "greed" alone were a factor, why have so many people
"sold" themselves so cheaply?

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Computer and Human Security
</A>
</H3>
<address>
&lt;<A HREF="mailto:decwrl!decvax!LOCAL!utzoo!henry@ucbvax.Berkeley.EDU">
decwrl!decvax!LOCAL!utzoo!henry@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Tue, 5 Aug 86 21:41:12 edt
</i><PRE>

Lindsay F. Marshall writes, in part:

&gt; I feel that there are significant differences between the quality of the two
&gt; sorts of security... there are many instances where computer
&gt; security seems very much more superficial than human security...

The other side of this coin is that there are many instances where human
security is very much more superficial than computer security.  How many
times have you been waved through a gate by a guard who knows you?  Does
he really consider the possibility that your pass might have been revoked
yesterday?  Yes, I know, they're supposed to always check, but it often
doesn't work that way in practice.  Especially if there is something else
distracting them at the time.  An electronic pass-checker box, on the other
hand, does not get distracted and doesn't get to know you.  Human security
can be bribed, coerced, or tricked; these tactics generally don't work on
computers.  Their single-minded dedication to doing their job precisely
correctly and ignoring everything else blinds them to "out-of-band" signs
that subversion is taking place, but it also blinds them to "out-of-band"
methods of subversion.

The best approach is to combine the virtues of the two systems:  use
computers for mindless zero-defects jobs like checking credentials, and
use humans to watch for improper use of credentials, attempts to bypass
credential checking, and anomalies in general.  One gray area is checking
the match between credentials and credential-holders:  this generally has
to be done by humans unless the credentials are something like retinagrams.

				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Ozone Reference
</A>
</H3>
<address>
Eugene Miya
&lt;<A HREF="mailto:eugene@AMES-NAS.ARPA ">
eugene@AMES-NAS.ARPA 
</A>&gt;
</address>
<i>
Tue, 5 Aug 86 10:51:50 pdt
</i><PRE>
ReSent-To: RISKS@CSL.SRI.COM

I talked to one of our bio-geo-chemists.  There is a popular article which
he feels is a good introduction to the players of this research including
good references:
                    Nature, 321, June 19, 1986, pp. 729-730

To reiterate: all of the postings I have seen on Risks almost make this
sound like either a conspiracy or foot dragging by the earth science
community.  Eight years is nothing in the span of research in the earth
sciences.  That was also the length of time involved in the Palmdale Bulge
research which turned out to be erroneous.

My contact, Greg, has seen papers suggesting natural mechanisms for ozone
depletion in the Antarctic.  There is insufficient money and time to
research long-period phenomena.  Note: this brings up the issue of fast
developing trends with slow thinking scientific communities, but that is
another issue.
                                       --eugene miya, NASA Ames

    [The AAAS Science article is on page 1602 of the 27 June 1986 issue.
     It points out the increasing depletion (now 50%) in the ozone layer 
     for a short period in October compared with the 1979 norm.  It does
     not deal with the reported software problem.  PGN]

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Financial risks
</A>
</H3>
<address>
Robert Stroud 
&lt;<A HREF="mailto:robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Tue, 5 Aug 86 16:17:45 bst
</i><PRE>

There was an item on the ITV News at Ten last night about the record
62-point fall of the Dow Jones Index about a month ago. Since it was on TV,
I can't report it verbatim, but the gist was as follows:

  "Experts are convinced that the record fall was almost entirely due to the
  use of computer programs that automatically sell stock when certain
  conditions are triggered.  [...stuff about the cash index falling below the
  futures index...]  Whereas a fall of this magnitude would have been
  disastrous a few years ago, nowadays it hardly causes a hiccup. The big
  shareholders are quite capable of withstanding a swing of 40 points or more
  in a day, although the small investor suffers. Although computers are blamed
  for this sort of instability, they are also credited with keeping the market
  at its high level over the last 6 months.  However, members of the public
  would be concerned if they were aware of the increasing use of technology,
  not just because of the problems of the small investor but also because
  decisions are now being taken based solely on movements within the market,
  without consideration of external economic factors."

I also saw something in The Times suggesting that the fall was "aggravated"
by the use of such programs a few days after the incident occurred - maybe ITV
were reporting the result of an investigation into the causes.

There has been a recent trend towards relaxing controls and regulations in
the financial markets. There will shortly be what is known as the Big Bang
in the UK and this has caused a great deal of activity in the City with
companies that have traditionally performed separate functions being allowed
to merge, and several giant financial organisations forming. There has been
a lot of headhunting with astronomical (by British standards :-) salaries
being offered, first for dealers but more recently for those with computing
experience. Sophisticated computer systems are planned, and apart from just
displaying information, I expect there will be more programs to buy and sell
automatically. Another aspect of the mergers will be the need to establish what
are called Chinese Walls within institutions to prevent the unethical use
of confidential information. For example, one part of an institution may be
giving financial advice to some company which another part of the same
institution could use to speculate - the same institution would not have been 
allowed to perform both roles under the regulations before the Big Bang.

The Chinese Wall problem is really a standard security problem with the
computing system being divided up into multiple partitions between which
information flow is not allowed. Human leakage is likely to be more of a
problem. Increasing dependence on technology has obvious reliability
implications, but I am more concerned about whether automatic trading
is likely to have a destabilising influence. Modern telecommunication has made 
it possible to have a 24 hour world currency futures market in which vast sums 
(1 billion/day) are traded rapidly for minute gains. This is pure speculation, 
creating money out of nothing with no connection to the outside world, (unlike 
other futures markets which at least have some basis in reality providing a 
guaranteed market for some commodity). I feel that programs will be able to 
react too quickly for the wrong reasons with possibly disastrous consequences.
Equally, they could create a false sense of security and an artificially
inflated market by buying instead of selling.

Although some of these concerns are political rather than technical, and I
am in no sense a financial expert, I would appreciate a discussion of these 
issues and some information about the heuristics and safeguards built into 
these automatic trading programs.

Robert Stroud, Computing Laboratory, University of Newcastle upon Tyne.

ARPA robert%cheviot.newcastle@ucl-cs.ARPA
UUCP ...!ukc!cheviot!robert
JANET robert@newcastle.cheviot

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.30.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.32.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-18</DOCNO>
<DOCOLDNO>IA012-000123-B021-204</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.32.html 128.240.150.127 19970217003956 text/html 12333
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:38:26 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 32</TITLE>
<LINK REL="Prev" HREF="/Risks/3.31.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.33.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.31.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.33.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 32</H1>
<H2> Wednesday, 6 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
DC-10 Crash 
</A>
<DD>
<A HREF="#subj1.1">
Chuck Weinstock
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Earthquake Reporting 
</A>
<DD>
<A HREF="#subj2.1">
AP
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  The Recent Near-Disaster for the Shuttle Columbia 
</A>
<DD>
<A HREF="#subj3.1">
Peter G. Neumann
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Traffic lights in Austin 
</A>
<DD>
<A HREF="#subj4.1">
Alan Wexelblat
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Re: Laserprinter dangers 
</A>
<DD>
<A HREF="#subj5.1">
Graeme Hirst
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
DC-10 Crash
</A>
</H3>
<address>
&lt;<A HREF="mailto:Chuck.Weinstock@sei.cmu.edu">
Chuck.Weinstock@sei.cmu.edu
</A>&gt;
</address>
<i>
6 Aug 1986 09:19-EDT
</i><PRE>

I have also heard the stories about pilots following the procedures in the
manual not being able to save the aircraft.  In the case of the American
Airlines DC-10 accident, the pilot executed the correct maneuver for loss of
engine power, but the effects of the missing engine caused it to go into a
stall.  However, the correction for the stall is 180 degrees different from
the correction for the loss of engine power, and thus the plane was lost.
The pilot possibly could have saved the aircraft had he known what was going
on.  The reason the pilot didn't correct for the stall is that he didn't
know about it (or knew too late) -- because the missing engine supplied
power to the stall warning device.

Interestingly, at the time stories were circulating that some airlines
(e.g., United) had ordered their DC-10's with dual-redundant stall-warning
devices, powered off of multiple engines.

(I'm afraid I don't have a reference.  Probably Aviation Week and Space
Technology.)

Chuck

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Earthquake Reporting
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Wed 6 Aug 86 11:55:55-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

From the AP, Tuesday, 5 August 1986, Los Angeles:

  Three of five earthquakes that state agencies said rattled California on
  Sunday never happened, officials acknowledged yesterday.  The false reports
  by California's Office of Emergency Services and Department of Water
  Resources were blamed on static in the microwave system that transmits data
  from monitoring devices around the state to Sacramento.  Don Irwin, deputy
  director of Emergency Services, said his agency was trying to decide whether
  to change procedures and stop publicizing what he termed ``preliminary,
  unofficial information''.

  U.S. Geological Survey seismologists said yesterday that three small quakes
  shook the state on Sunday, two near San Jose and a third in the eastern
  Sierra Nevada.  No damage or injuries were reported.  The state agencies
  never reported one of the San Jose-area quakes, and reported three others
  that did not happen.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
The Recent Near-Disaster for the Shuttle Columbia
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Wed 6 Aug 86 13:22:33-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

From the San Francisco Chronicle, Wednesday, 6 August 1986:

      WASHINGTON - The space shuttle Columbia (the launch preceding the
  Challenger disaster) came within 31 seconds of being launched without
  enough fuel to reach its planned orbit on January 6 after weary Kennedy
  Space Center workers mistakenly drained 18,000 gallons of liquid oxygen
  from the craft, according to documents released yesterday by the White
  House panel that probed the shuttle program.  Although [NASA] said at 
  the time that computer problems were responsible for the scrubbed 
  launch, Representative Bill Nelson, D-Fla., who flew on the mission, said
  yesterday that he was informed of the fuel loss while aboard the
  spacecraft that day...

    According to the appendix [to the panel report], Columbia's brush with
  disaster ... occurred when Lockheed Space Operations Co. workers 
  "inadvertently" drained super-cold oxygen from the shuttle's external tank
  5 minutes before the scheduled launch.  The workers misread computer
  evidence of a failed valve and allowed a fuel line to remain open.  The
  leak was detected when the cold oxygen caused a temperature gauge to drop
  below approved levels, but not until 31 seconds before the launch was the
  liftoff scrubbed.

    NASA said then that the liftoff was scrubbed [until January 12] because
  computer problems delayed the closing of a valve.  Space agency
  spokeswoman Shirley Green said yesterday that the fuel loss did not become
  apparent until much later.

The NY Times (same day) noted that the potentially catastrophic launch of the
Columbia without adequate fuel to reach its intended orbit could be blamed
on human error caused by fatigue.  "Investigators also concluded that many
key people working for NASA and its contractors work an excessive amount of 
overtime that has the potential for causing catastrophic errors in judgment."

The Chronicle article goes on to state, quoting the panel report, that
fatigue may also have contributed "significantly" to the disputed decision
by NASA and Thiokol officials to launch the Challenger in cold weather --
despite strong evidence that the O-ring booster seals were ineffective.  The
panel said "certain key managers obtained only minimal sleep the night
before the teleconference" in which the fatal decision was made.
Furthermore, a study of 2900 workers' timecards in the weeks before that
showed an "unusally high amount of overtime", during which time there were
five aborted launches and two actual launches.

I am astounded to look back over my list of computer-related disasters (an
update will appear in RISKS at the beginning of Volume 4 -- it is now up to
5 pages) and find only one other space/missile/defense/aviation case that
could easily have been linked to fatigue.  That case was the KAL 007, whose
real cause is still a matter of much speculation.  (See ACM Software
Engineering Notes 9 1 and 10 3.)  One would expect that to be a more common
cause...

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Traffic lights in Austin
</A>
</H3>
<address>
Alan Wexelblat 
&lt;<A HREF="mailto:wex@mcc.com">
wex@mcc.com
</A>&gt;
</address>
<i>
Wed, 6 Aug 86 14:01:12 CDT
</i><PRE>

Yesterday, Austin experienced a sudden thunderstorm and some small power
failures.  One of the things knocked out by the power loss was the central
computer that coordinates the traffic lights in the downtown area.

The central controller is backed up by isolated controllers at each
intersection.  By my guesstimate, there are about 125 of these
intersections.  Two of the site controllers failed to operate, causing the
light at those two intersections to go out.

Is this a success or a failure for the system as a whole?  Of course we'd
like it if the backup was 100%, but is 2% an acceptable failure rate?

(Side note: the only adverse effect of the two failures was that humans
-- policemen - were required to stand in the downpour and direct traffic.)

Alan Wexelblat
UUCP: {ihnp4, seismo, harvard, gatech, pyramid}!ut-sally!im4u!milano!wex

    [Success -- like failure -- is relative.  Even the greatest successes
     can be disasters if we become overconfident.  Even the worst disasters
     can have some benefits if we learn from them.  In this case, the 
     result was clearly a qualified success, but would have been quite
     different if someone had been killed when the lights went out at one
     intersection.  PGN]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Re: Laserprinter dangers
</A>
</H3>
<address>
Graeme Hirst 
&lt;<A HREF="mailto:gh%ai.toronto.edu@CSNET-RELAY.ARPA">
gh%ai.toronto.edu@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Tue, 5 Aug 86 15:27:52 edt
</i><PRE>

&gt; Increasingly, large and "official" organisations [...] are using laser 
&gt; printers to print the bills and other requests for money [...]

I cannot believe this will be a serious problem.  In fact, most organizations
are still using pre-printed stock, even if they use the laser printer to
do smarter things on it.  For example, my Ontario motor vehicle registration
is laser-printed on banknote-style paper.  My credit card bills and bank
statements are laser-printed on pre-printed paper that is virtually identical
in design to the paper used when they were impact-printed.  (This also has
programming advantages.)

Similarly, a new ATM at my bank prints its receipts on a role of paper like
that of a cash register, instead of the pre-printed cards used by older
models.  But the paper used has the bank's logo printed on the back to
prevent easy forgery.

The one exception I can think of is my city tax and water bills, which have
(on plain colored paper) the most ornate laser-printing imaginable -- which
required some amazing hacking on the Xerox 9700.  Duplicating this would be of
the same level of complexity as forging pre-printed stock -- which was
always possible even in the days of hand-writing and typewriters.

\\\\   Graeme Hirst    University of Toronto	Computer Science Department
////   utcsri!utai!gh  /  gh@ai.toronto.edu   /  416-978-8747

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.31.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.33.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-19</DOCNO>
<DOCOLDNO>IA012-000123-B021-223</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.33.html 128.240.150.127 19970217004008 text/html 11193
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:38:37 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 33</TITLE>
<LINK REL="Prev" HREF="/Risks/3.32.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.34.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.32.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.34.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 33</H1>
<H2> Thursday, 7 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Air traffic computer failure 
</A>
<DD>
<A HREF="#subj1.1">
Hal Perkins
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Re: Laserprinter dangers 
</A>
<DD>
<A HREF="#subj2.1">
Sean Malloy
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: Expert system to catch spies 
</A>
<DD>
<A HREF="#subj3.1">
Rich Kulawiec
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Survey of Computer Professionals 
</A>
<DD>
<A HREF="#subj4.1">
Kurt Hyde
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Air traffic computer failure
</A>
</H3>
<address>
Hal Perkins
&lt;<A HREF="mailto:hal@gvax.cs.cornell.edu ">
hal@gvax.cs.cornell.edu 
</A>&gt;
</address>
<i>
Fri, 8 Aug 86 00:14:04 EDT
</i><PRE>

From the New York Times, Thursday, August 7, 1986, p. A10.

Computer Failure Snarls Chicago Air Traffic

  WASHINGTON, Aug. 6 (UPI) -- The main computer used by air traffic
controllers at Chicago Center, the Federal Aviation Administration's
busiest facility, failed Tuesday, delaying hundreds of flights, an
agency spokesman said today.

  The failure, which lasted two hours, during which a backup computer
operated, caused no safety-related incidents, a spokesman, Robert
Buckhorn, said.

  The incident at 2 P.M. was caused by the failure of a computer
element that feeds the computer radar information and other data
critical to tracking and directing flights in the crowded Chicago
airspace, agency sources familiar with the breakdown said.

  In Chicago, agency sources said some of the main computer's functions
were restored Tuesday afternoon.  Mr. Buckhorn said the problem was
completely corrected at about 6 A.M. today.

[Anybody know further details about this?  HP]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Re: Laserprinter dangers
</A>
</H3>
<address>
Sean Malloy
&lt;<A HREF="mailto:malloy@nprdc.arpa ">
malloy@nprdc.arpa 
</A>&gt;
</address>
<i>
Thu, 7 Aug 86 06:55:10 pdt
</i><PRE>
To: RISKS@CSL.SRI.COM
Cc: gh%ai.toronto.edu@CSNET-RELAY.ARPA

 &gt;From: Graeme Hirst &lt;gh%ai.toronto.edu@CSNET-RELAY.ARPA&gt;
 &gt;Subject: Re: Laserprinter dangers
 &gt;
 &gt;The one exception I can think of is my city tax and water bills, which have
 &gt;(on plain colored paper) the most ornate laser-printing imaginable -- which
 &gt;required some amazing hacking on the Xerox 9700.  Duplicating this would be 
 &gt;of the same level of complexity as forging pre-printed stock ...

This is less of a problem than you might imagine -- Any good laser printer
has a page control language, such as PostScript on the Imagen laser printer
at my office, that can output bitmap images. And with the availability of
graphic input devices like digitizing cameras and image scanners, the
problem of entering ornate output formats is due more to the price of the
input devices than the actual input itself.

And even if you have to put the paper through twice, once for the fixed
ornate work, and once for the text of the bill itself, the result is going
to look like the real thing. And with some of the page layout packages like
InterLeaf, the whole output can be laid up for each page on a single pass,
at the expense of speed of output (InterLeaf eats an amazing amount of CPU
time).

Simply having a complex output format isn't enough to prevent forgery --
all that will happen is that the forgers will have to resort to the
same technology that created the image in the first place.

	Sean Malloy, 	Naval Personnel R&amp;D Center, 	malloy@nprdc

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: Expert system to catch spies
</A>
</H3>
<address>
Whitewater Wombat 
&lt;<A HREF="mailto:rsk@purdue-asc.ARPA">
rsk@purdue-asc.ARPA
</A>&gt;
</address>
<i>
Thu, 7 Aug 86 22:57:24 est
</i><PRE>

Mr. Rosa's recommendation that expert systems be used in order to identify
potential spies certainly has some chilling Orwellian overtones, and also
highlights certain misconceptions about expert systems.

The cross-correlation of credit histories, bank records, major purchase
receipts, customs logs, and so on, is certainly a monumental task, given the
size of the databases involved if such a program were applied on a national
scale; but this sort of problem seems to me to be within the reach of
ordinary database query systems.  In my opinion, a program which performs
such searching operations is not an expert system, but a (smart) database
manager.  Calling it an expert system does not make it one.

Chris McDonald points out another important problem; "suitability", in terms
of whatever criteria are employed, does not necessarily imply guilt.  For
example, if I were to design the criteria, I might direct the program to
search for frequent overseas travellers with multiple bank accounts and
expensive automobiles.  Of course, the resultant list of "suspects" would be
huge, and would probably contain a great number of prominent business
executives.  Certainly, this is a facetious example, but extending and
refining the criteria will only partially reduce the list.  Given the
initial (huge) size of the search space, I wonder whether the reductions
would ever be sufficient to reduce it to a humanly-manageable size.  I
speculate that a case-by-case examination of the list would simply not be
feasible.

Finally, the public at large (apparently including Mr. Rosa) does not
seem to understand that expert systems are built to embody the
knowledge of human experts.  (Perhaps this will eventually change;
but I am as yet unaware of any self-taught expert system.)  System
architects spend a great deal of time querying human experts to find
out how they reason about the problem space, and then attempt to
construct a system that (loosely) mimics that process.  To a large
extent, the efficacy of an expert system depends upon the expertise
of those whose collective experiences were tapped to build it.  If a
spy-catching expert system is to be reasonably successful, then at
least one human expert must be found...but is there one?  Is there at
least one person whose acumen is comparable with, say, the medical
diagnostic skills of the physicians involved in the Mycin project?

My intuition says that there is not.  (But I'll hedge my bets by
observing that if the U.S. government actually had such a person in
their employ, they'd be unlikely to publicize that fact.)  It seems
to me that Mr. Rosa is invoking the modern magic buzzword "expert
system" as if he expects a team of software engineers to solve
national security problems for him.  Given the limited (impressive,
but limited) success that expert systems have enjoyed in such highly
restricted problem domains as mineralogical prospecting and computer
system configuration, I doubt that they'd be much help in such a
wide-open area as espionage.

Rich Kulawiec, pucc-j!rsk, rsk@j.cc.purdue.edu, rsk@purdue-asc.arpa

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Survey of Computer Professionals    [REPLY TO KURT, NOT RISKS]
</A>
</H3>
<address>
Kurt Hyde DTN 264-7759 MKO1-2/E02
&lt;<A HREF="mailto:hyde%vax4.DEC@decwrl.DEC.COM  ">
hyde%vax4.DEC@decwrl.DEC.COM  
</A>&gt;
</address>
<i>
Thursday,  7 Aug 1986 07:32:41-PDT
</i><PRE>

Survey of Computer Professionals Regarding Computerized Voting

Please return to TOPCAT::HYDE on Digital's Engineering Net by
Tuseday, August 12th. 

1) Would you trust a computerized voting system if did not allow you 
   to monitor how it worked nor did it allow you to inspect the ballot 
   it cast for you?

   YES, I would trust it         NO, I not would trust it         

2) Would you trust a computerized voting system if did allow you to
   monitor how it worked, but did not allow you to inspect the ballot 
   it cast for you?

   YES, I would trust it         NO, I not would trust it         

3) Would you trust a computerized voting system if did not allow you 
   to monitor how it worked, but it did allow you to inspect the ballot 
   it cast for you?

   YES, I would trust it         NO, I not would trust it         

4) Would you trust a computerized voting system if it allowed you to
   monitor how it worked and allowed you to inspect the ballot it 
   cast for you?

   YES, I would trust it         NO, I not would trust it         

         [Presumably Kurt will share the results with us.  A
          sequence of four answers (YES or NO) will suffice.  PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.32.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.34.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-20</DOCNO>
<DOCOLDNO>IA012-000123-B021-248</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.34.html 128.240.150.127 19970217004028 text/html 17934
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:38:57 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 34</TITLE>
<LINK REL="Prev" HREF="/Risks/3.33.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.35.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.33.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.35.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 34</H1>
<H2> Saturday, 9 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Non-Flying Airplanes and Flying Glass 
</A>
<DD>
<A HREF="#subj1.1">
Jim Horning
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Failure Recovery, Simulations, and Reality 
</A>
<DD>
<A HREF="#subj2.1">
Danny Cohen
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Ottawa Power Failure 
</A>
<DD>
<A HREF="#subj3.1">
Dan Craigen
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Liability for Software Problems 
</A>
<DD>
<A HREF="#subj4.1">
Peter G. Neumann
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Ozone hole 
</A>
<DD>
<A HREF="#subj5.1">
Hal Perkins
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Re: Survey of Trust in Election Computers 
</A>
<DD>
<A HREF="#subj6.1">
Chris Hibbert
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Nondelivery of <A HREF="/Risks/2.38.html">RISKS-2.38</A> (8 April 1986) and other mail     
</A>
<DD>
<A HREF="#subj7.1">
Communications Satellite [and PGN]
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Non-Flying Airplanes and Flying Glass
</A>
</H3>
<address>
Jim Horning
&lt;<A HREF="mailto:horning@src.DEC.COM ">
horning@src.DEC.COM 
</A>&gt;
</address>
<i>
Fri, 8 Aug 86 14:45:04 pdt
</i><PRE>

A number of people sent me information about the myth that the design flaw
in the Electras wasn't caught because of an undetected overflow.  (The most
detailed information came from someone who wishes to remain anonymous.)
Putting it all together, I am now convinced that the problem was not
undetected overflow.  Rather, it was a failure to simulate a dynamic effect
(gyroscopic coupling) that had never been significant in piston-engined
planes. So another myth bites the dust.  But the true story should remind us
that simulations are only as good as the assumptions on which they are based.

I solicit similar clarification of the story of the (then) new John Hancock
Building in Boston (the one that resonated and shed many of its exterior
glass panes when the wind came from a certain direction).  I know that there
was litigation about who was responsible for the additional costs: replacing
the glass; installing a huge lead deadweight mounted on shock absorbers in
an upper story to damp the oscillation; etc.  I don't recall the final
outcome. I do remember reading that there was a very narrow range of wind
directions that would excite the resonance, and that the simulations of the
design had unluckily missed that range.  Maybe some readers of Risks know the
details? Has there been a book or magazine article that explored the
computer angle (if indeed there is one)?

Jim H.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Failure Recovery, Simulations, and Reality
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
8 Aug 1986 18:38:58 PDT
</i><PRE>
From: COHEN@B.ISI.EDU
To: RISKS FORUM (Peter G. Neumann -- Coordinator) &lt;RISKS@CSL.SRI.COM&gt;

In <A HREF="/Risks/3.27.html">RISKS-3.27</A> Stephen Little, Computing &amp; Information Studies, of
Griffith Uni, Qld, Australia. reported that:

     I have been told of one major accident in which the pilot followed 
     the drill for a specific failure, as practiced on the simulator, 
     only to crash because a critical common-mode feature of the system
     was neither understood, or incorporated in the simulation.

Being a pilot I find this report most important and interesting.

I am sure that the readers of RISKS would be better served by having
evidence to support such reports.  Major (and responsible) newspapers
have a verification procedures.  Since RISKS cannot afford this I'd be
delighted to help this process.

The best way to verify such a report is by a reference to the official
accident investigation report.  I'd be delighted to pursue this
reference myself if anyone can give me details like the date
(approximately), place (country, for example), or the make and type of
the aircraft.

This is a plea to provide me with this information.

							Danny Cohen.

          [This is a very nice offer, and I hope someone can 
           provide enough details to take you up on it!  PGN]

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Ottawa Power Failure
</A>
</H3>
<address>
Dan Craigen  
&lt;<A HREF="mailto:CMP.CRAIGEN@R20.UTEXAS.EDU">
CMP.CRAIGEN@R20.UTEXAS.EDU
</A>&gt;
</address>
<i>
Sat 9 Aug 86 14:47:36-CDT
</i><PRE>
To: risks@CSL.SRI.COM

    A brief fire at Ottawa Hydro's Slater Street station on the morning of
August 7th resulted in a loss of power to a substantial section of the
downtown core.  Even after 48 hours of effort, sections of the city were
still without power.

[From the Ottawa Citizen (Friday, 8 August 1986)]

     Top officials from Ontario and Ottawa Hydro today [Friday] are
  re-examining long accepted system reliability standards...
     Ottawa Hydro engineering manager Gordon Donaldson said ``the system is
  built to be 99.99 per cent reliable ... now we will be looking at going to
  another standard of reliability -- 99.999 per cent.''
     He also said that the cost would be huge -- many times the $10 million
  cost of the Slater Street station -- and hydro customers may not be prepared
  to accept the cost. ...
     The Slater station is the biggest and was considered the most reliable of
  the 12 across the city. It has three units, each of which is capable of
  carrying the whole system in an emergency.
     But ... all three were knocked out. ...
     The culprit, an Ontario Hydro board [called a ``soupy board''] which
  monitors the equipment at the substation, didn't even have anything directly
  to do with providing power to the thousands of people who work and live in
  the area.
     ... its job is to make the system safer, cheaper and more reliable....
     The board is considered so reliable that it doesn't have its own backup
  equipment. [!]

The economic costs of the power failure are expected to be in the millions of
dollars. It is unlikely that the Ottawa birthrate will increase. As columnist
Charles Lynch noted: ``The Ottawa power failure took place during the
breakfast hour, not normally a time when Ottawans are being polite to one
another, let alone intimate.''

We, at I.P. Sharp (Ottawa), lost both our VAXs; I have been unable to get onto
Tymnet for the past two days; ATMs as far as a 100 miles distant from
Ottawa were knocked out of commission -- the central computer that controls
them is in the area of outage; Many traffic signals are still out; and
a number of businesses still shut.
                                               Dan Craigen

      [Add this to the growing collection of problems in which a redundant
      system failed because of a weakest link in the redundancy itself!  PGN]

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Liability for Software Problems
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Sat 9 Aug 86 11:48:40-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

All week long I have been waiting for either someone else to submit it or
for me to have a few spare moments to enter it:  an item from the Wall
Street Journal of last Monday, 4 August 1986, "Can Software Firms Be Held
Responsible When a Program Makes a Costly Error", by Hank Gilman and William
M. Bulkeley.  A few excerpts are in order.

  Early last year, James A. Cummings Inc. used a personal computer to prepare
  a construction bid for a Miami office-building complex.  But soon after the
  bid was accepted, the Fort Lauderdale firm realized that its price didn't
  include $254,000 for general costs.  Cummings blamed the error on the
  program it had used, and last October filed suit in federal court in Miami
  against the software maker, Lotus Development Corp.  The suit, which seeks
  $254,000 in damages, contends that Lotus' "Symphony" business program didn't
  properly add the general expenses, resulting in a loss in completing the
  contract.

  Lotus, based in Cambridge, Mass., disputes that contention, araguing that
  Cummings made the error.  The case, however, has had a chilling effect on
  the software industry.  For the first time, industry officials say, a case
  ma go to court that could determine if makers of software for personal
  computers are liable for damages when the software fails.  Some software
  makers also worry that such a case, regardless of the outcome, may lead
  to other suits by disgruntled consumers.  [...]

  Software makers are particularly concerned about paying for damages
  resulting from faulty software -- rather than just replacing the software.
  Such "consequential" damages have been awarded in suits involving larger
  computers.  Other types of damages from computer disputes "come from
  saying what benefits you were supposed to get compared with what benefits
  you didn't get," says Richard Perez, an Orinda, Calif., lawyer.  Mr. Perez
  won a $2.3 million judgment against NCR Corp. for Glovatorium, Inc., a dry
  cleaner that said its computers didn't work as promised.

The article goes on to note that most PC software comes on an "as-is" basis,
which doesn't provide for correction of errors.  Under the limited
warranties, the buyer does not even "own" the program.  Illinois and
Louisiana have passed "shrink-wrap" laws which imply that when you open the
package, that is equivalent to signing a contract that lacks guarantee and
prevents copying.

In the case of Cummings, they noticed they had left out the general costs,
and added them as the top line of a column of figures.  The new entry showed
on the screen, but was not included in the total.  Keep your eyes open for
whether the blame is placed on a naive user not following his instructions,
or on the software not doing what it was supposed to (or both).

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Ozone hole
</A>
</H3>
<address>
Hal Perkins
&lt;<A HREF="mailto:hal@gvax.cs.cornell.edu ">
hal@gvax.cs.cornell.edu 
</A>&gt;
</address>
<i>
Fri, 8 Aug 86 03:17:48 EDT
</i><PRE>

In response to PGN's request for sources on the ozone hole...

The New York Time's Science Times section on July 29, 1986 had a long
story on this (it starts on page C1).  The gist of the story is that
there's a big hole in the ozone layer over the south pole, nobody knows
how it got there, nobody knows what it means, it could be a very
serious problem, and scientists are investigating the situation.

As for computers and such, here are a couple of relevant paragraphs:

"The initial report of the hole by British scientists in March 1985
caused little excitement, partly because the British team in Antarctica
was not well known among atmospheric scientists.  Also, since their
data came from ground instruments measuring the ozone in a direct line
upward, they did not show the extent of the hole.

"But later last year, scientists at the National Aeronautics and Space
Administration produced satellite data confirming the British findings
and showing how big the hole was.  NASA scientists found that the
depletion of ozone was so severe that the computer analyzing the data
had been suppressing it, having been programmed to assume that
deviations so extreme must be errors.  The scientists had to go back
and reprocess the data going back to 1979."

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
 Re: Survey of Trust in Election Computers 
</A>
</H3>
<address>
&lt;<A HREF="mailto:Hibbert.pa@Xerox.COM">
Hibbert.pa@Xerox.COM
</A>&gt;
</address>
<i>
Fri, 8 Aug 86 10:30:03 PDT
</i><PRE>
To: hyde%vax4.DEC@decwrl.DEC.COM
cc: risks@CSL.SRI.COM

I'm afraid your questions are too vague for me to give yes or no answers.
(I hope you'll give a count of non-respondents when you tell us how many
YESes and NOs you got.)  I'm not at all sure what it would mean for a voting
system to allow me to monitor how it worked.  Would it print out a trace of
its execution?  Would it let me know the running total of votes it had
collected?

What would it mean for the system to allow me to inspect the ballot it
cast for me?  Does that mean the "computerized" aspect is merely a
printer for ballots that will be counted later by hand or some other
computer?  Or does that mean that before I accept my votes it displays a
summary for me to approve, and it then adds them into its running total?

I'm not convinced I would ever trust a system that only kept running
tallys in software.  If there aren't paper ballots printed, then there
is no way to recheck the results.  In this situation, the machine that
later counts the paper ballots is much more important, and your
questions don't address this part of the process.

Chris
        [We await Kurt Hyde's results...]

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
[Nondelivery of <A HREF="/Risks/2.38.html">RISKS-2.38</A> (8 April 1986) and other mail]
</A>
</H3>
<address>
Communications Satellite 
&lt;<A HREF="mailto:COMSAT@MC.LCS.MIT.EDU">
COMSAT@MC.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Fri,  8 Aug 86 19:43:54 EDT
</i><PRE>
To: RISKS@CSL.SRI.COM

============ A copy of your message is being returned, because: ============
"HEWITT-RISKS" at MC.LCS.MIT.EDU is an unknown recipient.
============ Failed message follows: ============
Received: from MX.LCS.MIT.EDU by MC.LCS.MIT.EDU via Chaosnet; 8 AUG 86  19:42:12 EDT
Date: Tue 8 Apr 86 21:15:55-PST
From: RISKS FORUM    (Peter G. Neumann, Coordinator) &lt;RISKS@SRI-CSL.ARPA&gt;

                                             [REST OF MESSAGE TRUNCATED...]

   [For the past week or so, I have been getting sequential notices of
    undeliverable mail from "Communications Satellite" -- four months after
    the original mailings of RISKS, and just another risk of running a forum.

    There was a news item last week about an entire bag of US mail from aboard
    the Liberty Ship Caleb Strong from World War II (May 1944) that was just 
    found undelivered by an exterminator in an attic in North Carolina.
    The Postal Service is trying to find the addressees, but was quick to add
    that it did not happen on their shift! (It blamed a soldier, who has since
    died.)  Here are two related items that I just happen to have filed away.

      Herb Caen's SF Chron column of 18 December 1973 noted a 1940 calendar
      mailed in 1939 to a customer in Utah that was returned "Addressee
      Unknown" during that week in 1973.

      The Martha's Vineyard Gazette of 30 March 1973 noted a postcard mailed
      in Asbury Park NJ, postmarked 11 August 1914, addressed to West Summit
      NJ and forwarded to Edgartown, Mass.  It arrived at that post office
      on 26 March 1973.

    With sleet and snow and dark of night, now computers are doing it,
    too -- and they don't even need to find excuses.  PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.33.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.35.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-21</DOCNO>
<DOCOLDNO>IA012-000123-B021-261</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.35.html 128.240.150.127 19970217004040 text/html 14689
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:39:09 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 35</TITLE>
<LINK REL="Prev" HREF="/Risks/3.34.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.36.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.34.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.36.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 35</H1>
<H2> Monday, 11 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Flying windows on the Hancock Building 
</A>
<DD>
<A HREF="#subj1.1">
Remy Malan
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Pilots and counter-intuitive maneuvers 
</A>
<DD>
<A HREF="#subj2.1">
Martin Minow
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Mail adrift 
</A>
<DD>
<A HREF="#subj3.1">
Mike McLaughlin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Laserprinter dangers 
</A>
<DD>
<A HREF="#subj4.1">
Niall Mansfield
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  A bit of humor and even philosophy 
</A>
<DD>
<A HREF="#subj5.1">
Willis Ware
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Official Report on Chernobyl disaster 
</A>
<DD>
<A HREF="#subj6.1">
Robert Stroud
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Flying windows on the Hancock Building
</A>
</H3>
<address>
Remy Malan 
&lt;<A HREF="mailto:remym%tekig5.tek.csnet@CSNET-RELAY.ARPA">
remym%tekig5.tek.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Sun, 10 Aug 86 08:37:32 PDT
</i><PRE>
To: risks@CSL.SRI.COM

While at school in Cambridge, MA. I took a course in decision analysis.
One of the examples given in class was the case of the Hancock Building.
This is how I remember it:

A model of the Hancock Building and the surrounding structures was tested in
a wind tunnel.  The wind direction in the initial tests was incremented by
45 degree intervals.  The model behaved well for these tests.  Later, after
the problem occurred on the real structure, more testing [at a finer mesh]
revealed very narrow bands in wind direction in which resonance did occur.
The 45 degree increments were too coarse to pick out the resonant zones.

(I believe that their initial tests were done informally, as a kind of
favour, and so were not very rigourous.)

*This is all from memory, so my apologies if I didn't get it quite right.

A. Remy Malan

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Pilots and counter-intuitive maneuvers
</A>
</H3>
<address>
Martin Minow, DECtalk Engineering ML3-1/U47 223-9922
&lt;<A HREF="mailto:minow%rex.DEC@decwrl.DEC.COM  ">
minow%rex.DEC@decwrl.DEC.COM  
</A>&gt;
</address>
<i>
10-Aug-1986 0025
</i><PRE>

This is from memory, and it's late, so bear with me: 

A very recent Smithsonian (June 86?) had an article on flight simulators --
the same month as the Scientific American article. In it, the chief instructor 
for one of the airlines related that, a few months ago, he flew as the
flight engineer on a commercial flight.  The plane encountered a wind-shear
situation on take off. The instructor, from his flight engineer's position,
reminded the pilot that the correct recovery for wind-shear is opposite to
the correct recovery for a stall (which has a similar appearance to the pilot).

Hope this reassures your pilot subscribers. By the way, accident investigation 
reports are usually summarized in Aviation Week and Space Technology.

Martin Minow
minow%rex.dec@decwrl.dec.com

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
mail adrift
</A>
</H3>
<address>
Mike McLaughlin
&lt;<A HREF="mailto:mikemcl@nrl-csr ">
mikemcl@nrl-csr 
</A>&gt;
</address>
<i>
Sun, 10 Aug 86 11:24:12 edt
</i><PRE>

Personal item, no documentation known:  I once purchased a used USPS station
wagon at GSA auction for $350.  While cleaning it out, my wife and I found
well over a hundred pieces of undelivered mail.  We trashed all but the
first class - and dropped 30 or 40 pieces into the nearest mail box.  Some
were over five (5) years old.  We watched the paper for days, but saw no
items about late mail.

Only relevance to RISKS is that people will _always_ be imperfect.

	- Mike
                      [And how often do we assume that a system will work
                       properly in the face of that statement?!  PGN]

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
       Laserprinter dangers
</A>
</H3>
<address>
            Niall Mansfield  
&lt;<A HREF="mailto:MANSFIEL%DHDEMBL5.BITNET@WISCVM.ARPA">
MANSFIEL%DHDEMBL5.BITNET@WISCVM.ARPA
</A>&gt;
</address>
<i>
Mon 11 Aug 86 18:29:51 N
</i><PRE>

 &gt;From: Graeme Hirst &lt;gh%ai.toronto.edu@CSNET-RELAY.ARPA&gt;
 &gt;Subject: Re: Laserprinter dangers

Sean Malloy dealt with the ease of forging with laser printers.  A more
general point is that forging ANY computer-produced item, be it a hard-copy
output or a message on a wire, is easier than forging old-style pieces of
paper, etc., because:-

1. The machinery involved is cheap - bytes on a wire which have come from a
cheapo toy computer just look just like expensive DEC or IBM bytes. (Coiners
need expensive metal presses)

2. You can realistically attain a 100% perfect forgery - my bogus bytes look
just the same as real ones.  (Coiners presumably have difficulty making the
right alloys, but worse, have to copy the shapes on the coin - how do they
know when their product is "good enough"?)

3. The skills required are, more or less, the same for producing ordinary
software as for producing forgeries - software is software, whether legal or
otherwise.  (It is also true that an engraver uses his same skills whether
he is forging banknotes or producing a bookplate; the big difference
however, is in the widespread distribution of skills needed for forging -
there are very few qualified engravers, but lots of "qualified"
programmers).

In summary, a lot of people are finding themselves in a position they were
never in before - not only have they all the skills and equipment necessary
for a particular type of crime, but increasingly they are being presented
with opportunities to commit those same crimes.  Ergo ...

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
A bit of humor and even philosophy
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Mon, 11 Aug 86 16:07:38 PDT
From: willis@rand-unix.ARPA

In the Washington Post, July 30 1986, pg A-23, columnist James J.
Kilpatrick discusses the nomination and confirmation of Daniel Manion as
appellate judge.  He laments at length the lack of support for the
individual, notes that a keen sense of justice is not all that important for
appellate judges anyway if they have a good knowledge of the structure of
law which is what they really rule on.  He goes on to note that the analysis
of pertinent law and the detailed writing will likely be turned over to law
clerks anyway.

The last paragraph of the article is the clincher and source of humor.

     "In sum, I fear not for the republic, or for the 7th Circuit, when
     Manion joins the club.  Give him an intelligent clerk and a good word
     processor, and the gentleman may look forward to many happy years on
     the bench."

Do you suppose it could be called an application of AI, when software
offsets presumed deficiencies of appointed officials?

Are things such as this off-the-cuff suggestion an early step of having
software front for the performance and/or the beavhior of public
officials?  And with what unseen, possibly unknowable, risks?

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Official Report on Chernobyl disaster
</A>
</H3>
<address>
Robert Stroud 
&lt;<A HREF="mailto:robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Mon, 11 Aug 86 15:01:08 bst
</i><PRE>

The following article appeared in yesterday's Observer, and is reproduced
here without permission:

Robert Stroud,
Computing Laboratory,
University of Newcastle upon Tyne.

ARPA robert%cheviot.newcastle@ucl-cs.ARPA
UUCP ...!cheviot!robert
          =======================================================
"Chernobyl report blames turbines" p.6 Observer, Sunday 10 August 1986

by Robin McKie and Laura Veltman

(c) Observer Newspapers

Soviet operators who experimented with turbines and alternators
at the Chernobyl plant are to be blamed for the nuclear disaster there.

Western experts who have recently visited Chernobyl say that the full
Soviet accident report which is expected to be published this week,
will blame 'human error' and 'misuse' of turbines for the chain of
events that led to the disaster in April.

But many believe the explanation is inadequate and that it is being 
promoted mainly to protect the country's nuclear construction programme.

'The theory moves the source of the accident from the reactor itself
to the turbines which are housed separately,' said Mr Peter Potter, a
British nuclear expert who has seen many Soviet reactors.

'By maintaining that human error and turbine problems were really to
blame, the Russians could say that their reactors have no serious design
flaws. They could then avoid calls for closures of other reactors or for
the implementation of drastic redesign work.'

The Soviet theory argues that the Chernobyl accident was caused by a
total loss of electricity supply to the pumps which circulate cooling
water through the heated reactor core. One Western scientist, Professor
Leslie Kemeny, of the University of New South Wales' nuclear engineering
group, does believe that an accident with the electricity-generating
turbines - which are worked by steam heated in the reactor - triggered
the disaster.

Prof Kemeny, who took detailed samples of air, water and soil 
contamination during a recent visit to the Chernobyl area, said:
'The loss of electricity to the pumps was due to human error. During
the night of 25 April, the turbo-alternator linked to Reactor 4 at
Chernobyl was undergoing a "run-down" experiment. In effect, this meant
that engineers were studying the behaviour of the turbines while they
were being run down. Throughout the hour of the experiment, alternative
energy sources should have supplied replacement power for the pumps.
But this did not function, and the reactor was left uncooled.'

Normally, the reactor's own electricity should have been used to run
the cooling pumps. During a run-down, an alternative source should
have been switched on automatically. It was this which failed at 
Chernobyl. Without cooling water, the reactor's temperature was sent
soaring - with dire effects on its uranium fuel, zirconium cladding
and graphite core.

First the remaining water inside the reactor heated up, forming steam
which began to react with the zirconium to produce hydrogen. The pressure
of the steam and the hydrogen eventually cracked the reactor core's
outer tube. Finally, when air mixed with the hydrogen, it exploded and
set fire to the graphite in the core. The result was an inferno which 
sent radioactive debris puring over much of Europe.

Despite his support for the accident theory, Prof Kemeny criticised
the Russians for failing to build pressure domes over the reactor core.
'I stand by my belief that the Chernobyl reactor was safety-deficient,'
he said. 'American, German, French and British reactors have pressure
vessels and strongly reinforced concrete structures to contain such
radiation releases.'

But other nuclear experts cast doubt on the turbine theory. 'I don't
think it is the whole story,' Mr Potter said. 'The explanation begs
some questions. Why didn't the alternative back-up power supples
switch on automatically, and what caused the power surge which the
Russians say occurred at the time of the accident? I think there was
another factor - concerned with the reactor itself - which was involved
but which the Russians do not want highlighted for political reasons.
They would find it very inconvenient if it was shown that there were
serious generic design faults in all their RBMK reactors, the ones like
the Chernobyl reactor. They are not going to let that idea spread'

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.34.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.36.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-22</DOCNO>
<DOCOLDNO>IA012-000123-B021-282</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.36.html 128.240.150.127 19970217004053 text/html 15473
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:39:22 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 36</TITLE>
<LINK REL="Prev" HREF="/Risks/3.35.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.37.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.35.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.37.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 36</H1>
<H2> Tuesday, 12 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Another Medical Risk? 
</A>
<DD>
<A HREF="#subj1.1">
Lee Breisacher
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  RISKy Business in Surgery 
</A>
<DD>
<A HREF="#subj2.1">
Mark Jackson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Reliance on word-processors discussed in the Israeli Supreme 
</A>
<DD>
<A HREF="#subj3.1">
Ady Wiernik
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Expert Systems - The New Cop on the Beat 
</A>
<DD>
<A HREF="#subj4.1">
Laws via Fred Ostapik
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Chernobyl 
</A>
<DD>
<A HREF="#subj5.1">
Art Evans
</A><br>
<A HREF="#subj5.2">
 Dick Karpinski
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Air Traffic Control computer failure 
</A>
<DD>
<A HREF="#subj6.1">
Dan Melson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Possible failures of BMD software 
</A>
<DD>
<A HREF="#subj7.1">
Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  A note about stories "from memory" 
</A>
<DD>
<A HREF="#subj8.1">
Henry Mensch
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Another Medical Risk?
</A>
</H3>
<ADDRESS>&lt;<A HREF="mailto:Breisacher.OsbuSouth@Xerox.COM">Breisacher.OsbuSouth@Xerox.COM</A>&gt;</ADDRESS>
<i>
12 Aug 86 09:25:49 PDT (Tuesday)
</i><PRE>
To: RISKS@CSL.SRI.COM

From the August PSA Airline magazine, extracted from an article about
inventors:

[There's a photo of Dr. Kwoh in surgical garb in an operating room leaning
over a dummy patient with some elaborate equipment surrounding its head.
The caption reads:]

Robotic surgery is a reality because of the obsessive work of Yik San
Kwoh, medical research and development director of Long Beach Memorial
Hospital.  His computer controlled "surgeon," capable of conducting
brain surgery within an accuracy of 1/2000 of an inch, was the result of
three years of incessant programming.

[From the text of the article:]

Yik San Kwoh, medical research and development director of Long Beach
Memorial Hospital, explains, "I've got two Apple computers at home and
three IBMs.  I spend so much time on those damn things that I get sick
of it.  Only then can I stop."

It took three years of programming and reprogramming for Kwoh to turn
and industrial robot into a surgical instrument capable of conducting
brain surgery.

[As usual, we must weigh the risks of using such equipment against the
risks of NOT using it.  On the other hand, the description makes it
sound like he programmed this thing the way I wrote my first couple
programs (FORTRAN in the early 70's) -- dive in and start writing code
then keep debugging til it sorta works.]

Lee

---------------------------------

Date: 12 Aug 86 07:56:26 EDT (Tuesday)
From: MJackson.Wbst@Xerox.COM
Subject: RISKy Business in Surgery
To: RISKS@CSL.SRI.COM

From /Programmers at Work (1st Series):  Interviews/, by Susan Lammers
(Microsoft Press, 1986):

"My most amazing experience, though, was a phone call I got right after
I started Iris, from a surgeon who was using Symphony for real-time data
analysis during open heart surgery.  It is sobering to think that
someone was lying on an operating table potentially relying upon my
program running properly.  It reminds one of the real responsibility to
the end users."

	-- Ray Ozzie
	   project leader for Symphony

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Reliance on word-processors discussed in the Israeli Supreme Court
</A>
</H3>
<address>
  Ady Wiernik   
&lt;<A HREF="mailto:ady%taurus.BITNET@WISCVM.ARPA">
ady%taurus.BITNET@WISCVM.ARPA
</A>&gt;
</address>
<i>
Tue, 12 Aug 86 21:19:31 -0300
</i><PRE>

     Rules of Court in Israel fix a time limit for bringing an appeal to the
Supreme Court against a decision of an inferior Court.

     A lawyer applied to Supreme Court for an extension of the period to
appeal. He has missed the statutory period by two days.  His excuse was that
the word-processor in his office (that has been recently installed)
malfunctioned.

     The text of the appeal that was typed into the computer has been erased
because of that computer malfunction.  He called the maintenance personnel.
They promised that the malfunction would be shortly repaired, but actually,
it lasted longer, causing him not to be able to bring the appeal at the same
day.

     The appellant claimed that the trouble with the computer was an "act
of god", Force Majeure, which is considered a special ground that entitles
him the desired extension.

     The court has rejected this argument.

     In his judgement, Registra Tzur of the Supreme Court said:  "Indeed,
the computer is very useful, but one must prepare for possible malfunctions
in its operation.  When there is no computer, the good old typewriter should
replace it."

     This decision is the first recorded judicial reference to the use of
word-processing devices in lawyer offices, and displays the dangerous
results of reliance on high-tech.

                                        Ady Wiernik

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Expert Systems - The New Cop on the Beat
</A>
</H3>
<address>
&lt;<A HREF="mailto:Laws@SRI-STRIPE.ARPA  [courtesy of Fred Ostapik]">
Laws@SRI-STRIPE.ARPA  [courtesy of Fred Ostapik]
</A>&gt;
</address>
<i>
Mon 4 Aug 86 22:38:23-PDT
</i><PRE>

The FBI has developed Big Floyd, an expert system to assist in criminal
investigations.  Similar programs are being developed to catch drug
smugglers and target potential terrorists.  The EPA wants to identify
polluters; the Treasury Department is looking for money-laundering
banks; the Energy Department would like to find contractors who cut
corners; the Customs service is after drug smugglers; the IRS is
developing a system to spot tax cheaters; the Secret Service is working
on a classified system to point out potential presidential assassins;
and the FBI's National Center for the Analysis of Violent Crimes is
developing expert systems to identify potential serial killers,
arsonists, and rapists.  Systems to target counterfeiters and bombers
are also being built.  -- Michael Schrage, The Washington Post National
Weekly Edition, Vol. 3, No. 40, August 4, 1986, p. 6.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Chernobyl
</A>
</H3>
<address>
"Art Evans" 
&lt;<A HREF="mailto:Evans@TL-20B.ARPA">
Evans@TL-20B.ARPA
</A>&gt;
</address>
<i>
Tue 12 Aug 86 11:34:21-EDT
</i><PRE>
To: Risks@CSL.SRI.COM

In <A HREF="/Risks/3.35.html">RISKS-3.35</A>, Robert Stroud comments on "Official Report on Chernobyl
disaster".  Although the discussion of what actually triggered that
disaster is interesting, I choose to focus instead on how the Russian
explanation was interpreted by others (not by Mr Stroud).

Quoting from the post:
    But many believe the explanation [offered by the Russians] is
    inadequate and that it is being  promoted mainly to protect the
    country's nuclear construction programme.
No justification is given for this belief.  A Peter Potter is quoted as saying
    By maintaining that human error and turbine problems were really to
    blame, the Russians could say that their reactors have no serious
    design flaws. They could then avoid calls for closures of other
    reactors or for the implementation of drastic redesign work.
This claim may in fact be true, but we are given no evidence.

Note what is happening: The Russians offer a technical explanation for
the disaster.  A western nuclear expert says the explanation is
inaccurate and was offered for political reasons.  But, no reason other
than political is given for this skepticism.  The Russians may well be
lying, and if there is evidence I would like to see it.  Lacking such
evidence, though, the public would be better served by less misleading
pronouncements by "experts".

Art Evans

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Chernobyl
</A>
</H3>
<address>
Dick Karpinski
&lt;<A HREF="mailto:dick@cca.ucsf.edu ">
dick@cca.ucsf.edu 
</A>&gt;
</address>
<i>
Tue, 12 Aug 86 11:13:17 PDT
</i><PRE>

The only unadvertised design deficiency that I know of in the Chernobyl
reactor is that it has a positive coeficient of reactivity with respect
to temperature.  That is, when the temperature goes up, so does the rate
of nuclear fission.  Such a design would be ruled out here, claims my
source, a former reactor containment vessel engineer.  Surely, such a
design would make the sort of accident which occurred more likely.
   Dick
Dick Karpinski    Manager of Unix Services, UCSF Computer Center
UUCP: ...!ucbvax!ucsfcgl!cca.ucsf!dick   (415) 666-4529 (12-7)
BITNET: dick@ucsfcca   Compuserve: 70215,1277  Telemail: RKarpinski
USPS: U-76 UCSF, San Francisco, CA 94143

</PRE>
<HR><H3><A NAME="subj5.2">
Air Traffic Control failure
</A>
</H3>
<address>
Dan Melson
&lt;<A HREF="mailto:crash!pnet01!dm@nosc.ARPA ">
crash!pnet01!dm@nosc.ARPA 
</A>&gt;
</address>
<i>
Mon, 11 Aug 86 23:47:21 PDT
</i><PRE>

Computer failures at Air Route Centers are not as uncommon as we'd like, but
they're not as nasty as they could be.  Despite the fact that the computers
currently used are more than fifteen years old, they seem to handle the load
well enough for the present.  When the primary computers (IBM 9020's) go down,
however, the DARC backup system does not furnish the controllers with nearly
as much data, and it is far more difficult to get automated tasks done.

There is currently a new computer system in the works, and when it is
operational, delays due to computer failure should dramatically decrease.  The
estimate for this is 'around 1990'.

At any rate, even the bachup systems are far more pleasant than doing all
of the work manually.  

                                                DM
                                        
</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Possible failures of BMD software
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Tue, 12 Aug 1986  00:38 EDT
</i><PRE>

I'm working on a paper on potential software-induced difficulties and
problems that might accompany the deployment of a BMD system.  I'd
like to enlist the collective imagination of the list on examples
apropos to this paper.

Please constrain your imagination by the limits of the possible (e.g.,
it is impossible for an X-ray laser to shoot x-rays at ground targets,
but it is not impossible that the firing of an X-ray laser creates an
electromagnetic pulse that has unanticipated effects).  Please specify
the scenario in as much detail as you can.  I am not specifying a
system architecture, so please tell me the one(s) you have in mind in
your scenario(s); that is necessary because softare -- by itself -- is
harmless no matter how buggy it is.  Also remember that BMD has
significant capability against satellites.


Thanks.  Acknowledgements will be provided if you so desire.

Herb Lin

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
A note about stories "from memory"
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Mon, 11 Aug 86 23:44:12 -0500
From: Henry Mensch &lt;henry@ATHENA.MIT.EDU&gt;

I hate to sound like a nit-picker but I've noticed a rash of stories
which begin with words like "If I remember correctly ..." or "It's 
pretty late, so expect errors."  Is this sort of thing a product of
having such powerful communications tools at our fingertips?

Once these things happen we seem to spend a lot of time saying "Well,
*I* thought it went this way. . . "  In discussing risks to the public,
we risk wasting our time doing these tasks, which could be avoided
with a bit of research.

Striving for better communications,

 - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Henry Mensch     |   Technical Writer  | MIT/Project Athena
henry@athena.mit.edu          ..!mit-eddie!mit-athena!henry

      [On the one hand, it is nice to be precise.  On the other hand,
       if the report is novel and interesting, perhaps RISKS provides 
       a medium for getting feedback from an expert on a matter that
       would otherwise go unreported.  But, I certainly appreciate it
       when contributors take a little time to track down the reference
       -- and especially when they cite that reference.  PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.35.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.37.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-23</DOCNO>
<DOCOLDNO>IA012-000123-B021-296</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.37.html 128.240.150.127 19970217004131 text/html 16822
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:39:45 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 37</TITLE>
<LINK REL="Prev" HREF="/Risks/3.36.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.38.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.36.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.38.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 37</H1>
<H2> Thursday, 14 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Computer Viruses 
</A>
<DD>
<A HREF="#subj1.1">
Robert Stroud
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  On knowing how hard a system is to make work 
</A>
<DD>
<A HREF="#subj2.1">
Bob Estell
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  COMSAT and the Nondelivery of Mail 
</A>
<DD>
<A HREF="#subj3.1">
Rob Austein
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Exploding Office Chairs 
</A>
<DD>
<A HREF="#subj4.1">
Jonathan Bowen
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Computer Viruses
</A>
</H3>
<address>
Robert Stroud 
&lt;<A HREF="mailto:robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Wed, 13 Aug 86 20:29:54 bst
</i><PRE>

Here is something I found in the Times yesterday. Since it is marked
"Reuters" I assume it originated in the States so you may have seen it
already. What is your policy on posting copyrighted articles?  This is the
entire text and I have not made any excerpts. On the other hand, I have
acknowledged the copyright. There has been a fuss about this in net.unix
recently, so I am rather concerned not to get myself, the University or you
into trouble.

   [RISKS is a non-profit educational operation.  I believe that it is
    quite appropriate to quote an article under such circumstances -- with
    attribution.  There is a burden on all of us to use it accordingly.  PGN]

One of the "computer comics" (free journal made up of half news/features
and half job adverts) called Datalink has a front page story about the X-ray
machine in Texas killing a patient. I remember this coming up in RISKS some
time ago, and you are quoted in the article as follows:

"Specialists in the field of software reliability have long been predicting 
fatalities caused by bugs. Peter Neumann of the US ACM claimed that the ACM's
software engineering group had monitored 16 deaths caused by defective
programs. "This is just the tip of the iceberg", he said.

    [Actually I thought I mentioned to him that there were at least 16
     CASES of computer-related deaths (a subsequent closer count by me
     shows that there are 24 different cases in my files).  The total 
     number of deaths in those cases is over 716.  There were also three 
     Soviet nuclear sub accidents with unknown tolls.  PGN]

Manny Lehman is also quoted as being "not surprised - this is merely the
front-runner of a thing we're going to see a lot of".

The same issue of Datalink also contains a story about how a problem with
some new software led to rumours that Tetley's brewery had stopped
production - while they were installing it, they ran into problems and to
save time, tried to contact the programmer who was on holiday in Scotland.
Somehow the messages got distorted en-route...

It's a nice anecdote but perhaps not really a RISK! However, I'll send it
in if you're interested. People can take beer very seriously in the UK...

   [Please send it!  PGN]

     ============================================================

Here is an article from yesterday's [London] Times (August 12th, "Computer
Horizons").  Although it is couched in somewhat exaggerated tones(!), the
consequences of failure are the same, whether induced by sinister bogeymen
or simply design faults.

By coincidence, I recently came across a reference to the paper by F. Cohen
of the University of Southern California entitled "Computer Viruses: Theory
&amp; Experiments", which apparently suggests that a Unix virus could gain root
privileges within an hour, so maybe there is something to be worried about
after all!  [A few minutes is well within an hour...  PGN]

Perhaps some of the "sources who spoke on condition they would not be 
identified" will read this and would like to comment further, (anonymously
of course...)

Robert Stroud, Computing Laboratory, University of Newcastle upon Tyne.

ARPA robert%cheviot.newcastle@ucl-cs.ARPA, UUCP ...!ukc!cheviot!robert

     ============================================================

"The 'virus' threat to defence secrets" (c) Times Newspapers Limited 1986

from Christopher Hanson in Washington

American Scientists are struggling to protect computer networks - vital in
areas ranging from national defence to banking and air traffic control -
against a potentially devastating weapon called a computer virus.

Computer security experts in the US government say the "virus" is a high
technology equivalent of germ warfare: a destructive electronic code that
could be inserted into a computer's program, possibly over a telephone line,
by a secret agent, terrorist or white collar criminal.

When a computer virus attacks it wipes out crucial memory data or otherwise
causes high technology equipment to behave erratically, according to sources
who spoke on condition they would not be identified.

They said a computer virus attack might bring a major weapons system to a
standstill, throw a computer-guided missile off course, or wipe out computer
stored intelligence. "The government is concerned and we are pursuing
solutions," one security official said.

Computer security experts have created experimental viruses in a bid to find
defences, but there had been no breakthroughs.

Both the military's computer nets and the highly automated US banking system 
are vulnerable to "catastrophic collapse", according to a recent Georgetown
University report by a group of government and private counter-terror experts.
Urging that the pace of defensive research be quickened, it said the computer
virus threat was "a matter of great concern...There do not appear to be any
quick and easy defences or overall solutions to the problem."

As to the banking system, the report warned: "The four major electronic
funds transfer networks alone carry the equivalent of the federal budget
every two to four hours. These almost incomprehensible sums of money are
processed solely between the memories of computers, using communications
systems that are vulnerable to physical disruption and electronic tampering."

Computer viruses are designed to replicate themselves like a living organism,
spreading throughout a computer netork, government scientists said. Viruses
can spread from one computer system to another during electronic linkups
and might lie undetected for months or years before going on the attack at a
pre-determined time.

Before it begins to disrupt a system, a computer virus would be inconspicuous,
containing only a few hundred "bytes" in a program that might total hundreds
of thousands. Even the most carefully designed computer security barriers can
be vulnerable, the Georgetown report said.

Another way the viruses could spread was through computer discs which computer
users often copy and share. Scientists say the computer virus idea may have
originated in a 1975 science fiction novel, "The Shockwave Rider". Intrigued
computer buffs began tinkering and by the early 1980s had turned fiction into
fact with experimental viruses. (Reuter)

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
On knowing how hard a system is to make work
</A>
</H3>
<address>
&lt;<A HREF="mailto:"SEFE::ESTELL" <estell%sefe.decnet@nwc-143b.ARPA> [or estell@nwc-143b]">
"SEFE::ESTELL" &lt;estell%sefe.decnet@nwc-143b.ARPA&gt; [or estell@nwc-143b]
</A>&gt;
</address>
<i>
14 Aug 86 11:06:00 PST
</i><PRE>
To: "risks" &lt;risks@csl.sri.com&gt;

I think there is a risk in solving computing problems too easily.  A San
Diego friend says that "The trouble with doing a project right the first
time is that no one knows how hard it was."  Though that happens
infrequently, he's got a point.  In most fields, accomplishment can be
measured by effort, along with talent, luck, and some other things.  The
scholar who breezes through school often knows how hard it is, based on the
hours spent in the library and the lab; the athlete whose graceful moves
seem effortless knows how close to the limit she plays.  But lots of "good"
computing systems are joint ventures between a hardware designer of generic
computer power, and a software designer of some particular algorithm;
neither really knows how hard the machine works to solve a particular
problem.  Often it's only after the system fails that we realize that it was
operating at its limit before we increased the load.  That's in part because
many programmers just write code, with little attention to thorough analysis
&amp; design as urged by Don Knuth's work; and in part because hardware designer
and software end-user often never meet; and in part because the field is so
broad and demanding that one person can't know it all.

There's another old saying, that an expert is someone who avoids all the
minor errors on his way to the colossal blunder.  That points up the risk of
being so bright (or lucky?) that one never fails (or is even stressed) by
routine assignments; and finally assumes a prominent role in a major, high
risk program.

Maybe we should give some thought to having major computing projects headed
by people who have reached their limits at least once along the way; not
that they have failed, but that they have had to try again.  [A winner is
one who gets up one more time than he goes down.]  With that in mind, does 
anyone know the "track record" of the leaders of some high risk projects; 
e.g., SDI?  I'm sure these folks have impressive credentials;
I just wonder if they've ever explored their own limits.

Bob

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
[Nondelivery of <A HREF="/Risks/2.38.html">RISKS-2.38</A> (8 April 1986) and other mail]
</A>
</H3>
<address>
Rob Austein 
&lt;<A HREF="mailto:SRA@XX.LCS.MIT.EDU">
SRA@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Thu, 14 Aug 1986  03:16 EDT
</i><PRE>

    Date: Friday, 8 August 1986  19:43-EDT
    From: Communications Satellite &lt;COMSAT@MC.LCS.MIT.EDU&gt;
       "[For the past week or so, I have been getting sequential notices of
        undeliverable mail from "Communications Satellite" -- four
        months after the original mailings of RISKS, ... PGN ]"

COMSAT stopped being able to deliver messages of any serious length sometime
around last December, and didn't really get fixed until mid-May (changing of
the guard, had to scare up a new COMSAT hacker).  During that time a couple
of Really Dedicated People were faithfully saving all the messages that
COMSAT was dropping on the floor.  Ever since COMSAT was fixed these
messages have been being dribbled back into the mail queue, 10 or 20 at a
time (not practical to filter them, given the volume).  The fact that it is
now August and we still aren't done should give you some idea of the volume
of mail that MC handles.

We announced this on Arpanet-BBoards (and other places) when we started
dribbling the mail back in.  Of course, that was a while ago....

--Rob

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Exploding Office Chairs [A Peripheral Risk of Sitting Before a VDT?]
</A>
</H3>
<address>
Jonathan Bowen 
&lt;<A HREF="mailto:bowen%sevax.prg.oxford.ac.uk@Cs.Ucl.AC.UK">
bowen%sevax.prg.oxford.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Thu, 14 Aug 86 15:16:30 GMT
</i><PRE>

Below are extracts from two reports in the Guardian; the first rather jokey
and the second less so, presumably after the journalist realised the
seriousness of the problem.

      Exploding chairs a pain in the office (Monday, 11th August 1986)

    A new hazard at work, the exploding office chair, is facing - or, rather,
  the reverse - Britain's white collar workers.  The problem is now under
  investigation so that up to 2 million minds, and a similar number of
  bottoms, may rest more easily.  So far, 11 swivel chairs around the country
  are known to have gone off with a bang.  In three cases the exploding chairs
  have caused injury, probably because the sitters have been sent sprawling as
  the bottom drops out of their world.
  
    The problem has cropped up with adjustable office chairs fitted with
  nitrogen gas cylinders in place of the conventional springs in their height
  control mechanism.  Preliminary findings suggest that metal fatigue cracks
  can develop in the cylinders, possibly caused by the poor chairs being asked
  to cope with more than they can bear.


      Exploding chairs' two-year history (Tuesday, 12th August 1986)
  
    The danger of office chairs exploding has not previously been made public
  because of official reluctance to raise an "alarmist scare," it emerged
  yesterday.  The public has not been warned about blasts scattering stell
  fragments and metal bolts caused by failures in adjustable chairs fitted
  with nitrogen cylinders instead of conventional springs. Cases of serious
  injury came to light two years ago. ...
  
    In September 1984, the Consumers' Association passed to the Health and
  Safety Executive (HSE) details reported by consumer organisations in Europe
  of incidents involving office chairs.  They included accounts of two deaths,
  one in Belgium and the other in West Germany, where, it was reported, a
  piece of steel had penetrated a victim's brain through the eye.  ....  The
  HSE has stressed that only 11 incidents, three of which caused injury, are
  known to have occurred in Britain - where up to 2 million of the chairs are
  in use.
  
Has this story broken in the US yet? How many of you are sitting at your VDU
on such a chair? This is the time to take a quick peek below you, and take
appropriate defensive action if necessary. You have been warned!
  
Jonathan Bowen, Research Officer, Distributed Computing Software Project
Oxford University Computing Laboratory, Programming Research Group
8-11 Keble Road, Oxford OX1 3QD, England, Tel:	+44-865-54141 x293
   JANET: bowen@uk.ac.oxford.prg 
   UUCP:  ...seismo!mcvax!ukc!ox-prg!bowen (bowen@ox-prg.uucp)

                         [Some persons talked into buying this chair 
                          were evidently given a bum steer!  PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.36.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.38.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-24</DOCNO>
<DOCOLDNO>IA012-000123-B021-320</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.38.html 128.240.150.127 19970217004146 text/html 11771
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:40:15 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 38</TITLE>
<LINK REL="Prev" HREF="/Risks/3.37.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.39.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.37.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.39.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 38</H1>
<H2> Sunday, 17 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Computer gives away California state funds 
</A>
<DD>
<A HREF="#subj1.1">
Rodney Hoffman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  High-Tech Sex Ring: Beware of Whose Database You Are In! 
</A>
<DD>
<A HREF="#subj2.1">
Peter G. Neumann
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Computer Viruses 
</A>
<DD>
<A HREF="#subj3.1">
Chris McDonald
</A><br>
<A HREF="#subj3.2">
 Paul Garnet
</A><br>
<A HREF="#subj3.3">
 Matt Bishop
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Computer Viruses and Air Traffic Control 
</A>
<DD>
<A HREF="#subj4.1">
Dan Melson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Re: Traffic lights in Austin 
</A>
<DD>
<A HREF="#subj5.1">
Bill Davidsen
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Computer gives away California state funds
</A>
</H3>
<address>
&lt;<A HREF="mailto:Hoffman.es@Xerox.COM">
Hoffman.es@Xerox.COM
</A>&gt;
</address>
<i>
15 Aug 86 13:51:39 PDT (Friday)
</i><PRE>
To: RISKS@CSL.SRI.COM

From the Los Angeles Times, August 15 1986, page 2:

  A computer error caused California's check-writing system to 
  issue $4 million in interest-payment checks to bondholders
  who hold a type of bond on which no such payments were due.
  Deputy state Treasurer Liz Whitney explained that those bonds
  are of the "zero coupon" type, which are held for a period of
  years and redeemed with accumulated interest at maturity
  rather than bearing interest on a monthly or yearly basis.
  The treasurer's office learned of the error last Friday, she
  said, when a recipient inquired about the check's validity,
  and stop-payment orders were issued.  By Wednesday, all but
  a few checks totaling $33,000 had been recovered.

No further details  are given about the nature of the computer error.

	-- Rodney Hoffman

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
High-Tech Sex Ring: Beware of Whose Database You Are In! 
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Fri 15 Aug 86 19:37:38-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

From the San Francisco Chronicle, Friday 15 August 1986:

  POLICE SAY ARRESTS IN MARIN SMASHED HIGH-TECH SEX RING
  by Torri Minton and Katy Butler
  
  A sophisticated prostitution ring that kept computerized records on more
  than 12,000 patrons has been broken after a three-month investigation,
  authorities in San Jose said yesterday.  The ring, known as EE&amp;L
  Enterprises, collected $3.5 million a year dispatching at least 117
  prostitutes by electronic beeper to cities all over Northern California from
  a computerized command center in San Rafael, according to San Jose vice
  Lieutenant Joe Brockman.  ``It's a top-class operation -- the largest
  prostitution ring, to our knowledge, in Northern California," Brockman said.
  He said that the business took in more than $25 million during the eight
  years it was in business...
  
  Records seized by police ... included customers' names, telephone numbers,
  credit card numbers, sexual preferences and comments by the prostitutes...
  The office was equipped with four desks, several IBM computers, a
  photocopier, a paper shredder and a wall poster announcing that ``Reality is
  nothing but a collective hunch.''
  
On-line SuperCalifornication?

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Computer Viruses
</A>
</H3>
<address>
Chris McDonald  SD 
&lt;<A HREF="mailto:cmcdonal@wsmr06.arpa">
cmcdonal@wsmr06.arpa
</A>&gt;
</address>
<i>
Fri, 15 Aug 86  7:47:01 MDT
</i><PRE>
To: RISKS FORUM    (Peter G. Neumann -- Coordinator) &lt;RISKS@CSL.SRI.COM&gt;

                   [This is included because so many of you do 
                    not seem to know the Cohen reference.  PGN

Robert Stroud references a paper by Fred Cohen on "Computer Viruses."  The full
text of the paper can be found in several public souces.  The most available
for US readers is the minutes of the 7th DoD/NBS Computer Security Conference,
Sept 24-26, 1984, pages 240-263.  The paper is not exclusively concerned with
any one particular operating system.  It defines a "virus" as "a program that
can infect other programs by modifying them to include a possibly evolved copy
of itself."  The paper references Ken Thompson's acceptance speech on the
Turing Award, "Reflections on Trusting Trust," which was published in the
August 1984 "Communications of the ACM."  The reference, however, is only for
purposes of illustrating what Fred proposes is a "limited" virus. 

    [That paper includes the wonderful C compiler Trojan horse lurking
     in wait for the next recompilation of the UNIX LOGIN procedure.  PGN]

A close reading of the paper would reveal that very specific factors have to
exist for a "virus" to become "virulent."  The most interesting facet of the
paper is really the question it raises as to whether the Bell-LaPadula and the
Biba models on mathematically defining "secure systems" even addresses the
potential of a "virus" attack.

</PRE>
<HR><H3><A NAME="subj3.2">
Computer Viruses
</A>
</H3>
<address>
&lt;<A HREF="mailto:pgarnet@nswc-wo.ARPA">
pgarnet@nswc-wo.ARPA
</A>&gt;
</address>
<i>
Fri, 15 Aug 86 12:14:22 edt
</i><PRE>

Another paper by Fred Cohen is "Recent Results in Computer Viruses", written
while at Lehigh University.  The copy I have does not have a date on it, but
I believe it was written sometime around the spring of 1985.

Anybody else know of any good, technical papers on the subject?

					Paul

</PRE>
<HR><H3><A NAME="subj3.3">
Re: Computer Viruses
</A>
</H3>
<address>
Matt Bishop 
&lt;<A HREF="mailto:mab@riacs.ARPA">
mab@riacs.ARPA
</A>&gt;
</address>
<i>

</i><PRE>
Date: Fri, 15 Aug 86 07:28:27 -0700

If anyone wants to read an interesting science fiction book about computer
viruses (and things of that ilk) try reading John Brunner's "Shockwave
Rider."  Briefly, it's about a man who puts computer viruses into the
worldwide data banks, enabling him to do all sorts of illegal things such as
change identities.  Quite interesting, at least from the viewpoint of
computer security!
                                        Matt Bishop

       [I think we included mention of "Shockwave Rider" in RISK long ago.
        However, with the interest in viruses and our large number of new
        readers, I am not trying to avoid all duplication -- especially with
        the distant past.  PGN]

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Computer Viruses and Air Traffic Control
</A>
</H3>
<address>
Dan Melson
&lt;<A HREF="mailto:crash!pnet01!dm@nosc.ARPA ">
crash!pnet01!dm@nosc.ARPA 
</A>&gt;
</address>
<i>
Sat, 16 Aug 86 01:13:47 PDT
</i><PRE>

Those who fly regularly will be somewhat relieved to note that all terminals
of the ARTS and NAS systems, except master consoles (and a few others hardwired
straight into the machine and on site) are limited in what they can input,
nor can they escape the ATC program.  Furthermore, I am not aware of any
means whereby employees can access any of the FAA's computers from other than
known sites.  This also explains why there are so few ATC's on any net, despite
the large amount of computer work associated with the job today.

                                                DM
      [Beware of Trojan horses bearing gifts that look like sound programs,
       officially installed through proper channels.  There is also the
       problem of accidental viruses such as the ARPANET collapse of 27 
       October 1980.  (See Eric Rosen's fine article in the ACM Software
       Engineering Notes 6 1 Jan 81, for those of you who have not seen
       it before.)  PGN]
     
</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Re: Traffic lights in Austin
</A>
</H3>
<address>
&lt;<A HREF="mailto:davidsen%kbsvax.tcpip@ge-crd.arpa">
davidsen%kbsvax.tcpip@ge-crd.arpa
</A>&gt;
</address>
<i>
15 Aug 86 10:57 EST
</i><PRE>
To: RISKS@CSL.SRI.COM
   [From: Davidsen &lt;davidsen%kbsvax@kbsvax.tcp-ip&gt;]

I would call a 2% clean failure rate a success. If the two intersections had
failed in an unsafe mode, such as green in both directions, it would not
have been acceptable. If the lights had "stuck" showing green one way and
red the other, it could have caused severe delays. For the light to cleanly
go out is probably acceptable.  Most drivers seeing a light with no signal
showing will use adequate caution to prevent accidents.
                                                            -bill davidsen

  ihnp4!seismo!rochester!steinmetz!--\
                                       \
                    unirot -------------&gt;---&gt; crdos1!davidsen
                          chinet ------/
         sixhub ---------------------/        (davidsen@ge-crd.ARPA)

"Stupidity, like virtue, is its own reward"

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.37.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.39.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-25</DOCNO>
<DOCOLDNO>IA012-000123-B022-18</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.39.html 128.240.150.127 19970217004156 text/html 13462
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:40:27 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 39</TITLE>
<LINK REL="Prev" HREF="/Risks/3.38.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.40.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.38.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.40.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 39</H1>
<H2> Tuesday, 19 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Nuclear false alarm 
</A>
<DD>
<A HREF="#subj1.1">
Robert Stroud
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Risk to beer production? 
</A>
<DD>
<A HREF="#subj2.1">
Robert Stroud
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: High Tech Sex 
</A>
<DD>
<A HREF="#subj3.1">
Lindsay F. Marshall
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  QA on nuclear power plants and the shuttle 
</A>
<DD>
<A HREF="#subj4.1">
Roy Smith
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Hackers in BITNET 
</A>
<DD>
<A HREF="#subj5.1">
Sterling Bjorndas
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Nuclear false alarm
</A>
</H3>
<address>
Robert Stroud 
&lt;<A HREF="mailto:robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Mon, 18 Aug 86 17:56:25 bst
</i><PRE>

"BT is blamed for HM's displeasure" (Computing, August 14th 1986)
by Angus McCrone (c) Computing

British Telecom (BT) is being blamed for a network fault which caused
nuclear attack sirens in Edinburgh to blare into action last month.  The
sirens disturbed thousands of people at 7.30 in the morning.  The incident
coincided with a visit by the Queen and Margaret Thatcher to watch the
Commonwealth Games.

A spokeswoman at the Home Office, which has the responsibility for civil
defence in the UK, said that BT was checking a carrier control unit in
Edinburgh. This is believed to have malfunctioned causing the alarm to go
off.  The carrier control unit, one of about 250 around the country, has the
job of connecting the Ministry of Defence's air operations computer centre
and local police stations which activate the alarm.

The Home Office has ruled out computer error as a reason for the mistake,
and seems convinced that human error or sabotage were not involved either.
This is despite the fact that no similar mistakes have been recorded in the
past 12 years, and that the incident happened at the height of a
controversial visit to Scotland by the Prime Minister.  A BT official
confirmed that a report on the alarm had been sent to the Home Office, but
would not say whether his company accepted responsibility for the mistake.

In time of war the Home Office consults with the MoD before ordering police
stations to switch on the alarms, which warn citizens to expect air or
nuclear attack.  The incident in Edinburgh last month caused little panic
because most people switched on their radios to check there was no real
emergency.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Risk to beer production?
</A>
</H3>
<address>
Robert Stroud 
&lt;<A HREF="mailto:robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Mon, 18 Aug 86 17:52:47 bst
</i><PRE>

"Minor bug starts mass beer panic" (Datalink, August 11th 1986)
by Dave Holmes (c) Datalink

A holidaying programmer sparked off a bizarre series of events last week
culminating in a rumour that Tetley's, Yorkshire's most famous brewery, had
stopped production.  Workmates had realised that they needed the advice of
the programmer, Richard Chubb, to sort out a problem with the control system
he was developing for Tetley.  Police were asked to help track him down on
holiday in Scotland, but a telex from Strathclyde police to seven Scottish
police divisions apparently suggested that the brewery had stopped
production because of a computer breakdown.

News of this got back to Yorkshire and last weekend Tetley was deluged with
calls from worried publicans afraid that supplies of Yorkshire's finest were
about to dry up.  David Gaskill, of the engineering company Turkington which
was installing the control system explained what had happened: "There was a
communications glitch between two systems we are installing at Tetley, and
the program is not fully documented yet. To go through the code was going to
take ages, but Richard could have sorted it out in 20 minutes," he said.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: High Tech Sex
</A>
</H3>
<address>
"Lindsay F. Marshall" 
&lt;<A HREF="mailto:lindsay%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
lindsay%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Tue, 19 Aug 86 15:21:58 bst
</i><PRE>

The interesting question that it raises is that of what has happened to the
information on the data-base.  Has it been destroyed, or has it been
incorporated into the Police computer records?
                                                      Lindsay

     [The implication of the article was that indeed the records had been
      confiscated.  With a shredder in the office, it could have been what
      was on the diskettes -- but more than likely there were simply 
      printouts lying around.  PGN]

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
QA on nuclear power plants and the shuttle
</A>
</H3>
<address>
Roy Smith
&lt;<A HREF="mailto:allegra!phri!roy@seismo.CSS.GOV ">
allegra!phri!roy@seismo.CSS.GOV 
</A>&gt;
</address>
<i>
Tue, 19 Aug 86 11:50:39 edt
</i><PRE>
Apparently-To: allegra!seismo!sri-csl.arpa!risks

	Last night I watched "The China Syndrome" on TV.  For those of you
not familiar with this moderately-trashy movie, it's about the threat of a
meltdown at a nuclear power plant.  It seems that when the plant was built,
the X-ray testing of the welds was faked, so a bad weld went unnoticed
(causing a pump to fail, etc).
                                 [That was taken from some real cases...  PGN]

	Anyway, at one point, the hero exclaims, "but our quality control
is second only to NASA's!"  Shows you the RISKS of making comparisons,
doesn't it?  Do nuclear plants have O-rings?

                                 [No, but they do have lots of reports of 
                                  equipment failures and human errors that
                                  don't seem to get wide public view.  PGN]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Hackers in BITNET
</A>
</H3>
<address>
&lt;<A HREF="mailto: BJORNDAS%CLARGRAD.BITNET@WISCVM.WISC.EDU">
 BJORNDAS%CLARGRAD.BITNET@WISCVM.WISC.EDU
</A>&gt;
</address>
<i>
18 AUG 86 12:43-PST
</i><PRE>

The following is an abridged version of an article from issue 3.3 of
VM-COM, an e-magazine published distributed in BITNET. It has been
edited with permission, by Sterling Bjorndahl (BJORNDAS@CLARGRAD).

               Life in the Fast Lane:  Column #2

                  Chris Condon BITLIB@YALEVM

   There are hackers in BITNET.  You aren't surprised, I'm sure.  Now, not
all hackers are slavering, demented, animals waiting to break into, crash,
and destroy systems, illegally using their resources, plundering userids
that are not their own, and making a general mess out of everything.
   Only some are.
   There exists in this network a group of hackers who broke into a userid
at Fermilab via BITNET.  They used the RELAY conference machine system to
keep in contact. Administration types at Cornell University, hearing of
this, came to this conclusion:

   "The Cornell Relay has been shut down forever due to the misuse of BITNET by
   some hackers in West Germany who discussed their trade on the Relay.  It is
   Cornell's desire to not be associated with the Relay system in the future..."

   The reaction by these people might seem a bit extreme, but it could be
even worse.  There are some people in BITNET who would like to see students
completely banned from the network, or chatting banned from the network, or
both.  These are people to be reckoned with. They are in positions of power
to do such things at their own nodes, given enough reason.  For Cornell, the
hackers breaking into Fermilab turned out to be an excellent excuse. It need
not be anything so extreme.
   Our actions are a reflection on the students in BITNET.  It has been said
(not enough) that BITNET usage is a privilege.  It brings with it a great
responsibility.  Everything we do may have far reaching effects without our
knowing it.  The hackers that broke into Fermilab were not from Cornell, had
no intention of getting that Relay shut down, and they probably did not
consider that it would happen.
   I posted a notice on this subject for the Usage Guidelines Group via
LISTSERV@BITNIC.  These are some of the responses (names withheld):

A. "The problem, as I see it, stems from a lack of moral and ethical 
   standards in the computer world, as well as the natural inquisitiveness 
   of young people specifically and computer type people generally."

[I disagree that "computer types" have any worse ethical standards
than the bulk of this society. They just have a lot of power. - S.B.]

B. "I don't know what, if any, audit trail is left from interactive traffic on
   the net. If there isn't any, I think there ought to be and installations
   with security concerns about chatting should monitor the traffic for
   suspicious activity."

C. "A totally restrictive policy, one that makes absolute and unbending
   restrictions, especially to undergraduate students, will have two effects.

     1: Those persons who are borderline on being responsible or abusive 
        with the system may just go the wrong way, partly out of challenged
        to their perception of a "cold-hearted" system.

     2: Students will lack (unless they break in and get away with it which
        is what we try to prevent) a practical education of how real life
        computers are implemented.  I know these things to be true from
        first hand experience, because I used to be such a hacker. I did get
        away with it and I did learn enough to go right into an upper level
        systems programming job right out of school... The school I attended
        had a very closed policy.  They were, however, not effective in
        implementing that policy, and so some of us got into the system."

D.     "My suggestion is that a policy be established to deal
        [constructively] with "curious students" who show promise.  Just
        how you do this has to depend on your resources."

   Like it or not, someone is looking over your shoulder.  Maybe you
won't get caught when you do something irresponsible via BITNET, but
somebody will pay the consequences.  Somebody out there is looking for
an excuse to shut you, or some other student, out of BITNET...  The
actions of some students have simply led  him to believe that shutting
students out is a good thing.  It will take your example to convince
him otherwise.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.38.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.40.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-26</DOCNO>
<DOCOLDNO>IA012-000123-B022-41</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.40.html 128.240.150.127 19970217004210 text/html 19362
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:40:37 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 40</TITLE>
<LINK REL="Prev" HREF="/Risks/3.39.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.41.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.39.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.41.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 40</H1>
<H2> Thursday, 21 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
QA on nuclear power plants and the shuttle 
</A>
<DD>
<A HREF="#subj1.1">
Eugene Miya
</A><br>
<A HREF="#subj1.2">
 Ken Dymond
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  CAD, Simulation, Armored Combat Earthmover, and Stinger 
</A>
<DD>
<A HREF="#subj2.1">
Mary C. Akers
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Risks Distribution List -- Private-Copy Subscribers PLEASE READ! 
</A>
<DD>
<A HREF="#subj3.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Could computers launch a nuclear attack? 
</A>
<DD>
<A HREF="#subj4.1">
Jeff Myers
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Re: QA on nuclear power plants and the shuttle (re "portary"-als)
</A>
</H3>
<address>
Eugene Miya
&lt;<A HREF="mailto:eugene@AMES-NAS.ARPA ">
eugene@AMES-NAS.ARPA 
</A>&gt;
</address>
<i>
20 Aug 1986 1045-PDT (Wednesday)
</i><PRE>

&gt; Date: Tue, 19 Aug 86 11:50:39 edt
&gt; From: allegra!phri!roy@seismo.CSS.GOV (Roy Smith)
&gt;
&gt; ... I watched "The China Syndrome" on TV... moderately-trashy movie...
&gt; Anyway at one point, the hero exclaims, "but our quality control
&gt; is second only to NASA's!"  Shows you the RISKS of making comparisons,
&gt; doesn't it?  Do nuclear plants have O-rings?
&gt;
&gt;             [No, but they do have lots of reports of equipment failures 
&gt;              and human errors that don't seem to get wide public view.  PGN]

Risks of films?

I saw China Syndrome the day TMI occurred.  It is a reasonably accurate
film, with a minimum of dramatic license (the "vibration" is an example of
this as control rooms tend to be more isolated.).  I don't regard the film
as trashy.  There are deliberate attempts by film makers to be "realistic",
and this film was well researched.  In contrast, War Games looked trashy to
computer people.  The screenplay writers gave a talk about the film at the
Palo Alto CPSR meeting.  They deliberately used obsolete hardware so that
companies like A*e might not sue them.

Sorry, Peter, you are wrong.  Reactors do use O-rings.  Your car uses
O-rings; one just failed in my VW Rabbit.  The problem of reporting is
historical and dates back to the late 40s and the "mysticism" on about
nuclear information.  It is very easy to classify nuclear information:  for
instance, it is not forbidden to have civilians in any nuclear control room
(they are not much different from coal fired plants in layout).  This was
driven by the concern for nuclear terrorism in the late 1970s.  It boils
down to whether nuclear power should be under civilian or military control:
I know civilian physicists at LLNL who think the original decision in the
1940s was a mistake.  (They feel it should have been kept a military secret.)

NASA's QA.  I've not worked on QA.  The problem might be in the Q:
The paperwork for individual Shuttle tiles weigh more than the tiles
themselves.  There is a photo in Scott Crossfield's autobiography (1964?)
showing paperwork for the X-15 exceeding 3 times the weight of the X-15.
We must not mistake quantity for real quality.  Maybe software should have
more paper....  Let's not confuse quantitative assurance and qualitative.

Lastly, (here's the nerve you hit), Hans Mark (currently head of the U of
Texas) gave a talk at Ames on Monday on Challenger and Chernobyl.  Hans is
and was in a unique position to talk about both.  He was a chief at LLL,
taught nuclear engineering at UCB for 10 years, ran Ames, ran the Air Force,
#2 man at NASA and made flight decisions for the first dozen flights (O-ring
charring on fights 2, 8 and later).  He was interviewed by the Rogers
Commission.  "O-rings, did not seem like that much of a problem in contrast
to other problems like nozzle burn thru..."  Mark has decided to write an
article based on this talk.

He feels somewhat responsible even though he is no longer with NASA.  He had
scheduled a review regarding O-rings during a period when he took his
new U-Texas job.  The review never took place.  (Lame duck administrator,
in his words.)  The men who made the final launch decisions were
and still are friends of his.  The Chernobyl portion was a recapping of known
information.  In both cases, Mark cites the need for communication
between management and workers.

--eugene miya
  NASA Ames Research Center
  eugene@ames-aurora.ARPA

    [I saw it the NIGHT BEFORE TMI! But I asked Gene about whether 
     those other O-rings also had problems at low temperatures.  (PGN)
     This was Gene's reply:]

        Cars: Mine was 8 years old.  It was an external seal, it failed at
        80 degs F.

        Power plants: probably not.

        I would think antarctic snow cars have O-rings and fan belts and all
        sorts of things that snap.

        --eugene

</PRE>
<HR><H3><A NAME="subj1.2">
Re: QA at Nuclear Plants
</A>
</H3>
<address>
"DYMOND, KEN" 
&lt;<A HREF="mailto:dymond@nbs-vms.ARPA">
dymond@nbs-vms.ARPA
</A>&gt;
</address>
<i>
21 Aug 86 09:41:00 EDT
</i><PRE>
To: "risks" &lt;risks@csl.sri.com&gt;

    PGN comments in RISKS 3-39 on "QA on nuclear power plants and the shuttle":

         &gt;No, but they [nuclear power plants] do have lots of reports
         &gt;of equipment failures and human errors that don't seem to
         &gt;get wide public view.

It may depend on how interested the public is.  These reports (and probably
PGN is referring to the Licensee Event Reports or LERs which are compiled by
the NRC from plants, i.e. holders of licenses to make electricity from
nuclear power) are matters of public record.  The NRC distributes them to
all plants as notices of the kinds of things that happen and should be
watched for.  They are also maintained in the NRC's public documents room in
the D.C. area and in a local public documents room near every nuclear plant.
I know of at least one public library (Wiscasset, Maine) that keeps LERs on
file because of public interest in the Maine Yankee plant nearby.

Most of the time LERs don't make exciting reading. I haven't seen an LER for
a while but a representative incident that comes to mind occurred at a plant
where the fuel tanks for the emergency diesel generators were allowed to get
300 gallons low (out of 3000 or 30000 gals., can't remember).  Some fuel is
used up in the weekly test of making sure the generators start and operate
and I guess the tanks are supposed to be topped up.  The 10 percent or so
shortfall of fuel would have been remedied at the next (I think it was
weekly) scheduled visit from the oilman.  I don't remember whether the NRC
levied a fine in this case.

The LERs serve as a record of errors in the industry, something that would
be a great help if it existed for software engineering.  Civil and
structural engineers investigate structural failures and publish detailed
results of the investigations in their literature, another practice that
software engineers might consider.

The LERs are supposed to be exhaustive and one thing the resident NRC
inspector at every plant does is to make sure that all events required by
regulations to be reported do get reported.  If the story about the
defective welds is true, it should be in an LER somewhere.

                                          Ken Dymond

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
CAD, Simulation, Armored Combat Earthmover, and Stinger
</A>
</H3>
<address>
"Mary C. Akers" 
&lt;<A HREF="mailto:makers@cct.bbn.com">
makers@cct.bbn.com
</A>&gt;
</address>
<i>
Thu, 21 Aug 86 10:26:23 EDT
</i><PRE>
To: risks@csl.sri.com

Recently the Risks list had a short discussion on the excessive use of CAD
systems.  The September 1986 issue of Discover Magazine has an article by
Wayne Biddle on the use and abuse of computer modeling and simulation.  It
is entitled "How Much Bang for the Buck?"  Here are a few interesting quotes:

     "I want to replace sterile computers simulations with more
     realistic testing under combat conditions," says Representative
     Charles Bennett of Florida, [...]"Weapons testing should ensure
     that our weapons work in combat, not just in the laboratory."  With
     that statement, Bennett zeroes in on the main bone of contention
     among those concerned with weapons design and testing: whether
     computer simulation and modeling can take the place of live
     trails with real equipment."

     "The thing we worry about most is validating our simulations (that 
     is, proving they're realistic), and validation is lagging, for sure.
     Without test data, an unvalidated simulation is all we have."

     "Simulated Flying is so different from real flying that the Navy
     finds that working in a simulator can be a detriment to safe
     operation of an airplane."

Some of the examples used in the article include:

     The Army's Armored Combat Earthmover (ACE) - "...which underwent
     18,000 hours of testing without ever being operated under field
     conditions.  [When it finally under went live trails at Fort Hood]
     ...the tests revealed that the ACE's transmission cracked, that is
     muffler caught fire, that the driver's hatch lid was too heavy to lift,
     and that doing certain maintenance work "could endanger the operator's
     life."

     "The Stinger, a 'man-portable' ground-to-air missile, proved too heavy
     for soldiers to carry on long marches; gunners must hold their breath
     after firing to avoid noxious fumes."

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Risks Distribution List -- Private-Copy Subscribers PLEASE READ!
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Wed 20 Aug 86 11:04:45-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

One of our readers asked to be removed from the RISKS list, forwarding this
somewhat heavy-handed note from an administrator at his institution:

  "Please unsubscribe from the lists you have joined.  At [...] individuals
  do not join mailing lists directly.  There will be a way for you to read
  the full distribution of lists in the fall.  For now I must ask you to
  stop receiving your own copies of everything."

When RISKS began a year ago, the initial intent was to provide individual
subscriptions only until appropriate BBOARDs could be set up.  For the
convenience of some individuals, we have continued to provide private
copies.  The local mailer overhead attributable to RISKS is nontrivial --
although the new intelligent mailers cut down on net traffic.  Disk storage
is now approaching 800 DEC-20 pages for the full collection to date.
Maintenance of the RISKS list continues to be a problem with all the address
changes, incessant notifications of individual nondeliveries (sorry if we
overflow your disk quotas!), host outages, etc.  [Welcome back, Dockmaster
-- which took months to recover from lightning hitting their IMP.]
Unfortunately, various BBOARDs have allocated enough space for only a few
recent back issues (presumably on the assumption that the earlier issues can
be FTPed or that they lose their timeliness).

If you receive a private copy and could conveniently be reading RISKS on a 
local BBOARD, please ask me to remove you from the list.  Thanks...  Peter

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Could computers launch a nuclear attack?  
</A>
</H3>
<address>
Jeff Myers 
&lt;<A HREF="mailto:myers@unix.macc.wisc.edu">
myers@unix.macc.wisc.edu
</A>&gt;
</address>
<i>
Thu, 21 Aug 86 09:41:49 cdt
</i><PRE>

                 [NEW ARTICLE ON OLD TOPIC.  Earlier followers of this 
                  story may wish to read the last three paragraphs.  PGN]

[from the August 20 *Guardian*, p. 9]
By Dave Kadlecek, *Guardian* Bureau

SAN FRANCISCO -- A Stanford University computer professional has sued
Secretary of Defense Caspar Weinberger, claiming that government plans
allowing computers to automatically launch a nuclear attack are
unconstitutional.

Clifford Johnson, a manager in Stanford's Information Technology Services,
filed the suit in federal district court in San Francisco June 17.  He
charged that the US government has a policy of operating a launch-on-warning
capability, under which the US would launch a retaliatory nuclear attack
against the USSR on the basis of a warning that Soviet missiles are on the
way, before unequivocal confirmation that an attack actually occurred.  Due
to the short times involved, such a launch capability relies upon
computerized warning systems which are prone to error and cannot allow for
meaningful human intervention in a launch decision.

This automatic decision illegally usurps congressional powers and delegates
presidential powers.  Thus, Johnson's suit argues, the resulting
``likelihood of a nuclear counterstrike and global environmental damage''
would deprive Johnson of life and property without due process of law,
giving him standing to sue now, since it would not be possible to do so
after a nuclear war.  He asked that the court declare that the secretary of
defense's oath of office ``obligates him to forthwith cease and desist from
operating his launch-on-warning capability.''

Under a cautious assumption that launch-on-warning is in continuous use only
during crisis situations, a number of studies have predicted that an
accidental nuclear war is statistically likely within the next 30 years.

Johnson maintains, however, that US policy already does continuously use
launch-on-warning capability by any normal interpretation of the word
``policy,'' but this denial means only that a formal decision will not be
made until a button is pushed when the warning occurs.  Indeed, a highly
sophisticated set of procedures and programs for a launch-on-warning is in
continuous operation, guarding against a feared ``bolt-from-the-blue''
attack by short-range submarine-launched ballistic missiles.  The Single
Integrated Operational Plan consists of a menu of nuclear ``attack options''
-- lists of targets with assignments of weapons to hit them.  The plan
contains launch-on-warning options, and procedures now in operation permit
the selection of a launch-on-warning option in response to a surprise
attack.

In support of Johnson's suit, Computer Professionals for Social
Responsibility (CPSR) emphasize the inevitability of some computer error in
a system as complex as a launch-on-warning system.  The most dangerous
computer errors are not failures of the device itself (hardware errors), but
of the programming (software errors), stemming ``not from inadequacies in
the technology, but rather from the inability of human beings to formulate
totally adequate plans (programs) for dealing with complicated, poorly
understood situations,'' says CPSR.  CPSR is ``concerned that the government
is pursuing a launch-on-warning capability, in the mistaken belief that
computer technology can safely be entrusted with important decisions
regarding the release of nuclear weapons.  If this course is allowed to
continue unchecked, it is only a matter of time before a catastrophic error
occurs.''

			GROUPS IN SUPPORT

Though not an attorney, Johnson filed suit on his own behalf, and will argue
his own case through the resolution of government motions to dismiss the
suit, on which hearings are expected this fall.  However, he will need to
hire a lawyer if the case goes to trial, and the Lawyer's Alliance for
Nuclear Arms Control (LANAC) and the Center for Constitutional Rights have
agreed to help at the appellate level.

In addition to CPSR, support has come from peace groups and from former
aerospace engineer Robert Aldridge, coauthor of ``First Strike'' and
co-editor of ``The Nuclear Time Bomb,'' and constitutional scholar Arthur
Miller.

Johnson had filed a similar suit in 1984.  He lost in district court when
the judge ruled that it was a political matter, not for the judiciary to
decide.  His appeal was rejected, not by upholding the lower court's
reasoning, but by ruling that since he then claimed only that the government
had a launch-on-warning capability, not necessarily a launch-on-warning
policy, the unused capability was not a threat over which he could sue.

Johnson's current suit includes sensitive information he had deliberately
excluded from his earlier suit, such as evidence that the Strategic Air
Command possesses the authorization codes needed to launch a nuclear attack.

``I've gone back, I've done my homework, I say we've got launch-on-warning
now and I'm prepared to prove it,'' said Johnson.  ``We're at peace, so why
risk my neck?''

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.39.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.41.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-27</DOCNO>
<DOCOLDNO>IA012-000123-B022-71</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.41.html 128.240.150.127 19970217004228 text/html 18617
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:40:52 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 41</TITLE>
<LINK REL="Prev" HREF="/Risks/3.40.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.42.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.40.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.42.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 41</H1>
<H2> Saturday, 23 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
$1 million bogus bank deposit 
</A>
<DD>
<A HREF="#subj1.1">
Hal Perkins
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Cheating of automatic teller machines 
</A>
<DD>
<A HREF="#subj2.1">
Jacob Palme
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Simulation, Armored Combat Earthmover, and Stinger 
</A>
<DD>
<A HREF="#subj3.1">
Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Report from AAAI-86 
</A>
<DD>
<A HREF="#subj4.1">
Alan Wexelblat
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
$1 million bogus bank deposit
</A>
</H3>
<address>
Hal Perkins
&lt;<A HREF="mailto:hal@gvax.cs.cornell.edu ">
hal@gvax.cs.cornell.edu 
</A>&gt;
</address>
<i>
Fri, 22 Aug 86 21:47:58 EDT
</i><PRE>

From the Chicago Tribune, Friday, Aug. 15, 1986.  sec. 3, p. 3:

Bank machine is no match for schoolboy with a lollipop

  AUCKLAND, New Zealand [UPI] -- A schoolboy outsmarted an automatic
bank machine by using the cardboard from a lollipop packet to
transfer $1 million New Zealand dollars into his account, bank
spokesmen said Thursday.

  Tony Kunowski, corporate affairs manager of the United Building
Society savings and loans institution, said the 14-year-old student
slipped the cardboard into an envelope and inserted it into the machine
while punching in a deposit of $1 million, the U.S. equivalent of
$650,000.

  "We are not amused, but we don't think this is the tip of an
iceberg," he said of the incident of three weeks ago.

  Kunowski said that when the boy, identified only as Simon, checked
his account a few days later, he was amazed to discover the money had
been credited.  He withdrew $10.

  When no alarm bells rang and no police appeared, he withdrew another
$500.  But his nerve failed and he redeposited the money.

  On Tuesday, Simon withdrew $1,500, Kunowski said.

  But his nerve failed again Wednesday, and he told one of his teachers
at Selwyn College, Kunowski said.  The school's headmaster, Bob Ford,
took Simon to talk with United Building Society executives.

  Ford said Simon had not been considered one of his brightest pupils,
"at least until now."

  It was unknown if Simon would be disciplined.

  Kunowski told reporters that Simon succeeded because of delays in
reconciling transactions in automatic tellers around the country with
United's central computer system.

  "The delay in toting up the figures would normally be four weeks and
that was how a schoolboy could keep a fake million dollars in his
account without anyone batting an eyelid," he said.

  "We are now looking very closely at our internal systems.  Human
error may also be involved," Kunowski said.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
    Cheating of automatic teller machines
</A>
</H3>
<address>
&lt;<A HREF="mailto:       Jacob_Palme_QZ%QZCOM.MAILNET@MIT-MULTICS.ARPA">
       Jacob_Palme_QZ%QZCOM.MAILNET@MIT-MULTICS.ARPA
</A>&gt;
</address>
<i>
21 Aug 86 02:45 +0200
</i><PRE>

Several young people have cheated automatic teller machines from
one of the largest Swedish bank chains in a rather funny way.

You use the machines by inserting your plastic card in a slot, then punching
the amount you want and your password, and then the card comes out of one
slot, and the money out of another slot.

The cheaters took a badge belonging to a large guard company, which looked
very reassuring, and fastened it with double-sticky tape in front of the
slot through which money comes out. They then faded into the background and
waited until someone came to get money from the machine. The person who
wanted to use the machine put in his card, punched his code and amount, and
the machine started to push out the money through the slot. When the money
could not get out, because of the obstruction, the machine noted this, and
gave a "technical error" message to the customer, who went away. Up came the
youngsters, who took away the badge, fetched the money behind it, and put up
the badge again for the next customer.

The cheatings described above have been going on for several months, but the
bank has tried to keep this secret, claiming that if more people knew about,
more would try to cheat them.  Since the money is debited on the account of
the customers, this means that those customers who did not complain lost the
money. The bank has now been criticised for keeping this secret, and has
been forced to promise that they will find all customers cheated (this is
possible because the temporary failure in getting the money out of the slot
was noted automatically by the machine) and refund the money lost.

The bank chain will now have to rebuild 700 automatic dispensing machines.
Most other banks in Sweden, except this chain, have a joint company
operating another kind of dispensing machines, from which you can take out
money from your account in any of these banks. Their dispensing machines
cannot be cheated in this way, because they have a steel door in front of
the machine which does not open until you insert a valid plastic card.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Simulation, Armored Combat Earthmover, and Stinger
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Fri, 22 Aug 1986  08:53 EDT
</i><PRE>

    From: Mary C. Akers &lt;makers at cct.bbn.com&gt;

    ... the main bone of contention among those concerned with weapons
    design and testing [is] whether computer simulation and modeling can
    take the place of live trials with real equipment.

The reason people want to do simulation testing is that they then don't have
to do real testing, for whatever reason.  Real testing is expensive and
time-consuming, and the very people who say that they want real testing are
often those who say that the weapons development process is too slow.

No one would argue that simulation testing is a bad thing in and of itself.
It is when you REPLACE real testing with simulation, rather than SUPPLEMENT
real testing, that you run into problems with validity.

     The Army's Armored Combat Earthmover (ACE) - "...which underwent
     18,000 hours of testing without ever being operated under field
     conditions.  [When it finally under went live trails at Fort Hood]
     ...the tests revealed that the ACE's transmission cracked, that its
     muffler caught fire, that the driver's hatch lid was too heavy to lift, 
     and that doing certain maintenance work "could endanger the operator's
     life."

It strengthens the point of the article to note that the 18,000 hours
of testing described was probably not simulation testing, but rather
developmental testing.  But that is the point of operational testing
(OT) -- to place it into a real life operational environment and see what
problems there are.  You EXPECT things to go wrong in OT. 

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Report from AAAI-86    [Really from Alan Wexelblat]
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
Fri, 22 Aug 86 13:05:57 CDT
</i><PRE>

I just got back from a week at AAAI-86.  One thing that might interest
RISKS readers was the booth run by Computer Professionals for Social
Responsibility (CPSR).  They were engaged in a valiant  (but ineffectual)
effort to get the AI mad-scientist types to realize what some of their
systems are going to be doing (guiding tanks, cruise missiles, etc.).

They were handing out some interesting stuff, including stickers that said
(superimposed over a mushroom cloud):  "It's 11 p.m.  Do you know what your
expert system just inferred?"

They also had a series of question-answer cards titled "It's Not Trivial."
Some of them deal with things that have come up in RISKS before.  [I left
them in for the sake of our newer readers.  PGN]    They are:

Q1:  How often do attempts to remove program errors in fact introduce one
	or more additional errors?

A1:  The probability of such an occurance varies, but estimates range from
	15 to 50 percent (E.N. Adams, "Optimizing Preventing Service of
	Software Products," _IBM Journal of Research and Development_,
	Volume 28(1), January 1984, page 8)

Q2:  True or False:  Experience with large control programs (100,000 &lt; x &lt;
	2,000,000 lines) suggests that the chance of introducing a severe
	error during the correction of original errors is large enough that
	only a small fraction of the original errors should be corrected.

A2:  True. (Adams, page 12)

Q3:  What percentage of federal support for academic Computer Science
	research is funded through the Department of Defense?

A3:  About 60% in 1984.  (Clark Thompson, "Federal Support of Academic
	Research in Computer Science," Computer Science Division, University
	of California, Berkeley, 1984)

Q4:  What fraction of the U.S. science budget is devoted to defense-related
	R&amp;D in the Reagan 1985/86 budget?

A4:  72%  ("Science and the Citizen,"  _Scientific American_ 252:6 (June
	1985), page 64)

Q5:  The Space Shuttle Ground Processing System, with over 1/2 million lines
	of code, is one of the largest real-time systems ever developed.
	The stable release version underwent 2177 hours of simulation
	testing and the 280 hours of actual use during the third shuttle
	mission.  How many critical, major, and minor errors were found
	during testing?  During the mission?

A5:  		Critical	Major	Minor
     Testing	   3		  76	 128
     Mission	   1		   3	  20
	(Misra, "Software Reliability Analysis," _IBM Sys. J. 1983, 22(3) )

Q6:  How large would "Star Wars" software be?

A6:  6 to 10 million lines of code, or 12 to 20 times the size of the Space
	Shuttle Ground Processing System.  (Fletcher Report, Part 5, page 45)

The World Wide Military Command and Control System (WWMCCS) is used by
civilian and military authorities to communicate with U.S. military forces
in the field.

Q7:  In November 1978, a power failure interrupted communications between
	WWMCCS computers in Washington, D.C. and Florida.  When power was
	restored, the Washington computer was unable to reconnect to the
	Florida computer.  Why?

A7:  No one had anticipated a need for the same computer (ie the one in
	Washington) to sign on twice.  Human operators had to find a way to
	bypass normal operating procedures before being able to restore
	communications.  (William Broad, "Computers and the U.S. Military
	Don't Mix," _Science_ Volume 207, 14 March 1980, page 1183)

Q8:  During a 1977 exercise in which WWMCCS was connected to the command and
	control systems of several regional American commands, what was the
	average success rate in message transmission?

A8:  38%  (Broad, page 1184)

Q9:  How much will the average American household spend in taxes on the
	military alone in the coming year?

A9:  $3,400 (Guide to the Military Budget, SANE)

[question 10 is unrelated to RISKS]

Q11: True or False?  Computer programs prepared independently from the same
	specification will fail independently.

A11: False.  In one experiment, 27 independently-prepared versions, each
	with reliability of more than 99%, were subjected to one million
	test cases.  There were over 500 instances of two versions failing
	on the same test case.  There were two test cases in which 8 of the
	27 versions failed.  (Knight, Leveson and StJean, "A Large-Scale
	Experiment in N-Version Programming,"  Fault-Tolerant Computing
	Systems Conference 15)

Q12: How, in a quintuply-redundant computer system, did a software error
	cause the first Space Shuttle mission to be delayed 24 hours only
	minutes before launch?

A12: The error affected the synchronization initialization among the 5
	computers.  It was a 1-in-67 probability involving a queue that
	wasn't empty when it should have been and the modeling of past
	and future time.  (J.R. Garman, "The Bug Heard 'Round the World,"
	_Software Engineering Notes_ Volume 6 #5, October 1981, pages 3-10)

Q13: How did a programming punctuation error lead to the loss of a Mariner
	probe to Venus?

A13: In a FORTRAN program, DO 3 I = 1,3 was mistyped as DO 3 I = 1.3 which
	was accepted by the compiler as assigning 1.3 to the variable DO3I.
	(_Annals of the History of Computing_, 1984, 6(1), page 6)

Q14: Why did the splashdown of the Gemini V orbiter miss its landing point
	by 100 miles?

A14: Because its guidance program ignored the motion of the earth around
	the sun. (Joseph Fox, _Software and its Development_, Prentice Hall,
	1982, pages 187-188)

[Questions 15-17 are not RISKS related]

Q18: True or False?  The rising of the moon was once interpreted by the
	Ballistic Missile Early Warning System as a missile attack on the US.

A18: True, in 1960.  (J.C. Licklider, "Underestimates and Overexpectations,"
	in _ABM: An Evaluation of the Decision to Deploy and Anti-Ballistic
	Missile_, Abram Chayes and Jerome Wiesner (eds), Harper and Row,
	1969, pages 122-123)

[question 19 is about the 1980 Arpanet collapse, which RISKS has discussed]

Q20: How did the Vancouver Stock Exchange index gain 574.081 points while
	the stock prices were unchanged?

A20: The stock index was calculated to four decimal places, but truncated
	(not rounded) to three.  It was recomputed with each trade, some
	3000 each day.  The result was a loss of an index point a day, or
	20 points a month.  On Friday, November 25, 1983, the index stood
	at 524.811.  After incorporating three weeks of work for consultants
	from Toronto and California computing the proper corrections for 22
	months of compounded error, the index began Monday morning at
	1098.892, up 574.081.  (Toronto Star, 29 November 1983)

Q21: How did a programming error cause the calculated ability of five
	nuclear reactors to withstand earthquakes to be overestimated, and
	the plants to be shut down temporarily?

A21: A program used in their design used an arithmetic sum of variables when
	it should have used the sum of their absolute values.  (Evars Witt,
	"The Little Computer and the Big Problem,"  AP Newswire, 16 March
	1979.  See also Peter Neumann, "An Editorial on Software Correctness
	and the Social Process,"  _Software Engineering Notes_, Volume 4(2),
	April 1979, page 3)

Q22: The U.S. spy ship Liberty was attacked in Israeli waters on June 8,
	1967.  Why was it there in spite of repeated orders from the U.S.
	Navy to withdraw?

A22: In what a Congressional committee later called "one of the most
	incredible failures of communications on the history of the
	Department of Defense," none of the three warnings sent by three
	different communications media ever reached the Liberty.  (James
	Bamford, _The Puzzle Palace_, Penguin Books, 1983, page 283)

Q23: AEGIS is a battle management system designed to track hundreds of
	airborne objects in a 300 km radius and allocate weapons sufficient
	to destroy about 20 targets within the range of its defensive
	missiles.  In its first operational test in April 1983, it was
	presented with a threat much smaller than its design limit:  there
	were never more than three targets presented simultaneously.  What
	were the results?

A23: AEGIS failed to shoot down six out of seventeen targets due to system
	failures later associated with faulty software.  (Admiral James
	Watkins, Chief of Naval Operations and Vice Admiral Robert Walters,
	Deputy Chief of Naval Operations.  Department of Defense
	Authorization for Appropriations for FY 1985.  Hearings before the
	Senate Committee on Armed Services, pages 4337 and 4379.)

Well, this message is long enough; I'll hold off on my personal commentaries.
People wanting more information can either check this sources given or
contact CPSR at P.O. Box 717, Palo Alto, CA  94301.

--Alan Wexelblat
ARPA: WEX@MCC.ARPA or WEX@MCC.COM
UUCP: {ihnp4, seismo, harvard, gatech, pyramid}!ut-sally!im4u!milano!wex

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.40.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.42.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-28</DOCNO>
<DOCOLDNO>IA012-000123-B022-94</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.42.html 128.240.150.127 19970217004241 text/html 17328
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:41:10 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 42</TITLE>
<LINK REL="Prev" HREF="/Risks/3.41.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.43.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.41.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.43.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 42</H1>
<H2> Monday, 25 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Re: $1 million bogus bank deposit 
</A>
<DD>
<A HREF="#subj1.1">
Barry Shein
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Sometimes things go right 
</A>
<DD>
<A HREF="#subj2.1">
Matt Bishop
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: Cheating of automatic teller machines 
</A>
<DD>
<A HREF="#subj3.1">
Dave Farber
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Keystroke Analysis for Authentication 
</A>
<DD>
<A HREF="#subj4.1">
rclex
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Computer Vote Counting In the News -- More 
</A>
<DD>
<A HREF="#subj5.1">
John Woods
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Re: $1 million bogus bank deposit
</A>
</H3>
<address>
Barry Shein 
&lt;<A HREF="mailto:bzs@BU-CS.BU.EDU">
bzs@BU-CS.BU.EDU
</A>&gt;
</address>
<i>
Sat, 23 Aug 86 20:14:53 EDT
</i><PRE>

   &gt;  "We are now looking very closely at our internal systems.  Human
   &gt;error may also be involved," Kunowski said.

There's that term "human error" again.  Note Chernobyl, TMI, etc.  They
also seemed to like to speak of "human error".

Is this a new form of excuse?  Is it supposed to have PR value?
What else? Alien-life-form error? Supernatural error?

I know most of you agree with me, and this is essentially trite.
I am just starting to sensitize badly to this techno-speak.

	-Barry Shein, Boston University

    [I have commented on this on various occasions.  Many of the problems
     that we find are deeper sorts of "human error" -- the requirements
     are established badly (the DIVAD?), the design is flawed (Challenger
     booster rockets), the implementation is faulty (the first Shuttle 
     launch), the patch was put in wrong (Viking), the system permits 
     operation in an unsafe mode (Sheffield), etc.  Those are clearly human
     errors, but they get treated in the opposite way -- not treated as
     human errors, but rather disanthropomorphized as "computer errors"!
     What you are saying is both essentially trite and very deep, both at
     the same time.  PGN]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Sometimes things go right
</A>
</H3>
<address>
Matt Bishop 
&lt;<A HREF="mailto:mab@riacs.ARPA">
mab@riacs.ARPA
</A>&gt;
</address>
<i>

</i><PRE>
Date: Mon, 25 Aug 86 08:19:14 -0700

All these letters about ATM's being outsmarted reminds me of an incident
where someone gambled on the inability of a bank to change the programming
for managing ATM's, and lost.  This incident is described in Donn Parker's
book on computer crime, which I seem to have left at home (so I can't give a
reference), and it's interesting because it shows the risks in assuming
things can't be done quickly.

In Japan, someone kidnapped a little girl, and told her father to open an
account at a bank which had ATM's throughout Tokyo, and put the ransom in
that account.  He was then to indicate the account number and password (in
the newspaper via what Sherlock Holmes would call the agony column, I
guess). The kidnapper would then withdraw the money from one of the ATMs.
He figured there weren't enough police to watch all the ATMs and even if
there were, they would have no way of distinguishing him from any of the
other patrons who made legitimate withdrawals.

Unfortunately for him, when the bank heard about this, they got
several programmers together and working all night they changed the
program controlling the ATMs to trap any transactions for that
particular account, and immediately notify the operators at which ATM
the withdrawal was taking place.  They then put police at as many ATMs
as they could.  The father made the deposit, the kidnapper withdrew
the money, and before he could get out of the ATM booth the police
grabbed him.  The girl was recovered safely.  The programmers got a
medal.  The kidnapper went to jail.

Kind of nice to know that sometimes things do go wrong for the better!

Matt Bishop

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: Cheating of automatic teller machines 
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Sat, 23 Aug 86 17:01:38 -0400
From: Dave Farber &lt;farber@huey.udel.EDU&gt;

That's the modern analog to the favorite telephone trick, stuff cotton [or
chewing gum] up the coin return, and come back latter to collect the coin
returns.  (It's harder to do with the new pay phones, but not impossible.)

    [Yes, many of the current tricks are reincarnations of earlier ones.
     But, as we get higher-tech, new tricks are emerging as well.  PGN]     

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Keystroke Analysis for Authentication (Re: <A HREF="/Risks/3.31.html">RISKS-3.31</A>)
</A>
</H3>
<address>
&lt;<A HREF="mailto:hplabs!caip!harvard!rclex!cdx39!jc@ucbvax.Berkeley.EDU">
hplabs!caip!harvard!rclex!cdx39!jc@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Wed, 20 Aug 86 10:07:37 edt
</i><PRE>
To: rclex!SRI-CSL.ARPA!RISKS

&gt;                                            ...  One gray area is checking
&gt; the match between credentials and credential-holders:  this generally has
&gt; to be done by humans unless the credentials are something like retinagrams.

Actually, this is easier to automate than most people would guess.

A few years back, I saw a demo of one solution, which is as accurate as
retinagrams, but is non-invasive.  This was the measurement of a "typing
profile" as a person typed something (it didn't much matter what) on a
keyboard that recorded and reported microsecond-precision timing info on
keystrokes.

The idea was to make a list of the most common 2-character pairs (th, he,
st, se, ...), calculate ratios of the top entries (th/he, he/st, th/st,
...), and normalize by dividing throughout by the mean value of the most
common pairs.  The resulting histogram turns out to be quite as specific as
retinagrams and fingerprints, and even harder to counterfeit.

Since then, I've been watching for applications, and have found instead that
most people 1) have never heard of it, and 2) don't believe that it works.
The people doing the demo weren't very concerned about either of these
"problems".  After all, only the ones making the decision to install it need
know about it; it's better if the subject not know or understand the
security system.  As for the second point, it doesn't really matter whether
the subject believe in it; it works regardless.

It's surprising how short a message it works with.  Obviously, you need at
least 3 characters; it turns out that you don't need more than about 10.  Of
course, there are failures.  But from a security viewpoint, they are in the
right direction of labeling a person as "unknown", typically when they are
typing irregularly due to fatigue or drugs.

The demo system had no sign-on.  You just started typing commands; the
machine determined for each command who had typed it and whether the person
was authorized to do what was asked.  In particular, they liked to show an
operator's console sitting in a non-secure area.  The machine would obey
commands typed by authorized operators, but not by anyone else.  It was
rather cute.  A lot of people who tried using it got very nervous looks on
their faces.  "The machine really does know who I am, doesn't it?"

Of course, you couldn't use this approach with just any commercial 
terminal.  How could you get the timing figures out of a VT100, 
for example?  But the data collection is well within the capabilities 
of the typical intelligent terminal with an 8-bit micro as a controller.

I've occasionally wondered whether there are any other non-invasive
identification techniques that are anywhere nearly as effective as
this one.  I haven't heard of any.  But then, they might not be very
widely advertised if they do exist.

I've also wondered about the feasibility of using this a a "user
friendliness" feature.  Imagine not needing to sign on to a system;
you just walk up to any terminal and start typing commands....

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Computer Vote Counting In the News      [SOME NEW STUFF, SOME OLD]
</A>
</H3>
<address>
John Woods
&lt;<A HREF="mailto:jfw@EDDIE.MIT.EDU ">
jfw@EDDIE.MIT.EDU 
</A>&gt;
</address>
<i>
Sat, 23 Aug 86 21:13:24 EDT
</i><PRE>
    [SEE SUMMARY OF EVA WASKELL'S EARLIER TALK BY RON NEWMAN in <A HREF="/Risks/2.42.html">RISKS-2.42</A>]

Use of computers in elections raises security questions
Boston Globe, 23 August 1986, page 17
By Gregory Witcher, Globe Staff

   The computer programs that will be used to count the votes in elections
this fall accross the United States, including a quarter of the votes in
Massachusetts, are vulnerable to tampering and fraud, according to computer
specialists, researchers, science writers and attorneys.
   Although no case of computer fraud has been proved, specialists say a
large potential exists because of the lack of mandatory federal or state
security guidelines to prevent it.
   In addition, they say, there are no independent means of auditing
programs to verify they are working properly and most local election
officials lack the computer skills necessary to detect if computer
programs are secretly altered.
   "It's like a black box," says Eva Waskell, a Reston, Va., science
writer who helped organize a recent two-day conference at Boston
University on the potential of computer fraud in voting.  "Election
officials have no hard data to back their claims that these
vote-counting programs are counting accurately."
   Sixty-five percent of the votes cast by Americans in the 1984
presidential election were tabulated by computer systems, according to
the Federal Election Commission.  In next month's Massachusetts primary,
computer programs will be used to tally the votes in 26 percent of the
state's 351 election precincts, the Secretary of State's office says.
   Four of every five of those votes will be tallied by a vote-counting
program that has been challenged in cases now pending in state and
federal courts in Indiana, West Virginia, and Maryland.  In Indiana and
West Virginia, the company was accused of helping to rig elections.
   The program was developed by Business Records Corp., formerly
Computer Election Systems, a Berkeley, Calif., company that federal
election officials estimate produces more than half the computer voting
equipment used nationwide.  Company officials in Berkeley and Chicago
could not be reached for comment yesterday.
   John Cloonan, director of the elections division of the Massachusetts
Secretary of State's office, said there have been no instances of
computer fraud reported since Massachusetts first began using a
computer-assisted voting system in 1967.
   Computerized voting is now used in Massachusetts jurisdictions ranging in
size from Worcester, the state's second largest city with about 80,000
registered voters, to Avon, where there are 3,000 registered voters, Cloonan
said.
   Voters in Boston and in one-third of all Massachusetts communities
cast their ballots on mechanical lever-type machines.  The remaining
cities and towns use paper ballots.
   According to David Stutsman, who participated in the two-day seminar
at BU, a recount of the votes cast in Elkhart County, Ind., in November
1982 showed that the computer program had improperly printed the results
of one race in another, failed to count all the votes for one candidate
and counted 250 more votes than there were voters in a third race.
   Stutsman is an attorney representing eight candidates who challenged
the election results in lawsuits alleging that the vote counting was
"false and fraudulent."
   Stutsman contended that a computer programmer from the company changed
the computer program's instructions on election night, but without a system
to record changes made in the pgram and without election officials
knowledgable about how the program worked, "it was impossible to say how the
votes were counted and whether they were counted accurately or not."
   In another case presented at the conference, a review of 1984 election
results showed that President Reagan received 159 votes in the Trinity River
Bottom precinct, defeating challenger Walter Mondale by a 3 to 1 margin in
the Texas district inhabited only by squirrels, rabbits and fish.
   "The computer invented those numbers.  The numbers could not have
gone into the program but they came out," said Terry Elkins, a political
researcher in Dallas who studied the election results.  "No one lives
there, so the fish must have voted."
   Despite reports like these, others remain confident that computer voting
is not terribly vulnerable to fraud or error.  "The smoke far outweighs the
fires," William Kimberling, a federal elections administrator in Washington,
said.  Kimberling said that none of the allegations of fraud raised in the
legal challenges has been upheld in court.

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Words, words, words... 
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Mon, 25 Aug 1986  15:08 EDT
</i><PRE>
In-reply-to: Msg of 3 May 1986  13:06-EDT from mikemcl at nrl-csr (Mike McLaughlin)

    The point is that a person who believes something, however
    erroneously, and espouses and publicly supports that belief, is *not*
    lying.  These are complex times.  There are many matters about which
    reasonable persons, even reasonable scientists, may differ.  There is
    no point in saying that a person lied when that person was doing the
    best work possible based on the knowledge and belief available at the
    time.  

I'd like to believe this, but I think you leave out a major category
-- how are we to classify what could be called "deliberate ignorance"?
That is probably the most charitable label that one could give to the
call for SDI -- a system that will eliminate the threat of nuclear
ballistic missiles.  Some people (some of them on RISKS) have called
such statements merely "political rhetoric".  But when the call is for
defense of the entire population, and NO ONE in the scientific
community believes that it is possible to frustrate a deliberate
Soviet attack on the U.S. population, isn't that either lying (at
worst) or deliberate dumbness at best?
-------


<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.41.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.43.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-29</DOCNO>
<DOCOLDNO>IA012-000123-B022-121</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.43.html 128.240.150.127 19970217004253 text/html 16917
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:41:23 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 43</TITLE>
<LINK REL="Prev" HREF="/Risks/3.42.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.44.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.42.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.44.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 43</H1>
<H2> Tuesday, 26 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Comment on PGN's comment on human error 
</A>
<DD>
<A HREF="#subj1.1">
Nancy Leveson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Keystroke Analysis for Authentication 
</A>
<DD>
<A HREF="#subj2.1">
Scott E. Preece
</A><br>
<A HREF="#subj2.2">
 Eugene Miya
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Risks of Mechanical Engineering [More on O-Rings] 
</A>
<DD>
<A HREF="#subj3.1">
Martin Harriman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re:  Words, words, words... 
</A>
<DD>
<A HREF="#subj4.1">
Mike McLaughlin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Comments on paper desired 
</A>
<DD>
<A HREF="#subj5.1">
Herb Lin
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Comment on PGN's comment on human error
</A>
</H3>
<address>
Nancy Leveson 
&lt;<A HREF="mailto:nancy@ICSD.UCI.EDU">
nancy@ICSD.UCI.EDU
</A>&gt;
</address>
<i>
26 Aug 86 11:52:32 PDT (Tue)
</i><PRE>

Both "human error" and "computer error" are meaningless words.  At least in
scientific discussions, we should attempt to use words that can be defined.
There are not "deeper" human errors (or "shallower" ones?).  There are
design flaws or inadequacies, operational errors, hardware "random"
(wear-out?) failures, management errors, etc.  In hardware, there are also
production errors.  These may not be good categories, and I welcome
suggestions for better ones.  But if we can categorize, then it may help us
to understand the issues involved in risks (by locating general themes) and
to devise fixes for them.

But in doing this we must be very careful.  Accident causes are almost
always multifactorial.  TMI, for example, involved all of the above
categories of errors including several hardware failures, operator errors,
management errors, and design flaws.  Challenger also appears to follow the
same trend. [I mention these two because they are both accidents which
involved extensive investigation into the causes].  According to my friends
in System Safety Engineering, this is true for ALL major accidents.  As I
have mentioned earlier in this forum, liability plays a major role in
attempts to ascribe accidents to single causes (usually involving operator
errors because they cannot be sued for billions).  Also, the nature of the
mass media, such as newspapers, is to simplify.  This is one of the dangers
of just quoting newspaper articles about computer-related incidents.  When
one reads accident investigation reports by government agencies, the picture
is always much more complicated. Trying to simplify and ascribe accidents to
one cause will ALWAYS be misleading.  Worse, it leads us to think that by
eliminating one cause, we have then done everything necessary to eliminate
accidents (e.g. train the operators better, replace the operators by
computers, etc.).

But even though it is difficult to ascribe a "cause" to a single factor, it
is possible to describe the involvement of the computer in the incident, and
this is what we should be doing.  We also need to understand more fully the
"system" nature of accidents and apply "system" approaches to preventing
them.  If accidents are caused by the interaction of several types of errors
and failures in different parts of the system, then it seems reasonable that
attempts to prevent accidents will require investigation into and
understanding of these interactions along with attempts to eliminate
individual problems.  Elsewhere I have given examples of serious
computer-related accidents that have occurred in situations where the
software worked "correctly" (by all current definitions of software
correctness) but where the computer software was one of the major factors
("causes") in the accident.

Since I specialize in software safety, I interact with a large number of 
companies and industries (aerospace, defense, medicine, nuclear power, etc.)
concerned with this problem.  The most successful efforts I have seen have 
involved companies where the software group and engineers have worked together. 
Unfortunately, this is rare.  The majority of the people who come to my talks 
and classes and with whom I work are engineers.  The software personnel 
usually argue that: 

   (1) safety is a system problem (not a software problem) and thus is 
       the province of the system engineer.  They are too busy doing their 
       own work developing software to participate in system safety meetings 
       and design reviews.

   (2) they already use good software engineering practices and therefore
       are doing everything necessary to make the software safe.
       
  I.e., "leave me alone and let me get back to my job of producing
  code, and don't waste my time by making me attend meetings with
  the system engineers.  They can do their job, and I'll do mine."

Unfortunately, almost all of the techniques that appear to be useful
in producing safer computer-controlled systems require the involvement
of the software designers and implementers.

      Nancy Leveson
      Information and Computer Science Dept.
      University of California, Irvine  

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Keystroke Analysis for Authentication (<A HREF="/Risks/3.42.html">RISKS-3.42</A>)
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%ccvaxa@GSWD-VMS.ARPA">
preece%ccvaxa@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Tue, 26 Aug 86 08:59:00 cdt
</i><PRE>

I would think this would only be safe if you had physical security
for the terminal -- otherwise the determined break-in artist could
record the appropriate sequences and play them back as desired.
Of course, if you allow that kind of intrusion any kind of password
scheme is also hopeless.

scott preece, gould/csd - urbana     uucp:	ihnp4!uiucdcs!ccvaxa!preece

</PRE>
<HR><H3><A NAME="subj2.2">
Re: Keystroke Analysis for Authentication (Re: <A HREF="/Risks/3.31.html">RISKS-3.31</A>)
</A>
</H3>
<address>
Eugene Miya
&lt;<A HREF="mailto:eugene@AMES-NAS.ARPA ">
eugene@AMES-NAS.ARPA 
</A>&gt;
</address>
<i>
Tue, 26 Aug 86 15:02:33 pdt
</i><PRE>

We just had a demostration of the keystroke authentication system
by Dr. John David Garcia.  To clarify a couple of things.  The
shortest realistic name should be 5 characters (Ed Ng).  10 characters
is better.  The system uses a statistical distance function and is based on
the old idea of telegraph key signatures.  It is not just a matter of
starting to type.  A user must do between 70-80 trials to train a
system to recognize a signature.  A lower figure is used for touch
typists.  Non-typists can be recognized with a sort of relaxation
phenomena when they adapt to using the system: users (believe it or not
go into "an alpha state" [not my quote]) have to relax in order to consistency
log in.  It seems other benefits or problems result: any significant
quantity of alcohol or other drug affects timing: three drinks and you
can't log in [good and bad].  The mechanism for determining timing
is not for general purpose typing, only particular strings.  This also
brings up the fact that some times you don't always log in on the first try.
Garcia is speaking to various Government agencies and computer manufacturers
about this system, but it would not be appropriate to say whom.
Signatures tend to be keyboard specific, so trials are required for different
keyboards.  Despite these draw backs, the system appears quite nice.
It does not require "microsecond timing," 60 Hz wall clock timing is
adequate.  There is probably be a demostration of the system at the next
Compcon in San Francisco.  The demo we saw was running on a Compauq written
in BASIC with a couple of assembly language kernels.

--eugene miya;   NASA Ames Research Center;   eugene@ames-aurora.ARPA
  {hplabs,hao,dual,ihnp4,decwrl,allegra,tektronix,menlo70}!ames!aurora!eugene

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
 Risks of Mechanical Engineering [More on O-Rings]
</A>
</H3>
<address>
    Martin Harriman 
&lt;<A HREF="mailto:MARTIN%SRUCAD%sc.intel.com@CSNET-RELAY.ARPA">
MARTIN%SRUCAD%sc.intel.com@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Fri, 22 Aug 86 10:40 PDT
</i><PRE>

O-rings are used in many applications where a reliable gas or liquid seal
is desired; they are generally the most reliable method for sealing a
joint that must be disassembled periodically.  There are lots of interesting
failure mechanisms (interesting if you are a mechanical engineer), but
I doubt any of them involve computers, except in the most peripheral fashion.

O-ring failures in automobiles are usually the result of hardening, either
due to chemical attack (usually methanol in gasohol), or heat.

The recent failures (NASA, Chernobyl, TVA) don't have a lot to do with
computers, per se--I claim each of these cases were due to poor management.
In NASA's case, we have the spectacle of NASA management ignoring engineering
concerns because of the pressure to launch.  So NASA will listen to the WCTU
(who convinced NASA to abandon their plans to include wine in Skylab's
rations), but won't listen to Morton Thiokol's engineers.

The Chernobyl accident was evidently the result of the local operators (and
management?) ignoring the procedures in the operating manual; the Soviets
claim that the local folks weren't supposed to have that much autonomy.  The
operators will take the rap--but the Soviet central management is
responsible for not doing a better job of supervising (and motivating?) the
local site people.

Right now, most of TVA's nuclear capacity is shut down; it seems that their
plants don't match their documentation, due to unrecorded (and perhaps
unauthorized) modifications during construction.  Since this problem
(at least at this magnitude) is unique to TVA, it seems that the fault
was management's attitude towards the importance of this documentation.
At least the NRC seems to think so, since a management reshuffle was one of
their conditions for relicensing the TVA reactors.

No one's mentioned the earlier famous O-ring/management failure (so I have
to, of course--):  the triple engine failure on a 727.  In this case,
the ever-so-reliable O-rings failed because they were omitted from a
maintenance kit--so they didn't get installed, so they didn't seal the
engine chip detectors, so all the oil ran out of the engine, so all
three engines failed en-route (over the ocean).  One restarted (it
still had a quart or so left), and the aircraft made it back to Miami.
The NTSB decided the problem was inadequate training and supervision;
the procedures for changing the O-rings had been changed, but no one
told the mechanic, or checked his work (as they were required to do).

Hope this kills all further interest in O-rings--
  --Martin Harriman          Intel Santa Cruz &lt;martin@srucad.sc.intel.com&gt;

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re:  Words, words, words...
</A>
</H3>
<address>
Mike McLaughlin
&lt;<A HREF="mailto:mikemcl@nrl-csr ">
mikemcl@nrl-csr 
</A>&gt;
</address>
<i>
Tue, 26 Aug 86 15:25:49 edt
</i><PRE>
Cc: neumann@sri
      [Herb's message got appended after the end of <A HREF="/Risks/3.42.html">RISKS-3.42</A>, and
       was not included in the Contents of that issue.  Sorry.  PGN]

"Deliberate dumbness" is delightful.  Reminds me of a friend's term,
"malicious obedience," referring to carrying out dumb orders in their
infinite complexity, regardless of the consequences, and without applying a
grain of common sense.

Used car salesmen and realtors sometimes exhibit deliberate dumbness when 
they discourage the owner from telling them about defects in a property or
automobile.  

I do not know that "NO ONE in the scientific community believes that it is
possible to frustrate a deliberate Soviet attack on the U.S. population..."
If there is a PhD in a science who believes that, is that person de facto 
excluded from the scientific community?

I do not know what "frustrat(ing) a deliberate... attack" means.  If it means
deterring the attack by reducing the cost/benefit ratio to an unacceptable
level, I believe that is possible (but I am not in the scientific community
and never have been).

If it means saving a significant number of civilian lives from an inevitable
attack, I believe that is possible (but... ).

If it means saving EVERY civilian life, I do not believe that, any more than
I believe the statement that "NO ONE... etc."

SDI involves more than science, it affects billions of people, millions of
military and defense industry people, and thousands of decisions makers on
both sides of the Curtain.  As such, it is not susceptible to the simple 
and elegant solutions of science - neither "It won't work" nor "It will work"
is adequate.  

I have five children.  I hope we, and the Russians, get it right, whatever
we decide to do.  

"Things are the way they are because if they were to be any different they
 wouldn't have come out like this." - Tevye (Sholom Aleichem)

	- Mike	&lt;mikemcl@nrl-csr.arpa&gt;

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Comments on paper desired
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Tue, 26 Aug 1986  19:42 EDT
</i><PRE>

I am currently writing a paper entitled COMPTER SOFTWARE AND STRATEGIC
DEFENSE, which should be available in preliminary draft form on August
29, Friday.  Comments are solicited by September 15.  It is too big to
mail, so FTP is the solution.  If you want to see a copy (in exchange
for a promise to make comments on it), please drop me a note.  A brief
abstract follows:

  Computer software will be an integral part of any strategic defense
  system (defined here to include BMD, ASAT, and air defense).  Several
  issues are addressed: The reliability of SDI software, the problem of
  system architecture, the problems that very short defensive time lines
  may introduce, the risk for accidental nuclear war, mechanisms for
  escalation control.  

Thanks.  Herb

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.42.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.44.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-30</DOCNO>
<DOCOLDNO>IA012-000123-B022-146</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.44.html 128.240.150.127 19970217004307 text/html 14377
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:41:35 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 44</TITLE>
<LINK REL="Prev" HREF="/Risks/3.43.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.45.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.43.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.45.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 44</H1>
<H2> Wednesday, 14 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
F-16 Problems 
</A>
<DD>
<A HREF="#subj1.1">
Bill Janssen
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Various clips from European Newspapers 
</A>
<DD>
<A HREF="#subj2.1">
Martin Minow
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Comment on Nancy Leveson's comment on... 
</A>
<DD>
<A HREF="#subj3.1">
Alan Wexelblat
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Words, words, words... 
</A>
<DD>
<A HREF="#subj4.1">
Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Software Safety 
</A>
<DD>
<A HREF="#subj5.1">
Paul Anderson
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
F-16 Problems (from Usenet net.aviation)
</A>
</H3>
<address>
Bill Janssen 
&lt;<A HREF="mailto:janssen@mcc.com">
janssen@mcc.com
</A>&gt;
</address>
<i>
Wed, 27 Aug 86 14:31:45 CDT
</i><PRE>

A friend of mine who works for General Dynamics here in Ft. Worth wrote some
of the code for the F-16, and he is always telling me about some
neato-whiz-bang bug/feature they keep finding in the F-16:

o Since the F-16 is a fly-by-wire aircraft, the computer keeps the pilot from 
  doing dumb things to himself. So if the pilot jerks hard over on the 
  joystick, the computer will instruct the flight surfaces to make a nice and 
  easy 4 or 5 G flip. But the plane can withstand a much higher flip than that. 
  So when they were 'flying' the F-16 in simulation over the equator, the 
  computer got confused and instantly flipped the plane over, killing the 
  pilot [in simulation].  And since it can fly forever upside down, it would
  do so until it ran out of fuel.

(The remaining bugs were actually found while flying, rather than in 
simulation):

o One of the first things the Air Force test pilots tried on an early F-16 
  was to tell the computer to raise the landing gear while standing still on
  the runway. Guess what happened? Scratch one F-16. (my friend says there
  is a new subroutine in the code called 'wait_on_wheels' now...) [weight?]

o The computer system onboard has a weapons management system that will
  attempt to keep the plane flying level by dispersing weapons and empty
  fuel tanks in a balanced fashion. So if you ask to drop a bomb, the
  computer will figure out whether to drop a port or starboard bomb in order
  to keep the load even. One of the early problems with that was the fact
  that you could flip the plane over and the computer would gladly let you
  drop a bomb or fuel tank. It would drop, dent the wing, and then roll off.

There are some really remarkable things about the F-16. And some even more
remarkable things in the new F-16C and D models: 

o They are adding two movable vents called 'canards' that will be installed
  near the engine intake vent under where the pilot sits. By doing some
  fancy things with the flight surfaces and slick programming, they can get
  the F-16 to fly almost sideways through the air. Or flat turns (no
  banking!). Or fly level with the nose pointed 30 degrees down or up (handy 
  for firing the guns at the ground or other aircraft).

I figured this stuff can't be too classified, since I heard the almost same
thing from two different people who work at GD. I hope the Feds don't get
too upset...

George Moore (gm@trsvax.UUCP)

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Various clips from European Newspapers
</A>
</H3>
<address>
&lt;<A HREF="mailto:minow%regent.DEC@decwrl.DEC.COM  ">
minow%regent.DEC@decwrl.DEC.COM  
</A>&gt;
</address>
<i>
27-Aug-1986 0835
</i><PRE>

From The [London] Guardian, Aug. 20-22 1986 (not sure of the exact date):

	Bank zaps `raid on computer'

Barclays Bank yesterday denied reports that computer experts had
"hacked" into its Whitehall computer and transferred 440,000 Lb.
Sterling to an overseas account. 

----

From Dagens Nyheter [Stockholm], Aug. 22, 1986.  My translation, abridged.

	Shock billing of private person
	Phone bill of 31,000 kronor [almost $2,600]

A woman in the Stockholm area received a record phone bill of 31,000
kronor. The amount is equivalent to local calls 24-hours per day for
nearly two years. 

The phone company's computers raised an alarm that the amount was
unreasonably high, but human error resulted in the bill being sent out
anyways.  The group that normally checks especially high invoices
never got to see this bill. 

The woman and the phone company have reached an agreement, whereby she
pays an average bill based on previous invoices.  Phone technicians
are now trying to discover whether an error occurred in the
computer-controlled phone exchange.  ...

"It's completely our fault," says phone company spokesman Kjell Palmqvist.

"What are you doing about it?" [asked the reporter.]

"First, we've come to an agreement with the woman.  She need not pay more
than normally.  We've also started an examination of what could have caused
the problem.... There could have been a problem in the computerized phone
exchange, or a cable-error or other type of interference."

"Is this sort of bill common?"

"No, theoretically, we expect one error in 10,000 years.  But no
technology is 100% perfect."  ...

The telephone exchange, in Oestermalm in Stockholm, uses an
AXE-exchange, a computerized telephone exchange [manufactured by LM
Ericsson] that is very advanced and reliable. 

----

From Dagens Nyheter [Stockholm], Aug. 22, 1986.  My translation, abridged.

		Battle over Databank

The chairman of the governmental data- and public-access committee
[offentlighetskommitt'en], Carl Axel Petri, rejects the criticisms which
have recently been brought by the moderate party [conservative] and
folk-party [liberal conservative] concerning sales of personal
information from computer data banks. 

   [Sweden has a "sunshine" law, almost 200 years old, that guarantees
    public access to almost all government documents.  As the information
    in the manual registers were considered public, so too is the same
    information in the computerised data bank.  Information which is not
    public is carefully	controlled.  Access is governed by the Swedish Data
    Law, which is now over 10 years old.]

"It is important to quickly get a law that stops general sales.  We
have allowed some exceptions, nine specified computer companies, but
even their sales shall, in the future, be controlled by parliament.
Nobody should be allowed to earn money by [selling] personal
information. Sales should have a public interest, in principle, the
new law will forbid sales" said Petri. ... 

The leader of the Moderate Party, Gunnar Hoekmark, says that Petri is
incorrect when he claims that the law will forbid sales of personal
information. 

"On the contrary," says Hoekmark, "the largest databases will continue
to be sold.  Without the committee's discussing what effect sales of
different personal information will have on individual personal
integrity, they propose that the largest database, Spar, may continue
to sell information on individuals income, personal identity number,
wealth, civil status, address, age, etc." 

Hoekmark points out that the majority [report?] of the inquiry didn't
answer the most basic questions on whether the government in general
shall have the right to sell information on private individuals'
economy and personal situation. 

The majority includes the Center Party's [liberal conservative] Olof
Johansson, who says that the important issue for the future isn't
whether the information ought to be sold, but what information should
be collected.  This includes, for example, the discussion on
limitations of use of the personal id number. 

Constitutional questions [the Sunshine Law is part of the Swedish
Constitution] and the future of the personal id number will remain for
the inquiry to solve by next spring. 

----

Sloppily translated by Martin Minow 

[Peter, I also have a long article on computer controlled airplanes
(fly by wire) from the Observer.  Mostly Sunday Paper background.
Too much to type in.  "... the pilot must have enough confidence
in the flight control computer, and the men who programmed its software,
to take off in an aircraft he cannot fly without them"  "there is
one more type of failure from which they [the pilots] cannot recover."]

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Comment on Nancy Leveson's comment on...
</A>
</H3>
<address>
Alan Wexelblat 
&lt;<A HREF="mailto:wex@mcc.com">
wex@mcc.com
</A>&gt;
</address>
<i>
Wed, 27 Aug 86 09:33:11 CDT
</i><PRE>

I agree in large part with Nancy Leveson's comments in <A HREF="/Risks/3.43.html">RISKS-3.43</A>.
Nevertheless, I find it interesting that she denies that there are "human
errors" but believes that there are "management errors."  It seems that the
latter is simply a subset of the former (at least, until we get computer
managers).  Also, it's not clear whether she includes things like `pushing
the wrong button' or `following the wrong procedure' under the category of
"operational errors."

--Alan Wexelblat	(WEX@MCC.COM)

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Words, words, words...
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Wed, 27 Aug 1986  15:05 EDT
</i><PRE>

    From: mikemcl at nrl-csr (Mike McLaughlin)

    I do not know that "NO ONE in the scientific community believes that it is
    possible to frustrate a deliberate Soviet attack on the U.S. population..."
    If there is a PhD in a science who believes that, is that person de facto 
    excluded from the scientific community?

I should have been more precise.  No person with technical credentials
has stated that it is possible to deny the Soviet Union the capability
to wreak significant damage on the U.S. population and industry.  

    I do not know what "frustrat[ing] a deliberate... attack" means.

    If it means deterring the attack by reducing the cost/benefit ratio to
    an unacceptable level, I believe that is possible (but I am not in the
    scientific community and never have been).

    If it means saving a significant number of civilian lives from an 
    inevitable attack, I believe that is possible (but... ).

I think the benchmark that Ashton Carter used in his Office of
Technology Assessment background paper on BMD was pretty good, and it
will serve as a starting point for discussion.  "Frustrate a
deliberate attack..." is taken to mean "preventing the Soviet Union
from delivering by ballistic missile 100 megatons of nuclear warhead
on U.S. cities and industry."  (Note well: WW II was a 5 MT war.)

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Software Safety
</A>
</H3>
<address>
Paul Anderson
&lt;<A HREF="mailto:anderson ">
anderson 
</A>&gt;
</address>
<i>
Wed, 27 Aug 86 09:43:03 edt
</i><PRE>

I have received a copy of a proposed revision of MIL-STD-882B (System Safety
Hazard Analysis) Task 212, Software Safety Analysis, that has been
distributed for formal coordination.  This task will be invoked on
contractors building systems containing software for DOD.  This task will
require the contractor to conduct safety analyses and testing of the
software, both on the software alone, and when integrated with the overall
system.

If anybody has thoughts, comments, or suggestions (or even recommended
wording), on what should be included in this task, please let me know
(preferably within the next week or so).

Paul Anderson
anderson@nrl-csr

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.43.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.45.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-31</DOCNO>
<DOCOLDNO>IA012-000123-B022-169</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.45.html 128.240.150.127 19970217004320 text/html 15916
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:41:48 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 45</TITLE>
<LINK REL="Prev" HREF="/Risks/3.44.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.46.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.44.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.46.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 45</H1>
<H2> Thursday, 28 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Nonviolent Resistor Destroys Aries Launch 
</A>
<DD>
<A HREF="#subj1.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Risks in the design of civil engineering projects 
</A>
<DD>
<A HREF="#subj2.1">
Annette Bauman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  ATMs 
</A>
<DD>
<A HREF="#subj3.1">
Lindsay F. Marshall
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: Typing Profiles 
</A>
<DD>
<A HREF="#subj4.1">
Lindsay F. Marshall
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Human errors prevail 
</A>
<DD>
<A HREF="#subj5.1">
Ken Dymond
</A><br>
<A HREF="#subj5.2">
 Nancy Leveson
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Nonviolent Resistor Destroys Aries Launch
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Thu 28 Aug 86 21:30:48-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

From SF Chronicle wire services, 28 Aug 1986: White Sands Missile Range NM

A rocket carrying a scientific payload for NASA was destroyed 50 seconds
after launch because its guidance system failed...  The loss of the $1.5
million rocket was caused by a mistake in the installation of a ... resistor
of the wrong size in the guidance system.  "It was an honest error", said
Warren Gurkin...  "This rocket has been a good rocket, and we continue to
have a lot of faith in it."  Saturday's flight was the 27th since the first
Aries was launched in 1973, and it was the third failure.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Risks in the design of civil engineering projects
</A>
</H3>
<address>
&lt;<A HREF="mailto:ABauman @ DDN1">
ABauman @ DDN1
</A>&gt;
</address>
<i>
28 Aug 86 06:40 EDT
</i><PRE>
To: risks @ csl.sri.com

Computer-Aided Engineering, Penton Publishing, Cleveland OH, April 1986 page 4:

"Impressive computer analysis, however, may tempt some engineers into
developing designs that barely exceed maximum expected operational loads.
In these cases there is no room for error, no allowance for slight
miscalculations, no tolerance for inaccuracy.  In engineering parlance, the
design is "close to the line".  The reasoning, of course, is that relatively
small safety factors are justified because computer analysis is so accurate.
     The major flaw in this logic, however, lies in the fact that the
initial mathematical model set up by the designer may itself contain gross
inaccuracies...  These errors are carried through the entire analysis by
thecomputer, of course, which uses the model as the sole basis for its
calculations...  And wrong answers are easily obsuured by flashy color
graphics and high-speed interactive displays.  In most cases, the engineer
must be extreamly familar with the design and the programs used in its
development to spot errors in results."  -John K. Krouse editor

Annette C. Bauman, DDN-PMO
Test &amp; Evaluation Branch, DDN Network software Test Director

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: ATMs
</A>
</H3>
<address>
"Lindsay F. Marshall" 
&lt;<A HREF="mailto:lindsay%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
lindsay%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Wed, 27 Aug 86 08:38:38 bst
</i><PRE>

&gt;....Their dispensing machines cannot be cheated in this way, because they have
&gt;a steel door in front of the machine which does not open until you insert a
&gt;valid plastic card.

People who swindle ATM's don't have cash cards?????

ATM swindle's don't seem to have caught on in the UK too much yet (at least
not that I've heard), but the new "vandal proof" phone boxes which have
special money compartments seem to be rather more vulnerable. I have heard
reports of people touring regions of the UK on a regular basis emptying
these phones.  Another interesting scam at the moment (which I presume has
swept the US long ago....) and which is not illegal is that of beating quiz
machines. Teams of 3 "experts" (sport, TV/film and general knowledge
usually) tour pubs and play the video quiz machines. These have money prizes
and they simply strip them of everything in them by answering all the
questions. Most landlords are now removing these games as they are losing
money.......  

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: Typing Profiles
</A>
</H3>
<address>
"Lindsay F. Marshall" 
&lt;<A HREF="mailto:lindsay%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
lindsay%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Wed, 27 Aug 86 08:29:32 bst
</i><PRE>

John Ellenby (of Grid systems) told me that they installed just such a thing
into an operating system they were building and used it to distinguish
between the various operators who used the console. The operators never
could work out how the system "knew" who they were. (I may say that I am not
totally convinced however - particularly in a non-keyboard oriented society
such as the UK where very few people can actually type properly.)

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Human errors prevail -- Comment on Nancy's Comment on ...
</A>
</H3>
<address>
"DYMOND, KEN" 
&lt;<A HREF="mailto:dymond@nbs-vms.ARPA">
dymond@nbs-vms.ARPA
</A>&gt;
</address>
<i>
28 Aug 86 14:11:00 EDT
</i><PRE>
To: "risks" &lt;risks@csl.sri.com&gt;

Nancy Leveson's comment (on PGN's comment on human error in <A HREF="/Risks/3.43.html">RISKS-3.43</A>)
makes some very good points.  We do need to discuss the terms we use to
describe the various ways systems fail if only because system safety and
especially software safety are fairly young fields.  And it seems natural
for practitioners of a science, young or not, to disagree on what they are
talking about.  (Recall the discussion a few years ago in SEN on what the
term "software engineering" meant and whether what software engineers did
was really engineering.)

But what scientists say in these discussions about science may not be
science, at least in the sense of experimental science -- it's more like
philosophy, especially when the talk is about "causes".  Aristotle, for one,
talked a lot about causes and categories.  When we are urged to constrain
our use of "cause" ("Trying to simplify and ascribe accidents to one cause
will ALWAYS be misleading.  Worse, it leads us to think that by eliminating
one cause, we have then done everything necessary to eliminate accidents
(e.g. train the operators better, replace the operators by computer,
etc.)"), we are being given a prescription, something value-laden.  (I don't
mean to imply that science is or should be value-free.)  The implication in
the prescription seems to be that we (those interested in software and
system safety) should avoid using "cause" in a certain way otherwise we are
in danger of seducing ourselves as well as everybody else not specifically
so interested (the public) into a dangerous (unsafe) way of thinking.

But a way of supplementing the philosophical or prescriptive bent to our
discussion about the fundamental words is to look at how other disciplines
use the same words.  For example structural engineers seem to be doing a lot
of thinking about what we would call safety.  They even say "Human error is
the major cause of structural failures." (Nowak and Carr, "Classification of
Human Errors," in Structural Safety Studies, American Society of Civil
Engineers, 1985.)  It may be that our discussions about the basic words we
use can be helped by consulting similar areas in more traditional types of
engineering.

There is another prescriptive aspect to the subject of constraining our
discourse as raised by Nancy, namely not admitting into that discourse
statements from certain sources.  ("Also, the nature of the mass media, such
as newspapers, is to simplify.  This is one of the dangers of just quoting
newspaper articles about computer-related incidents, When one reads accident
investigation reports by government agencies, the picture is always more
complicated.")  Our thinking about this prescription may also benefit from
looking at other engineering disciplines to see how they investigate and
report on failures and what criteria and categories (the jargon word is
"methodology") they use, implicitly or explicitly, in assigning causes to
failure.  "Over-simplified" might be the best adjective to describe some of
the contributions to RISKS from newspapers-- one doesn't know whether to
believe them or not.  A problem may arise when writers on safety start to
quote SEN and the safety material collected there, most of which is
previewed here on RISKS, as authoritative sources on computer and other
types of failures.  The question is whether SEN's credibility is being
lessened or the newspaper's enhanced by the one being the source for the
other.  Compare some of the newspaper stories reproduced on this list with
the lucidity and thoroughness of Garman's report on the "The 'Bug' Heard
'Round the World," (SEN, Oct. 1981).  That seems a model for a software
engineering analysis and report of a failure.  We might compare it to other
thorough engineering analyses of failures, say the various commissions'
reports on Three Mile Island or the NBS (no chauvinism intended) report on
the skywalk collapse at the Hyatt Regency in Kansas City.  (The report of
the Soviet government on Chernobyl will perhaps bear reading, too.)

If we evolve some kind of standard for analyzing and reporting system
failure, we'll be able to categorize the trustworthiness of newspaper and,
for that matter, any other failure reports so that their appearance on RISKS
will not necessarily count as an endorsement, either in our own minds or in
that of the public.
   
Ken Dymond, NBS

</PRE>
<HR><H3><A NAME="subj5.2">
Human errors prevail -- Comment on Alan Wexelblat's Comment on 
</A>
</H3>
<address>
Nancy Leveson 
&lt;<A HREF="mailto:nancy@ICSD.UCI.EDU">
nancy@ICSD.UCI.EDU
</A>&gt;
</address>
<i>
28 Aug 86 19:42:14 PDT (Thu)
</i><PRE>
   Nancy Leveson's... (ad infinitum?)      [but not quite yet ad nauseum!]

From Alan Wexelblat's comment on my comment on ... (<A HREF="/Risks/3.44.html">RISKS-3.44</A>):

    &gt;... she denies that there are "human errors" but believes that
    &gt;there are "management errors."  It seems that the latter is simply
    &gt;a subset of the former (at least until we get computer managers).

With some risk of belaboring a somewhat insignificant point, after reading
[Alan's message], it is clear to me that I did not make myself very clear.
So let me try again to make a more coherent statement.  I did not mean to
deny that there are human errors, in fact, the problem is that all "errors"
are human errors.

I divide the world of things that can go wrong into human errors and random
hardware failures (or "acts of God" in the words of the insurance
companies).  My real quibble is with the term "computer errors".  Since I do
not believe that computers can perform acts of volition (they tend to
slavishly and often frustratingly follow directions to my frequent chagrin),
erroneous actions on the part of computers must either stem from errors made
by programmers and/or software engineers (who, for the most part, are humans
despite rumors to the contrary) or from underlying hardware failures or a
combination of both.  I suppose we could also include operator errors such
as "pushing the wrong button" or "following the wrong procedure" as either
part of "computer errors" or as a separate category.  The point is that the
term "computer error" includes everything (or nothing depending on how you
want to argue) and the term "human error" includes most everything and
overlaps with most of the computer errors.  And the term "computer error" is
also misleading since to me (and apparently to others since they tend to
talk about human errors vs. computer errors and to imply that we will get
rid of human errors by replacing humans with computers) it seems to imply
some sort of volition on the part of the computer as if it were acting on
its own, without any human influence, to do these terrible things.

That is why I do not find the terms particularly useful in terms of
diagnosing the cause of accidents or devising preventative measures.  I was
just trying to suggest a breakdown of these terms into more useful
subcategories, not to deny that there are "human errors" (in fact, just the
opposite).  And in fact, to be useful, we probably need to further
understand and subdivide my four or five categories which included design
flaws, random hardware failures, operational errors, and management errors
(along with the possibility of including production or manufacturing errors
for hardware components).  Note that three out of the first four of these
are definitely human errors and manufacturing errors could be either
human-caused (most likely) or random.

Actually, I thought the part of my original comment that followed the 
quibbling about terms was much more interesting...

 Nancy Leveson
 ICS Dept.
 University of California, Irvine

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.44.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.46.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-32</DOCNO>
<DOCOLDNO>IA012-000123-B022-195</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.46.html 128.240.150.127 19970217004347 text/html 16020
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:42:09 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 46</TITLE>
<LINK REL="Prev" HREF="/Risks/3.45.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.47.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.45.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.47.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 46</H1>
<H2> Saturday, 30 August 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Human error 
</A>
<DD>
<A HREF="#subj1.1">
Nancy Leveson
</A><br>
<A HREF="#subj1.2">
 Lindsay F. Marshall
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Re: F-16 Tales 
</A>
<DD>
<A HREF="#subj2.1">
Earl Boebert
</A><br>
<A HREF="#subj2.2">
 Phil Ngai
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Correction to note about flight simulators 
</A>
<DD>
<A HREF="#subj3.1">
Martin Minow
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Supermarket grinds to a halt 
</A>
<DD>
<A HREF="#subj4.1">
David Sherman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Video processing 
</A>
<DD>
<A HREF="#subj5.1">
Guy Schafer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  ATMs 
</A>
<DD>
<A HREF="#subj6.1">
Jacob Palme
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Human error
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Fri, 29 Aug 1986  18:48 EDT
</i><PRE>

    From: Nancy Leveson &lt;nancy at ICSD.UCI.EDU&gt;

    ...  My real quibble is with the term "computer errors".  Since I do
    not believe that computers can perform acts of volition (they tend to
    slavishly and often frustratingly follow directions to my frequent 
    chagrin),..

While I agree with the above sentiment for the most part, it strikes me that
there is a sense in which it is not true.  Consider an situation in which a
very large computer program is used to control a very complex real-time
process.  Imagine that the designers are generally competent, but do not
understand very well certain aspects of the process or the environment in
which the process operates.  Something that they did not anticipate happens,
and the software performs an action that results in a disaster.  How are we
to describe this occurrence?

We can describe this as a design (and thus human) error.  But given
the circumstances, it is not implausible to argue that an "Act of God"
occurred.  It was a rare happening; we didn't understand it very well;
we could not predict that it would happen.  Is there a better
definition for "Act of God"?  While computers are in principle
deterministic, a sufficiently complex program operating in a highly
varied environment is for all practical purposes non-deterministic, in
that its behavior is unpredictable under some circumstances.

The problem with calling such an event an "Act of God" is that it
leaves open the door to using that kind of defense when it is entirely
unwarranted; I personally do not want to get away from the idea that
human beings are (or should be) in control of their computers.  I'm
only pointing out that sometimes they may not be, through no "fault"
of their own.  Maybe computers should be used only in situations that
we understand very well, and that are predictable.  My safety
instincts say that this should be true, but it's not clear that it is
practical.

    ... the term "computer error" is
    also misleading since to me ... it seems to imply
    some sort of volition on the part of the computer as if it were acting on
    its own, without any human influence, to do these terrible things.

While I agree it is misleading for the reasons I gave above, from the
standpoint of being a user trying to behave defensively (as a driver
drives defensively) it could be useful -- no user should approach a
computer system as though its behavior is predictable and/or sensible
under all circumstances, and in the absence of the system designers
and programmers, that is probably an adaptive strategy from an
evolutionary perspective.

</PRE>
<HR><H3><A NAME="subj1.2">
Re: Human Error
</A>
</H3>
<address>
"Lindsay F. Marshall" 
&lt;<A HREF="mailto:lindsay%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
lindsay%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Fri, 29 Aug 86 12:08:04 bst
</i><PRE>

Someone who has looked at the changing attitudes to "human error" as
against "mechanical failure" is Michael Lesk. He has been studying
reports of railway accidents in the UK to extract from them information
about the attitudes of the reporters and investigators towards the
causes of the accidents. I don't know if he has written this up anywhere
or not, nor do I know if he reads RISKS.  He is well worth talking to
about the subject however and has uncovered some exceedingly
interesting points.
                                  Lindsay F. Marshall

  [Will someone at Bell Labs who reads this please give Mike a nudge?  PGN]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
 Re: F-16 Tales
</A>
</H3>
<address>
&lt;<A HREF="mailto: Boebert@HI-MULTICS.ARPA">
 Boebert@HI-MULTICS.ARPA
</A>&gt;
</address>
<i>
Fri, 29 Aug 86 10:51 CDT
</i><PRE>
To:  Neumann@CSL.SRI.COM
ReSent-To: RISKS@CSL.SRI.COM

Weight on wheels is a basic sensor input that tells the flight program
whether or not the aircraft is airborne.  In advanced systems like the
F-16 its is probably confirmed by air data computer and inertial
platform inputs; in older systems, where the computer does just nav and
weapons delivery, it is the prime indicator.  It is therefore unlikely
in the extreme that this would be overlooked in a design or an ordnance
safety analysis ((weight_on_wheels = TRUE) &amp; (master_arm = TRUE) &amp;
(weapon_release = TRUE) is clearly an undesired state).  I am also
skeptical that the gear would be controlled by the flight computer, but
I am not familiar with the F-16 so cannot comment further.

</PRE>
<HR><H3><A NAME="subj2.2">
F-16 software
</A>
</H3>
<address>
Phil Ngai
&lt;<A HREF="mailto:amdcad!phil@decwrl.DEC.COM ">
amdcad!phil@decwrl.DEC.COM 
</A>&gt;
</address>
<i>
Fri, 29 Aug 86 19:57:30 pdt
</i><PRE>

It sounds very funny that the software would let you drop a bomb on the wing
while in inverted flight but is it really important to prevent this? Is it
worth the chance of introducing a new bug to fix this very minor problem? Is
it worth the chance of making the code too big to fit in memory? What is the
chance that a pilot would really make this mistake?

     [The probability is clearly NONZERO.  It is very dangerous to start
      making assumptions in programming about being able to leave out an
      exception condition simply because you think it cannot arise.  Such
      assumptions have a nasty habit of interacting with other assumptions
      or propagating.  PGN]

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Correction to note about flight simulators
</A>
</H3>
<address>
Martin Minow, DECtalk Engineering ML3-1/U47 223-9922
&lt;<A HREF="mailto:minow%regent.DEC@decwrl.DEC.COM  ">
minow%regent.DEC@decwrl.DEC.COM  
</A>&gt;
</address>
<i>
29-Aug-1986 1406
</i><PRE>

In a private mail exchange, Danny Cohen ("COHEN@B.ISI.EDU") was
kind enough to point out that I had mis-remembered my article
from Smithsonian where I claimed the article stated that a flight
instructor flew as a flight engineer on a commercial flight.

&gt; The plane encountered a wind-shear situation on take off. The
&gt; instructor, from his flight engineer's position, reminded the pilot
&gt; that the correct recovery for wind-shear is opposite to the correct
&gt; recovery for a stall (which has a similar appearance to the pilot)." 

According to Danny (I can't find my copy of this issue), the article does
not talk about anything being "opposite to the correct recovery for the stall."

I'm sorry for the confusion this might have caused anyone.  At least, I did
learn a lot about flying and recovery from dangerous conditions.  Danny did
ask me to clarify my purpose in submitting the article to RISKS -- whether
it was to show that computer-based simulators contribute to airline safety,
or to "highlight the risks in using computers for whatever purposes."  To
set the matter straight, it was to show that computer-based simulation is a
factor in increased airline safety, as it lets pilots learn about situations
that are either dangerous or unusual (or both) in real life.

Danny is still looking for pointers to accidents caused by computer-based
simulators.
                                           Martin

     [I don't mean to take a potshot at Martin, who has been a delightful
      contributor.  But PLEASE, all of you, if you see something that you 
      think is appropriate for RISKS, make a note of it at the time rather
      than subsequently half-remember it.  I keep a huge stack of old items
      next to my terminal just in case I have to dig back...  PGN]

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Supermarket grinds to a halt
</A>
</H3>
<address>
&lt;<A HREF="mailto:mnetor!lsuc!dave@seismo.CSS.GOV">
mnetor!lsuc!dave@seismo.CSS.GOV
</A>&gt;
</address>
<i>
Fri, 29 Aug 86 17:04:53 edt
</i><PRE>

Last week I went to our local Miracle Food Mart supermarket (in
northern Toronto) at 9 a.m. on a Sunday, when they were just opening.
They discovered that they couldn't get any of the cash registers
to work; something was down in the central system. So they had the
cashiers writing each number down on a pad of paper and totalling
them up by hand, which slowed checkout down to a crawl. After a
while, someone found a desk calculator with a paper tape, which made
things a bit faster.  When I left they had someone at the door warning
customers not to bother coming in because the terminals weren't working.

Obviously, this kind of thing can happen only where cash registers
are no longer cash registers but terminals connected to a central
system, which is becoming more and more the case.  I can't believe
MFM doesn't have some type of backup system, since they're a large
chain. My speculation is that someone wasn't prepared for the system
to be running on Sunday morning; supermarkets must be closed in
Ontario on Sundays, and the ones near us started opening only about
a month ago...

David Sherman, The Law Society of Upper Canada, Toronto   
{ ihnp4!utzoo  seismo!mnetor  utzoo  hcr  decvax!utcsri  } !lsuc!dave

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Video processing
</A>
</H3>
<address>
Guy Schafer
&lt;<A HREF="mailto:decwrl!amdcad!amdimage!prls!philabs!linus!axiom!gts@ucbvax.Berkeley.EDU ">
decwrl!amdcad!amdimage!prls!philabs!linus!axiom!gts@ucbvax.Berkeley.EDU 
</A>&gt;
</address>
<i>
Thu, 28 Aug 86 15:38:31 edt
</i><PRE>

Now that sophisticated hardware for capturing and altering video images
exists for even the modest IBM-PC (AT&amp;T's Truevision products), several
concerns arise:

Because images can be captured in real time (for less than $5000), and
it has been proven that at least one method exists for over-powering
('hi-jacking') a cable video broadcast, some program can be altered and
re-broadcast (with a delay equal to the video processing time).  This
could be especially dangerous if it is done to, say, 2 minutes of a
news broadcast or televised political proceedings.

Video post-processing can also be an effective means to control the behavior
of an individual by tapping directly into her cable coming into her house.
An appropriate stock tip given by a seemingly authentic Ruekeiser (sp?) might
cause a major stockholder to get on her phone to a broker with predictable
(and thus profitable) results.  We always knew a hacker with a PC had quite
a bit of power; if this hacker can alter someone's main source of
information (television broadcasts) he suddenly has quite a bit more.

Also, video tapes which are used as evidence in court can be changed without
simple means of detection.

Actors can be cheated out of royalties--especially in commercials where
post-processing 30 seconds of video could cost less than the royalties of
an often-repeated performance.  The features of the face can be "airbrushed"
or distinguishing marks can be added or removed (by software--e.g. TIPS by
AT&amp;T) and the actor told that someone else got the part.

Any comments?

	&gt;&lt; ...{ decvax!linus | seismo!harvard }!axiom!gts

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
    ATMs (<A HREF="/Risks/3.45.html">RISKS-3.45</A>)
</A>
</H3>
<address>
&lt;<A HREF="mailto:       Jacob_Palme_QZ%QZCOM.MAILNET@MIT-MULTICS.ARPA">
       Jacob_Palme_QZ%QZCOM.MAILNET@MIT-MULTICS.ARPA
</A>&gt;
</address>
<i>
30 Aug 86 16:57 +0200
</i><PRE>

&gt;&gt;..Their dispensing machines cannot be cheated in this way, because they have
&gt;&gt;a steel door in front of the machine which does not open until you insert a
&gt;&gt;valid plastic card.
&gt;
&gt;People who swindle ATM's don't have cash cards?????

If you have a legally obtained cash card, and insert it into the
machine, this act is immediately recorded, so that if the swindle
is detected, they can find out who did it.

If you have an illegally obtained cash card, you probably do not
know the password you have to input on the keyboard. When you
input the wrong password (or do not input any password at all),
the machine swallows the card, and you never get it back again.

At least that is the way the Swedish ATM's work.  

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.45.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.47.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-33</DOCNO>
<DOCOLDNO>IA012-000123-B022-212</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.47.html 128.240.150.127 19970217004401 text/html 18871
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:42:29 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 47</TITLE>
<LINK REL="Prev" HREF="/Risks/3.46.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.48.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.46.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.48.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 47</H1>
<H2> Monday, 1 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Flight Simulators Have Faults 
</A>
<DD>
<A HREF="#subj1.1">
Dave Benson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Re: QA on nuclear power plants, the shuttle, and beer 
</A>
<DD>
<A HREF="#subj2.1">
Henry Spencer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Acts of God vs. Acts of Man 
</A>
<DD>
<A HREF="#subj3.1">
Nancy Leveson -- two messages
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Computer Literacy 
</A>
<DD>
<A HREF="#subj4.1">
Mike McLaughlin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Another supermarket crash 
</A>
<DD>
<A HREF="#subj5.1">
Ted Lee
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  A supermarket does not grind to a halt 
</A>
<DD>
<A HREF="#subj6.1">
Brint Cooper
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
 Flight Simulators Have Faults
</A>
</H3>
<address>
Dave Benson 
&lt;<A HREF="mailto:benson%wsu.csnet@CSNET-RELAY.ARPA">
benson%wsu.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Sat, 30 Aug 86 23:08:47 pdt
</i><PRE>

I mentioned the F-16 RISKS contributions to my Software Engineering class
yesterday.  After class, one of the students told me the following story about
the B-1 Flight Simulator. The student had been employed over the summer to
work on that project, thus having first-hand knowledge of the incident.

Seems when a pilot attempts to loop the B-1 Flight Simulator that
the (simulated) sky disappears.  Why?  Well, the simulated aircraft
pitch angle was translated by the software into a visual image by
taking the trigonometric tangent somewhere in the code.  With the
simulated aircraft on its nose, the angle is 90 degrees and the
tangent routine just couldn't manage the infinities involved.  As I
understand the story, the monitors projecting the window view went blank.

Ah, me.  The B-1 is the first aircraft with the capability to loop?  Nope,
its been done for about 70 years now...  The B-1 Flight Simulator is the
first flight simulator with the capability to allow loops?  Nope, seems to
me I've played with a commercially available Apple IIe program in which a
capable player could loop the simulated Cessna 180.  $$ to donuts that
military flight simulators with all functionality in software have been
allowing simulated loops for many years now.

Dick Hamming said something to the effect that while physicists stand on one
another's shoulders, computer scientists stand on one another's toes.  At
least on the toes is better than this failure to do as well as a game
program...  Maybe software engineers dig one another's graves?

And this company wants to research Starwars software...  Jus' beam me up,
Scotty, there's no intelligent life here.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Re: QA on nuclear power plants, the shuttle, and beer
</A>
</H3>
<address>
&lt;<A HREF="mailto:decwrl!decvax!LOCAL!utzoo!henry@ucbvax.Berkeley.EDU">
decwrl!decvax!LOCAL!utzoo!henry@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Sun, 31 Aug 86 01:35:29 edt
</i><PRE>

Equipment failures and human errors are common enough in any human endeavor;
the question is not whether they happen, but whether they present actual or
potential risks of serious consequences.  In this context the lack of
publicity is not at all surprising: one form of serious consequence is public 
hysteria over insignificant trivia.  When an attempt to reach a vacationing
brewery programmer gets blown up into stories of a total production shutdown
and impending beer shortage -- this, mind you, in an industry which is *not*
the focus of hostile propaganda campaigns and widespread irrational fears --
the people involved with nuclear plants have every reason to be very quiet
about even routine, unexciting, non-hazardous problems.

				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Acts of God vs. Acts of Man
</A>
</H3>
<address>
Nancy Leveson 
&lt;<A HREF="mailto:nancy@ICSD.UCI.EDU">
nancy@ICSD.UCI.EDU
</A>&gt;
</address>
<i>
30 Aug 86 17:26:20 PDT (Sat)
</i><PRE>

   &gt;&gt; From: Nancy Leveson &lt;nancy at ICSD.UCI.EDU&gt;
   &gt;&gt;  ...  My real quibble is with the term "computer errors"...

 &gt; From: Herb Lin &lt;lin at XX.LCS.MIT.EDU&gt;
 &gt;While I agree with the above sentiment for the most part, it strikes
 &gt;me that there is a sense in which it is not true...

I would describe it as a mistake on the part of the designers, not the
computer.  It probably did exactly what the designers told it to do.
You certainly could not expect it somehow magically to come up with
a fix for the software bug that the designers put in.  Only under those
assumptions could the computer have made a mistake by not coming up
with such a fix.  

 &gt;We can describe this as a design (and thus human) error.  But given
 &gt;the circumstances, it is not implausible to argue that an "Act of God"
 &gt;occurred.  

But unless I am working very fast, am distracted, or have taken drugs, you
have described all situations in which I make mistakes.  By this argument,
as long as I work slowly, pay attention, and remain sober, then I will never
make a mistake again -- I can attribute any stupid thing I do to God and
never have to accept responsibility.  I will also never have to do anything
to improve my performance since it is God's fault (or the computer's fault
or the fault of the complexity of the problem I am working on), and
therefore there is nothing I can do about it.  There is a difference between
responsibility and liability.  I can be responsible without being negligent
(although I argue below that the designers could be considered negligent
under the above hypothesized circumstances).

By Herb's definition, no accident can ever be ascribed to human error since:

 &gt;It was a rare happening; 

All accidents are rare happenings.  And unless the human is extremely
incompetent, most human mistakes are rare happenings.  

 &gt;we didn't understand it very well;

I rarely make mistakes when I understand things well (unless I am working
fast, not paying attention, or stoned).  

 &gt;we could not predict that it would happen.  

If I could predict that I will make a mistake, then I either don't do
the thing or I fix the mistake I made.  I have yet to write a program
which I did not think was correct (although a few actually had mistakes
in them).  In fact, it is precisely the stochastic, hardware-type
failures which we can predict and for which we can  provide probabilities.  
It is human mistakes which are difficult to predict.

BUT it IS possible to predict that human-made flaws will occur in complex
designs (although the particular mistakes may not be predictable) and,
it IS possible to provide mechanisms to protect against them.  This is
precisely what engineers do when they put interlocks into potentially
dangerous systems.  Software and computers are not unique in this
respect.  One can argue that the humans were responsible for the
original design flaw hypothesized above (although not negligent) and
that they were responsible and liable for the mistake they made in not
doing something to avert disaster in case their assumptions were wrong.

Most readers will agree that the Russian explanation for the
accident at Chernobyl implies human error and not an act of God.
But certainly, the events were rare, the operators did not understand
what they were doing, and they could not predict the consequences
(or they certainly would not have done it).  Again, computer
software is not needed for these conditions to hold.

My real fear of this type of thinking is that it will be used to
justify not doing anything about software safety.  If all software design 
flaws are "acts of God," then we need not spend a lot of money for safety 
because nothing can be done about programmer mistakes which are not the
result of negligence or maliciousness.  This is, however, untrue.  System
safety engineers have developed scientific and engineering principles
to identify and control hazards (caused by human design flaws or
hardware failures) in complex systems through analysis, design, 
and management procedures.  We need to do the same for software before 
catastrophies result.  

     Nancy Leveson
     Information and Computer Science Dept.
     University of California, Irvine

     [There is a lengthy response to this message from Herb, but since
      he mostly agrees with Nancy or reemphasizes earlier points, it is not
      included in this issue -- except for a paragraph that prompted a
      further message from Nancy.  I am somewhat amused that this entire 
      debate began when Nancy responded to my original statement -- which
      enumerated a few causative factors, but which in no way intended to
      imply that those were the ONLY factors in those cases.  I have 
      maintained consistently throughout RISKS that a holistic approach is
      absolutely essential; any factor individually, or various factors in
      combination, could be devastating.  Herb concludes that he and Nancy
      really seem to agree, so I presume this exchange will now end.  PGN]

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Acts of God vs. Acts of Man, Round n+1 (eastbound)
</A>
</H3>
<address>
Nancy Leveson 
&lt;<A HREF="mailto:nancy@ICSD.UCI.EDU">
nancy@ICSD.UCI.EDU
</A>&gt;
</address>
<i>
30 Aug 86 23:30:36 PDT (Sat)
</i><PRE>

   &gt;From Herb Lin:
   &gt;But the number of assumptions that
   &gt;designers must make is enormously large, and it is essentially
   &gt;impossible to even articulate ALL of one's assumptions.  

Agreed.  But there are ways to determine which are the critical
assumptions with regard to particular hazards.  This is exactly what
some of my techniques, e.g. software fault tree analysis, attempt to
do.  In the Firewheel example that I published, we determined a critical
assumption which could have resulted in the satellite being destroyed.
That is, if there were two sun pulses detected within 64 milliseconds of
each other, the microprocessor interrupt system became hung which could
possibly result in destruction of the sensor booms (and thus the usefulness
of the satellite).  We found this assumption by working backward through
the software from the hazardous condition.  The solution, once the
critical assumption had been determined, was a simple blocking of the
second sun pulse interrupt.

I don't know for what size systems these backward analysis approaches are
practical.  It took Peter Harvey (my student) two days to analyze the
Firewheel software (which is about 1600 lines long) by hand.  Obviously, it
would be possible to analyze larger software, but we do not yet know how
much this will scale up practically.  We are working on a software tool to
automate as much as possible.  These techniques are, of course, no more
perfect than other more traditional software engineering techniques.  And
better ones may be found.  I am just not ready to say it is impossible
without first trying.

Backwards analysis, verification of safety, software interlocks,
software fault tolerance, fail-safe design, ... -- there are possible
solutions which we should be examining.
                                               Nancy Leveson

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Computer Literacy
</A>
</H3>
<address>
Mike McLaughlin
&lt;<A HREF="mailto:mikemcl@nrl-csr ">
mikemcl@nrl-csr 
</A>&gt;
</address>
<i>
Mon, 1 Sep 86 11:49:10 edt
</i><PRE>

From THE WASHINGTON POST, Monday, 1 Sept 86, page A14, Letters to the
Editor [ "..." indicates omissions].  While I do not entirely agree with
Mr. Jordan, much of what he says is directly applicable to Risks.

        [Before responding to this, please recall that this topic has
         already been discussed at some length in <A HREF="/Risks/2.36.html">RISKS-2.36</A> and 37, 
         and in <A HREF="/Risks/3.17.html">RISKS-3.17</A>, 19, 20, and 21.  PGN]

        +++++++++++++++++++++++++++++++++++++++++++++++++++++++

                         COMPUTER LITERACY

     Although I earn my living as a consultant in computerized data bases,
I strongly oppose the view... that computer literacy should be mandatory in
the secondary school curriculum.

     Computers are a device for performing some task that either is already
performed by other means or first must be understood in other terms,
usually a mathematical equation.  Learning how to operate a computer, or
program one, is not going to improve a student's knowledge of languages,
mathematics, history or political science.

     Alfred North Whitehead observed that civilization increases the number
of things that we can do without thinking, i.e., that we can take for
granted.  This is evident in the development of computers, which
increasingly are becoming like automobiles; anyone can drive them.
Learning the technology of computers has as much relevance to everyday life
as learning the technology of auto engines.

     Unquestionably there are tasks for which computers are indespensable,
but individuals will learn those functions as they become involved in the
task itself, whether it be medical diagnosis, controlling the flow of
electric power over a grid or determining the authorship of a 16th-century
poem.

     What students need to know is how to think, especially about the human
condition.  As more and more college students flock to "practical" majors,
the secondary schools should be concentrating on the liberal arts.  In this
perspective, "computer literacy" may be just another form of a larger
illiteracy.

          - John S. Jordan,  Washington, D.C.

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
 Another supermarket crash
</A>
</H3>
<address>
&lt;<A HREF="mailto: TMPLee@DOCKMASTER.ARPA">
 TMPLee@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Sat, 30 Aug 86 23:26 EDT
</i><PRE>
To:  Risks@CSL.SRI.COM

Same thing happened here (Minneapolis-St.Paul) a couple of years ago -- I
was in a major discount store (Target), during a normal busy time --
Saturday morning, I think -- only to find that all the cash registers wre
down because the central computer was down.  Don't know how long it lasted,
but at least long enough that by the time I got there the cashiers were
using paper and pencil.  Really, stupid -- (so he says as a so-called
computer expert) -- given that those registers probably had Z80's or 6502 or
such in them, a printed record, etc., so they could just as well have worked
off-line (except for not knowing the price on instant sale items, which
would I assume have been in the central machine.)

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
 A supermarket does not grind to a halt
</A>
</H3>
<address>
    Brint Cooper 
&lt;<A HREF="mailto:abc@BRL.ARPA">
abc@BRL.ARPA
</A>&gt;
</address>
<i>
Mon, 1 Sep 86 13:29:45 EDT
</i><PRE>

Last month, as I awaited checkout in a "Giant" supermarket in Bel Air,
MD, an area-wide power outage lasting several minutes occured.  The
following was the sequence of events:

	1.  All lights, outside and inside, instantly out.
	1A. Display on cash register_cum_terminal RETAINED display!
	2.  Some of the same lights came back almost immediately (seemed
            to be back-up power).
	3.  Several minutes passed; additional lights began to come on.
	4.  One-by-one, the terminals "beeped" and became functional.

No market employee with whom I spoke seemed to understand what actually
happened.  But the computer system obviously was protected from such
random power outages (which occur FREQUENTLY here).
                                                          Brint Cooper
                    UUCP:  ...{seismo,unc,decvax,cbosgd}!brl-smoke!abc

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.46.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.48.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-34</DOCNO>
<DOCOLDNO>IA012-000123-B022-237</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.48.html 128.240.150.127 19970217004426 text/html 19680
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:42:43 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 48</TITLE>
<LINK REL="Prev" HREF="/Risks/3.47.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.49.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.47.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.49.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 48</H1>
<H2> Tuesday, 2 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Aeromexico Crash 
</A>
<DD>
<A HREF="#subj1.1">
UPI via PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Air Force puts secrets up for sale 
</A>
<DD>
<A HREF="#subj2.1">
Peter G. Neumann
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Randi, Popoff, and Data Privacy Laws 
</A>
<DD>
<A HREF="#subj3.1">
Phil Karn via Geoff Goodfellow
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Flight Simulators Have Faults 
</A>
<DD>
<A HREF="#subj4.1">
Gary Whisenhunt
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  On-Line with Taco Bell Telephone 
</A>
<DD>
<A HREF="#subj5.1">
John Mulhollen
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Titanic photo expedition 
</A>
<DD>
<A HREF="#subj6.1">
Lindsay F. Marshall
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  New Zealand $1 million deposit 
</A>
<DD>
<A HREF="#subj7.1">
Dave Sherman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Examination Processing Error 
</A>
<DD>
<A HREF="#subj8.1">
Joe Stoy
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Aeromexico Crash
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Tue 2 Sep 86 09:59:20-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

The New York Times news summary, Tuesday, 2 Sept 1986, had this item
on the LA plane crash.

      New York - The California plane collision Sunday occurred in a
  government-established restricted zone where the private plane that was
  destroyed in the collision with an Aeromexico DC-9 was not authorized
  to fly, the Federal Aviation Administration said.  An FAA spokesman also
  said the controller guiding the DC-9 could not have radioed warnings to
  avert the collision because ''as far as we can determine'' no radar
  blip designating the small plane appeared on his scope.  The controller
  did not know of the small plane's existence, the spokesman said.

A SF Chron report on the same day indicated that the controller in question
was distracted by the pilot of another private plane, with whom he was
having a two-minute interaction -- during which time the crash occurred.

PBS added several more pieces to the puzzle.  The pilot of the private plane
(a Piper Archer) apparently had had a heart attack just before the crash.
The private plane did indeed appear on the controller's radar after all.
However, it was not equipped with an altitude-measuring transponder, so the
controller had no idea whether or not there was any danger.

The death toll is 64 on the jetliner, 3 on the Piper PA-28, and at least
18 on the ground.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Air Force puts secrets up for sale
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Tue 2 Sep 86 16:00:31-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

Fred Ostapik went off to Ashland, Oregon, for some Shakespeare plays, and
brought back this clipping from the local Ashland paper of 23 August 1986:

              Audit: Air Force put secrets up for sale

  Washington (UPI) -- A military audit, examining the latest lapse in
Pentagon security, says the Air Force inadvertently allowed computer tapes
containing ``sensitive, unclassified'' data to be auctioned off to the
public.
  The Air Force Audit Agency found more than 1,200 magnetic tapes
containing the data -- dealing with launch times, aircraft tests,
and launch and aircraft vehicles -- available for public purchase at
three key bases...
  Auditors said they found 1,980 analog tapes available for purchase, 64
percent of which had not been erased and contained sensitive unclassified
data.  Five of the seven installations checked had inadvertently made secret
tapes available to the public.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Randi, Popoff, and Data Privacy Laws
</A>
</H3>
<address>
Phil Karn 
&lt;<A HREF="mailto:karn@ka9q.bellcore.COM">
karn@ka9q.bellcore.COM
</A>&gt;
</address>
<i>

</i><PRE>
Date: 31 Aug 86 02:29:11 GMT
Organization: Bell Communications Research, Inc
ReSent-To: RISKS@CSL.SRI.COM
Original-Subject: I wonder if the Congress considered this one

I picked up a copy of the magazine "Free Inquiry" at the bookstore today.
The cover article was written by James Randi (the magician who debunks lots
of ESP frauds). In fact, the magazine seems to be run by the same folks who
do the Skeptical Inquirer, but is slanted more towards religious debunking.

Randi's article was titled "Peter Popoff Reaches Heaven via 39.17
Megahertz".  Popoff is one of the most notorious TV faith healers.  Randi's
group went to the shows and noticed that Popoff wore a hearing aid. Then
they got a scanner and quickly found the frequency his wife was using to
tell him the names and ills of people whom she had pumped for information
before the show.

Now ponder the fact that the proposed Communications Privacy Act now pending
in the US Senate would have made this expose' illegal.  The conversation was
meant to be private, and Popoff certainly would have objected to its
interception.

Could there be a connection here? Hmm......
                                                    Phil

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Flight Simulators Have Faults
</A>
</H3>
<address>
Gary Whisenhunt 
&lt;<A HREF="mailto:gwhisen%ccvaxa@GSWD-VMS.ARPA">
gwhisen%ccvaxa@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Tue, 2 Sep 86 10:35:47 cdt
</i><PRE>

    I developed flight simulators for over 7 years and could describe many such
bizarre incidents.  I seriously doubt that the sky went blank in the B-1
simulator when it was delivered to the government.  Military simulators have
formal acceptance tests that last for months.  The last one that I worked on
had a test procedure over 12 inches thick.  To point out a failure during
testing (or more likely development) seems meaningless.  Failures that make
it into the actual product are what should be of concern.
    Most flight simulators procured by the Air Force and the Navy require
Mil-Std 1644 or Mil-Std 1679 to be followed when developing software.  These
standards detail how software is to be developed and tested.  The standards
are fairly strict and exhaustive.  This is to ensure product correctness 
even if it incurrs greater costs.  It would be interesting study for a 
class in Software Engineering.
    The greatest risks that I see from flight simulators (especially
military) is that the simulator often lags behind the aircraft in
functionality by a year or 2.  Simulators require design data to be frozen
at a certain date so that the simulator can be designed using consistent,
tested data.  After 2 years of development, the aircraft may have changed
functionaly (sometimes in subtle ways) from the simulator design.  The
effect is much more dramatic for newer aircraft than it is for more
established ones.  The simulator is upgraded, but during the upgrade period
pilots train on a simulator that is mildly different from their aircraft.
    As for the effectiveness of simulators, I've been told by more than one
pilot that the simulator saved his life because he was able to practice
malfunction conditions in the simulator that prepared him for a real emergency
that occurred later.

Gary Whisenhunt
Gould Computer Systems Division
Urbana, Ill.

    [I thought that by now these simulators were designed so that they could
     be driven by the same software that is used in the live aircraft -- a
     change in one place would be reflected by the same change in the other,
     although changing the application code without having to modify the
     simulator itself.  Maybe not...  PGN]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
On-Line with Taco Bell Telephone
</A>
</H3>
<address>
John Mulhollen 
&lt;<A HREF="mailto:JOHNM@USC-ECLC.ARPA">
JOHNM@USC-ECLC.ARPA
</A>&gt;
</address>
<i>
Mon 1 Sep 86 22:32:00-PDT
</i><PRE>
To: Neumann@CSL.SRI.COM
ReSent-To: RISKS@CSL.SRI.COM

It seems that more and more fast food places are switching from the
old-fashioned cash register to computerized ones that enable management to
get reports on how many burgers we sold today between 10pm and 11pm, the
average number of tacos per patron, or how many french fries were wasted.
        [Results are automatically telecommunicated back to headquarters.  PGN]
However, along with the capability for better-informed management, the
capability for unbelievable confusion also increases. Case in point -- our
local Taco Bell has been "computerized" for almost 9 months now (equipment
from Par Microsystems in NY) and patrons and employees alike have become
accustomed to not getting receipts, and other quirks. Last week, the
computer "locked up" (their term) just as I arrived. It was also just before
the noon rush. The employees behind the counter did not know what to do. Do
we take orders (on paper) and wait for the machine to come back up? Do we
tell the customers to go away? It appears that with all this wonderful
automation, the employees were incapable of 1) figuring out what to do;
2) taking orders without the computer; and 3) figuring out not only the total
due for each patron, but the amount of change to return!!

	When I was working my way through school, I did a brief stint at a
local taco joint. We had an "old-fashioned" cash register (it didn't even 
compute the change -- how backward can you get!!) and we did just fine. When 
it didn't work, we just used a pad of paper (we knew all the prices and such).

	Apparently one of the risks to society of the increasingly wide-spread
use of computers is the possibility of losing the ability to think and reason.

JohnM

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Titanic photo expedition
</A>
</H3>
<address>
"Lindsay F. Marshall" 
&lt;<A HREF="mailto:lindsay%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK">
lindsay%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Mon, 1 Sep 86 09:10:44 gmt
</i><PRE>

There was a program last night on ITV about the Woods Hole expedition to the
Titanic. During the first dive, the program that was being used to help
locate the ship "developed a mind of its own" and the people on the support
ship had to guess headings for the sub to follow. Does any one have
information on this??
                                        	Lindsay

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
New Zealand $1 million deposit (<A HREF="/Risks/3.41.html">RISKS-3.41</A>)
</A>
</H3>
<address>
&lt;<A HREF="mailto:mnetor!lsuc!dave@seismo.CSS.GOV">
mnetor!lsuc!dave@seismo.CSS.GOV
</A>&gt;
</address>
<i>
Tue, 2 Sep 86 14:22:27 edt
</i><PRE>

  &gt;Bank machine is no match for schoolboy with a lollipop
  &gt;
  &gt;  AUCKLAND, New Zealand [UPI] -- A schoolboy outsmarted an automatic
  &gt;bank machine by using the cardboard from a lollipop packet to
  &gt;transfer $1 million New Zealand dollars into his account, bank
  &gt;spokesmen said Thursday.

As the article indicates, this wasn't caught because of delays in
reconciling the physical deposits with the computer records (4 WEEKS?
my bank does it in a day!).

I find it somewhat misleading and irritating that the media choose
to make a big deal about the lollipop packet. Obviously, he could
have fed in an empty envelope just as easily. But "outsmarted ...
by using the cardboard from..."?  I guess this is one of the RISKs
of having reporters who feel they need to make their stories interesting.

Dave Sherman, The Law Society of Upper Canada, Toronto
{ ihnp4!utzoo  seismo!mnetor  utzoo  hcr  decvax!utcsri  } !lsuc!dave

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
Examination Processing Error
</A>
</H3>
<address>
Joe Stoy 
&lt;<A HREF="mailto:stoy%sevax.prg.oxford.ac.uk@Cs.Ucl.AC.UK">
stoy%sevax.prg.oxford.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Mon, 1 Sep 86 13:56:43 GMT
</i><PRE>

EXAMINATION PROCESSING ERROR

The following is copied (without permission) from The Times (London).
(C) TIMES NEWSPAPERS LIMITED 1986.

[Glossary:
O level ("Ordinary level") - an exam. taken by children aged fifteen or so.
A level ("Advanced level") - an exam. taken two years after O level; a
  prerequisite for university entrance.
CSE ("Certificate of Secondary Education") - an exam. for children who are not
  up to O level standard.
GCSE ("General Certificate of Secondary Education") - a forthcoming
  amalgamation of O level and CSE, in preparation for which some boards are
  already setting papers common to both existing exams.]
[[American readers should note that Public School means Private School. PGN]]

[28 August 1986]
COMPUTER MARK STARTS O-LEVEL PANIC
By Lucy Hodges
Education Correspondent

Hundreds of pupils who took a new joint O level/CSE examination in chemistry
received the wrong grade because of a computer error.

It meant that no candidate received more than a grade C, the pass mark at O
level, sending many parents and their offspring into a panic.

Schools were telephoned to be asked if this meant that the pupils involved
would be prevented from doing chemistry at A level next year.  The schools
queried the grades with the boards and the rogue computer program was
discovered.

The examination boards involved are the three GCE boards, Cambridge, Oxford
and Cambridge, Southern Universities Joint, and the two CSE boards, West and
East Midlands.

These five boards are combining to form the Midlands Examining Group for the
new GCSE exam.  As part of their preparation they are running joint
examinations in certain subjects and new computer programs have had to be
set up.

"The boards have to collaborate and with new computer programs we cannot find
out mistakes until something happens," Mr. John Reddaway, secretary of the
Cambridge board, said.

A total of 12,000 students entered for the joint examination in chemistry, of
which 3,800 were awarded a grade C by the computer.  In fact 800 of these
should have been a grade A and 1,000 a grade B, Mr. Reddaway said.

The error appears to have occurred at the offices of the West Midlands CSE
board in Birmingham, which was administering this particular exam.  Mr.
Reddaway said that the mistaken grades had all been rectified.  "I hope schools
and colleges will receive them tomorrow."

Whitgift School in Croydon, a boys' public school which normally gets very
good results, was one of those involved.  It was surprised to find that all its
O-level pupils had been awarded a grade C.

"It was ridiculous in a school like this not to have any grades A or B," Miss
Patricia Dawson-Taylor, the school secretary, said.  "I told the board that we
would be querying them."

Parents of Whitgift boys have been informed by the school that there has been
an error and that some candidates may be upgraded.


[29 August 1986 -- excerpts from the follow-up report]

EXAMS RESULT IS CORRECTED

.... Because of what the Midlands Examining Group described as "a procedural,
rather than a computer error", none of the 12,000 entrants ... was awarded
more than a grade C ...

.... Mr John Reddaway, secretary of the Cambridge board, said that because of
misunderstandings between the five boards, the "hurdle" mark that
distinguishes an A or B grade was not programmed into the computer. ...

[1 September 1986 - Letters to the Editor]

O-LEVEL ERRORS
&gt;From Mr P.D.R. Talbot Willcox

Sir, The case reported in your columns today (August 28) of the computer error
affecting the grades of O-level candidates raises the question whether other
undetected computer errors are resulting in injustice and danger.  The
statement made by the Secretary of the Cambridge Board that "with new computer
programmes [sic] we cannot find out mistakes until something happens" is hardly
reassuring.

The error was sufficiently gross to excite determined questioning by those
most obviously affected.  But one dreads to think what might have happened if
only a smaller number of pupils had been affected.  There are many other
computer applications where errors of this kind would have more serious and
even disastrous implications, not least being medical and criminal records.

Is it not time for a Government enquiry to be held into ways and means of
legislating to ensure that all potentially dangerous programmes are thoroughly
checked before they are used?

Yours faithfully,
P.D.R. TALBOT WILLCOX, Rodwell House,Middlesex St, [London] E1, August 28.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.47.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.49.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-35</DOCNO>
<DOCOLDNO>IA012-000123-B022-260</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.49.html 128.240.150.127 19970217004441 text/html 14018
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:43:09 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 49</TITLE>
<LINK REL="Prev" HREF="/Risks/3.48.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.50.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.48.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.50.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 49</H1>
<H2>Thursday, 4 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Human Error 
</A>
<DD>
<A HREF="#subj1.1">
Dave Parnas
</A><br>
<A HREF="#subj1.2">
 Bill Anderson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Machine errors - another point of view  
</A>
<DD>
<A HREF="#subj2.1">
Bob Estell
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Flight simulators 
</A>
<DD>
<A HREF="#subj3.1">
Eugene Miya
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  F-16 software 
</A>
<DD>
<A HREF="#subj4.1">
Henry Spencer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Terminal (!) lockup 
</A>
<DD>
<A HREF="#subj5.1">
Ken Steiglitz
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
human error
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
Wed, 27 Aug 86 09:16:44 EDT
</i><PRE>
ReSent-To: RISKS@CSL.SRI.COM
[JUST RECEIVED FROM DAVE PARNAS  AFTER MAILER DIFFICULTIES]

   When people use the phrases "human error" and "computer error"
they are simply trying to distinguish between situations in which 
"the cause" of the accident was a human action that happened about the time
of the accident and the situations in which "the cause" of the 
error was a human action much earlier.  Obviously, we cannot
make a hard black/white distinction based on this continuum of
possibilities.  Only humans cause accidents because only humans
provide the problem statements that allow one to talk about
an accident or a failure.

Dave

</PRE>
<HR><H3><A NAME="subj1.2">
Re: <A HREF="/Risks/3.46.html">RISKS-3.46</A>: Human Error
</A>
</H3>
<address>
&lt;<A HREF="mailto:WAnderson.wbst@Xerox.COM">
WAnderson.wbst@Xerox.COM
</A>&gt;
</address>
<i>
3 Sep 86 09:57 EDT
</i><PRE>
To: LIN@XX.LCS.MIT.EDU
cc: RISKS@CSL.SRI.COM

Herb Lin writes:

  "no user should approach a computer system as though its behavior is
  predictable and/or sensible under all circumstances ...."

Stanislaw Lem has written some very amusing and thought provoking
stories about the relations between people and technology (including
automata) in the Tales of Pirx the Pilot (2 volumes, paperback).  Pirx
is just an ordinary space pilot who learns to approache the computer
systems he must use with a good deal of common sense.

Bill Anderson

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Machine errors - another point of view
</A>
</H3>
<address>
&lt;<A HREF="mailto:"SEFB::ESTELL" <estell%sefb.decnet@nwc-143b.ARPA> ">
"SEFB::ESTELL" &lt;estell%sefb.decnet@nwc-143b.ARPA&gt; 
</A>&gt;
</address>
<i>
3 Sep 86 12:19:00 PST
</i><PRE>
To: "risks" &lt;risks@csl.sri.com&gt;

	I'm not satisfied with the notion that computers don't make errors; 
that they ONLY suffer mechanical failures that can be fixed. 

 "Deep in a computer's hardware are circuits called arbiters whose function is 
 to select exactly one out of a set of binary singnals.  If one of the signals 
 can change from '0' to '1' while the selection is being made, the subsequent 
 behavior of the computer may be unpredictable.  It appears fundamentally 
 impossible to construct an arbiter that can reliably make its selection 
 within a bounded time interval."
	Peter J. Denning, in  AMERICAN SCIENTIST 73, no. 6 (Nov-Dec 1985)
	[also reprinted in RIACS TR  85.12]

	I'm not a hardware guru, nor a scholar in theoretical computer science; 
 but my practical experience says that Peter is right.  I've gotten very close 
 to the internals of only two computers; both were IBM second generation 
 machines, the 7074, and the 1401.  I programmed both in assembly and machine 
 code; even wrote diagnostics for the 7074.  I can guarantee that those 
 machines, much simpler in design than today's multi-processors,  and also 
 much slower, were somewhat unpredictable.   We found some nasty situations 
 that required special code loops to mask/unmask interrupts, so that the 
 machine could run.

	A "machine" as seen by the applications programmer, is already several 
 layers [raw hardware, microcode, operating system kernel, run-time libraries,  
 compiler]; and each layer is perhaps nearly a million pieces [IC's, lines of  
 (micro)code] that may interact with nearly a million other pieces in other 
 layers.

	What I suspect here is a "problem of scale" akin to the well know idea 
 that there are real limits to what one can build with a given material; e.g., 
 bones can't support animals much over 100 feet tall; because the internal 
 tensile and sheer stresses will at some point destroy the molecular integrity 
 of the materials.    We can analyse the few hundred lines of code, in the 
 kernel of an I/O driver, running on naked second generation hardware; I did 
 that.  But can we examine the millions of lines of code that comprise the 
 micro-instructions, the operating system, and the engineering applications
 on a multi-processor system, and hope to understand ALL the possible 
 side-effects?  Color me skeptical.   Thus, because we put machines "in 
 control" of significant events in our lives [ATM's, FFA stuff, weapons 
 simulators, etc.]; and because EVEN AFTER we've made our best personal and 
 professional attempt to eleminate the errors; and even after the system has
 run "a thousand test cases" it still has errors - not necessarily "hard 
 failures" that the C.E. can fix, but "transients" that are sensitive to 
 timing ; for all these reasons, I'll argue that "machines make errors" in 
 much the same sense that people mispronounce words or make mistakes driving.  
 It's not that we don't know better; it's not that we've suffered some damage;
 it's that we aren't perfect; neither are our computers.  And sometimes 
 there's "nothing wrong" that can be fixed.

	If we continue this discussion long enough, we'll approach the 
 metaphysical notion of "free will and determinism."  I don't think that's 
 necessary; I think our current  systems have already exceeded our ability 
 to predict them 100.0%, even in theory.

Bob

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Flight simulators
</A>
</H3>
<address>
Eugene Miya
&lt;<A HREF="mailto:eugene@AMES-NAS.ARPA ">
eugene@AMES-NAS.ARPA 
</A>&gt;
</address>
<i>
3 Sep 1986 1714-PDT (Wednesday)
</i><PRE>

    [I thought that by now these simulators were designed so that they could
     be driven by the same software that is used in the live aircraft &lt;PGN&gt;...

Don't forget that very few aircraft use "software."  Software is a radically
new concept to aircraft designers: F-16, F-18, X-29A, and so forth.

     change in one place would be reflected by the same change in the other,
     although changing the application code without having to modify the
     simulator itself.  Maybe not...  PGN]

The problem comes when it's asked "What do you simulate?"  The view? The
feeling?  Handling characteristic?-&gt;based on aerodynamics-&gt;computational
fluid dynamics-&gt;???  True, those games your can buy for an apple two are
simulators, and we have a $100 million test facility (6 stories high) which
is a simulator.  But there are limits to simulation:
we don't know how to simulate the flight characteristics of
a helicopter, we don't know how to automate a helicopter: (any one know
of a microprocessor which can withstand 800-1600 Gs?).  Anyway, Peter, you
are invited to talk to our simulator people if you want to answer this one,
as I don't have the time.  Danny Cohen has been here.

Another thought: as simulators become more "real" [as in some of ours]
they require increasing amounts of certification BEFORE you can use a
simulator [does this sound like a paradox in some ways? hope so].
I saw an experienced pilot told they he could not use a simulator
in some mode (motion base).

--eugene miya,   NASA Ames Research Center
  {hplabs,hao,dual,ihnp4,decwrl,allegra,tektronix,menlo70}!ames!aurora!eugene

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
F-16 software
</A>
</H3>
<address>
&lt;<A HREF="mailto:allegra!utzoo!henry@ucbvax.Berkeley.EDU">
allegra!utzoo!henry@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Wed, 3 Sep 86 23:59:56 PDT
</i><PRE>

Phil Ngai writes:

&gt; It sounds very funny that the software would let you drop a bomb on the wing
&gt; while in inverted flight but is it really important to prevent this? ...

This issue actually is even more complex than it sounds, because it may be
*desirable* to permit this in certain circumstances.  The question is not
whether the plane is upside down at the time of bomb release, but which way
the bomb's net acceleration vector is pointing.  If the plane is in level
flight upside-down, the vector points into the wing, which is a no-no.  But
the same thing can happen with the plane upright but pulling hard into a
dive.  Not common, but possible.  On the other side of the coin, some
"toss-bombing" techniques *demand* bomb release in unusual attitudes,
because aircraft maneuvering is being used to throw the bomb into an
unorthodox trajectory.  Toss-bombing is common when it is desired to bomb
from a distance (e.g. well-defended targets) or when the aircraft should
be as far away from the explosion as possible (e.g. nuclear weapons).
Low-altitude flight in rough terrain at high speeds can also involve quite
violent maneuvering, possibly demanding bomb release in other than straight-
and-level conditions.

				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Terminal (!) lockup
</A>
</H3>
<address>
&lt;<A HREF="mailto:princeton!ken@seismo.CSS.GOV">
princeton!ken@seismo.CSS.GOV
</A>&gt;
</address>
<i>
Wed, 3 Sep 86 01:34:17 EDT
</i><PRE>

From the User's manual for the Concept AVT terminal, p. 3-52 (Human Designed
Systems, Inc., 3440 Market Street, Philadelphia, PA 19104):

  "Note: since the Latent Expression is invoked automatically, it should not
  contain any command that resets the terminal, either implicitly or
  explicitly. If any such command is included in the Latent Expression, the
  terminal will go into an endless loop the next time it is reset (implicitly
  or explicitly) or powered up. The only way to break out of this loop is to
  disassemble the terminal and physically reset Non-Volatile Memory. ... "

Having a sequence of keystrokes that will physically disable a terminal
seems to me a bad thing. For one thing it makes me awfully nervous when I'm
changing the Latent Expression. For another, it opens up the possibility of
having my terminal physically disabled by people or events outside my
control. (I don't know whether this effect can also be caused by a sequence
of bits sent to the terminal.)

I wonder: How common is this property of terminals (or other equipment)?
Does the phenomenon have a (polite) name?  Is it so hard to avoid that we
should be satisfied to live with it? Is it clear how to test for the
possibility? Does anybody have any experience with this phenomenon?

           [You have found the tip of the iceberg of Trojan horses that 
            can take over your terminal, processes, files, etc.  PGN]

Ken Steiglitz, Dept. of Computer Science, Princeton Univ., Princeton, NJ 08544

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.48.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.50.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-36</DOCNO>
<DOCOLDNO>IA012-000123-B022-283</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.50.html 128.240.150.127 19970217004456 text/html 22194
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:43:22 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 50</TITLE>
<LINK REL="Prev" HREF="/Risks/3.49.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.51.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.49.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.51.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 50</H1>
<H2> Sunday, 7 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Enlightened Traffic Management 
</A>
<DD>
<A HREF="#subj1.1">
Alan Wexelblat
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Flight Simulator Simulators Have Faults 
</A>
<DD>
<A HREF="#subj2.1">
Dave Benson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: Flight Simulators and Software Bugs 
</A>
<DD>
<A HREF="#subj3.1">
Bjorn Freeman-Benson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Always Mount a Scratch Monkey 
</A>
<DD>
<A HREF="#subj4.1">
Art Evans
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Re: supermarket crashes 
</A>
<DD>
<A HREF="#subj5.1">
Jeffrey Mogul
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Machine errors - another point of view 
</A>
<DD>
<A HREF="#subj6.1">
Bob Estell
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Human Behv. &amp; FSM's 
</A>
<DD>
<A HREF="#subj7.1">
Robert DiCamillo
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Enlightened Traffic Management
</A>
</H3>
<address>
Alan Wexelblat 
&lt;<A HREF="mailto:wex@mcc.com">
wex@mcc.com
</A>&gt;
</address>
<i>
Thu, 4 Sep 86 09:58:49 CDT
</i><PRE>

The Austin rag carried the following brief item off the AP wire:

NEW DELHI, India (AP) - The computer lost the battle with the commuter.

"Enlightened traffic management" was the term for New Delhi's new
computerized bus routes, but four days of shattered windows, deflated tires
and protest marches convinced the bus company that its computer was wrong.

The routes dictated by the computer proved exceedingly unpopular
with passengers, who claimed that they were not being taken where
they wanted to go.

Bowing to demand, the New Delhi Transport Corp. scrapped the new
"rationalized" routes and restored 114 old routes.

"The computer has failed," shouted thousands of victorious commuters in
eastern New Delhi Tuesday night after transport officials drove around in
jeeps, using loudspeakers to announce the return of the old routes.

COMMENTS:  At first, I thought this was pretty amusing; deflated tires is a
computer risk I hadn't heard of before.  But the whole attitude of the
article (and seemingly the people) annoyed me.  The machine is taking the
rap and I'll bet that the idiot who programmed it to produce "optimal"
routes will get off scott free.  Not to mention the company execs who failed
to understand their customer base and allowed the computer to "dictate" new
routes.  ARGH!

Alan Wexelblat
UUCP: {seismo, harvard, gatech, pyramid, &amp;c.}!ut-sally!im4u!milano!wex

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Flight Simulator Simulators Have Faults
</A>
</H3>
<address>
Dave Benson 
&lt;<A HREF="mailto:benson%wsu.csnet@CSNET-RELAY.ARPA">
benson%wsu.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Wed, 3 Sep 86 17:01:17 pdt
</i><PRE>

  |I developed flight simulators for over 7 years and could describe many such
  |bizarre incidents.
Might be interesting for RISKS if these suggest problems in developing
risk-free software...
  |To point out a failure during
  |testing (or more likely development) seems meaningless.  Failures that make
  |it into the actual product are what should be of concern.
I do not agree.  We need to understand that the more faults found at
any stage to engineering software the less confidence one has in the
final product.  The more faults found, the higher the likelyhood that
faults remain.  I simply mentioned this one because it appears to
demonstrate that for all the claims made for careful analysis and
review of requirements and design, in fact the current practice leaves
such obvious faults to be found by testing.
  |As for the effectiveness of simulators...
Simulators are wonderful.  Surely nothing I wrote suggested otherwise.

Upon further inquiry, the blank sky was in a piece of software used
to simulate the flight simulator hardware.  The software specs essentially
duplicated the functions proposed for the hardware.  So the hardware was
going to take the trigonmetric tangent of the pitch angle.  The software
simulator of the flight simulator indeed demonstrated that one ought not
take the tangent of 90 degrees.

So somebody with presumably a good background in engineering mathematics
simply failed to think through the most immediate consequences of
the trigonometric tangent function.  Nobody noticed this in any kind
of review, nobody THOUGHT about it at all.

Since nobody bothered to think, the fault was found by writing a
computer program and then observing the obvious.  I suggest that
this inability to think bodes ill for the practice of
software engineering and the introduction of "advanced techniques"
such as fault-tree analysis.

I suggest that such examples of a pronounced inattention to well-known
mathematics are part of the reason for such lengthy testing sequences
as the military requires.	And I suggest that the fact that it
appears necessary to mention all this yet once again suggests that
there are many people doing "software engineering" who have failed to
grasp what a higher education is supposed to be about.  I certainly
do not expect perfection, but the trigonometric tangent is an
example of an elementary function.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: Flight Simulators and Software Bugs
</A>
</H3>
<address>
Bjorn Freeman-Benson
&lt;<A HREF="mailto:bnfb@uw-june.arpa ">
bnfb@uw-june.arpa 
</A>&gt;
</address>
<i>
Fri, 5 Sep 86 10:02:38 PDT
</i><PRE>

In <A HREF="/Risks/3.48.html">RISKS-3.48</A>, Gary Whisenhunt talks about how he developed flight simulators
and that he "..seriously doubt[s] that the sky went blank in the B-1 simulator
when it was delivered to the government."  And then he goes on to point out
all the specs it had to pass.  I don't know no way or the other, but I want to
point out that the sky going blank points out either a design problem or an
implementation problem.  If it is a design problem, who knows how many other
serious (sky blanking serious) problems exist?  Will the MIL standards catch
them all?  If it is an implementation error, who knows how many other similar
coding errors that sloppy/tired/etc engineer made?  If it's a sign problem,
what happens when you back the plane up?  Will it go into an infinite speed
reverse?  The point I'm trying to make is that bugs are not independent, and
if one shows up, other similar are usually in existence.

						Bjorn N Freeman-Benson
						U of Washington, Comp Sci

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Always Mount a Scratch Monkey
</A>
</H3>
<address>
"Art Evans" 
&lt;<A HREF="mailto:Evans@TL-20B.ARPA">
Evans@TL-20B.ARPA
</A>&gt;
</address>
<i>
Wed 3 Sep 86 16:46:31-EDT
</i><PRE>
To: Risks@CSL.SRI.COM

In another forum that I follow, one corespondent always adds the comment
	Always Mount a Scratch Monkey
after his signature.  In response to a request for explanation, he
replied somewhat as follows.  Since I'm reproducing without permission,
I have disguised a few things.
          
              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

My friend Bud used to be the intercept man at a computer vendor for
calls when an irate customer called.  Seems one day Bud was sitting at
his desk when the phone rang.
    
    Bud:	Hello.			Voice:	YOU KILLED MABEL!!
    B:		Excuse me?		V:	YOU KILLED MABEL!!

This went on for a couple of minutes and Bud was getting nowhere, so he
decided to alter his approach to the customer.
    
    B:		HOW DID I KILL MABEL?	V:	YOU PM'ED MY MACHINE!!

Well to avoid making a long story even longer, I will abbreviate what had
happened.  The customer was a Biologist at the University of Blah-de-blah,
and he had one of our computers that controlled gas mixtures that Mabel (the
monkey) breathed.  Now Mabel was not your ordinary monkey.  The University
had spent years teaching Mabel to swim, and they were studying the effects
that different gas mixtures had on her physiology.  It turns out that the
repair folks had just gotten a new Calibrated Power Supply (used to
calibrate analog equipment), and at their first opportunity decided to
calibrate the D/A converters in that computer.  This changed some of the gas
mixtures and poor Mabel was asphyxiated.  Well Bud then called the branch
manager for the repair folks:

    Manager:	Hello
    B:		This is Bud, I heard you did a PM at the University of
    		Blah-de-blah.
    M:		Yes, we really performed a complete PM.  What can I do
		for You?
    B:		Can You Swim?

The moral is, of course, that you should always mount a scratch monkey.

              ~~~~~~~~~~~~~~~~~~~~~~

There are several morals here related to risks in use of computers.
Examples include, "If it ain't broken, don't fix it."  However, the
cautious philosophical approach implied by "always mount a scratch
monkey" says a lot that we should keep in mind.

Art Evans
Tartan Labs

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Re: supermarket crashes
</A>
</H3>
<address>
Jeffrey Mogul
&lt;<A HREF="mailto:mogul@decwrl.DEC.COM ">
mogul@decwrl.DEC.COM 
</A>&gt;
</address>
<i>
4 Sep 1986 1614-PDT (Thursday)
</i><PRE>

One of the nearby Safeway supermarkets is open 24 hours, and is quite
popular with late-night shoppers (it's known by some as the "Singles
Safeway").  Smart shoppers, however, used to avoid visiting just before
midnight, because that's when all the cash registers were out of operation
while they went through some sort of ritual (daily balances or somesuch),
simultaneously.

I also discovered that this market, at least, is not immune to power
failures; I was buying a quart of milk one evening when a brief blackout hit
the area.  The lights were restored within minutes, but the computer was
dead and the cashiers "knew" it would be a long time before it would be up;
they weren't about to waste their fortuitous coffee-break adding things up
by hand, perhaps because they couldn't even tell the price of anything (or
indeed, what it was, in the case of produce) without the computer.

I don't often shop at that market, partly because the markets I do
use have cashiers who know what things are rather than relying on
the computer. Some day, just for fun, I might mark a pound of pecans
with the code number for walnuts, and see if I can save some money.

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Machine errors - another point of view
</A>
</H3>
<address>
&lt;<A HREF="mailto:LENOIL@XX.LCS.MIT.EDU">
LENOIL@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Thu, 4 Sep 1986  21:27 EDT
</i><PRE>

     	 A "machine" as seen by the applications programmer, is
     already several layers [raw hardware, microcode, operating system
     kernel, run-time libraries, compiler]; and each layer is perhaps
     nearly a million pieces [IC's, lines of (micro)code] that may
     interact with nearly a million other pieces in other layers.

    Interaction between one million pieces of a system is more than just
an exaggeration, it is horrendous engineering practice that should never
be seen.  Flow-graphs, dependency diagrams, top-down design - all are
ways of reducing interaction between system components to a small,
manageable size - the smaller the better.  The probability of designing
a working system of one million fully-connected components is near-zero.
Furthermore, you seem to imply that component interconnects can
transcend abstraction boundaries (e.g. microcode &lt;-&gt; run-time
libraries); this again is poor engineering practice.  I don't disagree
that rising system complexity is a problem today, but you are several
orders of magnitude off in your statement of the problem.

Robert Lenoil

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Human Behv. &amp; FSM's
</A>
</H3>
<address>
Robert DiCamillo 
&lt;<A HREF="mailto:rdicamil@cc2.bbn.com">
rdicamil@cc2.bbn.com
</A>&gt;
</address>
<i>
Fri, 5 Sep 86 16:27:45 EDT
</i><PRE>
To: risks@csl.sri.com
Cc: rdicamil@cc2.bbn.com

Comments on Bob Estell's "Machine Errors", Risks Vol. 3, #49
(FSM's need friends too)

I have often felt the same way Bob Estell does - that the full scope of
(software) engineering is too vast for a mere mortal to comprehend.
However, I usually reassure myself with a good dose of computational theory:

  * "... for all these reasons 'machines make errors' in much the same *
  * sense that people mispronounce words or make mistakes driving."    *

I agree with the apparent analogy, but still cringe at the actual usage of
the word error. Webster's Ninth New Collegiate dictionary defines error as
an "act involving unintentional deviation from truth or accuracy". If truth
or accuracy for computers or finite state automata is defined to be the
mapping of all possible input states to output states, then theoretically,
the only *unintentional* deviation from such truth (tables or such) is the
failure to map or correlate all possible input strings to known or desired
output states.

I have participated in the situation where the adoption of a non-standard
arbitration scheme did not take into account cycle stealing, and assembly
code actually had the value of operands corrupted so that a branch occurred
on the opposite condition to the true data. This was a bug that only a logic
analyzer could find, and set the hardware engineers back to their drawing
board. You have no idea how strange it feels to tell someone, that the code
actually took a branch wrong; prior to the branch the data was true, but it
always branched to the false address. The high level DDT would never show
the data to be false because of the particular timing coincidences involved
with using an in circuit emulator; very disturbing when even your debugger
says all is well, and tests still fail operationally in the real system.

In the case of bus arbitration, an entire realm of undesirable input strings
should be eliminated if the timing constraints between competing processes
are properly enforced in hardware.  If they are not, "unintentional
deviation" from the arbitration scheme will occur, but that "deviation" is
really only another set of output states that serves no desirable function.
However, you could sit down with a logic analyzer and painfully construct a
mapping of all possible input timing states to a bus arbitration scheme, and
map the output. Hopefully, the design engineers did this when they made the
specifications, even if they were not exhaustive in testing every possible
input string.

I believe it is improper to construe human behavior, especially
*unpredictability* to the results of input strings that fall outside the
desired function of a finite state automata. In theory, a FSM can have an
undefined output for a given input, but in practice the definition of this
output usually depends upon the resolution of your measuring instruments. If
an arbitration scheme appears to yield an indeterminate output when all
inputs are still within spec ( proper input strings), then the
characteristic function of the FSM is not complete (well defined).
Practically, this could mean that a timing situation arose they couldn't or
didn't see - maybe their analyzer didn't have the resolution ?  But it is
still ultimately, and sometimes easily attributable to a human oversight.
How much of the FSM characteristic function do you know about ? The part you
never dealt with is not necessarily "unpredictable".

Many important computational theories hinge on the conception that any
"solvable" problem can be realized in an arbitrarily complex FSM. While it
may not be practical to build the machine, no one yet has been able to
disprove such assertions as Church's thesis with current silicon built
architectures. Computational theory still clings to this viewpoint, which I
practically see as - if output states seem indeterminate, you still haven't
found the correct way to cast inputs in a reliably measurable form.

  * "But can we examine the millions of lines of code that comprise the *
  * micro-instructions, the operating system, and the engineering       *
  * applications on a multi-processor system, and hope to understand    *
  * ALL the possible side-effects."                                     *

Goals of good software/hardware design are to make it easy to categorize all
possible input strings, especially when they are countably infinite. This is
not the same as viewing the machine as somehow irrational and unpredictable.
Good designs may have an ease to their completeness of their characteristic
function (CF). This does not mean bad designs are unpredictable, just maybe
too complex to realize or measure. Anthropomorhizing is all too tempting.
Systems with many architectural layers have complex interactions. Recent
discussion in RISKS has highlighted the small percentage of total execution
paths that are ever actually traced, but perhaps in well characterized
FSM's, such exhaustive testing can be cautiously minimized. If in fact the
range of the CF is countably infinite, then some method of limited testing
is usually mandatory. Its the part of the FSM you don't know that you tend
to ascribe human behavior to !

Maybe it does take some exposure to developing systems with both complete
and incomplete characteristic functions to get an intuition about how closed
the FSM has to be to give satisfactory performance, for a specific
application. Bus arbitration is a relatively critical control function in
most architectures, and should be given a high priority. I'm sure there are
many systems out there that work just on the verge of catastrophe as
sloppily implemented FSM's, at numerous levels.

Writing  microcode, I tend to look at design issues architecturally;
however, some experts believe that new architectures may be invented
that will not be encompassed by contemporary computational theory.
In the August 1986 SPECTRUM (from IEEE), the series of articles
on optical computing addresses this problem:

  * "In C. Lee Giles view, (program manager of the Air Force Office of *
  * Scientific Research in Washington, D.C.), theoretical computer     *
  * science has 'stuck its neck out' by saying that computational      *
  * models define anything that is computable, since it is unknown     *
  * whether there are tasks these models cannot perform that the       *
  * human brain can."   .....                                          *
*                                                                    *
  * (from the author Trudy E. Bell), "it remains to be seen whether    *
  * (optical) neural network architectures represent a new             *
  * computational model."

I would love to prove some philosophers wrong about how "computable
tasks" can ultimately be cast in the form of FSM's. The dawn of
the general purpose optical computer architecture may well introduce
new models that require a new breed of non FSM computational theory. 
However, I think that computer engineering will focus on getting
good "old fashioned" FSM's to work in the real world for a long time,
and even at this level of complexity there will always be bugs from
human behavior, not "machine behavior".

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.49.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.51.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-37</DOCNO>
<DOCOLDNO>IA012-000123-B022-303</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.51.html 128.240.150.127 19970217004510 text/html 16993
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:43:37 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 51</TITLE>
<LINK REL="Prev" HREF="/Risks/3.50.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.52.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.50.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.52.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 51</H1>
<H2> Sunday, 7 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Computer almost created swing vote 
</A>
<DD>
<A HREF="#subj1.1">
Bjorn Freeman-Benson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Computer Sabotage of Encyclopedia Brittania 
</A>
<DD>
<A HREF="#subj2.1">
Rosanna Lee
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  F-16 software 
</A>
<DD>
<A HREF="#subj3.1">
Wayne Throop
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Arbiter failures and design failures 
</A>
<DD>
<A HREF="#subj4.1">
Martin Harriman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Systems errors (hardware AND humans) 
</A>
<DD>
<A HREF="#subj5.1">
Bill Janssen
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Re: Terminal (!) lockup 
</A>
<DD>
<A HREF="#subj6.1">
Roy Smith
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Computer almost created swing vote
</A>
</H3>
<address>
Bjorn Freeman-Benson
&lt;<A HREF="mailto:bnfb@uw-june.arpa ">
bnfb@uw-june.arpa 
</A>&gt;
</address>
<i>
Sun, 7 Sep 86 10:44:01 PDT
</i><PRE>

Quoted without permission from the Seattle Times, Sunday Sep. 7, 1986:

AP, PHOENIX, Ariz. -- Tuesday's primary elections in Maricopa County would
have been a mess if officials hadn't figured out that a compuer was set up
to give all Republican votes to the Democrats and vice versa.
    "If it had gone undetected, there would have a major, major problem with
the election," County Recorder Keith Poletis said Friday.  Poletis said that,
if the compuer hadn't been fixed, a race with three Republicans and one
Democrat would given the Democrat's votes to one of the Rebulicans.
    Votes cast for the remaining Republicans would have been zapped into the
void by the computer, because the software would find no Democratic opponents.
"The computer sorts the cards into two piles, and it was sorting the
Republicans into the Democrats' slots and the Democrats into the Republican
slots," Poletis said.
    A clerical error made the computer's cards were ordered was to blame for
the mix-up, said Joe Martina, director of the county's computer systems.  The
error was caught during the secretary of state's test of the country's cards
late Thursday.

End quote.

In my mind, this brings up an interesting question: should errors like this
be reported (1) to the general public and (2) to the software engineering
community?  I think the answer to (2) is yes -- the more data we have on the
types of errors that occur involving computers, the better grasp we will have
on solving them.  However, for (1), I see this arguement:
    Con - The testing procedures before acceptance caught the error.
        - The public will just lose faith in computers.
    Pro - The public should know, because what if the testing hadn't?
    Con - The public in general is not knowledgeable about computers and
	  software, and the general press is sensationalist.  Thus any
	  case reported will not be studied in the necessary depth.
	- An analogy with civil engineering: should the public know that
	  the first design for a bridge collapsed during testing?  Or is
	  it just enough to know that the final bridge works?

						Bjorn N Freeman-Benson

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Computer Sabotage of Encyclopedia Brittania
</A>
</H3>
<address>
Rosanna Lee 
&lt;<A HREF="mailto:rosanna@CSL.SRI.COM">
rosanna@CSL.SRI.COM
</A>&gt;
</address>
<i>
Sat 6 Sep 86 18:09:51-PDT
</i><PRE>
To: risks@CSL.SRI.COM

Chicago Tribune [From San Jose Mercury News, Friday, Sept 5, 1986]

LAID-OFF WORKER SABOTAGES ENCYCLOPEDIA

CHICAGO - An employee of the Encyclopedia Britannica, disgruntled at having
been laid off, apparently committed computer sabotage by altering portions of
the text being prepared for updated editions of the renowned encyclopedia.

The discovery has prompted an exhaustive check to ensure that painstaking work,
in the words of the editor, "is not turned into garbage."

"We have uncovered evidence of deliberate sabotage in the EB computer files,"
editor-in-chief Tom Goetz disclosed in an Aug. 28 memo to editorial personnel
at the chicago headquarters of the oldest continually published reference work
in the English language.

The unidentified former employee has confessed and is helping undo the damage,
a spokesman said, although the company may press criminal charges.  He said the
44-million word 1987 edition is safe, but employees are believed to be laboring
overtime to catch alterations that could find their way into the 1988 edition.

Among the former employee's more vivid changes, sources said, was changing
references to Jesus Christ to Allah, the Moslem name for God.

Goetz declined to comment Thursday other than to say, "Everything is under
control."  Another industry executive said, "In the computer age, this is
exactly what we have nightmares about."

In the first of three memos to editorial staffers, Goetz wrote, "What is 
perhaps most distressing for each of us is the knowledge that some of our hard
work has been turned into garbage by either a very sick or a very vicious 
person."

At the time, he said that the actions constituted a crime under Illinois law,
that the company planned to pursue legal actions "vigorously" and that it was
issuing new computer passwords to employees.

In a staff memo dated Wednesday, Goetz informed employees that "we have
successfully concluded the matter of the sabotage of the encyclopedia's
data base.

"The 1987 printing is secure," Goetz stated.

The publication first was alerted to a problem, sources said, when a worker
scanned the computer data base and discovered the clearly odd insertion of
the names of a company executive and a private consulting firm apparently

   [There are several problems in believing that this audit-trail approach
    is fool-proof.  First of all, it relies on a password.  Masquerading is
    therefore a concern.  The second is probably more important -- any
    self-respecting system programmer or cracker is probably able to alter
    the audit trail.  It is dangerous to assume that the only disgruntled
    employess are those who are NOT computer sophisticates... PGN]

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
F-16 software
</A>
</H3>
<address>
&lt;<A HREF="mailto:rti-sel!dg_rtp!throopw%mcnc.csnet@CSNET-RELAY.ARPA">
rti-sel!dg_rtp!throopw%mcnc.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Fri, 5 Sep 86 13:19:25 edt
</i><PRE>
Apparently-To: mcnc!csl.sri.com!risks

&gt; It sounds very funny that the software would let you drop a bomb on the wing
&gt; while in inverted flight but is it really important to prevent this? Is it
&gt; worth the chance of introducing a new bug to fix this very minor problem?

&gt;      [The probability is clearly NONZERO.  It is very dangerous to start
&gt;       making assumptions in programming about being able to leave out an
&gt;       exception condition simply because you think it cannot arise.  Such
&gt;       assumptions have a nasty habit of interacting with other assumptions
&gt;       or propagating.  PGN]

It is also dangerous to start making assumptions about the ways in which
the system will be used.  Can you really not think of a reason why one
would want to "drop" a bomb while the dorsal surface of the plane points
towards the planet's center (a possible interpretation of "inverted")?
I can think of several.

I am trying to make the point that the gross simplification of
"preventing bomb release while inverted" doesn't map very well to what I
assume the actual goal is: "preventing weapons discharge from damaging
the aircraft".  This is yet another instance where the assumptions made
to simplify a real-world situation to manageable size can easily lead to
design "errors", and is an architypical "computer risk" in the use of
relatively simple computer models of reality.

In addition to all this, it may well be that one doesn't *want* to
prevent all possible modes weapons discharge that may damage the
aircraft...  some of them may be useful techniques for use in extreme
situations.

   The more control,
   The more that requires control.
   This is the road to chaos.
                                --- PanSpechi aphorism {Frank Herbert}

Wayne Throop      &lt;the-known-world&gt;!mcnc!rti-sel!dg_rtp!throopw

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
 Arbiter failures and design failures
</A>
</H3>
<address>
    Martin Harriman 
&lt;<A HREF="mailto:MARTIN%SRUCAD%sc.intel.com@CSNET-RELAY.ARPA">
MARTIN%SRUCAD%sc.intel.com@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Fri, 5 Sep 86 09:38 PDT
</i><PRE>

Bob Estell raises two quite different failure mechanisms in his message.
The first mechanism he mentions is the well known problem of making a
reliable arbiter; he then goes on to discuss the quite different problem
of design errors in hardware, microcode, or systems software.

The arbiter problem is well known; fundamentally, there is no absolutely
reliable way to sample asynchronous signals in a synchronous system,
though there are ways of greatly reducing the probability of failure.
In this sense, no computer which incorporates asynchronous interrupts
is deterministic, since you can not predict its behavior cycle by cycle.
It is important to take this effect into account in the design of
the system and any software which cares about the timing of these external
events.

There are other interesting failure modes, where the arbiter essentially
says "maybe," instead of giving a clear yes or no answer; careful
circuit design can reduce the probability of these failures to one
failure every few thousand years (at least according to our last set
of simulations).

The bulk of Bob's message is a discussion of the probability of design
bugs.  Anyone who has seen the errata sheets for a microprocessor
or the ECO history of a mainframe will know that computer hardware
is imperfect.  This may be news to some computer programmers, but
there is such a thing as a computer error; for instance, the first
stepping of Intel's 80186 was convinced that the product of two
negative numbers was negative.

  --Martin Harriman (martin%srucad@sc.intel.com)
    Intel Santa Cruz

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Systems errors (hardware AND humans)
</A>
</H3>
<address>
Bill Janssen 
&lt;<A HREF="mailto:janssen@mcc.com">
janssen@mcc.com
</A>&gt;
</address>
<i>
Fri, 5 Sep 86 18:07:08 CDT
</i><PRE>

Bob Estell's note on machine errors made me think of an error that
I found some years ago.  I was writing a C program that, among other
things, provided a virtual connection between two serial ports.  The
code looked something like this:

	while (in_connection_mode)
		{
		if (input_available(port1))
			port2-&gt;output = port1-&gt;input;
		else if (input_available(port2))
			port1-&gt;output = port2-&gt;input;
		}

where `port1' and `port2' were pointers to register banks on the
serial port controllers.  When we tried it out, it didn't work.  To
make a long story short, it didn't work because assembly code for
"port2-&gt;output = port1-&gt;input" was produced very efficiently as
(something like) "MOVB 4(A4),8(A5)", which would still have been
OK, except that both serial ports were on the same chip and the chip
needed a recovery interval after doing a read before doing a write.
Working code used the line "port2-&gt;output = temp = port1-&gt;input", to
introduce a slight delay!!

Now, where's the source of the error here?  What bugs me is that you
can PROVE that the (non-functional) code functions properly... if you
ignore the hardware quirks, which aren't documented.  And if the
compiler produced less efficient code (load register; store register)
the HLL code would work.  And if the machine architecture didn't have
memory-to-memory move instructions the code would work.  And if the
computer clock was slower, the code would work.  I tend to think that
the error was in the characterization of the hardware, which described
the two serial ports as independent.  But perhaps the error is
actually in not VERIFYING the hardware characterization...

Bill
--
 Bill Janssen, MCC Software Technology
 9430 Research Blvd, Austin, Texas  78759
 ARPA:  janssen@mcc.com            PHONE:  (512) 339-3682
 UUCP:  {ihnp4,seismo,harvard,gatech,pyramid}!ut-sally!im4u!milano!janssen

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Re: Terminal (!) lockup
</A>
</H3>
<address>
Roy Smith
&lt;<A HREF="mailto:cmcl2!phri!roy@seismo.CSS.GOV ">
cmcl2!phri!roy@seismo.CSS.GOV 
</A>&gt;
</address>
<i>
Fri, 5 Sep 86 18:23:34 edt
</i><PRE>

	I wonder: How common is this property [being able to break it
	by pushing the wrong combination of buttons] of terminals (or
	other equipment)?

	We have some CTS-2400 auto-dial modems that let you set all sorts
of parameters that get stored in eeprom.  It's not too hard to set it up so
it doesn't echo and doesn't produce any output at all.  This condition
persists even after power-cycling.  It's not really dead, but unless you
realize what you did and know the magic sequence to turn back on echoing
and command processing, it sure looks that way (if it looks like a duck and
quacks like a duck ...)

	Take a typical time-sharing system, erase the boot block from disk
and turn it off.  You've sure done a nice imitation of breaking it (I
consider having to toggle in a binary boot program as very much akin to
opening up a terminal to fix it).  If you've got a writeable control store,
you could mess yourself up even more.

	The (clever) people who designed the Apple LaserWriter must have
been thinking along these lines.  There are 2 serial interfaces on the LW.
You can run a little PostScript to change the baud rate (stored in eeprom)
on either or both.  If you want to disable one interface, you just set its
baud rate to 0.  According to the documention (I've never tried it :-)) it
won't let you set both channels to 0 baud.  If you could, there would be no
way to talk to it short of yanking the eeprom.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.50.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.52.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-38</DOCNO>
<DOCOLDNO>IA012-000123-B022-324</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.52.html 128.240.150.127 19970217004530 text/html 25118
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:43:55 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 52</TITLE>
<LINK REL="Prev" HREF="/Risks/3.51.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.53.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.51.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.53.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 52</H1>
<H2> Tuesday, 9 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Re:  F-16 software 
</A>
<DD>
<A HREF="#subj1.1">
Nancy Leveson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Upside-down F-16's and "Human error" 
</A>
<DD>
<A HREF="#subj2.1">
Jon Jacky
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  F-16 software 
</A>
<DD>
<A HREF="#subj3.1">
Scott E. Preece
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Do More Faults Mean More Faults? 
</A>
<DD>
<A HREF="#subj4.1">
Ken Dymond
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Why components DON'T interact more often 
</A>
<DD>
<A HREF="#subj5.1">
Bob Estell
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Computer almost created swing vote 
</A>
<DD>
<A HREF="#subj6.1">
Scott E. Preece
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Computer Sabotage [MISSING LAST LINE FROM <A HREF="/Risks/3.51.html">RISKS-3.51</A>]
</A>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Computer Sabotage of Encyclopedia Brittanica 
</A>
<DD>
<A HREF="#subj8.1">
Scott E. Preece
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj9">
  Captain Midnight &amp; military satellites 
</A>
<DD>
<A HREF="#subj9.1">
Werner Uhrig
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj10">
  Re: always mount a scratch monkey 
</A>
<DD>
<A HREF="#subj10.1">
Alexander Dupuy
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj11">
  Erroneous computer printout used in public debates 
</A>
<DD>
<A HREF="#subj11.1">
Chris Koenigsberg
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Re:  F-16 software
</A>
</H3>
<address>
Nancy Leveson 
&lt;<A HREF="mailto:nancy@ICSD.UCI.EDU">
nancy@ICSD.UCI.EDU
</A>&gt;
</address>
<i>
08 Sep 86 09:53:29 PDT (Mon)
</i><PRE>

Wayne Throop writes:  

   &gt;it may well be that one doesn't *want* to prevent all possible 
   &gt;modes weapons discharge that may damage the aircraft ... some of
   &gt;them may be useful techniques for use in extreme situations.

This raises some extremely important points that should be remembered
by those attempting to deal with risk.

   1) nothing can be made 100% safe under all circumstances.  In papers I
      have written I have pointed out that safety razors and safety matches
      are not completely safe, they are only *safer* than their alternatives.
      Drinking water is usually considered safe, but drinking too much water
      can cause kidney failure.  
   
   1) the techniques used to make things safer usually involve
      limiting functionality or design freedom and thus involve tradeoffs
      with other desirable characteristics of the product.

All we can do is attempt to provide "acceptable risk."  What is "acceptable" 
will depend upon moral, political, and practical issues such as how much
we are willing to "pay" for a particular level of safety.

I define "software safety" as involving procedures to ensure that the
software will execute within a system context without resulting in
unacceptable risk.  This implies that when building safety-critical systems, 
one of the first and most important design problems may be in identifying
the risks and determining what will be considered acceptable risk for that
system.  And just as important, our models and techniques are going to have
to consider the tradeoffs implicit in any attempt to enhance safety and
to allow estimation of the risk implicit in any design decisions.  
If we have such models, then we can use them for decision making, including 
the decision about whether acceptable risk can be achieved (and thus
whether the system can and should be built).  If it is determined that
acceptable risk can be achieved, then the models and techniques should 
provide help in making the necessary design decisions and tradeoffs.
The important point is that these decisions should be carefully considered
and not subject to the whim of one programmer who decides in an ad hoc
fashion whether or not to put in the necessary checks and interlocks.

      Nancy Leveson
      Information &amp; Computer Science
      University of California, Irvine

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Upside-down F-16's and "Human error"
</A>
</H3>
<address>
Jon Jacky
&lt;<A HREF="mailto:jon@uw-june.arpa ">
jon@uw-june.arpa 
</A>&gt;
</address>
<i>
Mon, 8 Sep 86 16:55:19 PDT
</i><PRE>

&gt; (... earlier postings mentioned "fly-by-wire" F-16 computer would 
&gt; attempt to raise landing gear while aircraft was sitting on runway,
&gt; would attempt to drop bombs while flying inverted, and other such 
&gt; maneuvers -- in response to pilot's commands

These are regarded as errors?  Maybe I'm missing something, but it sounds 
like the right solution is to remind the pilots not to attempt obviously
destructive maneuvers.  I detect a notion floating about that software 
should prevent any unreasonable behavior.  This way lies madness.  Do we have 
to include code to prevent the speed from exceeding 55 mph while taxiing down
an interstate highway?

My point is, if you take the approach that the computer is supposed to check
for and prevent any incorrect behavior, then you have saddled yourself with
the task enumerating every possible thing the system should NOT do.  Such a 
list of prohibited behaviors is likely to be so long it will make the 
programming task quite intractable, not to mention that you will never get all
of them.

I suggest that the correct solution is the time-honored one: the operator must
be assumed to possess some level of competence; no attempt is made to 
protect against every conceivable error that might be committed by a flagrantly
incompetent or malicious operator.

Note that all non-computerized equipment is designed this way.  If I steer my
car into a freeway abutment, I am likely to get killed.  Is this a "design
flaw" or an "implementation bug?"  Obviously, it is neither.  People who are
drunk or suicidal are advised not to drive.

This relates to the ongoing discusssion about "human error."  This much-abused
term used to refer to violations of commonly accepted standards of operator
performance -- disobeying clear instructions, attempting to work when drunk, 
things like that.  Apparently it has come to refer to almost any behavior which,
in retrospect, turns out to have unfortunate consequences.  It is sometimes 
applied to situations for which the operator was never trained, and which the 
people who installed the system had not even anticipated.  

When abused in this way, the term "human error" can be a transparent attempt
to deflect blame from designers and management to those with the least control
over events.  Other times, however, it is evidence of genuine confusion over
who is responsible for what.  Right at the beginning, designers must draw a
clear line between what the automated system is supposed to do and what the
operators must do.  This may require facing the painful truth that there 
may be situations where, if the operator makes a mistake, a real disaster
may occur.  The choice is then one of ensuring the trustworthiness of the
operators, or finding an alternative approach to the problem that is more
robust.  

I suggest that if additional computer-based checking against operator errors
keeps getting added on after the system has been installed, it is evidence that
the role of the operator was not very clearly defined to begin with.

-Jonathan Jacky
University of Washington

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
F-16 software
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%ccvaxa@GSWD-VMS.ARPA">
preece%ccvaxa@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Mon, 8 Sep 86 09:36:42 cdt
</i><PRE>

&gt; From: amdcad!phil@decwrl.DEC.COM (Phil Ngai)

&gt; It sounds very funny that the software would let you drop a bomb on the
&gt; wing while in inverted flight but is it really important to prevent
&gt; this?

Others have already pointed out that sometimes you may WANT to
release the bomb when inverted.  I would ask the more obvious
question: Would a mechanical bomb release keep you from releasing
the bomb when inverted?  I tend to doubt it.  While it's nice
to think that a software controlled plane should be smarter than
a mechanical plane, I don't think it's fair to cite as an error
in the control software that it isn't smarter than a mechanical
plane...

If, in fact, the mechanical release HAD protected against inverted
release, I would have expected that to be part of the specs for
the plane; I would also expect that the acceptance tests for the
software comtrolled plane would test all of the specs and that
the fault would have been caught in that case.

scott preece
gould/csd - urbana
uucp:	ihnp4!uiucdcs!ccvaxa!preece
arpa:	preece@gswd-vms

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Do More Faults Mean More Faults?
</A>
</H3>
<address>
"DYMOND, KEN" 
&lt;<A HREF="mailto:dymond@nbs-vms.ARPA">
dymond@nbs-vms.ARPA
</A>&gt;
</address>
<i>
8 Sep 86 09:18:00 EDT
</i><PRE>
To: "risks" &lt;risks@csl.sri.com&gt;

In RISKS 3.50 Dave Benson comments in "Flight Simulator
Simulators Have Faults" that

    &gt;We need to understand that the more faults found at
    &gt;any stage to engineering software the less confidence one has in the
    &gt;final product.  The more faults found, the higher the likelyhood that
    &gt;faults remain.  
 
This statement makes intuitive sense, but does anyone know of any data 
to support this ?  Is this true of any models of software failures ?  
Is this true of the products in any of the hard engineering fields -- civil, 
mechanical, naval, etc. -- and do those fields have the confirming data ?

Ken Dymond, NBS

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
why components DON'T interact more often
</A>
</H3>
<address>
"SEFB::ESTELL" 
&lt;<A HREF="mailto:estell%sefb.decnet@nwc-143b.ARPA">
estell%sefb.decnet@nwc-143b.ARPA
</A>&gt;
</address>
<i>
8 Sep 86 08:12:00 PST
</i><PRE>
To: "risks" &lt;risks@csl.sri.com&gt;
cc: lenoil@xx.lcs.mit.edu
Reply-To: "SEFB::ESTELL" &lt;estell%sefb.decnet@nwc-143b.ARPA&gt;

I guess I neglected to emphasize a key word: "MAY."
My original posting said "...may interact..."
I am well aware that components SHOULD *NOT* interact.
I am also well aware that hardware designers labor to make sure that 
the actual interactions are 
 (1) very infrequent; and 
 (2) not terribly damaging when they inevitably do occur.  
Similarly, software designers [good ones!] labor to restrict the 
inevitable interactions; and limit the damage done when they occur.
Since each layer of a well designed, carefully implement system 
"filters" faithfully, the result is a system that will run for months 
[years?] without random failures.
But random failures do occur.  My ancient stories were replaced by more
current ones in later RISKS postings.
Until the theory and the practice of computing systems design EACH admit
that random error [including, but not limited to, interactions] is real,
we'll continue to build systems and applications less reliable than they
could be - or should be.

Bob

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Computer almost created swing vote
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%ccvaxa@GSWD-VMS.ARPA">
preece%ccvaxa@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Mon, 8 Sep 86 10:02:18 cdt
</i><PRE>

&gt; From: bnfb@uw-june.arpa (Bjorn Freeman-Benson)
&gt;  In my mind, this brings up an interesting question: should errors like
&gt; this be reported (1) to the general public and (2) to the software
&gt; engineering community?
----------
I don't think errors like this should be HIDDEN, but I also don't
think this demands issuing a press release.  The reason you do
a test is to determine whether your procedures are working --
it shouldn't be thought newsworthy that you find mistakes in
testing.  If, on dry run day, a manual election counting system
had mistakenly recorded the Democratic votes on the master tally
sheet on the Republican line and vice versa, the counter would
have been apprised of the error and instructed in proper
procedure, but I don't think they'd have issued a press release.

The problem in this particular case wasn't that the system
didn't work, but that the operators didn't understand the
operating procedures.  That's no big deal, but the election
judges should be warned what to look for on election night to
see that the control information is correctly set up (regression
testing).

-- 
scott preece
gould/csd - urbana
uucp:	ihnp4!uiucdcs!ccvaxa!preece
arpa:	preece@gswd-vms

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Computer Sabotage  [LAST LINE MISSING FROM <A HREF="/Risks/3.51.html">RISKS-3.51</A>]
</A>
</H3>
<address>
Rosanna Lee 
&lt;<A HREF="mailto:rosanna@CSL.SRI.COM">
rosanna@CSL.SRI.COM
</A>&gt;
</address>
<i>
Sat 6 Sep 86 18:09:51-PDT
</i><PRE>
To: risks@CSL.SRI.COM

   [LAST LINE INADVERTENTLY TRUNCATED... COMPLETE LAST PARAGRAPH FOLLOWS.]
The publication first was alerted to a problem, sources said, when a worker
scanned the computer data base and discovered the clearly odd insertion of
the names of a company executive and a private consulting firm apparently
viewed by the former employee as partly responsible for the layoff decision.

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
Computer Sabotage of Encyclopedia Brittanica
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%ccvaxa@GSWD-VMS.ARPA">
preece%ccvaxa@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Mon, 8 Sep 86 10:12:54 cdt
</i><PRE>

&gt;  [There are several problems in believing that this audit-trail approach
&gt;   is fool-proof.  First of all, it relies on a password.  Masquerading is
&gt;   therefore a concern.  The second is probably more important -- any
&gt;   self-respecting system programmer or cracker is probably able to alter
&gt;   the audit trail.  It is dangerous to assume that the only disgruntled
&gt;   employess are those who are NOT computer sophisticates... PGN]
----------
Clearly the audit trail is not enough to protect against insider
damage by systems programmers, but the article says nothing about
whether there are other tools designed to deal with such users --
just that audit trail methods were sufficient in this case.  Let's
not jump to conclusions.

Curiously, embezzlement, fraud, doctored documents, and disgruntled
employee sabotage all pre-dated computers.  It appears to be the
case (I haven't heard the details yet) that in this case the fact
that the system was computerized allowed them to identify the damage
quickly and repair it.  If the files were on paper and the
saboteur had simply altered and replaced random pages of random
articles in the files, the damage would have been worse and much
harder to trace and fix.  The system doesn't have to be foolproof to
be an improvement over manual systems.

-- 
scott preece
gould/csd - urbana
uucp:	ihnp4!uiucdcs!ccvaxa!preece
arpa:	preece@gswd-vms

</PRE>
<A NAME="subj9"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj9.1">
Captain Midnight &amp; military satellites (Mother Jones, October 86)
</A>
</H3>
<address>
Werner Uhrig  
&lt;<A HREF="mailto:CMP.WERNER@R20.UTEXAS.EDU">
CMP.WERNER@R20.UTEXAS.EDU
</A>&gt;
</address>
<i>
Mon 8 Sep 86 00:01:30-CDT
</i><PRE>
To: telecom@R20.UTEXAS.EDU, risks@R20.UTEXAS.EDU
Message-ID: &lt;12237198949.8.CMP.WERNER@R20.UTEXAS.EDU&gt;

[ pointer to article in print:  Mother Jones, Oct '86 Cover Story on Satellite
  Communications Security (or lack thereof) ]

(p.26)	CAPTAIN MIDNIGHT, HBO, AND WORLD WAR III - by Donald Goldberg
	John "Captain Mignight" MacDougall has been caught but the flaws he
exposed in the U.S. military and commercial ssatellite communications system
are still with us and could lead to far scarier things than a $12.95 monthly
cable charge.

(p.49)	HOME JAMMING: A DO-IT-YOURSELF GUIDE - by Donald Goldberg
	What cable companies and the Pentagon don;t want you to know.

PS: Donald Goldberg is described as "senior reporter in Washington, D.C., for
the syndicated Jack Anderson column

[ this is not an endorsement of the article, just a pointer.
  you be the judge of the contents. ]

</PRE>
<A NAME="subj10"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj10.1">
Re: always mount a scratch monkey
</A>
</H3>
<address>
Alexander Dupuy 
&lt;<A HREF="mailto:dupuy%amsterdam@columbia.edu">
dupuy%amsterdam@columbia.edu
</A>&gt;
</address>
<i>
Mon, 8 Sep 86 03:00:30 EDT
</i><PRE>

Here's another version of this story, from the ever reliable usenet net.rumor.
The existence of the alternate versions puts both pretty much in the realm of
apocrypha.  It's still a good story though...

From: moroney@jon.DEC (Mike Moroney)
Newsgroups: net.rumor
Subject: Re: Computer war stories
Date: 19 Mar 86 18:19:22 GMT
Organization: Digital Equipment Corporation


Yet another old classic war story.
--

It seems that there was a certain university that was doing experiments in
behavior modification in response to brain stimulation in primates.  They had
this monkey with a number of electrodes embedded in it's brain that were hooked
up to a PDP-11.  They had several programs that would stimulate different parts
of the monkey's brain, and they had spent over a year training the monkey to
respond to certain stimuli.  Well, eventually the PDP developed problems, and
field service was called in.  Due to some miscommunication, the field service
representative was not informed of the delicacy of this particular setup, and
the people running the experiment were not informed that field service was
coming to fix the machine.  The FS representative then booted up a diagnostic
system I/O exerciser.  After several minutes of gyrations, the monkey expired,
it's brain fried.

The moral, of course, is "Always mount a scratch monkey"

</PRE>
<A NAME="subj11"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj11.1">
Erroneous computer printout used in public debates
</A>
</H3>
<address>
Chris Koenigsberg
&lt;<A HREF="mailto:ckk@andrew.cmu.edu ">
ckk@andrew.cmu.edu 
</A>&gt;
</address>
<i>
Mon,  8 Sep 86 10:23:56 edt
</i><PRE>

[Brief background on this story: In Pennsylvania, all sales of wine and hard
liquor are made at State Stores, run by the Liquor Control Board. The
Governor has been trying to abolish the board and let private industry take
over the liquor business, but LCB employee unions have been fighting against
him. The LCB held a 20% discount sale on the Saturday before Labor Day, and
the unions were outraged because the LCB's mission is actually to control
alcohol, not promote it, and the sale seemed to encourage consumption on the
holiday weekend. The debate over how much was sold, how much profit or loss
was made, and the effects on holiday weekend drunk driving were hot news all
week. Now this report of a computer error comes after public debate already
occurred, in which people relied on the incorrect sales figures.]
          ++++++++++++++++++++++++++++++++++++++++++++++++++
Article from the Pittsburgh Post-Gazette, Saturday, September 6, 1986,
written by Gary Rotstein (copyright 1986 PG Publishing Co.)

"20% discount sale brought LCB $18.9 million, not $8.5 million"

Admitting a $10 million flub in a computer printout, the Pennsylvania Liquor
Control Board reported yesterday that it sold a one-day record high of $18.9
million of alcohol last Saturday. LCB Chairman Daniel Pennick told reporters
Wednesday that the second 20 percent discount sale in the agency's history
had grossed only about $8.5 million. That would have been $5 million more
than was sold on the comparable date a year ago, but less than the $11
million one-day high recorded during a similar sale in June.

LCB spokesman Robert Ford said the agency's comptroller's office reviewed the
figures yesterday morning and realized an important digit - the numeral 1
indicating $10 million - had been unable to fit on the initial computer
printout tallying the sales figure. Once a correction was made and final
purchases from Saturday were tacked on, the LCB learned it had sold $18.9
million in goods.

Ford noted that the comptroller's office personnel responsible for the
mistake are employees of the governor's budget office rather than the LCB.

"The fact that someone made an error doesn't bother us," Ford said. "We're
just happy about the sales figures."

Whether the higher sales is good or bad news for the LCB, however, is in
dispute between the agency and its longtime critic, Governor Thornburgh.
Thornburgh's budget office has estimated, based on an analysis of the LCB's
receipts and costs last year, that when the price of a bottle is reduced by
20 percent the agency loses an average of $1.13 on each item sold. That
scenario means it's worse for the LCB's financial picture to have $18.9
million in discount sales than $8.5 million, administration spokesman Michael
Moyle pointed out.

Ford maintained that the sale only cut into the size of the LCB's profits and
did not actually amount to a net loss.

"We didn't lose a penny on any bottle sold," he said.
          ++++++++++++++++++++++++++++++++++++++++++++++++++
[notice that Ford emphasizes how the mistake was made by the governor's
budget office (the same office responsible for the disputed estimate that a
20% sale would lose $1.13 on each item), and not by his LCB employees - his
agency is under fire and is close to being dissolved by the legislature. But
it's made very clear that a human error led to the missing digits, rather
than trying to "blame it on the computer"]
Christopher Koenigsberg
Center for Design of Educational Computing
Carnegie-Mellon University
(412)268-8526
ckk@andrew.cmu.edu

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.51.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.53.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-39</DOCNO>
<DOCOLDNO>IA012-000123-B022-339</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.53.html 128.240.150.127 19970217004552 text/html 11481
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:44:17 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 53</TITLE>
<LINK REL="Prev" HREF="/Risks/3.52.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.54.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.52.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.54.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 53</H1>
<H2>Wednesday, 10 September 1986</H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Hardware/software interface and risks 
</A>
<DD>
<A HREF="#subj1.1">
Mike Brown
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  More on Upside down F-16s 
</A>
<DD>
<A HREF="#subj2.1">
Mike Brown
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  "Unreasonable behavior" and software 
</A>
<DD>
<A HREF="#subj3.1">
Gary Chapman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: supermarket crashes 
</A>
<DD>
<A HREF="#subj4.1">
Scott Preece
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Hardware/software interface and risks
</A>
</H3>
<address>
&lt;<A HREF="mailto:mlbrown@nswc-wo.ARPA">
mlbrown@nswc-wo.ARPA
</A>&gt;
</address>
<i>
Tue, 9 Sep 86 09:48:31 edt
</i><PRE>

In RISKS 3.51 Bill Janssen writes of errors made in failing to consider
the interaction of the hardware and the software under design.  This
failing was all too common in the writing of assembly and machine code 
during the early days of programming.  Discrete wired machines often had
OP codes that were not generally well known (i.e. the computer designers
kept it secret).  Interestingly, these unknown OP codes were included when
more modern machines emulated the original discrete design.  An excellent 
example is the old IBM 4-PI CP-3 and the IBM 360.  

The hardware/software interface within the machine can create significant
problems from both a software safety and software security standpoint.  
Software designers will have to have an increasingly detailed knowledge of
the total system to produce the safe, secure software that critical systems
will require.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
More on Upside down F-16s
</A>
</H3>
<address>
&lt;<A HREF="mailto:mlbrown@nswc-wo.ARPA">
mlbrown@nswc-wo.ARPA
</A>&gt;
</address>
<i>
Tue, 9 Sep 86 10:00:00 edt
</i><PRE>

In RISKS 3.52 Jon Jacky writes:

&gt;..it sounds like the right solution is to remind the pilots not to attempt
&gt; obviously destructive maneuvers.  ...if you take the approach that the 
&gt; computer is supposed to check for and prevent any incorrect behavior, then
&gt; you have saddled yourself with the task enumerating every possible thing the 
&gt; system should not do."

Perhaps a solution is to remind the pilots not to attempt obviously destruc-
tive maneuvers however, relying on procedures to eliminate or reduce the
risk of hazards is the least acceptable way.  Pilots are human and as such
are prone to making errors.  Look at the safety record for general aviation
and the Navy - both are dismal and are often reported to be due to pilot
error.  Its fine to tell the pilot "Lower your wheels before you land, not
after" but we still have gear up landings.  We should not concern ourselves
with checking for and preventing any incorrect behavior but we should preclude
that behavior which will result in damage to or loss of the aircraft or the
pilot.  We do not need to anticipate every possible mistake that he can make
in this regard either - all we need to do are to identify the hazardous operational modes and prevent
their occurrence.

Mike Brown, Chairman Triservice Software Systems Safety Working Group

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
"Unreasonable behavior" and software
</A>
</H3>
<address>
Gary Chapman 
&lt;<A HREF="mailto:chapman@russell.stanford.edu">
chapman@russell.stanford.edu
</A>&gt;
</address>
<i>
Tue, 9 Sep 86 14:28:24 pdt
</i><PRE>
To: RISKS@CSL.SRI.COM

Jon Jacky wrote:

	I detect a notion floating about that software should 
	prevent any unreasonable behavior.  This way lies mad-
	ness.  Do we have to include code to prevent the speed
	[of an F-16] from exceeding 55 mph while taxiing down
	an interstate highway?

I certainly agree with the thrust of this.  But we should note that there is
plenty of evidence that coding in prohibitions on unreasonable behavior will
be required, particularly in the development of "autonomous" weapons that
are meant to combat the enemy without human "operators" on the scene.

Here's a description of a contract let by the United States Army Training and
Doctrine Command (TRADOC), Field Artillery Division, for something called a
"Terminal Homing Munition" (THM):

	Information about targets can be placed into the munitions
	processor prior to firing along with updates on meteorologi-
	cal conditions and terrain.  Warhead functioning can also be
	selected as variable options will be available.  The intro-
	duction of VHSIC processors will give the terminal homing
	munitions the capability of distinguishing between enemy and
	friendly systems and finite target type selection.  Since
	the decision of which target to attack is made on board the
	weapon, the THM will approach human intelligence in this area.
	The design criteria is pointed toward one munition per target
	kill.

(I scratched my head along with the rest of you when I saw this;  I've always
thought if you fire a bullet or a shell out of a tube it goes until it hits
something, preferably something you're aiming at.  But maybe the Army has
some new theories of ballistics we don't know about yet.)

As Nancy Leveson notes, we make tradeoffs in design and functionality for
safety, and how many and what kinds of tradeoffs are made depends on ethical,
political and cost considerations, among other things.  Since, as Jon Jacky
notes, trying to prohibit all unreasonable situations in code is itself un-
reasonable, then one wonders what sorts of things will be left out of the code
of terminal homing munitions?  What sorts of things will we have to take into
account in the code of a "warhead" that is supposed to find its own targets?
What level of confidence would we have to give soldiers (human soldiers--we
may have to get used to using that caveat) operating at close proximity to
THMs that the things are "safe"?

I was once a participant in an artillery briefing by a young, smart artillery
corps major.  This officer told us (a bunch of grunts) that we no longer needed
"forward observers," or guys attached to patrols to call in the ranges on
artillery strikes.  In fact, said the major, we don't need to call in our
artillery stikes at all--his methods had become so advanced  would
just know where and when we needed support.  We all looked at him like he had
gone stark raving mad.  An old grizzled master sergeant who had been in the Army
since Valley Forge I think, got up and said, "Sir, with all due respect, if I
find out you're in charge of the artillery in my sector, I will personally come
back and shoot you right between the eyes."  (His own form of THM "approaching
human intelligence", no doubt.) (I wouldn't be surprised if this major wrote
the language above.)

What is "unreasonable" behavior to take into account in coding software?  The
major's or the sergeant's?
							-- Gary Chapman

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: supermarket crashes
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%ccvaxa@GSWD-VMS.ARPA">
preece%ccvaxa@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Mon, 8 Sep 86 09:54:06 cdt
</i><PRE>

&gt; From: mogul@decwrl.DEC.COM (Jeffrey Mogul)

&gt; I don't often shop at that market, partly because the markets I do use
&gt; have cashiers who know what things are rather than relying on the
&gt; computer. Some day, just for fun, I might mark a pound of pecans with
&gt; the code number for walnuts, and see if I can save some money.
----------
Does the word "fraud" mean anything to you?

Even if your pet cashier can tell at sight a pound of peanuts from a pound
of walnuts, I don't see any reason to assume he would know what the correct
price of either was or even which was more expensive on a particular day.
The cashier is just as dependent on price stickers in a piece marked store
as the scanner is on the UPC label in a scanner store.

If I were designing a cash register, I'd make sure it could retain the
current session through a power outage (no re-ringing the stuff already in
the bags), but I don't think I'd require it to work while the power was off.
Personally, if I were in a store when the power went out, I would leave
quickly.  If power loss is COMMON in the area where the store is built, the
designers should work around it (perhaps by providing battery-powered
scanners or emergency backup power); in my neighborhood I think it's
reasonable to write off as a minor inconvenience -- the speed and efficiency
of the scanners when the power is on is a more than reasonable trade for the
inconvenience the tiny part of the time it isn't.

-- 
scott preece
gould/csd - urbana
uucp:	ihnp4!uiucdcs!ccvaxa!preece
arpa:	preece@gswd-vms

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.52.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.54.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-40</DOCNO>
<DOCOLDNO>IA012-000123-B022-358</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.54.html 128.240.150.127 19970217004609 text/html 15689
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:44:37 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 54</TITLE>
<LINK REL="Prev" HREF="/Risks/3.53.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.55.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.53.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.55.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 54</H1>
<H2> Monday, 15 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Ada Inherently Secure? 
</A>
<DD>
<A HREF="#subj1.1">
Mike McLaughlin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  A million lines of code works the first time? 
</A>
<DD>
<A HREF="#subj2.1">
Ken Calvert
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Computers and Ethics 
</A>
<DD>
<A HREF="#subj3.1">
Mark S. Day
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  New book: HUMAN RELIABILITY: With Human Factors 
</A>
<DD>
<A HREF="#subj4.1">
Elizabeth ?
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Answers to WWMCCS Intercomputer Network questions 
</A>
<DD>
<A HREF="#subj5.1">
Harold E. Russell
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Ada Inherently Secure?
</A>
</H3>
<address>
Mike McLaughlin
&lt;<A HREF="mailto:mikemcl@nrl-csr ">
mikemcl@nrl-csr 
</A>&gt;
</address>
<i>
Fri, 12 Sep 86 09:14:09 edt
</i><PRE>

The 8 September 1986 issue of InformationWEEK carried an article "Ada Goes
to Work."  A box in that article is "Ada is Finding More Job Opportunities
With European Telecommunications and Banking Corporations" by Philip Hunter.
The following excerpts are from the box.  Deletions... (bridges) [comment]:

"... The Finnish bank Kansallis Osake Pankki has standardized on Ada for 
some... systems, having decided that the language is much better than 
Cobol for developing secure fail-safe applications, with sound structure 
and strong management control."

"... Barclays privately admits that Ada could be the logical successor to 
Cobol for financial systems where security and fail-safe operation are 
essential, ..."

"... its chief appeal to banks is the rigorous structure... This prevents
individual(s)... from making changes that affect other parts of the system. 
The ... application is then to a large extent shielded both from careless 
coding... and from deliberate tampering-including the insertion of logic
time [sic] bombs... "

"... Ada... helps project managers construct secure reliable systems." 

[several paragraphs omitted]

"... British Petroleum and Shell... are evaluating its use for telemetry,
... The... Schlumberger group has... standardize(d) on Ada for oil-field
simulation systems, ..."

"Corporations here in the (U.S.) also are taking up Ada for simulation 
applications, but Europe is way ahead in use (of Ada) for telecommuni-
cations, ..." 

"(other uses include)... Computer Integrated Manufacturing, where a uni-
versal applications-programming environment is needed ... to drive a 
variety of devices, such as robots, machine tools, and vision systems. "

[I have left out several concluding paragraphs.  The thrust of the article
(Ada doing fine in Europe) is skewed by my selection of matters relating
to safety, security, and reliability.] 

	- Mike McLaughlin  &lt;mikemcl@nrl-csr.arpa&gt;

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
A million lines of code works the first time?
</A>
</H3>
<address>
Ken Calvert
&lt;<A HREF="mailto:calvert@sally.utexas.edu.UTEXAS.EDU ">
calvert@sally.utexas.edu.UTEXAS.EDU 
</A>&gt;
</address>
<i>
Fri, 12 Sep 86 11:52:37 cdt
</i><PRE>



Heard on NPR's "All Things Considered" yesterday evening:
An Air Force Lt. Col., speaking about a kinetic energy weapons
test earlier this week, which apparently went better than expected
in several respects.  If this isn't an exact quote (I heard it
twice, but didn't write it down at the time), it's real close:
"We wrote about a million lines of new computer code, and tested
them all for the first time, and they all worked perfectly."

"Interesting if true - and interesting anyway." - Mark Twain.

Ken Calvert
Univ. of Texas Computer Sciences

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Computers and Ethics
</A>
</H3>
<address>
Mark S. Day 
&lt;<A HREF="mailto:MDAY@XX.LCS.MIT.EDU">
MDAY@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Thu 11 Sep 86 09:48:37-EDT
</i><PRE>
To: RISKS@CSL.SRI.COM

In a recent issue of Risks, a contributor suggested the possibility of
substituting the UPC code for walnuts on a package of pecans, to "save
some money".  While I am fairly sure that person was joking, it does
point out an interesting phenomenon in the area of computer-related
risks.  That is, as soon as a computer is involved, people seem more
willing to commit acts of fraud, theft, and espionage than they would
in the absence of a computer.  Thus, people will talk about switching
UPC price tags who would view switching non-computerized price tags as
fraud.  Similarly, people will read mail and data files stored on a
timesharing system, even though it's unacceptable to rifle through
people's desks.

I don't believe that this is due to inadequate security measures on computers.
My desk is unlocked, but that hardly constitutes license for people to paw
through it, even in my absence.  Two possible explanations that occur to me
are 

1) Novelty -- computers are sufficiently new that they haven't been included
              in people's "social conditioning".  All of the little stories
              that tell children not to steal, not to lie, etc. don't seem
              to apply to computers and bits.

2) Distance -- computers serve as intermediaries distancing the perpetrator
               from the victim.  It is easier to consider and carry out 
               unethical actions when they appear to be carried out on a 
               machine rather than a person.

What, if anything, can/should be done about this problem?

--Mark

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
New book: HUMAN RELIABILITY: With Human Factors
</A>
</H3>
<address>
&lt;<A HREF="mailto:ELIZABETH%OZ.AI.MIT.EDU@XX.LCS.MIT.EDU">
ELIZABETH%OZ.AI.MIT.EDU@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Wed, 10 Sep 1986  13:47 EDT
</i><PRE>

A blurb from Pergamon Press came in the mail today; I thought RISKS
readers might be interested in this book.

Title: _HUMAN_RELIABILITY:_With_Human_Factors_
Author: Balbir S. Dhillon, Mechanical Engineering/University of Ottawa
Other: 1986; 272 pp.; softcover 24.50, harcover 43.50

Blurb: "This first-of-its-kind text explains the important role people
play in the overall reliability of engineering systems, since various
systems are interconnected by human links.  Detailed coverage of these
systems and links are given through data collection and analysis,
development of reliability prediction methods and techniques, and
numerous ready-to-use formulas and mathematical models for predicting
human reliability in a variety of situations.  The introductory
material eliminates the need for prior knowledge of mathematics and
reliability.  Exercises and references follow each chapter.

"Designed for upper-level undergraduate and graduate students, this
text will find application across many disciplines since human error
is a common problem..."

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
AAAI-86 Report, RISKS 3.41
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Thu, 11 Sep 86 14:25:48 -0500
From: Harold E. Russell &lt;russell@mitre.ARPA&gt;
Bcc: russell                         [sic!  INTERESTING MAILER GLITCH!]

The report from AAAI-86 (RISKS 3.41, Alan Wexelblat, wex@mcc.arpa) 
had two questions (Q7 &amp; Q8) relating to the WWMCCS Intercomputer
Network (WIN).

The WIN, which is the communications component of WWMCCS, has
received a great deal of bad press dating from the period 1977-79.
Some of it may be pertinent to RISKS FORUM.

RISK:  Using obsolete data for system evaluation.

The most vociferous complaints about WIN date from the period 1977-79
which was a transition phase from prototype (PWIN) to operational 
status.  Use of data from that period may be of academic interest but
it is not relevant to the present WIN which has current technology
hardware and vastly improved software.  I visited several WWMCCS sites
after the transition and found satisfied users who were doing things
that they considered impractical a few years before.  In some cases, 
the WIN was outperforming every other communications medium to the 
point of operating where the parallel communication channels failed 
or were hopelessly saturated.  WIN is now handling more data and 
serving more users than was originally anticipated.  There are still
people whose contempt for WIN is based on data from the transition
era. 

RISK:  Premature transition from prototype to operational status.

Transitioning from a prototype to production or operational status is
always a calculated risk.  This was no different in the case of the WIN.
Go ahead was given based IN PART on the following:

1.  There were still minor but correctable technical flaws in the WIN.
2.  Even in its imperfect state, the WIN provided capabilities which
were not otherwise available.
3.  A situation existed where no applications software was being 
developed for WIN because WIN was not yet available for development of 
applications software.
4.  There would be a learning curve for the applications development
people where the remaining WIN technical problems could be resolved 
before the learning curve started to rise significantly.
5.  There was no way of economically or effectively modeling or
testing the full-blown military network.
6.  Certain categories of highly sensitive military messages would be
prohibited in the WIN.  No reliance would be placed on an unproven
system.

In the case of the WIN, the gamble paid off handsomely, but there are
still numerous criticisms from people who could not or would not
understand the situation that existed in the late 1970s.

RISK:  Adaptation of technology from a different environment.

The WIN was directly derived from the purportedly highly successful
ARPANET which dated back to the late 1960s.  The ARPANET of that era was
essentially a heterogeneous network linking universities and government
research houses.  There were however flaws in the network architecture
and implementation that were unknown, unrecognized or otherwise not 
recorded, which came to light in the homogeneous military environment.  
No one much knew or cared if the University of West Academia unexpectedly 
dropped out of the network because of failures in home-grown software 
or hardware.  In the WIN, a lot of people will take notice if the 
Pentagon suddenly drops out of the network.  Much of the development
effort and many of the problems reported in the 1977-79 period were
associated with correcting deficiencies in the ARPANET architecture and
implementation.  The ARPANET was and still is a very good research 
network where problems are analyzed and corrected on a time-, money-, 
and talent-available basis.  There may be serious problems in the
wholesale transfer of laboratory technology to other environments 
especially critical large-scale military installations.

RISK:  Becoming a victim of one's own success.

At well-managed and well-run sites the WWMCCS/WIN provides good service
and reliability to those who understand its capabilities and limitations.
This results in a good reputation which causes the demands for service
extension to new users beyond those originally intended or causes
existing users to increase their utilization of the system.  Failure to
accommodate these demands yields criticisms of poor response and
inadequate support.  In order to support more users or increased 
utilization, the site equipment would probably require additional 
hardware which is difficult to formally justify and fund.  At the 
present time a typical WWMCCS site has less than half the equipment 
that the vendor defines as a maximum hardware configuration.  If more 
users are granted access than the equipment can support, then
performance can be expected to degrade and complaints to increase.
The WIN provides solid, reliable, effective communications among the 
WWMCCS sites for file transfer, teleconferencing, remote terminal access, 
and mail, but it has throughput limitations.  Performance tests, which 
I conducted two years ago, showed that minimal WWMCCS ADP computers are 
capable of driving the communications lines at near theoretical capacity.
Some people understand why their M16 rifle can't shoot 10 miles but will 
not be convinced that it takes a while to transfer a megabyte file over 
a 56K baud communications link.

WWMCCS ADP and the WIN have a lot of room for technical improvement.  
However, the biggest problems are not technical, but government 
regulations, redtape, funding, and retention of trained, capable 
personnel.  The continual references to statistics and data nearly a 
decade old is misleading and masks current problems and issues.

As always, these opinions may not reflect those of my employers,
associates, or customers, past or present.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.53.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.55.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-41</DOCNO>
<DOCOLDNO>IA012-000123-B022-375</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.55.html 128.240.150.127 19970217004623 text/html 14526
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:44:50 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 55</TITLE>
<LINK REL="Prev" HREF="/Risks/3.54.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.56.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.54.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.56.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 55</H1>
<H2> Monday, 15 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Hardware/software interface and risks 
</A>
<DD>
<A HREF="#subj1.1">
Kevin Kenny
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  F-16 
</A>
<DD>
<A HREF="#subj2.1">
Holleran
</A><br>
<A HREF="#subj2.2">
 Ken Miya
</A><br>
<A HREF="#subj2.3">
 Ihor Kinal
</A><br>
<A HREF="#subj2.4">
 Doug Wade
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Hardware/software interface and risks
</A>
</H3>
<address>
Kevin Kenny
&lt;<A HREF="mailto:kenny@b.cs.uiuc.edu ">
kenny@b.cs.uiuc.edu 
</A>&gt;
</address>
<i>
Thu, 11 Sep 86 10:37:05 CDT
</i><PRE>

In RISKS 3.53 mlbrown at ncsc-wo writes:

&gt;In RISKS 3.51 Bill Janssen writes of errors made in failing to consider
&gt;the interaction of the hardware and the software under design.  This
&gt;failing was all too common in the writing of assembly and machine code 
&gt;during the early days of programming.  Discrete wired machines often had
&gt;OP codes that were not generally well known (i.e. the computer designers
&gt;kept it secret)...

This posting raises another interesting issue; in any system with a
long service life, there is a likelihood that the underlying hardware
technology will change.  Use of anything undocumented on a particular
machine is asking for trouble when that machine is replaced with a
``compatible'' one that lacks the undocumented feature.

In fact, the undocumented op-codes on 4-pi and 360 were not ``kept
secret'' by the machine designers; in many cases they simply were not
foreseen.  It turned out that the combinations of operations that were
performed by certain bit patterns did something useful.  The modern
microprocessors have this tendency also; witness the plethora of
undocumented opcodes on the Z80.

The modern mainframe manufacturers have all been burned at one time or
another by users who take advantage of undocumented features and then
have their programs fail when transported to a ``compatible'' machine
using newer technology; the IBM 1401 compatibles brought out by
Honeywell after IBM dropped the product line are the classic example.
Some of the manufacturers now consider it worth the cost to add logic
to verify that a program is using only documented instructions
(generate a machine fault rather than an undocumented result); their
experience is that documenting something to be forbidden doesn't keep
the hackers from using it.  There's some justification for the
``everything not permitted is forbidden'' attitude; I've seen
mysterious failures years after a machine conversion caused by hardware
incompatibilities in little-used areas of the software.

I have also discovered successful penetrations of security on systems
in which undocumented opcodes allowed user programs to perform
privileged operations.  I will deliberately refrain from discussing
these further since some of the designs thus penetrated are still in
service in the field.

The goal that the hardware designers should aim for is to provide
predictable results under all circumstances, even the cases that are
documented to be illegal.

Kevin Kenny

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
 F-16 exceeding 55 mph
</A>
</H3>
<address>
&lt;<A HREF="mailto: Holleran@DOCKMASTER.ARPA">
 Holleran@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Thu, 11 Sep 86 00:49 EDT
</i><PRE>
To:  RISKS@CSL.SRI.COM

  I would like to provide some diversion on Jon Jacky's comments.

         &gt;Date: Mon, 8 Sep 86 16:55:19 PDT
         &gt;From: jon@uw-june.arpa (Jon Jacky)
         &gt;Subject: Upside-down F-16's and "Human error"

         &gt;... should prevent any unreasonable behavior.  This way lies
         &gt;madness.  Do we have to include code to prevent the speed from
         &gt;exceeding 55 mph while taxiing down an interstate highway?

  I agree with this and subsequent statements about the capabilities of
the operator (the pilot).

  Let's examine a silly analysis of providing that particular code.  After you
code the routine to prevent the " exceeding the speed", you are going to have
to test it.  Thus, the F-16 will have to "attempt" to exceed 55 mph on the
expressway.  Whether the code is there or not, the trooper is still going to
give you a ticket.  You have already made his day, but no one will believe him
without the pilot getting a ticket.  Besides he has to make his quota.  So you
may as well save your money for more important coding.  Then the pilot will
appear on either 60 minutes or Johnny Carson to explain his side of the
problem.  The analysis could go further but it belongs in a comedian's
dialogue now.

  I would say that many "unreasonable behavior" situations being analyzed in a
silly mode would show that some coding efforts should not be done.  You may
find out that certain situations cannot be tested in a justifiable fashion.
As Jon Jacky and others have concluded, lets be reasonable in the questions
responsible people should be addressing vice situations which have little
chance of occuring.  Good analysis will be better if common sense helps us to
priortize these situations.

</PRE>
<HR><H3><A NAME="subj2.2">
Re: F-16 software
</A>
</H3>
<address>
Eugene Miya
&lt;<A HREF="mailto:eugene@AMES-NAS.ARPA ">
eugene@AMES-NAS.ARPA 
</A>&gt;
</address>
<i>
10 Sep 1986 1317-PDT (Wednesday)
</i><PRE>

It seems F-16's are a hot topic everywhere.  I think it's novelty
thing like computers except for aeronautics.

&gt; I am trying to make the point that the gross simplification of
&gt; "preventing bomb release while inverted" doesn't map very well to what I
&gt; assume the actual goal is: "preventing weapons discharge from damaging
&gt; the aircraft".  This is yet another instance where the assumptions made
&gt; to simplify a real-world situation to manageable size can easily lead to
&gt; design "errors", and is an architypical "computer risk" in the use of
&gt; relatively simple computer models of reality.
&gt; 
&gt; Wayne Throop      &lt;the-known-world&gt;!mcnc!rti-sel!dg_rtp!throopw

Excellent point.

Several things strike me about this problem.  First, the language used
by writers up to this point don't use words like "centrifugal force"
and "gravity."  This worries me about the training of some computer people
for jobs like writing mission critical software [Whorf's "If the word
does not exist, the concept does not exist."]  I am awaiting a paper
by Whitehead whch I am told talks about some of this.

It can certainly be acknowledged that there are uses which are novel
(Spencer cites "lob" bombing, and others cite other reasons [all marginal])
equal concern must be given to straight-and-level flight AND those
novel cases.  In other words, we have to assume some skill on the part of
pilots [Is this arrogance on our part?]. 

Another problem is that planes and Shuttles do not have the types of sensory
mechanisms which living organisms have.  What is damage if we cannot
"sense it?"  Sensing equipment costs weight.  I could see some interesting
dialogues ala "Dark Star."

Another thing is that the people who write simulations seem to have the
great difficulty discriminating between the quality of thier simulations
and "real world" in the presence of incomplete cues (e.g., G-forces,
visual cues, etc.) when solely relying on things like instrument disk
[e.g., pilot: "Er, you notice that we are flying on empty tanks?" disturbed
pilot expression,  programmer: "Ah, it's just a simulation."]
Computer people seem to be "ever the optimist."  Besides, would you ever
get into a real plane with a pilot who's only been in simulators?

Most recently, another poster brought up the issue of autonmous weapons.
We had a discussion of of this at the last Palo Alto CPSR meeting.
Are autonmous weapons moral?  If an enemy has a white flag or hand-ups,
is the weapon "smart enough" to know the Geneva Convention (or is too
moral for programmers of such systems)?

On the subject of flight simulators: I visited Singer Link two years
ago (We have a DIG 1 system which we are replacing).  I "crashed" underneath
the earth and the polygon structure became more "visible."  It was like
being underneath Disneyland.

--eugene miya			sorry for the length, RISKS covered alot.
  NASA Ames Research Center
  President
  Bay Area ACM SIGGRAPH

</PRE>
<HR><H3><A NAME="subj2.3">
Re: <A HREF="/Risks/3.53.html">RISKS-3.53</A>
</A>
</H3>
<address>
&lt;<A HREF="mailto:cbosgd!mtung!ijk@ucbvax.Berkeley.EDU">
cbosgd!mtung!ijk@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Fri, 12 Sep 86 07:44:24 PDT
</i><PRE>

Mike Brown wrote:

&gt;&gt;  Its fine to tell the pilot "Lower your wheels before you land, not
&gt;&gt; after" but we still have gear up landings.  We should not concern ourselves
&gt;&gt; with checking for and preventing any incorrect behavior but we should preclude
&gt;&gt; that behavior which will result in damage to or loss of the aircraft or the
&gt;&gt; pilot.  We do not need to anticipate every possible mistake that he can make
&gt;&gt; in this regard either - all we need to do are to identify the hazardous operational modes and prevent
&gt;&gt; their occurrence.

I disagree that software MUST prevent: what about the case when an
aircraft can lower only ONE side of its landing gear????  A belly-up
landing is then the only way to go [ assume combat damage, or something,
so that the pilot can't eject, and the computer INSISTS on lowering
the landing gear whenever you attempt to go under 50 feet, or
something stupid like that].

On the other hand, some of the latest experimental planes are
totally UNFLYABLE by normal human control -- for those planes,
the software better be reliable, because there is no backup!!!

Obviously, one can present arguments for each side [human vs computer
having the last say -- at TMI, computers were right, but ...]  I
would say that if humans do override CRITICAL computer control [like
TMI], then some means of escalating the attention level must be
invoked [ e.g., have the computers automatically notify the NRC].
Again, there's lots of tradeoffs to be made [seriousness of the problem,
timeliness of the response necessary, etc.] which means thats there's
NO PAT answer in most cases, just hope that people involved in these
cases realize the possible consequences of their work.  In that
case one could argue for professional certification in these fields
[ we're software ENGINEERS, right?!? : you wouldn't to go over a bridge
built by an uncertified mechanical enginerr, would you??  What if the
software he used was written by a flake? ]; if not certification,
then perhaps the software should undergo wide scrutiny by independent
evaluators [ I'd feel a lot better if I knew that the software controlling
nuclear plants had undergone such scrutiny].

Enough said, I believe.
Ihor Kinal
ihnp4!mtung!ijk.

</PRE>
<HR><H3><A NAME="subj2.4">
re. F-16 Software.
</A>
</H3>
<address>
&lt;<A HREF="mailto:Doug_Wade%UBC.MAILNET@MIT-MULTICS.ARPA">
Doug_Wade%UBC.MAILNET@MIT-MULTICS.ARPA
</A>&gt;
</address>
<i>
Wed, 10 Sep 86 11:42:14 PDT
</i><PRE>


Reading comments about putting restraints on jet performance within
the software reminded me of a  conversation I had a few years ago
at an air-show.
In talking to a pilot who flew F-4's in Vietnam he mentioned that
the F-4 specs said a turn exerting more than say 8 G's would cause
the wings to "fall off". However in avoiding SAMs or ground-fire
they would pull double? this with no such result.
  My comment to this, is what if a 8G limit had been programmed into
the plane (if it had been fly-by-wire). Planes might have been hit and
lost which otherwise were saved by violent maneuvers. With a SAM targeted
on your jet, nothing could be lost by exceeding the structural limitations
of the plane since it was a do-or-die situation.
I'm sure 99.99% of the lifetime of a jet is spent within designed
specifications, but should software limit the plane the one time
a pilot needs to override this constraint?

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.54.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.56.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-42</DOCNO>
<DOCOLDNO>IA012-000123-B022-393</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.56.html 128.240.150.127 19970217004637 text/html 15138
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:45:06 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 56</TITLE>
<LINK REL="Prev" HREF="/Risks/3.55.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.57.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.55.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.57.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 56</H1>
<H2>Tuesday, 16 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Massive UNIX breakins at Stanford 
</A>
<DD>
<A HREF="#subj1.1">
Brian Reid
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Massive UNIX breakins at Stanford
</A>
</H3>
<address>
Brian Reid
&lt;<A HREF="mailto:reid@decwrl.DEC.COM ">
reid@decwrl.DEC.COM 
</A>&gt;
</address>
<i>
16 Sep 1986 1519-PDT (Tuesday)
</i><PRE>

    Lessons learned from a recent rash of Unix computer breakins

Introduction
   A number of Unix computers in the San Francisco area have
   recently been plagued with breakins by reasonably talented
   intruders. An analysis of the breakins (verified by a telephone
   conversation with the intruders!) show that the networking
   philosophy offered by Berkeley Unix, combined with the human
   nature of systems programmers, creates an environment in which
   breakins are more likely, and in which the consequences of
   breakins are more dire than they need to be.

   People who study the physical security of buildings and military
   bases believe that human frailty is much more likely than
   technology to be at fault when physical breakins occur. It is
   often easier to make friends with the guard, or to notice that he
   likes to watch the Benny Hill show on TV and then wait for that
   show to come on, than to try to climb fences or outwit burglar
   alarms.

Summary of Berkeley Unix networking mechanism:

   The user-level networking features are built around the
   principles of "remote execution" and "trusted host". For example,
   if you want to copy a file from computer A to computer B, you
   type the command
	   rcp A:file B:file
   If you want to copy the file /tmp/xyz from the computer that you
   are now using over to computer C where it will be called
   /usr/spool/breakin, you type the command
	   rcp /tmp/xyz C:/usr/spool/breakin
   The decision of whether or not to permit these copy commands is
   based on "permission" files that are stored on computers A, B,
   and C. The first command to copy from A to B will only work if
   you have an account on both of those computers, and the
   permission file stored in your directory on both of those
   computers authorizes this kind of remote access. 

   Each "permission file" contains a list of computer names and user
   login names. If the line "score.stanford.edu reid" is in the
   permission file on computer "B", it means that user "reid" on
   computer "score.stanford.edu" is permitted to perform remote
   operations such as rcp, in or out, with the same access
   privileges that user "reid" has on computer B.

How the breakins happened.

   One of the Stanford campus computers, used primarily as a mail
   gateway between Unix and IBM computers on campus, had a guest
   account with user id "guest" and password "guest". The intruder
   somehow got his hands on this account and guessed the password. 
   There are a number of well-known security holes in early releases
   of Berkeley Unix, many of which are fixed in later releases.
   Because this computer is used as a mail gateway, there was no
   particular incentive to keep it constantly up to date with the
   latest and greatest system release, so it was running an older version
   of the system. The intruder instantly cracked "root" on that
   computer, using the age-old trojan horse trick. (He had noticed
   that the guest account happened to have write permission into a
   certain scratch directory, and he had noticed that under certain
   circumstances, privileged jobs could be tricked into executing
   versions of programs out of that scratch directory instead of out
   of the normal system directories).

   Once the intruder cracked "root" on this computer, he was able to
   assume the login identity of everybody who had an account on that
   computer. In particular, he was able to pretend to be user "x" or
   user "y", and in that guise ask for a remote login on other
   computers. Sooner or later he found a [user,remote-computer] pair
   for which there was a permission file on the other end granting
   access, and now he was logged on to another computer. Using the
   same kind of trojan horse tricks, he was able to break into root
   on the new computer, and repeat the process iteratively.

   In most cases the intruder left trojan-horse traps behind on
   every computer that he broke into, and in most cases he created
   login accounts for himself on the computers that he broke into.
   Because no records were kept, it is difficult to tell exactly how
   many machines were penetrated, but the number could be as high as
   30 to 60 on the Stanford campus alone. An intruder using a
   similar modus operandi has been reported at other installations.

How "human nature" contributed to the problem

   The three technological entry points that made this intrusion
   possible were:

      * The large number of permission files, with entirely
	too many permissions stored in them, found all over the campus
	computers (and, for that matter, all over the ARPAnet).

      * The presence of system directories in which users have write
	permission.

      * Very sloppy and undisciplined use of search paths in privileged
        programs and superuser shell scripts.


Permissions: Berkeley networking mechanism encourages carelessness.

   The Berkeley networking mechanism is very very convenient. I use
   it all the time. You want to move a file from one place to
   another? just type "rcp" and it's there. Very fast and very
   efficient, and quite transparent. But sometimes I need to move a
   file to a machine that I don't normally use. I'll log on to that
   machine, quickly create a temporary permission file that lets me
   copy a file to that machine, then break back to my source machine
   and type the copy command. However, until I'm quite certain that
   I am done moving files, I don't want to delete my permission file
   from the remote end or edit that entry out of it. Most of us use
   display editors, and oftentimes these file copies are made to
   remote machines on which the display editors don't always work
   quite the way we want them to, so there is a large nuisance
   factor in running the text editor on the remote end. Therefore
   the effort in removing one entry from a permission file--by
   running the text editor and editing it out--is high enough that
   people don't do it as often as they should. And they don't want
   to *delete* the permission file, because it contains other
   entries that are still valid. So, more often than not, the
   permission files on rarely-used remote computers end up with
   extraneous permissions in them that were installed for a
   one-time-only operation. Since the Berkeley networking commands
   have no means of prompting for a password or asking for the name
   of a temporary permission file, everybody just edits things into
   the permanent permission file. And then, of course, they forget
   to take it out when they are done.


Write permission in system directories permits trojan horse attacks.

   All software development is always behind schedule, and
   programmers are forever looking for ways to do things faster. One
   convenient trick for reducing the pain of releasing new versions
   of some program is to have a directory such as /usr/local/bin or
   /usr/stanford/bin or /usr/new in which new or locally-written
   versions of programs are kept, and asking users to put that
   directory on their search paths. The systems programmers then
   give themselves write access to that directory, so that they can
   intall a new version just by typing "make install" rather than
   taking some longer path involving root permissions. Furthermore,
   it somehow seems more secure to be able to install new software
   without typing the root password. Therefore it is a
   nearly-universal practice on computers used by programmers to
   have program directories in which the development programmers
   have write permission. However, if a user has write permission in
   a system directory, and if an intruder breaks into that user's
   account, then the intruder can trivially break into root by using
   that write permission to install a trojan horse.

Search paths: people usually let convenience dominate caution.

   Search paths are almost universally misused. For example, many
   people write shell scripts that do not specify an explicit search
   path, which makes them vulnerable to inheriting the wrong path.
   Many people modify the root search path so that it will be
   convenient for systems programmers to use interactively as the 
   superuser, forgetting that the same search path will be used by
   system maintenance scripts run automatically during the night.
   It is so difficult to debug failures that are caused by incorrect
   search paths in automatically-run scripts that a common "repair"
   technique is to put every conceivable directory into the search
   path of automatically-run scripts. Essentially every Unix
   computer I have ever explored has grievous security leaks caused
   by underspecified or overlong search paths for privileged users.

Summary conclusion: Wizards cause leaks

   The people who are most likely to be the cause of leaks are
   the wizards. When something goes wrong on a remote machine, often
   a call goes in to a wizard for help. The wizard is usually busy
   or in a hurry, and he often is sloppier than he should be with
   operations on the remote machine. The people who are most likely
   to have permission files left behind on stray remote machines are
   the wizards who once offered help on that machine. But, alas,
   these same wizards are the people who are most likely to have
   write access to system directories on their home machines,
   because it seems to be in the nature of wizards to want to
   collect as many permissions as possible for their accounts. Maybe
   that's how they establish what level of wizard that they are. The
   net result is that there is an abnormally high probability that
   when an errant permission file is abused by an intruder, that it
   will lead to the account of somebody who has an unusually large
   collection of permissions on his own machine, thereby making it
   easier to break into root on that machine.

Conclusions.

   My conclusions from all this are these:
      * Nobody, no matter how important, should have write permission
	into any directory on the system search path. Ever.

      * Somebody should carefully re-think the user interface of the
	Berkeley networking mechanisms, to find ways to permit people to
	type passwords as they are needed, rather than requiring them to
	edit new permissions into their permissions files. 

      * The "permission file" security access mechanism seems
        fundamentally vulnerable. It would be quite reasonable
	for a system manager to forbid the use of them, or to 
	drastically limit the use of them. Mechanized checking is 
	easy.

      * Programmer convenience is the antithesis of security, because
	it is going to become intruder convenience if the programmer's 
	account is ever compromised. This is especially true in
        setting up the search path for the superuser.



Lament
   I mentioned in the introduction that we had talked to the
   intruders on the telephone. To me the most maddening thing about
   this intrusion was not that it happened, but that we were unable
   to convince any authorities that it was a serious problem, and
   could not get the telephone calls traced. At one point an
   intruder spent 2 hours talking on the telephone with a Stanford
   system manager, bragging about how he had done it, but there was
   no way that the call could be traced to locate him. A few days
   later, I sat there and watched the intruder log on to one
   Stanford comptuer, and I watched every keystroke that he typed on
   his keyboard, and I watched him break in to new directories, but
   there was nothing that I could do to catch him because he was
   coming in over the telephone. Naturally as soon as he started to
   do anything untoward I blasted the account that he was using and
   logged him off, but sooner or later new intruders will come
   along, knowing that they will not be caught because what they are
   doing is not considered serious. It isn't necessarily serious,
   but it could be. I don't want to throw such people in jail,
   and I don't want to let them get away either. I just want to
   catch them and shout at them and tell them that they are being
   antisocial.

Brian Reid
DEC Western Research and Stanford University

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.55.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.57.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-43</DOCNO>
<DOCOLDNO>IA012-000123-B022-418</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.57.html 128.240.150.127 19970217004654 text/html 17964
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:45:18 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 57</TITLE>
<LINK REL="Prev" HREF="/Risks/3.56.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.58.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.56.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.58.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 57</H1>
<H2>Tuesday, 16 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Computers and the Stock Market (again) 
</A>
<DD>
<A HREF="#subj1.1">
Robert Stroud
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  The Old Saw about Computers and TMI 
</A>
<DD>
<A HREF="#subj2.1">
Ken Dymond
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Do More Faults Mean (Yet) More Faults? 
</A>
<DD>
<A HREF="#subj3.1">
Dave Benson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  A critical real-time application worked the first time 
</A>
<DD>
<A HREF="#subj4.1">
Dave Benson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Autonomous weapons 
</A>
<DD>
<A HREF="#subj5.1">
Eugene Miya
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  "Unreasonable behavior" and software 
</A>
<DD>
<A HREF="#subj6.1">
Eugene Miya on Gary Chapman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Risks of maintaining computer timestamps revisited 
</A>
<DD>
<A HREF="#subj7.1">
John Coughlin
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Computers and the Stock Market (again)
</A>
</H3>
<address>
Robert Stroud 
&lt;<A HREF="mailto:robert%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK">
robert%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Mon, 15 Sep 86 16:53:37 gmt
</i><PRE>

The computers had a hand in the dramatic fall on Wall Street last week
according to an item on the BBC TV news. Apparently, the systems were
not designed to cope with the sheer volume of sales, (anybody know more
about this?). The report continued

	"In London they still do it the old fashioned way with bits
	of paper, which makes people think twice before joining in
	a mindless selling spree. However, all this could change in
	October with the Big Bang..."

What price progress?

Robert Stroud,
Computing Laboratory,
University of Newcastle upon Tyne.

ARPA robert%cheviot.newcastle@ucl-cs.ARPA
UUCP ...!ukc!cheviot!robert

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
The Old Saw about Computers and TMI
</A>
</H3>
<address>
"DYMOND, KEN" 
&lt;<A HREF="mailto:dymond@nbs-vms.ARPA">
dymond@nbs-vms.ARPA
</A>&gt;
</address>
<i>
16 Sep 86 09:25:00 EDT
</i><PRE>
To: "risks" &lt;risks@csl.sri.com&gt;
cc: dymond      
Reply-To: "DYMOND, KEN" &lt;dymond@nbs-vms.ARPA&gt;

Ihor Kinal says in <A HREF="/Risks/3.55.html">RISKS-3.55</A>

	&gt;Obviously, one can present arguments for each side [human
	&gt; vs computer having the last say -- at TMI, computers
	&gt;were right, but ...]   I would say that if humans do
	&gt;override CRITICAL computer control [like TMI], then
	&gt;some means of escalating the attention level must be
	&gt;invoked [e.g., have the computers automatically notify
	&gt;the NRC].

This belief keeps surfacing but is false.  There was no computer
control in safety grade systems at TMI -- see the documentation in
the Kemeny report and probably elsewhere.  There was a computer in
the control room but it only drove a printer to provide a hardcopy
log of alarms in the sequence in which they occurred.  The log is
an aid in diagnosing events.  The computer (a Bendix G-15 ??) did 
play a role in the emergency since at one point its buffer became 
full and something like 90 minutes of alarms were not recorded, thus
hampering diagnosis.  

On a couple of occasions I have asked NRC people why computers aren't
used to control critical plant systems and have been told that "they aren't
safety grade."  I'm not quite sure what this means, but I take it
to mean that computers (and software) aren't trustworthy enough for
such safety areas as the reactor protection system.  This is not to
say that computers aren't used in monitoring plant status, quite
different from control.

Ken Dymond
(the opinions above don't necessarily reflect those of my employer
or anybody else, for that matter.)

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Do More Faults Mean (Yet) More Faults?
</A>
</H3>
<address>
Dave Benson 
&lt;<A HREF="mailto:benson%wsu.csnet@CSNET-RELAY.ARPA">
benson%wsu.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Sun, 14 Sep 86 19:00:30 pdt
</i><PRE>

  |In RISKS 3.50 Dave Benson comments in "Flight Simulator
  |Simulators Have Faults" that
  |
  |    &gt;We need to understand that the more faults found at
  |    &gt;any stage to engineering software the less confidence one has in the
  |    &gt;final product.  The more faults found, the higher the likelyhood that
  |    &gt;faults remain.
  |
  |This statement makes intuitive sense, but does anyone know of any data
  |to support this ?  Is this true of any models of software failures ?
  |Is this true of the products in any of the hard engineering fields -- civil,
  |mechanical, naval, etc. -- and do those fields have the confirming data ?
  |
  |Ken Dymond, NBS

Please read the compendium of (highly readable) papers by M.M.Lehman and
L.A.Belady, Program Evolution: Processes of Software Change, APIC Studies
in Data Processing No. 27, Academic Press, Orlando, 1985.  This provides data.
It is (sorry-- should be, but probably isn't) standard in software quality
assurance efforts to throw away modules which show a high proportion of
the total evidenced failures.  The (valid, in my opinion) assumption is
that the engineering on these is so poor that it is hopeless to continue
to try to patch it up.

Certain models of software failure place increased "reliablity" on software
which has been exercised for long periods without fault.  One must
understand that this is simply formal modelling of the intuition that
some faults means (yet) more faults.  This is certainly true of all
engineering fields.  While I don't have the "confirming data" I suggest you
consider your car, your friends car, etc.  Any good history of engineering
will suggest that many designs never are marketed because of an unending
sequence of irremediable faults.

The intuitive explaination is: Good design and careful implementation works.
This is teleological.  We define good design and careful implementation by
"that which works".

However, I carefully said "confidence".  Confidence is an intuitive
assessment of reliability.  I was not considering the formalized notion
of "confidence interval" used in statistical studies.  To obtain high
confidence in the number of faults requires observing very many errors,
thus lowering one's confidence in the product.  To obtain high confidence
in a product requires observing very few errors while using it.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
I found one! (A critical real-time application worked the first time)
</A>
</H3>
<address>
Dave Benson 
&lt;<A HREF="mailto:benson%wsu.csnet@CSNET-RELAY.ARPA">
benson%wsu.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Sun, 14 Sep 86 22:40:21 pdt
</i><PRE>

Last spring I issued a call for hard data to refute a hypothesis which I,
perhaps mistakenly, called the Parnas Hypothesis:
	No large computer software has ever worked the first time.
Actually, I was only interested in military software, so let me repost the
challenge in the form I am most interested in:
	NO MILITARY SOFTWARE (large or small) HAS EVER WORKED IN ITS FIRST
	OPERATIONAL TEST OR ITS FIRST ACTUAL BATTLE.
Contradict me if you can. (Send citations to the open literature
to benson@wsu via csnet)

Last spring's request for data has finally led to the following paper:
	Bonnie A. Claussen, II
	VIKING '75 -- THE DEVELOPMENT OF A RELIABLE FLIGHT PROGRAM
	Proc. IEEE COMPSAC 77 (Computer Software &amp; Applications Conference)
	IEEE Computer Society, 1977
	pp. 33-37

I offer some quotations for your delictation:

	The 1976 landings of Viking 1 and Viking 2 upon the surface of
	Mars represented a significant achievement in the United States
	space exploration program. ... The unprecented success of the Viking
	mission was due in part to the ability of the flight software
	to operate in an autonomous and error free manner. ...
	Upon separation from the Oribiter the Viking Lander, under autonomous
	software control, deorbits, enters the Martian atmosphere,
	and performs a soft landing on the surface. ... Once upon the surface,
	... the computer and its flight software provide the means by
	which the Lander is controlled.  This control is semi-autonomous
	in the sense that Flight Operations can only command the Lander
	once a day at 4 bit/sec rate.

(Progress occured in a NASA contract over a decade ago, in that)

	In the initial stages of the Viking flight program development,
	the decision was made to test the flight algorithms and determine
	the timing, sizing and accuracy requirements that should be 
	levied upon the flight computer prior to computer procurement.
	... The entire philosophy of the computer hardware and
	software reliability was to "keep it simple."  Using the
	philosophy of simplification, modules and tasks tend toward 
	straight line code with minium decisions and minimum
	interactions with other modules.

(It was lots of work, as)

	When questioning the magnitude of the qulity assurance task,
	it should be noted that the Viking Lander flight program development
	required approximately 135 man-years to complete.

(But the paper gives no quantitative data about program size or complexity.)

Nevertheless, we may judge this as one of the finest software engineering
acomplishments to date.  The engineers on this project deserve far more
plaudits than they've received.  I know of no similar piece of software
with so much riding upon its reliable behavior which has done so well.
(If you do, please do tell me about it.)

However, one estimates that this program is on the order of kilolines of FORTRAN
and assembly code, probably less than one hundred kilolines.  Thus
Parnas will need to judge for himself whether or not the Viking Lander
flight software causes him to abandon (what I take to be) his hypothesis
about programs not working the first time.

It doesn't cause me to abandon mine because there were no Martians shooting
back, as far as we know...

David B. Benson, Computer Science Department, Washington State University,
Pullman, WA 99164-1210  csnet: benson@wsu

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Autonomous weapons
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Tue, 16 Sep 1986  08:31 EDT
</i><PRE>
In-reply-to: Msg of 10 Sep 1986  16:17-EDT from eugene at AMES-NAS.ARPA (Eugene Miya)


    From: eugene at AMES-NAS.ARPA (Eugene Miya)

    ... another poster brought up the issue of autonmous weapons.
    We had a discussion of of this at the last Palo Alto CPSR meeting.
    Are autonmous weapons moral?  If an enemy has a white flag or hand-ups,
    is the weapon "smart enough" to know the Geneva Convention (or is too
    moral for programmers of such systems)?

What do you consider an autonomous weapon?  Some anti-tank devices are
intended to recognize tanks and then attack them without human
intervention after they have been launched (so-called fire-and-forget
weapons).  But they still must be fired under human control.  *People*
are supposed to recognize white flags and surrendering soldiers.

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
"Unreasonable behavior" and software
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Tue, 16 Sep 1986  09:01 EDT
</i><PRE>

    From: Gary Chapman &lt;chapman at russell.stanford.edu&gt;
    	Information about targets can be placed into the munitions
    	processor prior to firing along with updates on meteorologi-
    	cal conditions and terrain.  Warhead functioning can also be
    	selected as variable options will be available.  The intro-
    	duction of VHSIC processors will give the terminal homing
    	munitions the capability of distinguishing between enemy and
    	friendly systems and finite target type selection.  Since
    	the decision of which target to attack is made on board the
    	weapon, the THM will approach human intelligence in this area.
    	The design criteria is pointed toward one munition per target
    	kill.

    (I scratched my head along with the rest of you when I saw this;
    I've always
    thought if you fire a bullet or a shell out of a tube it goes until it hits
    something, preferably something you're aiming at.  But maybe the Army has
    some new theories of ballistics we don't know about yet.)

The THM is an example of what the army calls a "fire-and-forget"
munition. A human being fires it in the general direction of the
target, and then the munition seeks out its target without further
intervention.  The munition has mechanisms to alter its course from a
ballistic trajectory.

    What level of confidence would we have to give soldiers (human soldiers--we
    may have to get used to using that caveat) operating at close proximity to
    THMs that the things are "safe"?

That is indeed the question.  My own guess is that THMs and other
smart munitions will never be able to distinguish between friend or
foe.  That's why most current concepts are directed towards attacking
enemy forces deep behind enemy lines, where you can ASSUME that
anything you see is hostile.

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
 Risks of maintaining computer timestamps revisited
</A>
</H3>
<address>
      John Coughlin  
&lt;<A HREF="mailto:JC%CARLETON.BITNET@WISCVM.WISC.EDU">
JC%CARLETON.BITNET@WISCVM.WISC.EDU
</A>&gt;
</address>
<i>
15 Sep 86 12:14:00 EDT
</i><PRE>


Some  time ago I  submitted an item  to RISKS describing  the way in which the
CP-6 operating system  requires the time to be set  manually during every warm
or cold boot.  The latest release  of this OS contains an improvement: in most
cases the time need only be  manually set on a cold boot.  Unfortunately, with
this enhancement came an unusual bug.

The timestamp is  stored in a special hardware register,  which is modified by
certain  diagnostic procedures  run during  preventive maintenance.   It seems
these diagnostic  procedures were not modified  to reflect the new  use put to
the timestamp register.  As a result, any time a warm boot was performed after
PM,  the monitor  would freak  out at  the illegal  timestamp and mysteriously
abort the boot  with a memory fault.  Until this bug  was patched the only fix
was to power the computer down, thus clearing the offending value.

Luckily, the  PM procedure set the timestamp  register to an impossible value,
rather than a realistic but incorrect value.  Therefore the problem manifested
itself in  an obvious way, instead  of subtly changing the  date and time.  Of
course  this was  at the  cost of  having to  fix a  hung system.  This is yet
another illustration of the risk of breaking one thing while fixing another.

                                                                     /jc

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.56.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.58.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-44</DOCNO>
<DOCOLDNO>IA012-000123-B022-439</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.58.html 128.240.150.127 19970217004707 text/html 19046
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:45:35 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 58</TITLE>
<LINK REL="Prev" HREF="/Risks/3.57.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.59.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.57.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.59.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 58</H1>
<H2> Wednesday, 17 Sept 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Massive UNIX breakins 
</A>
<DD>
<A HREF="#subj1.1">
Dave Curry
</A><br>
<A HREF="#subj1.2">
 Brian Reid
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  "Atlanta's been down all afternoon" 
</A>
<DD>
<A HREF="#subj2.1">
Alan Wexelblat
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  F-16 software 
</A>
<DD>
<A HREF="#subj3.1">
Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Viking Project 
</A>
<DD>
<A HREF="#subj4.1">
Eugene Miya
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Protection of personal information 
</A>
<DD>
<A HREF="#subj5.1">
David Chase
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Autonomous Weapons 
</A>
<DD>
<A HREF="#subj6.1">
Ken Laws
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Re: computers and petty fraud 
</A>
<DD>
<A HREF="#subj7.1">
Col. G. L. Sicherman
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Massive UNIX breakins
</A>
</H3>
<address>
Dave Curry
&lt;<A HREF="mailto:davy@ee.ecn.purdue.edu ">
davy@ee.ecn.purdue.edu 
</A>&gt;
</address>
<i>

</i><PRE>
Date: Wed, 17 Sep 86 08:01:03 EST

Brian -

    I feel for you, I really do.  Breakins can be a real pain in the
neck, aside from being potentially hazardous to your systems.  And, we
too have had trouble convincing the authorities that anything serious
is going on.  (To their credit, they have learned a lot and are much
more responsive now than they were a few years ago.)

    I do have a couple of comments though.  Griping about the Berkeley
networking utilities is well and good, and yes, they do have their
problems.  However, I think it really had little to do with the
initial breakins on your system.  It merely compounded an already
exisiting breakin several fold.

    Two specific parts of your letter I take exception to:

	One of the Stanford campus computers, used primarily as a mail
	gateway between Unix and IBM computers on campus, had a guest
	account with user id "guest" and password "guest". The intruder
	somehow got his hands on this account and guessed the
	password. 

    Um, to put it mildly, you were asking for it.  "guest" is probably
the second or third login name I'd guess if I were trying to break
in.  It ranks right up there with "user", "sys", "admin", and so on.
And making the password to "guest" be "guest" is like leaving the
front door wide open.  Berkeley networking had nothing to do with your
initial breakin, leaving an obvious account with an even more obvious
password on your system was the cause of that.

	There are a number of well-known security holes in early
	releases of Berkeley Unix, many of which are fixed in later
	releases.  Because this computer is used as a mail gateway,
	there was no particular incentive to keep it constantly up to
	date with the latest and greatest system release, so it was
	running an older version of the system.

    Once again, you asked for it.  If you don't plug the holes, someone
will come along and use them.  Again Berkeley networking had nothing to
do with your intruder getting root on your system, that was due purely
to neglect.  Granted, once you're a super-user, the Berkeley networking
scheme enables you to invade many, many accounts on many, many machines.

    Don't get me wrong.  I'm not trying to criticize for the sake of
being nasty here, but rather I'm emphasizing the need for enforcing
other good security measures:

	1. Unless there's a particularly good reason to have one, take
	   all "generic" guest accounts off your system.  Why let
	   someone log in without identifying himself?

	2. NEVER put an obvious password on a "standard" account.
	   This includes "guest" on the guest account, "system" on the
	   root account, and so on.

	   Enforcing this among the users is harder, but not
	   impossible.  We have in the past checked all the accounts
	   on our machines for stupid passwords, and informed everyone
	   whose password we found that they should change it.  As a
	   measure of how simple easy passwords make things, we
	   "cracked" about 400 accounts out of 10,000 in one overnight
	   run of the program, trying about 12 passwords per account.
	   Think what we could have done with a sophisticated attack.

	3. FIX SECURITY HOLES.  Even on "unused" machines.  It's amazing
	   how many UNIX sites have holes wide open that were plugged
	   years ago.  I even found a site still running with the 4.2
	   distributed sendmail a few months ago...

	4. Educate your police and other authorities about what's going
	   on.  Invite them to come learn about the computer.  Give
	   them an account and some documentation.  The first time we
	   had a breakin over dialup (1982 or so), it took us three
	   days to convince the police department that we needed the
	   calls traced.  Now, they understand what's going on, and
	   are much quicker to respond to any security violations we
	   deem important enough to bring to their attention.  The
	   Dean of Students office is now much more interested in
	   handling cases of students breaking in to other students'
	   accounts; several years ago their reaction was "so what?".
	   This is due primarily to our people making an effort to
	   educate them, although I'm sure the increased attention
	   computer security has received in the media (the 414's, and
	   so on) has had an effect too.

--Dave Curry
Purdue University
Engineering Computer Network

</PRE>
<HR><H3><A NAME="subj1.2">
Massive UNIX breakins
</A>
</H3>
<address>
Brian Reid
&lt;<A HREF="mailto:reid@decwrl.DEC.COM ">
reid@decwrl.DEC.COM 
</A>&gt;
</address>
<i>
17 Sep 1986 0729-PDT (Wednesday)
</i><PRE>

The machine on which the initial breakin occurred was one that I didn't
even know existed, and over which no CS department person had any
control at all. The issue here is that a small leak on some
inconsequential machine in the dark corners of campus was allowed to
spread to other machines because of the networking code. Security is
quite good on CSD and EE machines, because they are run by folks who
understand security. But, as this episode showed, that wasn't quite good
enough.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
"Atlanta's been down all afternoon" (!?)
</A>
</H3>
<address>
Alan Wexelblat 
&lt;<A HREF="mailto:wex@mcc.com">
wex@mcc.com
</A>&gt;
</address>
<i>
Wed, 17 Sep 86 14:38:59 CDT
</i><PRE>

Last Friday, we attempted to phone (ATT) long distance to Atlanta.  After
two hours of busy signals we finally decided to try and reach the Atlanta
operator.  She said that Atlanta had been "down all afternoon."

Does anyone have any info about this?

Alan Wexelblat
ARPA: WEX@MCC.ARPA or WEX@MCC.COM
UUCP: {seismo, harvard, gatech, pyramid, &amp;c.}!ut-sally!im4u!milano!wex

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
F-16 software
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Tue, 16 Sep 1986  17:43 EDT
</i><PRE>

I spoke to an F-16 flight instructor about this business concerning bomb
release when the plane is upside down.  He said the software OUGHT to
prevent such an occurrence.  When the plane is not at the right angle of
attack into the air stream, toss-bombing can result in the bomb being thrown
back into the airplane.  

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: <A HREF="/Risks/3.57.html">RISKS-3.57</A> Viking Project
</A>
</H3>
<address>
Eugene Miya
&lt;<A HREF="mailto:eugene@AMES-NAS.ARPA ">
eugene@AMES-NAS.ARPA 
</A>&gt;
</address>
<i>
16 Sep 1986 2213-PDT (Tuesday)
</i><PRE>

Sorry Dr. Benson, I wish to correct you on several points.  First off,
NASA is the CIVILIAN space agency.  NASA takes great pains to emphasize
this.  We are frequently accused of being puppets of the military and
we cannot deny that the DOD are customers and joint researchers, but
the DOD also causes us problems.  Many scientists in NASA (myself included)
work here to try an benefit ALL mankind.

The Viking Project, in particular, is not a military project and the scientists
that I know such as Conway Snyder and others would take great offense to
your implication.  (I think Sagan would be amused and offended, too.)
I can tell you there were bugs in the program.  Not
all was perfect.  Note the mission had redundency built into it.

What I can tell you about the physical systems is that spacecraft memories
at that period of time were very small and quite crude.  We are talking
hundreds of words of storage not K.  We are not talking sophisticated
programming either (more like hard coded routines). We are not talking
FORTRAN except for the trajectory and orbit determination programs (still
in use with 400K to 1M lines of code: Univac FORTRAN V and now VAX VMS
FORTRAN).  This code may be purchased from COSMIC (I think something like
$2K I can look this up).  Regarding other project documentation about
the nature of the Viking computers and their software, this is all in the
public domain in the form of NASA TRs.  (Don't ask for all, we are talking
TONS of documentation, you want to ask for specifics. and I might be
able to help a little [emphasis] by giving you contacts to phone at JPL).

(Un)happily? no Martians shot at the Landers.  I don't know how we would have
faired.  The system had no AI, it's really was not a concurrent system,
it had strictly local real-time processing, but not by choice (one-way
signal time to Mars is 7 minutes).

Valhalla: that place where Viking Project members go to retire.

--eugene miya
  ex-Voyager Program member
  ex-JPL/CIT
  NASA Ames Research Center

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
 Protection of personal information
</A>
</H3>
<address>
David Chase 
&lt;<A HREF="mailto:rbbb@rice.edu">
rbbb@rice.edu
</A>&gt;
</address>
<i>
Tue, 16 Sep 86 23:37:47 CDT
</i><PRE>
To: risks@csl.sri.com

A friend of mine attending a large state university is preparing to
interview for jobs.  At this university the powers that bureaucratically
be "require" that you fill out a form that among other things has your
social security number and a statement that (if signed) authorizes release
of transcript to people who might wish to employ you.  Other things on
this form include percentage of college expenses earned, and similar rot
that one might wish to keep private.  No form, then no on campus
interviews.

Just to make things interesting, they wish to place this info in an
"experimental" database.

When faced with something like this, what does one person (out of 48000
students, most of them cooperating like sheep) do to get any assurance
that private information is not released to undesirables (where the set of
"undesirables" is defined by the one person, NOT the university)?  This
same problem pops up with utilities in this state also, and the bargaining
position is even worse than the student's ("I'm sorry sir, but we can't
turn on your power until I complete this form, and I can't complete it
without your social security number").

Does anyone have any experience with this sort of thing?  I read a little
blurb while waiting to get my drivers license that told all about how one
should most definitely keep one's social security number in confidence, so
handing out (without permission) even those 9 digits to an alleged
prospective employer is out of line.  Never mind that those same 9 digits
are your "student number" at this school.

(Perhaps this belongs on Human Nets, but I feel this is a risk--if nothing
else, it raises my blood pressure to dangerous levels to hand out private
information to pig-headed idiots.  I'd also rather prevent some of this
now than be the subject of an amusing/shocking anecdote later)

David

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Autonomous Weapons
</A>
</H3>
<address>
Ken Laws 
&lt;<A HREF="mailto:Laws@SRI-STRIPE.ARPA">
Laws@SRI-STRIPE.ARPA
</A>&gt;
</address>
<i>
Wed 17 Sep 86 07:10:43-PDT
</i><PRE>
To: Risks@CSL.SRI.COM
Message-ID: &lt;12239658229.17.LAWS@SRI-STRIPE.ARPA&gt;

Eugene Miya asks whether autonomous weapons can be considered moral.  Brief
thoughts (since Risks may not be the right forum):

Dumb weapons or those guided incompetently are no better -- was the
accidentaly bombing of the French Embassy in Libya moral?

Autonomous vehicles (or, for that matter, bombs) are not smart enough
to perform trivial civilian duties in cooperative environments (e.g.,
driving to the grocery store or picking weeds in a corn field).
Someday they may be, in which case questions about their intelligence
and morality may be worth debating.  For now, the assumption is that
they are only to be used in situations where anything that moves is
a legitimate target and where taking out the wrong "target" is better
than taking out no target.  This is rather similar to the situation
facing nukes, and the moral choices in initiating use are the same.
The advantages of autonomous weapons over nukes should be obvious,
although there will always be philosophers and humanists who mourn an
isolated wrongful death as much as the destruction of a city.

					-- Ken Laws

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Re: computers and petty fraud
</A>
</H3>
<address>
"Col. G. L. Sicherman" 
&lt;<A HREF="mailto:colonel%buffalo.csnet@CSNET-RELAY.ARPA">
colonel%buffalo.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Wed, 17 Sep 86 15:33:21 EDT
</i><PRE>
Newsgroups: mod.risks
To: risks%sri-csl.arpa@CSNET-RELAY.ARPA
References: &lt;8609160017.AA09578@ucbvax.Berkeley.EDU&gt;

In <A HREF="/Risks/3.54.html">RISKS-3.54</A> Mark S. Day inquires why computerization encourages people
to defraud shop clerks.
 
&gt;                            ... Thus, people will talk about switching
&gt; UPC price tags who would view switching non-computerized price tags as
&gt; fraud.

This is partly because it's less easily detected.  Replacing price tags
with bar codes means that the clerk has little or no opportunity to
consider whether the price is reasonable.  The effect resembles what
happened when hand calculators replaced slide rules.  By eliminating
the element of clerical surveillance, the manager increases efficiency
at the cost of security.  It's a typical trade-off.

As for the customers ... perhaps the general run of people were never
very ethical to begin with?

&gt;         Similarly, people will read mail and data files stored on a
&gt; timesharing system, even though it's unacceptable to rifle through
&gt; people's desks.

There are two active changes here.  First, a time-sharing system is
perceived as a shared facility (even if it runs VM! :-), a commune
rather than an apartment house, so to speak.  This has been reinforced
by the development of message systems.  Second, the phenomenal progress
in communication in recent years has undermined public support for
privacy.  The subject of privacy has been vexing and misleading pundit
lately; the best treatment of it is to be found in _The Gutenberg
Galaxy_ by H. M. McLuhan.  (It's nothing like the typical liberal or
illiberal arguments one normally reads.)

A third factor, and I think a significant one, is the re-alignment of
popular loyalty.  Large societies are products of the age of print.
In particular, print provides the inspiration for uniform, stable
laws, language, and conventions; it also creates the necessary illusion
of commonality by virtue of the physical uniformity of print and the
impersonality of publishing.  (One could add that large states and
countries are perceptible chiefly by virtue of printed maps.)
In an age of fast, easy communication, artifacts like countries
grow to appear unreal and arbitrary.  People are coming to prefer
to deal directly with one another, and personal loyalties are out-
weighing loyalties to abstractions like country and society.  I do
not believe that this is a bad thing; it increases strife, but
reduces international war.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.57.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.59.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-45</DOCNO>
<DOCOLDNO>IA012-000123-B023-20</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.59.html 128.240.150.127 19970217004721 text/html 18473
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:45:48 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 59</TITLE>
<LINK REL="Prev" HREF="/Risks/3.58.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.60.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.58.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.60.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 59</H1>
<H2>Saturday, 20 September 1986</H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Computers and Wall Street 
</A>
<DD>
<A HREF="#subj1.1">
Robert Stroud
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Report from the Computerized Voting Symposium 
</A>
<DD>
<A HREF="#subj2.1">
Kurt Hyde
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Computers, TMI, Chernobyl, and professional licensing 
</A>
<DD>
<A HREF="#subj3.1">
Martin Harriman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Failsafe software 
</A>
<DD>
<A HREF="#subj4.1">
Martin Ewing
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Software vs. Mechanical Interlocks 
</A>
<DD>
<A HREF="#subj5.1">
Andy Freeman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  How Not to Protect Communications 
</A>
<DD>
<A HREF="#subj6.1">
Geoff Goodfellow
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Computers and Wall Street
</A>
</H3>
<address>
Robert Stroud 
&lt;<A HREF="mailto:robert%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK">
robert%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Thu, 18 Sep 86 14:07:59 gmt
</i><PRE>

I came across an article in Computing which gives more details about the
way in which computer systems are influencing the stock market. It suggests
that dealers are forced to rely on the "intuition" of their system, even
against their better judgement, for fear of being caught out. Personally
I find this trend very alarming, but perhaps the fluctuations on the stock
market are just "noise" with no lasting influence on the real economy.
Unfortunately, the "noise" can be heard around the world.

Robert Stroud,
Computing Laboratory,
University of Newcastle upon Tyne.

ARPA robert%cheviot.newcastle@cs.ucl.ac.uk (or ucl-cs.ARPA)
UUCP ...!ukc!cheviot!robert

----------------------------------------------------------------------------
Reproduced without permission from Sep 18th Computing, (c) Computing.

"Technology led Wall Street to drop prices" by Alex Garrett

The crash in prices which wiped a record amount off the value of shares
on Wall Street last week was largely the result of computerised dealing
systems failing to read the market.

Computer generated selling of shares was estimated to account for almost
50% of the transactions that caused a record volume of 240 million shares
to change hands last Friday. But it is believed that the effect of the
computers was to exaggerate the underlying movement in the market, so
that many shares were sold unnecessarily.

The problem has arisen as a number of factors conspired to make the US
stock markets subject to increasing fluctuations, which in turn has caused
stockbrokers to rely far more heavily upon the split-second advice of their
computer systems. In particular, many systems are triggered by a drop in
share price to instruct a dealer to sell, and he will often do so, even against
his better nature, for fear of being caught out.

.... this kind of feature has yet to be adopted in the UK.

Ian Reid ... said that although shares will often recover their price within
a short time, some of the computer systems in the US do not have the intuition
to see this.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Report from the Computerized Voting Symposium
</A>
</H3>
<address>
Jekyll's Revenge 264-7759 MKO1-2/E02
&lt;<A HREF="mailto:hyde%abacus.DEC@decwrl.DEC.COM  ">
hyde%abacus.DEC@decwrl.DEC.COM  
</A>&gt;
</address>
<i>
Friday, 19 Sep 1986 11:37:13-PDT
</i><PRE>

Belated Report from the  Symposium  on  Security  and  Reliability  of
Computers in the Electoral Process -- August 14th &amp; 15th, 1986

The participants came from many backgrounds, computer people, writers,
attorneys,  and  even  one Secretary of State.  Some of the highlights
emphasized by one or more speakers were:

      o  Lever voting machines are still  the  fastest  way  to  count
         votes.   The  computerized  vote counting machines are slower
         than lever machines, but faster than paper ballots.

      o  Lever voting machines still appear to be the  safest  way  to
         count votes.

      o  The  State  of  Illinois  tested  its   computerized   voting
         equipment  and  found  numerous  instances  of errors in vote
         counting, primarily in undervotes,  overvotes,  and  straight
         party crossovers.

         NOTE:  An undervote is voting for fewer candidates  than  the
         maximum  allowed  for  an  office.  An overvote is voting for
         more candidates than allowed for an office.  A straight party
         crossover is casting a vote to be applied to all members of a
         party and then switching one or more votes to candidates from
         another party.

      o  A group of Computer Science students  at  Notre  Dame  (South
         Bend,  IN)  tested a punch card voting system with a group of
         test ballots.  By altering only the control cards  they  were
         able  to  manage  the  vote  totals  to predictable incorrect
         totals.

Some of the recommendations made by one or more speakers were:

      o  Five percent  of  all  votes  cast  should  be  recounted  by
         different method than the original count.

      o  Security  standards  for  computerized  voting   are   needed
         immediately.  The expanding use of computerized vote counting
         equipment may preclude an effective implementation of such  a
         standard.

      o  Punch card ballots should be redesigned  to  make  the  punch
         card  into  a ballot that is readable by the voter as well as
         by the computer.

      o  Internal procedures of computerized voting equipment must  be
         open  to  the public in order to let the public be in control
         and to assure public confidence in the electoral process.

      o  Computerized voting equipment must  have  the  capability  of
         allowing  the  voter  to  monitor  the  ballots  cast  by the
         computer to be sure it has voted as instructed.

      o  There should be public domain vote counting software in order
         that   companies   not   have  to  keep  their  programs  for
         proprietary ownership reasons.

         NOTE:  Does anyone know of a Computer Science student looking
         for a project?  I'm willing to share my notes.

         Is there anyone with the resources to build  prototypes  that
         have security features, such as voter-readable punch cards or
         a computer-generated, recountable ballot?


Bill Gardner, New Hampshire's Secretary of State, informed us that New
York  City  is  planning  to  purchase  new voting equipment.  This is
likely to become a de facto standard for New York State and, possibly,
for  whole  the  nation.  Risks Forum people who'd like to contact the
New York City Task Force should contact:

David Moscovitz
New York City Elections Project
2 Lafayette Street, 6th Floor
New York, NY 10007
(212) 566-2952


The results of my informal poll  on  trusting  a  computerized  voting
system:

                                       Trust     Not Trust   Undecided

(1) Internal Procedures secret          2/40         38/40           0
    Results not monitored by voter 

(2) Internal Procedures Revealed        6/40         34/40           0
    Results not monitored by voter 

(3) Internal Procedures secret         10/40         28/40        2/40
    Results can be monitored by voter 

(4) Internal Procedures Revealed       24/40         11/40        5/40
    Results can be monitored by voter 

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
 Computers, TMI, Chernobyl, and professional licensing
</A>
</H3>
<address>
    Martin Harriman 
&lt;<A HREF="mailto:MARTIN%SRUCAD%sc.intel.com@CSNET-RELAY.ARPA">
MARTIN%SRUCAD%sc.intel.com@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Wed, 17 Sep 86 09:42 PDT
</i><PRE>

The NRC does require testing and certification of the software used in the
design of nuclear power plants:  this includes the software used for seismic
simulations, fueling studies, and simulations of coolant behavior (which
can get quite complex in BWR designs).

The reactors themselves are designed to be stable, so they do not require
a complex control system for safe operation (unlike military aircraft with
negative aerodynamic stability).  Incidentally, the feedback mechanisms
used to produce stability in US reactor designs are missing from graphite
moderated, water damped designs like Chernobyl; this lack of stability
contributed to the initial explosion at Chernobyl.

Professional licensing is state-regulated; I'm not aware of any states with
a professional engineer exam for software engineers.  I don't believe that
professional licensing is all that useful; I'm more interested in quality
assurance for safety-related software (and hardware) than in ensuring that
some fraction of the people developing the software passed an examination.
It would be fairly amusing if PE registration became popular with software
engineers, since it would mean they would all need to learn a fair chunk
of civil engineering (the Engineer In Training exam requires it).

  --Martin Harriman &lt;martin%srucad@sc.intel.com&gt;

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
 Failsafe software
</A>
</H3>
<address>
Martin Ewing
&lt;<A HREF="mailto:    mse%Phobos.Caltech.Edu@DEImos.Caltech.Edu ">
    mse%Phobos.Caltech.Edu@DEImos.Caltech.Edu 
</A>&gt;
</address>
<i>
Thu, 18 Sep 86 09:57:27 PDT
</i><PRE>
To:       arms-d%Phobos.Caltech.Edu@DEImos.Caltech.Edu,
          risks%Phobos.Caltech.Edu@DEImos.Caltech.Edu

How can we even dream of SDI or fly-by-wire aircraft when I just received
12 nearly identical copies of the last ARMS-D mailing, at 33 KB a crack?

Seriously, this is an example of failsafe:  if some transmission error
occurs before a message transmission is complete, send it again, and again,
and again...  And no one is even shooting at the net, as far as I know.

  Martin Ewing

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Software vs. Mechanical Interlocks
</A>
</H3>
<address>
Andy Freeman 
&lt;<A HREF="mailto:FREEMAN@SUMEX-AIM.ARPA">
FREEMAN@SUMEX-AIM.ARPA
</A>&gt;
</address>
<i>
Thu 18 Sep 86 10:16:01-PDT
</i><PRE>
To: risks@CSL.SRI.COM

One current advantage of mechanical interlocks is that they can (usually) be
bypassed or modified in the field.  If I went on a special toss-bombing
mission, I'd be much happier hearing "the mechanical upside-down
bomb-release interlock has been removed" than "we just patched out that
section of the code and burned a new prom".
                                               -andy

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
How Not to Protect Communications
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
20 Sep 1986 06:52-PDT
</i><PRE>
From: the tty of Geoffrey S. Goodfellow &lt;Geoff @ csl.sri.com&gt;
To: risks@CSL.SRI.COM
Cc: security@RED.RUTGERS.EDU
	
  [The New York Times, September 13, 1986]

  BALTIMORE - The Senate should avoid repeating the mistake made by the 
House when it unanimously passed the Electronic Communications Privacy 
Act.  Purportedly a benign updating of the 1968 Federal wiretap law 
designed to guarantee privacy in the electronic age, the bill actually 
promotes the cellular telephone industry at the expense of the public 
good.

  True enough, obsolete language in the existing wiretap law fails to 
address digital, video, and other new forms of communications.  The 
proposed law would fix that.  But it would also declare certain 
communications legally private regardless of the electronic medium 
employed to transport them.  The mere act of receiving radio signals, 
except for certain enumerated services like commercial broadcasts, would 
become a federal crime.

  To disregard the medium is to ignore the essence of the privacy issue.  
Some media, such as wire, are inherently private.  That is, they are 
hard to get at except by physical intrusion into a residence or up a 
telephone pole.  Others media, notably radio signals, are inherently 
accessible to the public.  Commercial radio and television broadcasts, 
cellular car telephone transmissions and other "two-way" radio 
communications enter our homes and pass through our bodies.  Cellular 
phone calls, in fact, can be received by most TV sets in America on UHF 
channels 80 through 83.

  If radio is public by the laws of physics, how can a law of Congress 
say that cellular communications and other forms of radio are private?  
The unhappy answer is that the proposed law appears to be a product of 
technological ignorance or wishful thinking.  A similar edict applied to 
print media would declare newspapers, or portions of them, to be as 
private as first class mail.  The result is plainly absurd and contrary 
to decades of reasonable legislative and judicial precedent.

  In contrast, present Federal statute prescribes a sensible policy for 
oral communications, protecting only those "uttered by a person 
exhibiting an expectation that such communication is not subject to 
interception under circumstances justifying such expectation."  To 
illustrate, a quiet chat in one's parlor would likely be protected.  
Substitute for the parlor a crowded restaurant or the stage of a packed 
auditorium, the expectation of privacy is no longer justified.  The law 
would not grant it.
  
  Congress should apply this same logic to electronic communications.  
The broadcasting of an unencrypted radio telephone call, or anything 
else, is an inherently public act, whether so intended or not.  Thus it 
violates the "justifiable expectation" doctrine, and warrants no Federal 
privacy protection.  

  Protection or no, people will not be stopped from receiving radio 
signals.  Even Representative Robert W. Kastenmeier, Democrat of 
Wisconsin, who championed the bill in the House, confesses that its 
radio provisions are essentially unenforceable.  They will have no 
deterrent effect, and they will not increase the privacy of cellular 
phone calls or other broadcasts.  Worse, the act would lull the public 
into a false presumption of privacy.

  On further examination, it appears that the legislation is really more 
a sham than an honest, if puerile, attempt by Congress to deal with new 
technology.  Its sponsors say they aim to protect all electronic 
communications equally.  Yet the bill sets out at least four categories 
of phone calls, with varying penalties for interception.  Cellular radio 
calls are guarded by threat of prison, but there is no interdiction 
whatsoever against eavesdropping on "cordless" telephones of the sort 
carried around the apartment backyard.

  So Congress is about to give the cellular telephone industry ammunition 
for advertising and bamboozling, promising privacy that does not 
actually exist.  Cellular service companies thereby hope to avoid losing 
revenue from customers who might use the service less if they understood 
its vulnerability.

  If Congress were serious about privacy in the communications age, it 
would scrap the Electronic Communications Privacy Act and begin anew.  
Legislators and the public must first grasp the true properties of new 
technologies.  Are those properties inadequate or unsavory?  If so, 
relief will only come from research and more technology not wishful 
legislation.

  ------------
  Robert Jesse is a technology consultant.    [known to us all as rnj@brl]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.58.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.60.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-46</DOCNO>
<DOCOLDNO>IA012-000123-B023-43</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.60.html 128.240.150.127 19970217004738 text/html 23963
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:46:02 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 60</TITLE>
<LINK REL="Prev" HREF="/Risks/3.59.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.61.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.59.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.61.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 60</H1>
<H2>Saturday, 20 September 1986</H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Sanity checks 
</A>
<DD>
<A HREF="#subj1.1">
Roy Smith
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Viking Flight Software working the `first' time? 
</A>
<DD>
<A HREF="#subj2.1">
Greg Earle
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  A million lines of code works the first time?    
</A>
<DD>
<A HREF="#subj3.1">
Anonymous
</A><br>
<A HREF="#subj3.2">
 Dave Benson
</A><br>
<A HREF="#subj3.3">
 Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: Massive UNIX breakins at Stanford 
</A>
<DD>
<A HREF="#subj4.1">
Scott E. Preece
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Re: Protection of personal information 
</A>
<DD>
<A HREF="#subj5.1">
Andy Mondore
</A><br>
<A HREF="#subj5.2">
 Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Announcement of Berkeley Conference on the SDI 
</A>
<DD>
<A HREF="#subj6.1">
Eric Roberts
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Sanity checks
</A>
</H3>
<address>
Roy Smith
&lt;<A HREF="mailto:allegra!phri!roy@seismo.CSS.GOV ">
allegra!phri!roy@seismo.CSS.GOV 
</A>&gt;
</address>
<i>
Fri, 19 Sep 86 16:43:39 edt
</i><PRE>
Organization: Public Health Research Institute, NYC, NY

	I'd like to relate 3 incidents along the lines of people willing to
believe anything the computer tells them, what I call the "if it's on
green-bar, it it must be true" syndrome.

	Incident #1 was two weeks ago.  I got 2 items for $5.95 and $8.95
at our local Radio Shack.  There was no tax on this sale and I quickly came
up with $14.90 in my head (if that's not right, I'm going to be *really*
embarrassed).  The sales clerk grabbed a calculator and came up with $14.93.
I'm not so upset at the fact that he came up with the wrong sum, but that
he didn't apply the trivial check that if you have a bunch of numbers, all
ending in 0 or 5, the sum must also end in 0 or 5.  Moral:  Always check
your results for sanity and never trust the clerks in Radio Shack.

	Incident #2 was a few days later.  In a discussion of very large
memories I mentioned that 200 bits is the biggest address you would ever
need and that 2^200 was about 10^40 (see usenet's net.arch for the past few
weeks).  How did I come up with that?  Easy, I just fired up a desk
calculator program, typed "2^200" at it and it typed back "1.70141e+38".

	Now, I *knew* this was too small (at 3 or 4 bits per decimal digit
I expected about 10^65) so I tried it again.  Since it gave the same answer
again, I figured it must be right.  Of course the problem was overflow (you
would think that by now any time I see a Vax print out 1.7e38 a bell would
go off in my head).  This is even worse than the clerk in Radio Shack; here
I had 2 reasons to suspect the answer was wrong and I still blindly
believed what the computer told me!  Moral: Always check your results for
sanity and don't get a big head thinking you're smarter than the clerks at
Radio Shack.

	Incident #3 was a few years ago.  We got a FORTRAN program to
predict protein secondary structure (feed it a sequence and it says where
it's alpha-form and where it's beta).  We fired it up and it ran so we put
it into production use.  It showed a lot more beta then we expected, but it
never occurred to us to suspect the program -- the algorithm was known to
slightly over-predict beta and we were perfectly willing to believe that
the outrageous amount of beta we were getting was due to that.

	To get to the point, the program was from a Vax and we were running
it on a pdp-11.  The input (3-letter codes) was stored in INTEGER*4's,
quitely truncated to INTEGER*2's by the compiler.  Most of the codes are
distinct in the first 2 letters so this was usually ok.  It was, however,
turning aspartic acid into asparagine (asp-&gt;asn) and glutamic acid into
glutamine (glu-&gt;gln); both those substitutions tend to result in more beta
form!  It was weeks before somebody spotted that the annotated sequence the
program printed out didn't match the input.  Moral #1: Always use sanity
checks, but don't blindly rely on them; if your check is "x &gt; y", think
before you accept "x &gt;&gt; y" .  Moral #2: If the program provides aids like
echo printing of input, use them.  Moral #3: If you're modifying a program
or porting it to a new machine, do regression testing.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Viking Flight Software working the `first' time?
</A>
</H3>
<address>
Greg Earle
&lt;<A HREF="mailto:elroy!smeagol!gorbag!earle@csvax.caltech.edu ">
elroy!smeagol!gorbag!earle@csvax.caltech.edu 
</A>&gt;
</address>
<i>
Wed, 17 Sep 86 21:35:44 pdt
</i><PRE>

Correct me if I am wrong, but for any spacecraft that I know of, virtually
every major spacecraft function can be exaustively tested on the ground
before the thing ever leaves the pad.  About the only thing you can't test
(obviously) is the software to actually physically separate the lander from
the command module on descent into the atmosphere.  Everything else, to
my knowledge, can be covered pretty thoroughly.  The projects that I am
associated with, here at JPL, are involved with test sets that test all
the functions of the spacecraft Command Data Subsystem (CDS) which is also
called the Payload Data System (PDS) on Mars Observer.  In other words,
this exercises the flight software that resides in the command data subsystem,
and telemetry streams are initiated, commands are uplinked, etc. etc.

Now maybe we want to pick nits and say "Well it worked the first time in
Actual Outer Space Usage", which is true, but considering the amount of
testing done beforehand (we are now testing breadboard CDS's for missions
that won't launch until at least 1991), 'tis not all that surprising
when it works ...

	Greg Earle		UUCP: sdcrdcf!smeagol!earle; attmail!earle
	JPL			ARPA: elroy!smeagol!earle@csvax.caltech.edu
        AT&amp;T: +1 818 354 0876   earle@JPL-MILVAX.ARPA
				
</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Anonymous contribution
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
18 Sep 86 20:21:00 EDT
</i><PRE>
To: risks-request@sri-csl
Re: "a million lines of code and it worked the first time", or words to 
    that effect, from an SDI spokesman referring to a recent test.

Let's take this with a grain of salt.  I have seen a large system (over
100,000 lines of high-level language) "work the first time". By this I
mean that in the first live test of the system, it performed as designed
with no errors.  That software had been designed and programmed by a
small, close-knit group of experienced real-time programmers, and had
been extensively tested at the module level with drivers and stubs, and
also at the system level using a very realistic simulation. (Also bear
in mind that the first live test of *any* system is likely to be quite
conservative in its objectives; it's likely that only a small fraction
of all possible paths through the code will actually be exercised.) 
Furthermore, the 100K lines of code that made it to the first live test
were by no means the original, first-cut 100K lines written (although a
gratifyingly large percentage of them were, thanks to good design
practices.) 

If the SDI test were a similar situation -- well-designed, thoroughly
pre-tested software that worked well on its initial, conservative live
test -- then it's at least plausible.  If, on the other hand, the
spokesman actually meant "we coded up 1,000,000 lines and then tried
them and they all worked" -- then I'd have to see some proof (in fact, a
*lot* of proof) before I'd believe it. 

</PRE>
<HR><H3><A NAME="subj3.2">
A million lines of code works the first time?
</A>
</H3>
<address>
Dave Benson 
&lt;<A HREF="mailto:benson%wsu.csnet@CSNET-RELAY.ARPA">
benson%wsu.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Tue, 16 Sep 86 16:56:14 pdt
</i><PRE>

 |Heard on NPR's "All Things Considered" yesterday evening:
 |An Air Force Lt. Col., speaking about a kinetic energy weapons
 |test earlier this week, which apparently went better than expected
 |in several respects.  If this isn't an exact quote (I heard it
 |twice, but didn't write it down at the time), it's real close:
 |"We wrote about a million lines of new computer code, and tested
 |them all for the first time, and they all worked perfectly."

Hoo boy!  I would appreciate any and all leads by which I might track
this to some reliable source.  Thank you,  David B. Benson, Computer
Science Department, Washington State University, Pullman, WA 99164-1210.
csnet: benson@wsu

</PRE>
<HR><H3><A NAME="subj3.3">
I found one! (A critical real-time application worked the first time)
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Wed, 17 Sep 1986  12:44 EDT
</i><PRE>

    From: Dave Benson &lt;benson%wsu.csnet at CSNET-RELAY.ARPA&gt;

        The unprecented success of the Viking
    	mission was due in part to the ability of the flight software
    	to operate in an autonomous and error free manner. ...
    	Upon separation from the Oribiter the Viking Lander, under autonomous
    	software control, deorbits, enters the Martian atmosphere,
    	and performs a soft landing on the surface. ... Once upon the surface,
    	... the computer and its flight software provide the means by
    	which the Lander is controlled.  This control is semi-autonomous
    	in the sense that Flight Operations can only command the Lander
    	once a day at 4 bit/sec rate.

    Nevertheless, we may judge this as one of the finest software engineering
    acomplishments to date.  The engineers on this project deserve far more
    plaudits than they've received.  I know of no similar piece of software
    with so much riding upon its reliable behavior which has done so well.

While I certainly agree that the Viking software is an example of very
fine software, its subsequent history is one that is less laudatory.
Ground control lost contact with Viking 1, apparently due to a
software change transmitted to the lander that was accidentally
overlaid upon some mission-critical software already in the lander's
computer. (Bruce Smith, "JPL Tries to Revive Link with Viking 1",
@ux(Aviation Week and Space Technology), April 4, 1983, Volume
118(14), page 16.)

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: Massive UNIX breakins at Stanford
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%ccvaxa@GSWD-VMS.ARPA">
preece%ccvaxa@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Thu, 18 Sep 86 09:12:59 cdt
</i><PRE>

&gt; From: reid@decwrl.DEC.COM (Brian Reid) The machine on which the initial
&gt; breakin occurred was one that I didn't even know existed, and over
&gt; which no CS department person had any control at all. The issue here is
&gt; that a small leak on some inconsequential machine in the dark corners
&gt; of campus was allowed to spread to other machines because of the
&gt; networking code. Security is quite good on CSD and EE machines, because
&gt; they are run by folks who understand security. But, as this episode
&gt; showed, that wasn't quite good enough.
----------

No you're still blaming the networking code for something it's not supposed
to do.  The fault lies in allowing an uncontrolled machine to have full
access to the network.  The NCSC approach to networking has been just that:
you can't certify networking code as secure, you can only certify a network
of machines AS A SINGLE SYSTEM.  That's pretty much the approach of the
Berkeley code, with some grafted on protections because there are real-world
situations where you have to have some less-controlled machines with
restricted access.  The addition of NFS makes the single-system model even
more necessary.

scott preece, gould/csd - urbana, uucp:	ihnp4!uiucdcs!ccvaxa!preece

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Re: Protection of personal information
</A>
</H3>
<address>
&lt;<A HREF="mailto:Andy_Mondore%RPI-MTS.Mailnet@MIT-MULTICS.ARPA">
Andy_Mondore%RPI-MTS.Mailnet@MIT-MULTICS.ARPA
</A>&gt;
</address>
<i>
Fri, 19 Sep 86 10:00:10 EDT
</i><PRE>

David Chase wrote in Risks 3.58 that at his university, students were
required to give a lot of personal information on a form before they could
sign up for on-campus job placement interviews and that by signing this
form, they authorized the university to release their transcripts to
potential employers.  He also complained about the use of the social
security number as the student number.

Here at RPI, I think the only form you are required to fill out before
getting on-campus interviews is a resume form.  I work in the Registrar's
office and we release a transcript only if we have received a signed
statement from the student authorizing release of the transcript to a
specific person or company.  As far as I know, we don't accept "blanket"
releases.

As for the use of social security numbers as student numbers -- we also use
social security numbers for this purpose.  One of the reasons we do this is
that if you are receiving financial aid, we must verify your attendance
every semester to the agency supplying the aid.  Very often, this
verification is in the form of a computer-generated list or tape from the
agency and the only way to cross-reference their list to our file is via the
social security number.  It is usually difficult to do a computer-match on
name because of differences in how the names might be formatted.  There is
the same problem when a student has an on-campus job -- the payroll office
needs to verify that the student is registered and they need the social
security number for tax purposes, so they prefer to use it as their primary
means of identifying the student (or any other employee).

In terms of requiring you to give us your social security number, federal
law prohibits us from requiring you to give it to us except for tax or
social security purposes.  However, the law has also been interpreted to
mean that we also have the option of not servicing you if you refuse to give
it.  (I don't think that has ever happened here, however.)

For the final word on what can and cannot be done with personal
information, I suggest you check the Family Rights to Privacy
Act (popularly known as the Buckley Amendment).

</PRE>
<HR><H3><A NAME="subj5.2">
Protection of personal information
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Thu, 18 Sep 1986  21:26 EDT
</i><PRE>

My understanding is that use of one's SS number must be authorized by law.
There are times when others ask, but you are not required to give it to them.

Under those circumstances, I don't believe it it is illegal to give a
fake SSN.  The way to protect yourself is to give your real SSN,
except for a small error that you can later blame on an entry error.

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Announcement of Berkeley Conference on the SDI
</A>
</H3>
<address>
Eric Roberts
&lt;<A HREF="mailto:roberts@src.DEC.COM ">
roberts@src.DEC.COM 
</A>&gt;
</address>
<i>
Thu, 18 Sep 86 13:25:05 pdt
</i><PRE>

The Dave Redell/Hugh DeWitt panel (Saturday morning) should be of special
interest to RISKS readers and the rest of the program of general interest.


                      STAR WARS AND NATIONAL SECURITY

             A Conference on the Strategic Defense Initiative
          October 9-11, 1986, University of California, Berkeley


--------------------  PART ONE: Exploring the Issues  --------------------

             Thursday Evening, 8:00-10:30, Wheeler Auditorium

Opening Debate:  "Technical Feasibility and Strategic Policy Implications
of the SDI"
   Andrew Sessler (moderator), Former Director of Lawrence Berkeley
      Laboratory; Member of American Physical Society Panel on Directed
      Energy Weapons.
   Lowell Wood, leader of "O Division," Lawrence Livermore National
      Laboratories.
   Richard Garwin, IBM Research Fellow; Adjunct Professor of Physics,
      Columbia University; Adjunct Research Fellow, Center for Science and
      International Affairs, Kennedy School of Government, Harvard
      University.
   Colin Gray, President, National Institute for Public Policy; Member of
      the President's General Advisory Committee on Arms Control and
      Disarmament.
   John Holdren, Professor of Energy and Resources, UC Berkeley; Chairman,
      U.S. Pugwash Committee; Former Chairman, Federation of American
      Scientists.

               Friday Morning, 9:00-11:00, Sibley Auditorium

Legislative Hearing: "Keeping California Competitive in R&amp;D: The Impacts of
Increased Military Spending, the SDI, and Federal Tax Reform" (This event
will be co-sponsored by the California Assembly Committee on Economic
Development and New Technologies.)
   Glenn Pascall, Senior Research Fellow, Graduate School of Public
      Affairs, University of Washington; President, Columbia Group Inc.
   Jay Stowsky, Research Economist, Berkeley Roundtable on the
      International Economy, UC Berkeley.
   Ted Williams, Chief Executive Officer, Bell Laboratories [invited].
   Robert Noyce, Vice-Chairman of the Board, Intel [invited].
   Ralph Thompson, Senior Vice-President for Public Affairs, American
      Electronics Association.
   John Holdren, Professor of Energy and Resources, UC Berkeley; Chairman,
      U.S. Pugwash Committee; Former Chairman, Federation of American
      Scientists.

Documentary Film: "Star Wars: A Search for Security," produced by Ian
Thiermann for PSR, 11:30-12:00 and 2:00-2:30, Room 4, Dwinell Hall.

              Friday Afternoon, 3:00-5:00, Wheeler Auditorium

Panel Discussion: "The Effects of SDI on Universities"
   Marvin Goldberger (moderator), President, Caltech.
   Vera Kistiakowsky, Professor of Physics, MIT.
   John Holdren, Professor of Energy and Resources, UC Berkeley; Chairman,
      U.S. Pugwash Committee; Former Chairman, Federation of American
      Scientists.
   Clark Thompson, Professor of Computer Science, University of Minnesota.
   Danny Cohen, Director, Systems Division, Information Sciences Institute,
      University of Southern California; Chairman, SDIO Committee on
      Computing in Support of Battle Management.


--------------------  PART TWO: Responses to the SDI  --------------------

              Saturday Morning, 9:00-1:00, Wheeler Auditorium

Panel Discussion: "Demystification of the SDI: Software, Hardware, and the
Appropriateness of Technological Solutions to Political Problems" 9:00-
10:30
   Hugh DeWitt, Physicist, Lawrence Livermore National Laboratory.
   Dave Redell, Computer Scientist, Systems Research Center, Digital
      Equipment Corporation.

Panel Discussion: "Alternatives to the SDI: The Peaceful Uses of Space
Technology and Alternative Security Strategies" 10:45-1:00
   Congressman George Brown (D. CA), Co-Chair, Congressional Space Caucus,
      a leading advocate for space cooperation and opponent of space
      militarization.
   Dan Deudney, author of "Forging Missles into Spaceships," World Policy
      Journal, Spring 1985, and "Whole Earth Security: Toward a Geopolitics
      of Peace," Worldwatch Paper No. 55.
   Mark Sommer, Co-founder of the Exploratory Project on the Conditions of
      Peace (EXPRO) and author of Beyond the Bomb.
   Anne Ehrlich, Senior Research Associate in Biological Sciences, Stanford
      University; member of the Sierra Club Committee on the Environmental
      Aspects of Warfare.
   Vivienne Verdon-Roe, Co-founder of the Educational Film and Video
      Project; her films include "In the Nuclear Shadow" and "Women--For
      America, For The World."

            Saturday Afternoon, 2:00-5:30, Room 10, Evans Hall

National and Local Political Strategies, 2:00-3:30
   Congressman George Brown (D. CA), Co-Chair, Congressional Space Caucus.
   Jerry Sanders, Senior Research Fellow, World Policy Institute; author of
      Peddlers of Crisis.
   Michael Shuman, President, Center for Innovative Diplomacy.
   Lee Halterman, Legal Counsel to Congressman Ron Dellums.
   Robert Ferrell, member, Los Angeles City Council; National Democratic
      Committee.

Organizing Strategies for Universities, 3:30-5:00
   Leonard Minsky, Executive Director, National Coalition of Universities
      in the Public Interest.
   Keith Miller, Professor of Mathematics, UCB; Chairman of the SDI
      Roundtable.
   Ted Forrester, Professor of Physics, UCLA; Chairman of Concerned
      Faculty.
   Roger Axford, Professor of Education, University of Arizona; Chairman
      of the Coalition for World Peace.

Concluding Remarks, 5:00-5:30


Conference sponsors include: Federation of American Scientists, National
Coalition for Universities in the Public Interest, Physicians for Social
Responsibility, Computer Professionals for Social Responsibility,
Progressive Space Forum, Student Pugwash, Peace and Conflict Studies (UCB),
ASUC.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.59.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.61.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-47</DOCNO>
<DOCOLDNO>IA012-000123-B023-68</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.61.html 128.240.150.127 19970217004807 text/html 14540
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:46:27 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 61</TITLE>
<LINK REL="Prev" HREF="/Risks/3.60.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.62.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.60.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.62.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 61</H1>
<H2> Sunday, 21 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Computers and Ethics 
</A>
<DD>
<A HREF="#subj1.1">
Robert Reed
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Autonomous weapons 
</A>
<DD>
<A HREF="#subj2.1">
Wayne Throop
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Simulation risk 
</A>
<DD>
<A HREF="#subj3.1">
Rob Horn
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Viking software 
</A>
<DD>
<A HREF="#subj4.1">
James Tomayko
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Risks of passwords on networks 
</A>
<DD>
<A HREF="#subj5.1">
Bruce
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  More on digital jets; Sanity checks 
</A>
<DD>
<A HREF="#subj6.1">
Eugene Miya
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Computers and Ethics
</A>
</H3>
<address>
&lt;<A HREF="mailto:bobr%zeus.tek.csnet@CSNET-RELAY.ARPA">
bobr%zeus.tek.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
19 Sep 86 13:36:54 PDT (Fri)
</i><PRE>

In <A HREF="/Risks/3.54.html">RISKS-3.54</A> Mark S. Day writes:
&gt; ...people will read mail and data files stored on a timesharing system, even
&gt; though it's unacceptable to rifle through people's desks. [...]

It occurs to me that each of these suggested mechanisms can be interpreted
in different ways which may provide new insights into the problem.

Novelty.  Social conditioning aside, the thrill of adventure in a new
		environment leads many people to explore the system in a
		quest for new understanding about it.  It is perhaps easier
		to lay the moral questions aside when caught in the fervor
		of covering new ground.  In fact the thrill is enhanced by
		doing something slightly larcenous.

Distance.  Certainly the distance between people is greater, but the
		distance between private pathways is shorter.
		Psychologically, I feel closer to your portion of the file
		system than I do to the contents of your desk drawers.
		Especially if working in an environment where limited
		sharing of files is part of the norm, the sense of
		territorial lines is less distinct within such an electronic
		medium

There is a third aspect which is related to the thrill factor, and that is
the threat of being caught.  If I am found in your office with my hand in
your desk, the evidence is pretty compelling and not easy to hide.  Within a
computer system, we are all little "virtual people", moving silently around
the directory tree, and so much less likely to arouse suspicions, so even
when ethical considerations are present, the concern about getting caught is
lessened by the nature of the medium.

Robert Reed, Tektronix CAE Systems Division, bobr@zeus.TEK

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Autonomous weapons
</A>
</H3>
<address>
&lt;<A HREF="mailto:rti-sel!dg_rtp!throopw%mcnc.csnet@CSNET-RELAY.ARPA">
rti-sel!dg_rtp!throopw%mcnc.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Fri, 19 Sep 86 16:46:17 edt
</i><PRE>

&gt; eugene@AMES-NAS.ARPA (Eugene Miya)
&gt; Most recently, another poster brought up the issue of autonmous weapons.

It is worth pointing out that we are *currently* using autonomous weapons
and they are *not* smart enough to distinguish signs of surrender.  Give up?
I'm talking about, for example, hand grenades or landmines.  These are
autonomous (after being thrown or burried) and their mission (guided by a
particularly simple "computer") is to saturate their environment with
shrapnel after a suitable delay.  Bombs with proximity fuses, self-guided
missiles, and so on, where there is "intelligence" in the weapon and a
significant time delay between the decision to deploy and the weapon's
effective discharge can all be considered cases of "autonomous weapons".  We
are (in this view) simply trying to make the beasties smarter, so that they
eventually *will* be able to recognize signs of surrender or cease-fire or
other cases of cessation of hostilities.  (Picture land-mines getting up and
"trooping" back to an armory after the war is over... )

Perhaps this is more appropos to one of the "arms" lists, but I think it is
worth noting that we are allowing some *very* simple "computers" to be in
charge of some *very* powerful weapons right now.  It is an interesting
question to ask if we really *want* to make the weapons smarter.  But I
don't think it is a question of whether to use autonomous weapons at all...
we're already using them.

Wayne Throop      &lt;the-known-world&gt;!mcnc!rti-sel!dg_rtp!throopw

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Simulation risk
</A>
</H3>
<address>
Rob Horn
&lt;<A HREF="mailto:harvard!wanginst!infinet!rhorn@seismo.CSS.GOV ">
harvard!wanginst!infinet!rhorn@seismo.CSS.GOV 
</A>&gt;
</address>
<i>
Sat, 20 Sep 86 16:11:42 edt
</i><PRE>

One kind of risk that I have not seen discussed here is the problems posed
by using computer simulation models that are not adequate.  In particular I
am refering to situations where due to either insufficient computer
resources, or insufficient mathematical analysis, the really accurate model
results are not available.  Usually more primitive, inaccurate model results
are available and being used by the ideologues on both sides of an issue.
This places the responsible scientists and engineers in a difficult
situation.  How do you say ``I don't know yet'' and how do you deal with
making recommendations in the absence of adequate information.

I can think of two such situations that have major public decision-making
impact.

The first is the ``nuclear winter'' situation.  I remember many years ago
reading the sensitivity analysis of the one-dimensional and two-dimensional
climate models to solar input.  They were hyper-sensitive, with variations
on the order of measurement error causing massive climate change.  It was
not until recently (1982) that the vectorized Climate Model was analyzed and
shown to be reasonably well behaved.  And even it has some contentious
approximations.  This model requires 15 hours on a CRAY-1 to analyze one
situation for one season.

When the nuclear winter stories came out I had my doubts.  Where did these
people find a solid month (12 seasons x 4(?) test cases) of CRAY time?  Had
they used one of the hyper-sensitive 1 or 2-dimensional models.  What would
the accurate models find?  And how should I respond when I knew that it
would probably be a year or more before that much CRAY time and post-
simulation analysis could be finished?  (Fortunately I only had to handle
party conversation with people who knew that I had worked in that field.)

The same kind of problem occured in the ozone layer issues during the mid
70's.  The more accurate model had two extremely severe problems: 1) it was
unconditionally unstable when phrased as a finite difference problem or
exceedingly temperamental when phrased as an implicit differencing problem.
2) It involved solving extremely stiff differential equations.  In this case
the official answer given was ``we don't know.  It will take several years
of mathematical research effort to make this problem tractable.  The real
answer is anyone's guess.  The published model answers are meaningless.''  A
truthful answer but of little value to decision makers.  (There was a brute
force throw-computers-at-it solution.  Estimated run-time on a CRAY was
about 1,000 years per simulated year.  Easier to wait and see what
happened.)
 
How often are we placed in a situation where the inaccurate computer
simulation is available, but the accurate simulation unavailable?  
What is an appropriate way to deal with this problem? 
 
				Rob  Horn
	UUCP:	...{decvax, seismo!harvard}!wanginst!infinet!rhorn
	Snail:	Infinet,  40 High St., North Andover, MA

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Viking software
</A>
</H3>
<address>
&lt;<A HREF="mailto:James.Tomayko@sei.cmu.edu">
James.Tomayko@sei.cmu.edu
</A>&gt;
</address>
<i>
Sunday, 21 September 1986 09:25:25 EDT
</i><PRE>

The Viking software load for the lander was 18K words stored on plated wire
memory. The Martin Marietta team decided to use a 'software first' approach
to the development of the flight load. This meant a careful examination of
the requirements, a serious memory estimate, and then commitment by the
project team to stay within that memory estimate. The software was developed
on an emulator that used microcoded instructions to simulate the
as-yet-unpurchased computer. Sources for this are a Rome Air Development
Center report that studied software development, later summarized in a book
by Robert L. Glass. The Viking software documents for the orbiter, developed
by JPL, are so good I use them as examples of tracability in my current
software engineering courses.

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Risks of passwords on networks
</A>
</H3>
<address>
&lt;<A HREF="mailto: BRUCE%UC780.BITNET@WISCVM.WISC.EDU">
 BRUCE%UC780.BITNET@WISCVM.WISC.EDU
</A>&gt;
</address>
<i>
20 SEP 86 14:57-EST
</i><PRE>

A few thoughts about networks which ask for passwords to send files.  Take a
computer network with three computers.  Call them computer A, B, and C.
Computer user on A wants to send a file to their account on C through
computer B.  No problem, we invoke the command to send files, supply it with
a password (and maybe a username at computer C) and off the files go.  But,
on computer B, there is a "smart" systems programmer who monitors all
network traffic through his/her node.  How interesting... A file copy
operation with a user name/password attached.

The point?  Just a password is not a good solution.  Maybe one
would need to encrypt the packets through the network (so that
intermediate nodes couldn't read them).

        Bruce

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
More on digital jets; Sanity checks
</A>
</H3>
<address>
Eugene Miya
&lt;<A HREF="mailto:eugene@AMES-NAS.ARPA ">
eugene@AMES-NAS.ARPA 
</A>&gt;
</address>
<i>
Sat, 20 Sep 86 11:40:44 pdt
</i><PRE>

Talk about timing:

In the latest copy of IEEE Spectrum (why didn't anyone else post this?)

%A Cary R. Spitzer
%Z NASA LaRC, Hampton, VA
%T All-digital Jets are Taking Off
%J IEEE Spectrum
%V 23
%N 9
%D September 1986
%P 51-56
%X Covers F-14D, F-16[CD], A-3[012]) airbus, 7J7, MD-11, and
other 1st and emerging 2nd generation digital systems.
Has good basic references.

Added note.  I will be contacting some old Viking friends for a further
detailed description and references as requested (probably next Tu. or We
when they come up here).

On Sanity checks:

I had a similar incident in a Silicon Valley Mexician restaurant
which I reported in a early RISK to the pocket book. This issue
has appeared other news groups like mod.comp-soc on the USENET.
I offer the following reference:

%A Jon L. Bentley
%Z ATT BL (research!)
%T The Envelope is Back
%J Communications of the ACM
%S Programming Pearls
%V 29
%N 3
%D March 1986
%P 176-182
%K rules of thumb, cost, orders of magnitude, quick calculations,
Litle's Law
%X JLB's principles include:
Familiarity with numbers
Willingness to experiment [actively, discussing this one with Denning]
Discipline in checking answers
Mathematics, when you need it
He also gives the "Back of the Envelope" column in the
American Journal of Physics as good reading.

I am reminded of a quote by Eric Shipton, an early English Mt. Everest veteran
who died recently: (paraphased) Never go on an expedition which you can't
plan on the back of an envelope.  I know this is how spaceflight is
frequently done.

--eugene miya
  NASA ARC

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.60.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.62.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-48</DOCNO>
<DOCOLDNO>IA012-000123-B023-108</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.62.html 128.240.150.127 19970217004832 text/html 16317
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:46:51 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 62</TITLE>
<LINK REL="Prev" HREF="/Risks/3.61.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.63.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.61.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.63.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 62</H1>
<H2> Monday, 22 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Massive UNIX breakins at Stanford                
</A>
<DD>
<A HREF="#subj1.1">
Jerry Saltzer
</A><br>
<A HREF="#subj1.2">
 Rob Austein
</A><br>
<A HREF="#subj1.3">
 Andy Freeman
</A><br>
<A HREF="#subj1.4">
 Scott Preece
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  F-16 Software 
</A>
<DD>
<A HREF="#subj2.1">
Henry Spencer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  1,000,000 lines of correct code? 
</A>
<DD>
<A HREF="#subj3.1">
Stephen Schaefer
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Massive UNIX breakins at Stanford
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
Mon, 22 Sep 86 11:04:16 EDT
</i><PRE>
From: Jerome H. Saltzer &lt;Saltzer@ATHENA.MIT.EDU&gt;

In <A HREF="/Risks/3.58.html">RISKS-3.58</A>, Dave Curry gently chastises Brian Reid:

&gt; . . . you asked for it. . . Berkeley networking had nothing to
&gt; do with your intruder getting root on your system, that was due purely
&gt; to neglect.  Granted, once you're a super-user, the Berkeley networking
&gt; scheme enables you to invade many, many accounts on many, many machines.

And in <A HREF="/Risks/3.59.html">RISKS-3.59</A>, Scott Preece picks up the same theme, suggesting that
Stanford failed by not looking at the problem as one of network security,
and, in the light of use of Berkeley software, not enforcing a no-attachment
rule for machines that don't batten down the hatches.

These two technically- and policy-based responses might be more tenable if
the problem had occurred at a military base.  But a university is a
different environment, and those differences shed some light on environments
that will soon begin to emerge in typical commercial and networked home
computing settings.  And even on military bases.

There are two characteristics of the Stanford situation that
RISK-observers should keep in mind:

     1.  Choice of operating system software is made on many factors,
not just the quality of the network security features.  A university
has a lot of reasons for choosing BSD 4.2.  Having made that choice,
the Berkeley network code, complete with its casual approach to
network security, usually follows because the cost of changing it is
high and, as Brian noted, its convenience is also high.

     2.  It is the nature of a university to allow individuals to do
their own thing.  So insisting that every machine attached to a
network must run a certifably secure-from-penetration configuration
is counter-strategic.  And on a campus where there may be 2000
privately administered Sun III's, MicroVAX-II's, and PC RT's all
running BSD 4.2, it is so impractical as to be amusing to hear it
proposed.  Even the military sites are going to discover soon that
configuration control achieved by physical control of every network
host is harder than it looks in a world of engineering workstations.

Brian's comments are very thoughtful and thought-provoking.  He describes
expected responses of human beings to typical current-day operating system
designs.  The observations he makes can't be dismissed so easily.

					Jerry Saltzer

</PRE>
<HR><H3><A NAME="subj1.2">
Massive UNIX breakins at Stanford
</A>
</H3>
<address>
Rob Austein 
&lt;<A HREF="mailto:SRA@XX.LCS.MIT.EDU">
SRA@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Mon, 22 Sep 1986  23:03 EDT
</i><PRE>

I have to take issue with Scott Preece's statement that "the fault
lies in allowing an uncontrolled machine to have full access to the
network".  This may be a valid approach on a small isolated network or
in the military, but it fails horribly in the world that the rest of
us have to live in.  For example, take a person (me) who is
(theoreticly) responsible for what passes for security on up to half a
dozen mainframes at MIT (exact number varies).  Does he have any
control over what machines are put onto the network even across the
street on the MIT main campus?  Hollow laugh.  Let alone machines at
Berkeley or (to use our favorite local example) the Banana Junior
6000s belonging to high school students in Sunnyvale, California.

As computer networks come into wider use in the private sector, this
problem will get worse, not better.  I'm waiting to see when AT&amp;T
starts offering a long haul packet switched network as common carrier.

Rule of thumb: The net is intrinsicly insecure.  There's just too much
cable out there to police it all.  How much knowledge does it take to
tap into an ethernet?  How much money?  I'd imagine that anybody with
a BS from a good technical school could do it in a week or so for
under $5000 if she set her mind to it.

As for NFS... you are arguing my case for me.  The NFS approach to
security seems bankrupt for just this reason.  Same conceptual bug,
NFS simply agravates it by making heavier use of the trusted net
assumption.

Elsewhere in this same issue of RISKS there was some discussion about
the dangers of transporting passwords over the net (by somebody other
than Scott, I forget who).  Right.  It's a problem, but it needn't be.
Passwords can be tranmitted via public key encryption or some other
means.  The fact that most passwords are currently transmitted in
plaintext is an implementation problem, not a fundamental design
issue.

A final comment and I'll shut up.  With all this talk about security
it is important to keep in mind the adage "if it ain't broken, don't
fix it".  Case in point.  We've been running ITS (which has to be one
of the -least- secure operating systems ever written) for something
like two decades now.  We have surprisingly few problems with breakins
on ITS.  Seems that leaving out all the security code made it a very
boring proposition to break in, so almost nobody bothers (either that
or they are all scared off when they realize that the "command
processor" is an assembly language debugger ... can't imagine why).
Worth thinking about.  The price paid for security may not be obvious.

--Rob Austein &lt;SRA@XX.LCS.MIT.EDU&gt;

</PRE>
<HR><H3><A NAME="subj1.3">
Massive UNIX breakins at Stanford 
</A>
</H3>
<address>
Andy Freeman 
&lt;<A HREF="mailto:ANDY@Sushi.Stanford.EDU">
ANDY@Sushi.Stanford.EDU
</A>&gt;
</address>
<i>
Mon 22 Sep 86 11:07:04-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM, preece%ccvaxa@GSWD-VMS.ARPA

Scott E. Preece &lt;preece%ccvaxa@GSWD-VMS.ARPA&gt; writes in <A HREF="/Risks/3.60.html">RISKS-3.60</A>:

    reid@decwrl.DEC.COM (Brian Reid) writes:
	The issue here is that a small leak on some [unknown]
	inconsequential machine in the dark corners of campus was
	allowed to spread to other machines because of the networking code.

    No, you're still blaming the networking code for something it's not
    supposed to do.  The fault lies in allowing an uncontrolled machine to
    have full access to the network.  The NCSC approach to networking has
    been just that: you can't certify networking code as secure, you can
    only certify a network of machines AS A SINGLE SYSTEM.  That's pretty
    much the approach of the Berkeley code, with some grafted on
    protections because there are real-world situations where you have to
    have some less-controlled machines with restricted access.  The
    addition of NFS makes the single-system model even more necessary.

Then NCSC certification means nothing in many (most?) situations.  A
lot of networks cross adminstrative boundaries.  (The exceptions are
small companies and military installations.)  Even in those that
seemingly don't, phone access is often necessary.

Network access should be as secure as phone access.  Exceptions may
choose to disable this protection but many of us won't.  (If Brian
didn't know about the insecure machine, it wouldn't have had a valid
password to access his machine.  He'd also have been able to choose
what kind of access it had.)  The only additional problem that
networks pose is the ability to physically disrupt other's
communication.

-andy             [There is some redundancy in these contributions, 
                   but each makes some novel points.  It is better
                   for you to read selectively than for me to edit. PGN]

</PRE>
<HR><H3><A NAME="subj1.4">
Massive UNIX breakins at Stanford (<A HREF="/Risks/3.60.html">RISKS-3.60</A>)
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%mycroft@GSWD-VMS.ARPA">
preece%mycroft@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
22 Sep 1986 16:24-CST
</i><PRE>
To: ANDY@SUSHI.STANFORD.EDU, RISKS%CSL.SRI.COM@CSNET-RELAY.ARPA

	Andy Freeman writes [in response to my promoting the view
	of a network as a single system]:
	
&gt;       Then NCSC certification means nothing in many (most?) situations.
--------

Well, most sites are NOT required to have certified systems (yet?). If they
were, they wouldn't be allowed to have non-complying systems.  The view as a
single system makes the requirements of the security model feasible.  You
can't have anything in the network that isn't part of your trusted computing
base.  This seems to be an essential assumption.  If you can't trust the
code running on another machine on your ethernet, then you can't believe
that it is the machine it says it is, which violates the most basic
principles of the NCSC model. (IMMEDIATE DISCLAIMER: I am not part of the
group working on secure operating systems at Gould; my knowledge of the area
is superficial, but I think it's also correct.)  
                   [NOTE: The word "NOT" in the first line of this paragraph
                    was interpolated by PGN as the presumed intended meaning.]

--------
        Network access should be as secure as phone access.  Exceptions may
        choose to disable this protection but many of us won't.  (If Brian
        didn't know about the insecure machine, it wouldn't have had a valid
        password to access his machine.  He'd also have been able to choose
        what kind of access it had.)  The only additional problem that
        networks pose is the ability to physically disrupt other's
        communication.
--------

Absolutely, network access should be as secure as phone access,
IF YOU CHOOSE TO WORK IN THAT MODE.  Our links to the outside
world are as tightly restricted as our dialins.  The Berkeley
networking software is set up to support a much more integrated
kind of network, where the network is treated as a single system.
For our development environment that is much more effective.
You should never allow that kind of access to a machine you don't
control.  Never.  My interpretation of the original note was that
the author's net contained machines with trusted-host access
which should not have had such access; I contend that that
represents NOT a failing of the software, but a failing of the
administration of the network.

scott preece
gould/csd - urbana, uucp:	ihnp4!uiucdcs!ccvaxa!preece

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
 F-16 Software
</A>
</H3>
<address>
&lt;<A HREF="mailto:ihnp4!utzoo!henry@ucbvax.Berkeley.EDU">
ihnp4!utzoo!henry@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Mon, 22 Sep 86 18:07:11 PDT
</i><PRE>

Doug Wade notes:

&gt;   My comment to this, is what if a 8G limit had been programmed into
&gt; the plane (if it had been fly-by-wire)...

My first reaction on this was that military aircraft, at least front-line
combat types, obviously need a way to override such restrictions in crises,
but civilian aircraft shouldn't.  Then I remembered the case of the 727 that
rolled out of control into a dive a few years ago.  The crew finally managed
to reduce speed enough to regain control by dropping the landing gear.  The
plane was at transonic speed at the time -- there was some speculation, later
disproven, that it might actually have gone slightly supersonic -- and was
undoubtedly far above the official red-line maximum airspeed for the
landing gear.  It would seem that even airliners might need overrides.

				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
1,000,000 lines of correct code?
</A>
</H3>
<address>
Stephen Schaefer 
&lt;<A HREF="mailto:schaefer%research.bgsu.edu@CSNET-RELAY.ARPA">
schaefer%research.bgsu.edu@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Mon, 22 Sep 86 19:15:31 edt
</i><PRE>

  The Plain Dealer (Cleveland), Tuesday, September 16, 1986
  Excerpted without permission.

  "Protecting the secrets of success"

  Dayton(AP) - [Most of article dealing with foreign contractors
  omitted] [Col. Thomas D.] Fiorino also said a Sept. 5 experiment using
  two satellites that measured the plume of a rocket exhaust in space
  and then collided was a success.  Some critics, noting the experiment
  took 1 million lines of computer code, said a full SDI system would
  take tens or hundreds of millions.
	Fiorino said there was a computer on board that processed 2
  billion operations a second, about four times faster than current
  "supercomputers."
	"It did not represent our full technological potential," he
  said, pointing out that it did not use very high speed integrated
  circuits still under development.

On the one hand, I am incredulous, but on the other, I'd be utterly
horrified to find them directing misinformation to the small number of
people knowledgeable enough to understand.  I hope this ruggedized,
portable, Cray class machine is commercially available in a couple
years.  Failing that, I hope the reporter was simply "innumerate"
and heard "billion" for "million" somewhere.

I must repeat the quote of Mark Twain by the original poster:
"Interesting if true - and interesting anyway."

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.61.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.63.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-49</DOCNO>
<DOCOLDNO>IA012-000123-B023-128</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.63.html 128.240.150.127 19970217004845 text/html 14425
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:47:14 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 63</TITLE>
<LINK REL="Prev" HREF="/Risks/3.62.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.64.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.62.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.64.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 63</H1>
<H2>Wednesday, 24 September 1986</H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
NOTROJ (a Trojan Horse) 
</A>
<DD>
<A HREF="#subj1.1">
James H. Coombs via Martin Minow
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Massive UNIX breakins at Stanford 
</A>
<DD>
<A HREF="#subj2.1">
Scott Preece [two more messages!]
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
NOTROJ (a Trojan Horse)
</A>
</H3>
<address>
&lt;<A HREF="mailto:minow%regent.DEC@decwrl.DEC.COM  ">
minow%regent.DEC@decwrl.DEC.COM  
</A>&gt;
</address>
<i>
23-Sep-1986 1644
</i><PRE>

Found on a local bboard:

Date:	    Sat, 20 Sep 86 04:17:50 EDT
From:	    "James H. Coombs"  &lt;JAZBO%BROWNVM.BITNET@WISCVM.WISC.EDU&gt;
Subject:    NOTROJ--it IS a trojan
 
Distribute far and wide!
(C)Nobodaddy, 1986
 
                       A Story of a Trojan Horse
            With Some Suggestions for Dismounting Gracefully
 
                                   by
                            James H. Coombs
 
 
NOTROJ.COM is a TROJAN HORSE (comes in NOTROJ.ARC--for now).
 
I first became aware of NOTROJ when a member of The BOSS BBS community
reported his belief that the program destroyed the directory of his hard
disk.  After two days of restoring his files, he concluded:
 
         This Trojan was written by a real Pro---he knows his ASM and
         uses it as a weapon---not a tool.  From lokkin' at the job he
         did on me, I tendto doubt that I would have found the bomb has I
         been smart enough to look. ---PLEASE!!!!!  Spread the word 'bout
         this one.  It's a Killer!
 
In the next couple of days, I saw a similar note on the Boston Computer
Society bulletin board.  This victim rather pathetically credits NOTROJ
with a "valiant" attempt at saving his data.
 
         The program in question is a time-bomb (about 10 minutes) and
         works by the "SOFTGUARD UNFORMAT" method of attack.  I'm not
         sure what it did, or how it did it, or even how I could have
         recovered the disk but the NOTROJ program I had in the
         background alerted me to the fact, and tried a valiant attempt
         to shut down the hard disk.  To no avail, though.
 
Since my hard disk was becoming fragmented anyway, I decided to test
NOTROJ.  Everything looked pretty reasonable from the start; in fact, the
program looks like a very useful tool (although I'm not in love with the
interface).  One loads NOTROJ resident and then accesses the options menu
through Alt-N.  The menu contains about fifteen items, some of them
annotated "DANGER", e.g., "Format track (DANGER!)".  For each parameter,
the user can select one of four responses: Proceed, Timeout, Reboot, or
Bad Command.  The menu also provides a fifth option--"Pause&amp;Display"--
which provides the user with full information on the activity that the
currently active program is trying to perform and prompts for one of the
four primary actions, e.g, Proceed.
 
I selected "Pause&amp;Display" for all of the DANGERous parameters.
Everything worked fine, although I found that iteratively selecting
"Timeout" in response to the "Write sectors" interrupt hung up the
machine.  I fooled around with a number of commands and finally
reproduced the disk crash.  At the time, I was running the DOS ERASE
command (I had been suspicious of that one for quite some time anyway).
I don't have the full message that the program displayed, but I did write
down this much "Softguard-style low-level disk format."  (Keep those
words in mind.)
 
In spite of the fact that I had prepared for a disk crash, it took me at
least an hour to get running again.  When I booted the machine, I was
thrown into BASIC and could not get back to the system.  I put a DOS
diskette in, and got an invalid drive error message when I tried to
access the hard disk.  Here is the recovery procedure for this and most
disk crashes:
 
1) Insert DOS system disk in drive A.
2) Reboot the machine.
3) Run FDISK and install a DOS partition on the hard disk.
4) Format the hard disk with the '/S' option.
5) Restore files from the most recent full-disk Bernoulli or tape
   backup.
6) Restore files modified since the most recent full-disk Bernoulli
   or tape backup.
 
Once I got a minimal system running, I decided to reproduce the crash to
ensure that this was not some quirk of bad programming.  What, ho!  I got
bored playing around with COPY and ERASE and a few other programs.  I
waited for a while, read a magazine--no signs of a simple timing
technique.  I began to think that NOTROJ might be more incompetent than
vicious.  Something about the documentation made it seem unlikely that
the author was a criminal.  It occurred to me, however, that the author
might have had some time to waste on this program.  Does he, perhaps,
check to see how full the hard disk is?  It would be reasonable to evade
detection immediately after a bomb by making it impossible to reproduce
the crash.  In addition, it would be much more painful for people if they
have restored all of their files or gradually rebuilt their hard disks
before they discover that this is a trojan horse.  So, I restored all of
my files.
 
This time, Norton's NU command turned out to be the great blackguard that
was trying to format my disk (according to NOTROJ--although it was only
reading the FAT).  So, I restored my hard disk.  All of the while,
however, I had the nagging feeling that the documentation did not reflect
the personality of someone vicious.  When I got running again, I took a
look into NOTROJ.COM.  Nowhere could I find the words from the message
"Softguard-style low-level disk format."  That convinced me.  I have
concealed passwords on mainframes by assembling strings dynamically
instead of storing them statically.  Our trojanette must have used the
same technique so that no one would spot the suspicious messages.  I had
counted on being able to get them directly from the program so that I
would not have to take the time to write the whole message down while my
system was being operated on.  I do recall NOTROJ patting itself on the
back, however, for preventing "further damage."
 
As I think back on it, the documentation contains something of a rant
against copy-protection schemes, including Softguard.  In addition, I had
always been troubled by the fact that the name NOTROJ is an acrostic for
TROJAN and also an assertion that the program is not itself a trojan.
The documentation is also very badly written.  One has to experiment to
make sense of it, although that is nothing new in software documentation.
Also, the style is something of a pidgin English, which seems consistent
with the fact that the author has an Oriental name (Ng, or is that for
"no good"?).  Well, since the author's name and address are listed in the
documentation, I decided to give him a call.  Mirabile dictu!  It's a
real name, and I got a real number--I just didn't get an answer, even at
2 a.m.  It doesn't make much difference anyway, there's nothing that he
can say to convince me that he had legitimate reasons for concealing
error messages and that his program is not a trojan horse.  There is also
the possibility that the person listed as author has nothing to do with
the program.  Could the pidgin style of the documentation be the work of
a clever linguist--an acrostic fan--a sick person who considers himself
to be the bozo that Sherlock Holmes was always after?  Who knows?  I have
to write a book.  No time to play with these fools.
 
So, be careful.  Note that sysops don't have the time to test every
program extensively.  If a program like NOTROJ requires that a disk be
more than 70% full, for example, a lot of people may never have any
problems with it.  What else can we do?  Does someone want to try to
prosecute the author of NOTROJ?  And how do we keep ourselves from
becoming paranoid about new noncommerical software?
 
Eventually, I think it will all shake out just fine.  Those of us who are
prepared for problems provide others with the testing and filtering.
Junk like NOTROJ just does not make it into my community.  Actually, I
find mediocre software much more of a problem.  I have spent a lot of
time and money sorting through megabytes of chaff to find but a few
grains of wheat.  I would like to see us find some way to constrict the
growth of chaff and worms both.  If we can't do this, many of us may
have to switch to commercial software.
                                                   --Jim
Replies may be made to:
BITNET:  JAZBO@BROWNVM
BBS:     The BOSS, BCS, Hal's, et passim
BIX:     jcoombs

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Massive UNIX breakins at Stanford
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%ccvaxa@GSWD-VMS.ARPA">
preece%ccvaxa@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Tue, 23 Sep 86 09:16:21 cdt
</i><PRE>

   [This was an addendum to Scott's contribution to <A HREF="/Risks/3.61.html">RISKS-3.61</A>.  PGN]

I went back and reviewed Brian Reid's initial posting and found myself more
in agreement than disagreement.  I agree that the Berkeley approach offers
the unwary added opportunities to shoot themselves in the foot and that
local administrators should be as careful of .rhosts files as they are of
files that are setuid root; they should be purged or justified regularly.

I also agree that it should be possible for the system administrator to turn
off the .rhosts capability entirely, which currently can only be done in the
source code and that it would be a good idea to support password checks (as
a configuration option) on rcp and all the other remote services.

scott preece, gould/csd - urbana, uucp:	ihnp4!uiucdcs!ccvaxa!preece

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: Massive UNIX breakins at Stanford
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%ccvaxa@GSWD-VMS.ARPA">
preece%ccvaxa@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Tue, 23 Sep 86 08:41:29 cdt
</i><PRE>

  &gt; From: Rob Austein &lt;SRA@XX.LCS.MIT.EDU&gt;

  &gt; I have to take issue with Scott Preece's statement that "the fault lies
  &gt; in allowing an uncontrolled machine to have full access to the network"...

I stand by what I said, with the important proviso that you notice the word
"full" in the quote.  I took the description in the initial note to mean
that the network granted trusted access to all machines on the net.  The
Berkeley networking code allows the system administrator for each machine to
specify what other hosts on the network are to be treated as trusted and
which are not.  The original posting spoke of people on another machine
masquerading as different users on other machines; that is only possible if
the (untrustworthy) machine is in your hosts.equiv file, so that UIDs are
equivalenced for connections from that machine.  If you allow trusted access
to a machine you don't control, you get what you deserve.

Also note that by "the network" I was speaking only of machines intimately
connected by ethernet or other networking using the Berkeley networking
code, not UUCP or telephone connections to which normal login and password
checks apply.

The description in the original note STILL sounds to me like failure of
administration rather than failure of the networking code.

scott preece

    [OK.  Enough on that.  The deeper issue is that most operating
     systems are so deeply flawed that you are ALWAYS at risk.  Some
     tentative reports of Trojan horses discovered in RACF/ACF2 systems
     in Europe are awaiting details and submission to RISKS.  But their
     existence should come as no surprise.  Any use of such a system in
     a hostile environment could be considered a failure of administration.
     But it is also a shortcoming of the system itself...  PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.62.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.64.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-50</DOCNO>
<DOCOLDNO>IA012-000123-B023-156</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.64.html 128.240.150.127 19970217004859 text/html 19652
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:47:27 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 64</TITLE>
<LINK REL="Prev" HREF="/Risks/3.63.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.65.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.63.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.65.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 64</H1>
<H2>Wednesday, 24 September 1986</H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Sane sanity checks /  risking public discussion 
</A>
<DD>
<A HREF="#subj1.1">
Jim Purtilo
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  More (Maybe Too Much) On More Faults 
</A>
<DD>
<A HREF="#subj2.1">
Ken Dymond
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: Protection of personal information 
</A>
<DD>
<A HREF="#subj3.1">
Correction from David Chase
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Towards an effective definition of "autonomous" weapons  
</A>
<DD>
<A HREF="#subj4.1">
Herb Lin
</A><br>
<A HREF="#subj4.2">
 Herb Lib
</A><br>
<A HREF="#subj4.3">
 Clifford Johnson
</A><br>
<A HREF="#subj4.4">
 Clifford Johnson
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Sane sanity checks /  risking public discussion
</A>
</H3>
<address>
Jim Purtilo 
&lt;<A HREF="mailto:purtilo@brillig.umd.edu">
purtilo@brillig.umd.edu
</A>&gt;
</address>
<i>
Tue, 23 Sep 86 12:54:10 EDT
</i><PRE>

  [Regarding ``sanity checks'']

Let us remember that there are sane ``sanity checks'' as well as the other 
kind. About 8 years ago while a grad student at an Ohio university that 
probably ought to remain unnamed, I learned of the following follies:

The campus had long been doing class registration and scheduling via
computer, but the registrar insisted on a ``sanity check'' in the form of
hard copy.  Once each term, a dozen guys in overalls would spend the day
hauling a room full of paper boxes over from the CS center, representing a
paper copy of each document that had anything to do with the registration
process.  [I first took exception to this because their whole argument in
favor of "computerizing" was based on reduced costs, but I guess that should
be hashed out in NET.TREE-EATERS.]

No one in that registrar's office was at all interested in wading through
all that paper. Not even a little bit.

One fine day, the Burroughs people came through with a little upgrade to the
processor used by campus administration.  And some "unused status bits"
happened to float the other way.

This was right before the preregistration documents were run, and dutifully
about 12,000 students preregistration requests were scheduled and mailed
back to them.  All of them were signed up "PASS/FAIL".  This was
meticulously recorded on all those trees stored in the back room, but no one
wanted to look.

I suppose a moral would be ``if you include sanity checks, make sure a sane
person would be interested in looking at them.''


  [Regarding break-ins at Stanford]

A lot of the discussion seems to revolve about ``hey, Brian, you got what
you asked for'' (no matter how kindly it is phrased).  Without making
further editorial either way, I'd like to make sure that Brian is commended
for sharing the experience.  Sure would be a shame if ``coming clean'' about
a bad situation will be viewed as itself constituting a risk...

               [I am delighted to see this comment.  Thanks, Brian!  PGN]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
More (Maybe Too Much) On More Faults
</A>
</H3>
<address>
"DYMOND, KEN" 
&lt;<A HREF="mailto:dymond@nbs-vms.ARPA">
dymond@nbs-vms.ARPA
</A>&gt;
</address>
<i>
23 Sep 86 09:18:00 EDT
</i><PRE>
To: "risks" &lt;risks@csl.sri.com&gt;

The intuitive sense made by Dave Benson's argument in RISKS 3.50, that

  &gt;We need to understand that the more faults found at any stage to
  &gt;engineering software the less confidence one has in the final product.  
  &gt;The more faults found, the higher the likelihood that faults remain.

seems to invite a search for confirming data because there are also counter-
intuitive possibilities.  For example there is the notion that the earlier
in the life cycle errors are detected, the cheaper to remedy them.  There is
a premium on finding faults early.  And the further notion that with tools
for writing requirements in some kind of formal language that can be checked
for syntactic and semantic completeness and consistency, it's possible to
detect at least some errors at requirements stage that may not have been
caught till later.  So SE projects using these and similar methods for other
stages in the life cycle would tend to show more errors earlier.  Would the
products from these projects be therefore less reliable than others made
with, say, more traditional, less careful, design and programming practice ?

Dave makes the further argument in RISKS 3.57:

  &gt;Certain models of software failure place increased "reliability" on
  &gt;software which has been exercised for long periods without fault. [...]

The models of software reliability exist to order our thinking about
reliability and to help predict behavior of software systems based on
observation of failure up to the current time.  The models that show
failures clustered early in time and then tapering off later do indeed model
an intuition but maybe not the one that more faults mean yet more faults.
Hence the need for data.  I suspect that the reality as shown by data, if it
exists, would be more complex than intuition allows.  More errors discovered
so far may just mean better software engineering methods.  As far as other
engineering fields, the failure vs time curve in manufactured products is
often taken to be tub-shaped, not exponentially decaying.  So more failures
are expected at the beginning and near the end of the useful life of a
"hard" engineered product.  Of course, "an unending sequence of irremediable
faults" should be the kiss of death for any product, whether from hard
engineering or soft.  But the trick is in knowing that the sequence is
unending.  The B-17, I seem to remember reading, had a rather rocky
development road in the 1930s, yet was not abandoned.  Was it just that the
aeronautical engineers at Boeing then had in mind some limit on the number
of faults and that this limit was not exceeded?  It might be easy to say in
hindsight.  On the other hand, sometimes foresight, in terms of spotting a
poor design at the outset, makes a difference, as in the only Chernobyl-type
power reactor outside the Soviet block.  It was bought by Finland (perhaps
this is what "Finlandization" means ?).  However the Finns also bought a
containment building from Westinghouse.

Ken Dymond

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: Protection of personal information
</A>
</H3>
<address>
David Chase 
&lt;<A HREF="mailto:rbbb@rice.edu">
rbbb@rice.edu
</A>&gt;
</address>
<i>
Tue, 23 Sep 86 08:56:18 EDT
</i><PRE>
                       [The two participants requested this clarification 
                        be included for the record...  PGN]

You misinterpreted my message in a small way; I was writing about a
university attended by a friend, NOT Rice university.  To my knowledge, Rice
has been very good about protecting its students' privacy.  My student
number is NOT my social security number, though the university has that
number for good reasons.  I do not want anyone to think that I was talking
about Rice.       David

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Towards an effective definition of "autonomous" weapons
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Tue, 23 Sep 1986  18:00 EDT
</i><PRE>

         [THE FOLLOWING DISCOURSE INVOLVING CLIFF AND HERB IS LIKELY
          TO CONTINUE FOR A WHILE ON ARMS-D.  PLEASE RESPOND TO HERB LIN, 
          NOT TO RISKS ON THIS ONE.  HERB HAS VOLUNTEERED TO SUBMODERATE,
          AND THEN SUBMIT THE RESULTS TO RISKS.  PGN]

    From: Clifford Johnson &lt;GA.CJJ at Forsythe.Stanford.Edu&gt;

    An "autonomous weapon" [should be] defined to be any weapons system
    which is de facto preprogrammed to take decisions which, under the law 
    of nations, require the exercise of political or military discretion.

It's not a bad first attempt, and I think it is necessary to get a
handle on this.  With the realization that you have done us a service
in proposing your definition, let me comment on it.

I don't understand what it means for a weapon to "take a decision".  Clearly
you don't intend to include a depth charge set to explode at a certain
depth, and yet a depth charge could "decide" to explode at 100 feet given
certain input.

What I think you object to is the "preprogrammed" nature of a weapon,
in which a chip is giving arming, targeting and firing orders rather
than a human being.  What should be the role of the human being in
war?  I would think the most basic function is to decide what targets
should be attacked.  Thus, one modification to your definition is

    An "autonomous weapon" [should be] defined to be any weapons 
    system which is preprogrammed to SELECT targets.

This would include things like roving robot anti-tank jeeps, and
exclude the operation of LOW for the strategic forces.

But this definition would also exclude "fire-and-forget" weapons, and
I'm not sure I want to do that.  I want human DESIGNATION of a target
but I don't want the human being to remain exposed to enemy fire after
he has done so.  Thus, a second modification is 

    An "autonomous weapon" [should be] defined to be any weapons 
    system which is preprogrammed to SELECT targets in the absence of
    direct and immediate human intervention.

But then I note what a recent contributor said -- MINES are autonomous
weapons, and I don't want to get rid of mines either, since I regard
mines as a defensive weapon par excellence.  Do I add mobility to the
definition?  I don't know.

</PRE>
<HR><H3><A NAME="subj4.2">
Towards an effective defintion of "autonomous" weapons
</A>
</H3>
<address>
Clifford Johnson 
&lt;<A HREF="mailto:GA.CJJ at Forsythe.Stanford.Edu">
GA.CJJ at Forsythe.Stanford.Edu
</A>&gt;
</address>
<i>
Monday, 22 September 1986  21:43-EDT
</i><PRE>

There's great difficulty in defining "autonomous weapons" so as to separate
some element that seems intuitively "horrible" about robot-decided death.
But a workable definition is necessary if, as CPSR tentatively proposes,
such weapons are to be declared illegal under international law, as have
chemical and nuclear weapons.  (Yes, the U.N. has declared even the
possession of nukes illegal, but it's not a binding provision.)

The problem is, of course, that many presently "acceptable" weapons already
indiscrminately-discriminate targets, e.g.  target-seeking munitions and
even passive mines.  Weapons kill, and civilians get killed too, that's war.
Is there an element exclusive to computerized weapons that is meaningful?

I don't have an answer, but feel the answer must be yes.  I proffer two
difficult lines of reasoning, derived from the philosophy of automatic
decisionmaking rather than extant weapon systems.  First, weapon control
systems that may automatically target-select among options based upon a
utility function (point score) that weighs killing people against destroying
hardware would seem especially unconscionable.  Second, but this presumes a
meaningful definition of "escalation," any weapons system that has the
capability to automatically escalate a conflict - and is conditionally
programmed to do so - would also seem unconscionable.

Into the first bracket would conceivably fall battle management software and
war games, into the second would fall war tools that in operation (de facto)
would take decisions which according to military regulations would otherwise
have required the exercise of discretion by a military commander or
politician.  The latter category would embrace booby-trap devices activated
in peacetime, such as mines and LOWCs; and here there is the precedent of
law which prohibits booby traps which threaten innocents in peacetime.
Perhaps the following "definition" could stand alone as *the* definition of
autonomous weapons to be banned:

An "autonomous weapon" is defined to be any weapons system which is
de facto preprogrammed to take decisions which, under the law of
nations, require the exercise of political or military discretion.

This might seem to beg the question, but it could be effective - military
manuals and international custom is often explicit on each commanders'
degree of authority/responsibility, and resolving whether a particular
weapon was autonomous would then be a CASE-BY-CASE DETERMINATION.  Note that
this could, and would, vary with the sphere of application of the weapons
system.  This is reasonable, just as there are circumstances in which
blockades or mining is "legal" and "illegal."

Of course, a case-in-point would be needed to launch the definition.
Obviously, I would propose that LOWCs were illegal.  How about battle
management software which decides to engage seemingly threatening entities
regardless of flag, in air or by sea?  Any other suggestions?  Does anyone
have any better ideas for a definition?

</PRE>
<HR><H3><A NAME="subj4.3">
Towards an effective definition of "autonomous" weapons
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Tue, 23 Sep 1986  18:09 EDT
</i><PRE>

In thinking about this question, I believe that ARMS-D and RISKS could
perform a real service to the defense community.  There is obviously a
concern among some ARMS-D and RISKS readers that autonomous weapons
are dangerous generically, and maybe they should be subject to some
legal restrictions.  Others are perhaps less opposed to the idea.

It is my own feeling that autonomous weapons could pose the same danger to
humanity that chemical or biological warfare pose, though they may be
militarily effective under certain circumstances.

I propose that the readership take up the questions posed by Cliff's recent
contribution:

    What is a good definition of an autonomous weapon?  

    What restrictions should be placed on autonomous weapons, and why?

    How might such limits be verified?

    Under what circumstances would autonomous weapons be militarily
    useful?

    Should we be pursuing such weapons at all?

    How close to production and deployment of such weapons are we?

Maybe a paper could be generated for publication?

</PRE>
<HR><H3><A NAME="subj4.4">
 Towards a definition of "autonomous" weapons
</A>
</H3>
<address>
Clifford Johnson 
&lt;<A HREF="mailto:GA.CJJ@Forsythe.Stanford.Edu">
GA.CJJ@Forsythe.Stanford.Edu
</A>&gt;
</address>
<i>
Tue, 23 Sep 86 18:16:46 PDT
</i><PRE>

&gt; I don't understand what it means for a weapon to "take a decision".
&gt; Clearly you don't intend to include a depth charge set to explode at a
&gt; certain depth, and yet a depth charge could "decide" to explode at 100
&gt; feet given certain input.

My concept of "decision" embraces *all* manner of conditional
executions, delimited expressly by the customary law that recognizes
certain "special" decisions require human participation.  I know of
no precedent suggesting that "aiming" at a properly comprehended
target requires such discretion, which is the logical mechanism that
eliminates depth charges from my definition.

&gt; What should be the role of the human being in
&gt; war?  I would think the most basic function is to decide what targets
&gt; should be attacked.

The question I'm concerned with is "What should/could be the role of
law in precluding the unconscionable automation of war?"  With the
basic thrust on target selection I agree, but to include it I would
amend my definition by referencing "fatal consequences"
(generalization of "targets"):

AN "AUTONOMOUS WEAPON" IS DEFINED TO BE ANY WEAPONS SYSTEM WHICH
IS DE FACTO PREPROGRAMMED TO TAKE DECISIONS WHICH, DUE TO THEIR
POTENTIALLY FATAL CONSEQUENCES, UNDER THE LAW OF NATIONS REQUIRE
THE EXERCISE OF HUMAN DISCRETION AT THE TIME THEY ARE TAKEN.

N.B. The "de facto" is important to exclude the excuse that a
human might "override" the weapons' decision, when, in practice,
he/she wouldn't be competent to do so.

&gt; But then I note what a recent contributor said -- MINES are autonomous
&gt; weapons, and I don't want to get rid of mines either, since I regard
&gt; mines as a defensive weapon par excellence.  Do I add mobility to the
&gt; definition?  I don't know.

It seems such confusions are inevitable when seeking a
primarily technological definition of "autonomous" weapons.  My
"trick" is to reverse the analysis, placing the burden upon the
human judgment that some kinds of decisions are unconsionably
automated.  The nature of and context of the particular decision
becomes the focus. This isn't a cop-out, but would build upon
established military/international custom.  As I have intimated, it
would seek precedents where particular decisions would "initiate" or
"escalate" a conflict, or be beyond the ordinary authority of the
officer operating the system.

                             [RESPONSES TO LIN@XX.LCS.MIT.EDU please.]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.63.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.65.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-51</DOCNO>
<DOCOLDNO>IA012-000123-B023-176</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.65.html 128.240.150.127 19970217004913 text/html 13975
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:47:42 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 65</TITLE>
<LINK REL="Prev" HREF="/Risks/3.64.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.66.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.64.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.66.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 65</H1>
<H2>Wednesday, 25 September 1986</H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
UNIX and network security again 
</A>
<DD>
<A HREF="#subj1.1">
Andy Freeman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  F-16 software 
</A>
<DD>
<A HREF="#subj2.1">
Wayne Throop
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  NYT feature article on SDI software 
</A>
<DD>
<A HREF="#subj3.1">
Hal Perkins
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Autonomous widgets 
</A>
<DD>
<A HREF="#subj4.1">
Mike McLaughlin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Robottle Management Software? 
</A>
<DD>
<A HREF="#subj5.1">
PGN
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
UNIX and network security again
</A>
</H3>
<address>
Andy Freeman 
&lt;<A HREF="mailto:ANDY@SUSHI.STANFORD.EDU">
ANDY@SUSHI.STANFORD.EDU
</A>&gt;
</address>
<i>
Mon 22 Sep 86 17:09:27-PDT
</i><PRE>
To: preece%mycroft@GSWD-VMS.ARPA
cc: RISKS%CSL.SRI.COM@CSNET-RELAY.ARPA

preece%mycroft@gswd-vms.ARPA (Scott E. Preece) writes:

    If you can't trust the code running on another machine on your
    ethernet, then you can't believe that it is the machine it says it is,
    which violates the most basic principles of the NCSC model.

That's why electronic signatures are a good thing.

    I wrote (andy@sushi):
    &gt;   Then NCSC certification means nothing in many (most?) situations.

    Well, most sites are required to have certified systems (yet?). If
    they were, they wouldn't be allowed to have non-complying systems.

The designers of the Ford Pinto were told by the US DOT to use $x as a
cost-benefit tradeoff point for rear end collisions.  Ford was still
liable.  I'd be surprised if NCSC certification protected a company
from liability.  (In other words, being right can be more important
than complying.)

       [This case was cited again by Peter Browne (from old Ralph Nader
        materials?), at a Conference on Risk Analysis at NBS 15 September
        1986:  Ford estimated that the Pinto gas tank would take $11 each to
        fix in 400,000 cars, totalling $4.4M.  They estimated 6 people might
        be killed as a result, at $400,000 each (the going rate for lawsuits
        at the time?), totalling $2.4M.  PGN]

    Absolutely, network access should be as secure as phone access, IF YOU
    CHOOSE TO WORK IN THAT MODE.  Our links to the outside world are as
    tightly restricted as our dialins.  The Berkeley networking software
    is set up to support a much more integrated kind of network, where the
    network is treated as a single system.  For our development
    environment that is much more effective.  You should never allow that
    kind of access to a machine you don't control.  Never.  My
    interpretation of the original note was that the author's net
    contained machines with trusted-host access which should not have had
    such access; I contend that that represents NOT a failing of the
    software, but a failing of the administration of the network.

My interpretation of Brian's original message is that he didn't have a
choice; Berkeley network software trusts hosts on the local net.  If
that's true, then the administrators didn't have a chance to fail; the
software's designers had done it for them.  (I repeated all of Scott's
paragraph because I agree with most of what he had to say.)

-andy

    [I think the implications are clear.  The network software is weak.  
     Administrators are often unaware of the risks.  Not all hosts are
     trustworthy.  The world is full of exciting challenges for attackers.  
     All sorts of unrealistic simplifying assumptions are generally made.  
     Passwords are typically stored or transmitted in the clear and easily
     readable or obtained -- or else commonly known.  Encryption is still
     vulnerable if the keys can be compromised (flawed key distribution,
     unprotected or subject to bribable couriers) or if the algorithm is 
     weak.  There are lots of equally devastating additional vulnerabilities
     waiting to be exercised, particularly in vanilla UNIX systems and 
     networks thereof.  Remember all of our previous discussions about not
     trying to put the blame in ONE PLACE.  PGN]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
F-16 software
</A>
</H3>
<address>
&lt;<A HREF="mailto:rti-sel!dg_rtp!throopw%mcnc.csnet@CSNET-RELAY.ARPA">
rti-sel!dg_rtp!throopw%mcnc.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Tue, 23 Sep 86 19:12:33 edt
</i><PRE>
Apparently-To: mcnc!csl.sri.com!risks

  &gt; I spoke to an F-16 flight instructor about this business concerning bomb
  &gt; release when the plane is upside down.  He said the software OUGHT to
  &gt; prevent such an occurrence.  When the plane is not at the right angle of
  &gt; attack into the air stream, toss-bombing can result in the bomb being 
  &gt; thrown back into the airplane.  

Hmpf.  *I* spoke to an ex Air-Force pilot.  He said if *any* restriction on
bomb release is incorporated it should be to prevent it when the plane (or
more specificially, the bomb itself... there *is* a difference, and you had
better realize it!) is pulling negative G's.  This was my original point...
"upside down" or "inverted" isn't the correct thing to worry about, it is
the wrong mindset entirely, too simple a notion.

He went on to back up this assertion by pointing out that there is a
common (well... well-known anyhow) bombing technique, called "over the
shoulder" bombing, that requires release while inverted.  Consider the
following diagram.  (Note that the trajectory shapes are unrealistic and
the scales are exagerated.  Limitations of the terminal, don't y'know.)
                                 _
                               /   \
                              /      \
             ________________________ |
            &lt;               /        \r
                           /          \
                          |            |
                          v            /
B &gt;___________________________________/
                          T

Now, we have bomber B, release of bomb r, and target T.  The bomber makes a
fast, low-level run over the target (to avoid radar, and to let the
bombsight get a good look).  Then, soon after the overfly, pulls sharply up
and over, and *while* *inverted* releases the bomb.  The bomb lofts high
into the air over the target whilst the plane scoots for home (rolling out
of the inversion, presumably but not necessarily), and the bomb eventually
lands splat on the target.

Basically, if you want the flight computer to wet-nurse the pilot at all in
this regard, it ought to have a sensor to detect strain on the bomb
restraints, and refuse to release them if the bomb isn't currently "trying"
to "fall" away from the aircraft.  (Even this isn't foolproof, of course,
but it comes close.)  Tying this into the *attitude* of the *aircraft*
*itself* is *WRONG* *WRONG* *WRONG*, and is, as I said before, an
architypical computer risk, in that it is an overly simple and misleading
model of the situation.

The conversation I had with my friend makes a lot of sense to me, and the
above somewhat vague stuff about the angle of attack does not.  It could be
I'm just missing something obvious, but I stand by my earlier position.

   The desire for safety stands against every great and noble enterprise.
                                --- Tacitus

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
NYT feature article on SDI software
</A>
</H3>
<address>
Hal Perkins
&lt;<A HREF="mailto:hal@gvax.cs.cornell.edu ">
hal@gvax.cs.cornell.edu 
</A>&gt;
</address>
<i>
Wed, 24 Sep 86 11:32:59 EDT
</i><PRE>
To: risks@csl.sri.com

The science section of last Tuesday's New York Times (16 Sept 1986) had a
feature article on the SDI software problem starting on page C1.  The
headline is

	Software Seen As Obstacle In Developing 'Star Wars'
	Serious problems have forced dramatic changes in planning.

	by Philip M. Boffey

The article is much too long to type in -- anyone interested can easily
find a copy.  The author has done his homework.  He gives a good
overview of the problems and of the issues in the SDI software debate
and seems to have talked to the main people involved, several of whom
are quoted.  There's not much here that will be new to computer people
who have been following the debate, but it's definitely worth reading.

Hal Perkins, Cornell CS

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Autonomous widgets
</A>
</H3>
<address>
Mike McLaughlin
&lt;<A HREF="mailto:mikemcl@nrl-csr ">
mikemcl@nrl-csr 
</A>&gt;
</address>
<i>
Wed, 24 Sep 86 10:32:29 edt
</i><PRE>

The discussion of Autonomous Weapons should be expanded, considerably.
Consider the following devices, soon to be found at your local dealer:

	Autonomous Lumberjack - locates and cuts down designated
trees (pulp, hardwood, diseased... )

	Autonomous Booter - identifies automobiles with more than 
n dollars in overdue tickets.  

	Autonomous Streetsweeper - clears your street of any immobile
object other than licensed vehicles (see A. Booter, above).

	Autonomous NightWatchman - passive notifies authorities,
active counteracts intruders. 

N.B.:  My "passive autonomous nightwatchman" is available at your friendly
Heath/Zenith store _now_!  Sorry, don't have a catalog at hand, or I'd
provide ordering information.
                                    Mike McLaughlin

                                     [Mike, Now that it is FALL, you must be 
                                      feeling AUTUMNMATED.  Autonomous Bosh]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Robottle Management Software?  (Wine nought?)
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Wed 24 Sep 86 06:57:03-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

The following news item appeared in the 15 Sept 1986 issue of Digital Review, 
roundabout from the 26 June 1985 issue of the Halifax Gazette.  But it is
RISKY enough to report here.

    EDINBURGH (Reuters) -- A robot dressed in a black hat and bow tie
    appeared in court on Tuesday after running amok in a restaurant
    where it was employed to serve wine. 
         
         Within its first hour on the job, the secondhand robot became
    uncontrollable, knocking over furniture, frightening customers and 
    spilling a glass of wine, the court was told.  The following day, 
    the robot, exhibited Tuesday in the court, was still incapable of 
    controlling the wine glasses, testimony said.  Eventually its head
    fell into a customer's lap.

A tipsy-turvy robot?  Did the firsthand know what the secondhand was doing?
Asimov's Nth Law of Robotics might read, "A robot must not spill wine on the
customers unless enforcing this Law would conflict with Laws 1,2, and 3."
But maybe the program instructed the robot to put on "glasses" (ambiguously)
so it could see better.  Punishment: Send the robot to a OENAL COLONY?
[Apologies in advance.  I've been up too late recently.]  Peter

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.64.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.66.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-52</DOCNO>
<DOCOLDNO>IA012-000123-B023-201</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.66.html 128.240.150.127 19970217004929 text/html 17345
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:47:56 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 66</TITLE>
<LINK REL="Prev" HREF="/Risks/3.65.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.67.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.65.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.67.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 66</H1>
<H2>Thursday, 25 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Follow-up on Stanford breakins: PLEASE LISTEN THIS TIME! 
</A>
<DD>
<A HREF="#subj1.1">
Brian Reid
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  F-16 software [concluded?] 
</A>
<DD>
<A HREF="#subj2.1">
Herb Lin
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Follow-up on Stanford breakins: PLEASE LISTEN THIS TIME!
</A>
</H3>
<address>
Brian Reid
&lt;<A HREF="mailto:reid@decwrl.DEC.COM ">
reid@decwrl.DEC.COM 
</A>&gt;
</address>
<i>
25 Sep 1986 0014-PDT (Thursday)
</i><PRE>

   "What experience and history teach is that people have never learned
    anything from history, or acted upon principles deduced from it."
		-- Georg Hegel, 1832

Since so many of you are throwing insults and sneers in my direction, I feel
that I ought to respond. I am startled by how many of you did not understand
my breakin message at all, and in your haste to condemn me for "asking for
it" you completely misunderstood what I was telling you, and why.

I'm going to be a bit wordy here, but I can justify it on two counts. First,
I claim that this topic is absolutely central to the core purpose of RISKS (I
will support that statement in a bit). Second, I would like to take another
crack at making you understand what the problem is. I can't remember the
names, but all of you people from military bases and secure installations who
coughed about how it was a network administration failure are completely
missing the point. This is a "risks of technology" issue, pure and simple.

As an aside, I should say that I am not the system manager of any of the
systems that was broken into, and that I do not control the actions of any
of the users of any of the computers. Therefore under no possible explanation
can this be "my fault". My role is that I helped to track the intruders down,
and, more importantly, that I wrote about it.

I am guessing that most of you are college graduates. That means that you
once were at a college. Allow me to remind you that people do not need badges
to get into buildings. There are not guards at the door. There are a large
number of public buildings to which doors are not even locked. There is not a
fence around the campus, and there are not guard dogs patrolling the
perimeter.

The university is an open, somewhat unregulated place whose purpose is the
creation and exchange of ideas. Freedom is paramount. Not just academic
freedom, but physical freedom. People must be able to walk where they need to
walk, to see what they need to see, to touch what they need to touch.
Obviously some parts of the university need to be protected from some people,
so some of the doors will be locked. But the Stanford campus has 200
buildings on it, and I am free to walk into almost any of them any time that
I want. More to the point, *you* are also free to walk into any of them.

Now let us suppose that I am walking by the Linguistics building and I notice
that there is a teenager taking books out of the building and putting them in
his car, and that after I watch for a short while, I conclude that he is not
the owner of the books. I will have no trouble convincing any policeman that
the teenager is committing a crime. More important, if this teenager has had
anything resembling a normal upbringing in our culture, I will have no
trouble convincing the teenager that he is committing a crime. Part of the
training that we receive as citizens in our society is a training in what is
acceptable public behavior and what is not. The books were not locked up, the
doors to the library were not locked, but in general people do not run in and
steal all of the books.

Or let me suppose instead that I am a reporter for the Daily News. I have a
desk in a huge room full of desks. Most of the desks are empty because the
other reporters are out on a story. You've seen scenes like this in the
movies. It is rare in small towns to find those newsrooms locked. Here in
Palo Alto I can walk out of my office, walk over to the offices of the Times
Tribune a few blocks away, walk in to the newsroom, and sit down at any of
those desks without being challenged or stopped. There is no guard at the
door, and the door is not locked. There are 50,000 people in my city, and
since I have lived here not one of them has walked into the newsroom and
started destroying or stealing anything, even though it is not protected.
Why not? Because the rules for correct behavior in our society, which are
taught to every child, include the concept of private space, private
property, and things that belong to other people. My 3-year-old daughter
understands perfectly well that she is not to walk into neighbors' houses
without ringing the doorbell first, though she doesn't quite understand why.

People's training in correct social behavior is incredibly strong, even
among "criminals". Murderers are not likely to be litterbugs. Just because
somebody has violated one taboo does not mean that he will immediately and
systematically break all of them.

In some places, however, society breaks down and force must be used. In the
Washington Square area of New York, for example, near NYU, you must lock
everything or it will be stolen.  At Guantanamo you must have guards or the
Cubans will come take things. But in Palo Alto, and in Kansas and in Nebraska
and Wisconsin and rural Delaware and in thousands of other places, you do not
need to have guards and things do not get stolen.

I'm not sure what people on military bases use computer networks for, but
here in the research world we use computer networks as the building blocks of
electronic communities, as the hallways of the electronic workplace. Many of
us spend our time building network communities, and many of us spend our time
developing the technology that we and others will use to build network
communities. We are exploring, building, studying, and teaching in an
electronic world. And naturally each of us builds an electronic community
that mirrors the ordinary community that we live in. Networks in the Pentagon
are built by people who are accustomed to seeing soldiers with guns standing
in the hallway. Networks at Stanford are built by people who don't get out of
bed until 6 in the evening and who ride unicycles in the hallways.

Every now and then we get an intruder in our electronic world, and it
surprises us because the intruder does not share our sense of societal
responsibilities. Perhaps if Stanford were a military base we would simply
shoot the intruder and be done with it, but that is not our way of doing
things. We have two problems. One is immediate--how to stop him, and how
to stop people like him. Another is very long-term: how to make him and his
society understand that this is aberrant behavior.

The result of all of this is that we cannot, with 1986 technology, build
computer networks that are as free and open as our buildings, and therefore
we cannot build the kind of electronic community that we would like.

I promised you that I would justify what this all has to do with RISKS. 

We are developing technologies, and other people are using those
technologies. Sometimes other people misuse them. Misuse of technology is one
of the primary risks of that technology to society. When you are engineering
something that will be used by the public, it is not good enough for you to
engineer it so that if it is used properly it will not hurt anybody. You must
also engineer it so that if it is used *improperly* it will not hurt anybody.
I want to avoid arguments of just where the technologist's responsibility
ends and the consumer's responsibility begins, but I want to convince you,
even if you don't believe in the consumer protection movement, that there is
a nonzero technologist's responsibility.

Let us suppose, for example, that you discovered a new way to make
screwdrivers, by making the handles out of plastic explosives, so that the
screwdriver would work much better under some circumstances. In fact, these
screwdrivers with the gelignite handles are so much better at putting in
screws than any other screwdriver ever invented, that people buy them in
droves. They have only one bug: if you ever forget that the handle is
gelignite, and use the screwdriver to hit something with, it will explode and
blow your hand off. You, the inventor of the screwdriver, moan each time you
read a newspaper article about loss of limb, complaining that people
shouldn't *do* that with your screwdrivers.

Now suppose that you have invented a great new way to make computer networks,
and that it is significantly more convenient than any other way of making
computer networks. In fact, these networks are so fast and so convenient that
everybody is buying them. They have only one bug: if you ever use the network
to connect to an untrusted computer, and then if you also forget to delete
the permissions after you have done this, then people will break into your
computer and delete all of your files. When people complain about this, you
say "don't connect to untrusted computers" or "remember to delete the files"
or "fire anyone who does that".

Dammit, it doesn't work that way. The world is full of people who care only
about expediency, about getting their screws driven or their nets worked. In
the heat of the moment, they are not going to remember the caveats. People
never do. If the only computers were on military bases, you could forbid
the practice and punish the offenders. But only about 0.1% of the computers
are on military bases, so we need some solutions for the rest of us.

Consider this scenario (a true story). Some guy in the Petroleum Engineering
department buys a computer, gets a BSD license for it, and hires a Computer
Science major to do some systems programming for him. The CS major hasn't
taken the networks course yet and doesn't know the risks of breakins. The
petroleum engineer doesn't know a network from a rubber chicken, and in
desperation tells the CS student that he can do whatever he wants as long as
the plots are done by Friday afternoon. The CS student needs to do some
homework, and it is much more convenient for him to do his homework on the
petroleum computer, so he does his homework there. Then he needs to copy it
to the CS department computer, so he puts a permission file in his account on
the CSD computer that will let him copy his homework from the petroleum
engineering computer to the CSD computer. Now the CS student graduates and
gets a job as a systems programmer for the Robotics department, and his
systems programmer's account has lots of permissions. He has long since
forgotten about the permissions file that he set up to move his homework last
March. Meanwhile, somebody breaks into the petroleum engineering computer,
because its owner is more interested in petroleum than in computers and
doesn't really care what the guest password is. The somebody follows the
permission links and breaks into the robotics computer and deletes things.

Whose fault is this? Who is to blame? Who caused this breakin? Was it the
network administrator, who "permitted" the creation of .rhosts files? Was it
the person who, in a fit of expedience, created /usr/local/bin with 0776
protection? Was it the idiot at UCB who released 4.2BSD with /usr/spool/at
having protection 0777? Was it the owner of the petroleum engineering
computer? Was it the mother of the kid who did the breaking in, for failing
to teach him to respect electronic private property? I'm not sure whose fault
it is, but I know three things:

 1) It isn't my fault (I wasn't there). It isn't the student's fault (he
    didn't know any better--what can you expect for $5.75/hour). It isn't the
    petroleum engineer's fault (NSF only gave him 65% of the grant money he
    asked for and he couldn't afford a full-time programmer). Maybe you could
    argue that it is the fault of the administrator of the CSD machine, but in
    fact there was no administrator of the CSD machine because he had quit to
    form a startup company. In fact, it is nobody's fault.

 2) No solution involving authority, management, or administration will work
    in a network that crosses organization boundaries.

 3) If people keep designing technologies that are both convenient and 
    dangerous, and if they keep selling them to nonspecialists, then
    expedience will always win out over caution. Convenience always wins,
    except where it is specifically outlawed by authority. To me, this is
    one of the primary RISKs of any technology. What's special about
    computers is that the general public does not understand them well
    enough to evaluate the risks for itself.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
F-16 software [concluded?]
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Thu, 25 Sep 1986  09:39 EDT
</i><PRE>

    From: rti-sel!dg_rtp!throopw%mcnc.csnet at CSNET-RELAY.ARPA

      &gt; I spoke to an F-16 flight instructor about this business concerning
      &gt; bomb release when the plane is upside down.  He said the software 
      &gt; OUGHT to prevent such an occurrence.  When the plane is not at the
      &gt; right angle of attack into the air stream, toss-bombing can result
      &gt; in the bomb being thrown back into the airplane.  

    Hmpf.  *I* spoke to an ex Air-Force pilot.  He said if *any* restriction on
    bomb release is incorporated it should be to prevent it when the plane (or
    more specificially, the bomb itself... there *is* a difference, and you had
    better realize it!) is pulling negative G's.  This was my original point...
    "upside down" or "inverted" isn't the correct thing to worry about, it is
    the wrong mindset entirely, too simple a notion.

This dispute (well, sort of dispute anyway) is instructive -- each of us
consulted our own experts, and we come away with different answers.  It
suggests why even defining safety is so hard.  Maybe I misunderstood my
flight instructor's response, or maybe I posed the question to him
improperly, or maybe he just gave an off-the-cuff answer without thinking it
thorugh, or maybe he's wrong...

Moral: When you are lost and ask for directions, never ask just one person
for directions.  Ask two people, and you have a better chance of getting to
where you want to go.
                                            Herb

      [On the other hand, when the two people give you DIFFERENT DIRECTIONS,
       you must realize that AT LEAST ONE of them is wrong.  So, you may
       have to ask THREE PEOPLE before you get any agreement...  A further
       moral is that you should have some justifiable trust in those who 
       are giving you advice.  PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.65.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.67.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-53</DOCNO>
<DOCOLDNO>IA012-000123-B023-225</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.67.html 128.240.150.127 19970217004943 text/html 18853
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:48:11 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 67</TITLE>
<LINK REL="Prev" HREF="/Risks/3.66.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.68.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.66.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.68.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 67</H1>
<H2> Thursday 25 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Old GAO Report on Medical Device Software 
</A>
<DD>
<A HREF="#subj1.1">
Chuck Youman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Re: Stanford breakin, <A HREF="/Risks/3.62.html">RISKS-3.62</A> DIGEST 
</A>
<DD>
<A HREF="#subj2.1">
Darrel VanBuer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: Passwords and the Stanford break-in (<A HREF="/Risks/3.61.html">RISKS-3.61</A>) 
</A>
<DD>
<A HREF="#subj3.1">
Dave Sherman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: role of simulation - combat simulation for sale 
</A>
<DD>
<A HREF="#subj4.1">
Jon Jacky
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  MIT Symposium on economic impact of military spending 
</A>
<DD>
<A HREF="#subj5.1">
Richard Cowan
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  "Friendly" missiles and computer error -- more on the Exocet 
</A>
<DD>
<A HREF="#subj6.1">
Rob MacLachlan
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Old GAO Report on Medical Device Software
</A>
</H3>
<address>
Chuck Youman 
&lt;<A HREF="mailto:m14817@mitre.ARPA">
m14817@mitre.ARPA
</A>&gt;
</address>
<i>
Thu, 25 Sep 86 13:49:08 -0500
</i><PRE>

I recently ran across an old report (Aug. 5, 1981) from the U.S. General
Accounting Office (GAO) on the subject "Software Used in Medical Devices
Needs Better Controls To Avoid Compromising Patient Safety (AFMD-81-95).
I don't recall seeing it mentioned in this forum or in SEN.  The 
report is 8 pages and can be ordered from the GAO at P.O. Box 6015,
Gaithersburg, MD 20877.

To briefly summarize the report, they identified 78 cases involving
unreliable computerized medical devices that occurred from June 1976 to
August 1979.  They state that the believe this is only a small fraction of
the total cases that occurred.  They examined 24 of the cases and found 13
of them had software problems.  In their report they give two examples:  a
blood gas analyzer and a computerized electrocardiogram interpretation
software package.

They concluded:

Advances in computer technology have brought about far more reliable 
hardware.  However, software has been and remains a problem area, regardless
of whether it is used in medical or business applications.  We believe the
use of software in medical devices is emerging as a troublesome area and
requires the attention of the Bureau [i.e., the FDA].

The use of performance standards, as authorized by the Medical Device
Amendments of 1976, is a possible mechanism to help control the performance
of software in computerized medical devices.  Unfortunately, the time-
consuming process for developing standards together with the large number
of standards to be developed makes it very unlikely that any standards will
be available soon.  This, coupled with the relatively fast pace at which
computer technology changes, makes it unlikely that the standards when 
developed will be timely enough to validate software in medical devices.
Therefore, we believe the Bureau needs to explore other alternatives for
validating and certifying that the software in medical devices works as
expected.

Charles Youman (youman@mitre.arpa)

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Re: Stanford breakin, <A HREF="/Risks/3.62.html">RISKS-3.62</A> DIGEST
</A>
</H3>
<address>
Darrel VanBuer
&lt;<A HREF="mailto:hplabs!sdcrdcf!darrelj@ucbvax.Berkeley.EDU ">
hplabs!sdcrdcf!darrelj@ucbvax.Berkeley.EDU 
</A>&gt;
</address>
<i>
Wed, 24 Sep 86 09:35:37 pdt
</i><PRE>

I think many of the respondents misunderstand what went wrong: there was no
failure in the 4.2 trusted networking code.  It correctly communicated the
message that "someone logged in as X at Y wants to run program Z at W".  The
failure of security was that
  1)  the "someone" was not in fact X because of some failure of security
      (e.g. poor password).
  2)  the real X who had legitimate access on W had previously created a file
      under some user id at W saying X at Y is an OK user.
  3)  the real X was lazy about withdrawing remote privileges (not essential,
      but widens the window of opportunity.

There's a tough tradeoff between user convenience in a networked environment
and security.  Having to enter a password for every remote command is too
arduous for frequent use.  Interlisp-D has an interesting approach:
  1.  Try a generic userid and password.
  2.  Try a host-specific userid and password.
In either case, if it does not have these items in its cache, it prompts the
user.  The cache is cleared on logout and at certain other times which
suggest the user has gone away (e.g. 20 minutes without activity).
Passwords are never stored in long term or publically accessible locations.
It's also less convenient than 4.2 since you need to resupply IDs after
every cache flush.  It also has the opening for lazy users to use the same
ID and password at every host so that the generic entry is enough.

Darrel J. Van Buer, PhD, System Development Corp., 2525 Colorado Ave
Santa Monica, CA 90406, (213)820-4111 x5449
...{allegra,burdvax,cbosgd,hplabs,ihnp4,orstcs,sdcsvax,ucla-cs,akgua}
                                                            !sdcrdcf!darrelj
VANBUER@USC-ECL.ARPA

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: Passwords and the Stanford break-in (<A HREF="/Risks/3.61.html">RISKS-3.61</A>)
</A>
</H3>
<address>
&lt;<A HREF="mailto:mnetor!lsuc!dave@seismo.CSS.GOV">
mnetor!lsuc!dave@seismo.CSS.GOV
</A>&gt;
</address>
<i>
Thu, 25 Sep 86 12:48:55 edt
</i><PRE>

There's another risk which isn't related to the problems of the networking
code which Brian Reid described. Most users will have the same password on
all machines. So where the intruder becomes root on one machine, he need
merely modify login to store passwords for him, and will very quickly amass
a collection of login-password combinations which have a very high
probability of working all over the network.

I'm not sure what the solution is to this one, except, as has been pointed
out, to be aware that the network is as vulnerable as its weakest link.
Sure, people should use different passwords, but the burden of remembering
passwords for many different machines can become onerous. Perhaps building a
version of the machine name into the password can help mnemonically - i.e.
use the same password with a different final letter indicating which machine
it is.

I use two passwords for the several accounts I have: one for the machines
under my control and one for guest accounts on other organizations' systems.
That way no-one who collects passwords on someone else's system will be able
to use them to break into Law Society machines.

Dave Sherman, The Law Society of Upper Canada, Toronto
dave@lsuc.UUCP
{ ihnp4!utzoo  seismo!mnetor  utai  hcr  decvax!utcsri  } !lsuc!dave

    [Mnemonics with one-letter differences are clearly easy to break.
     Also, it does not really matter how many passwords you have if 
     they are stored somewhere for automatic remote access...  The
     more realistic point is that network security is an intrinsically 
     nontrivial problem.  PGN]

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: role of simulation - combat simulation for sale
</A>
</H3>
<address>
Jon Jacky
&lt;<A HREF="mailto:jon@june.cs.washington.edu ">
jon@june.cs.washington.edu 
</A>&gt;
</address>
<i>
Thu, 25 Sep 86 17:10:09 PDT
</i><PRE>

I came across the following advertisement in AVIATION WEEK AND SPACE TECHNOLOGY,
June 16, 1986, p. 87:

SURVIVE TOMORROW'S THREAT - &lt;illegible&gt; Equipment and Tactics Against Current
	and Future Threats

FSI's dynamic scenario software programs such as "War Over Land," "AirLand
Battle," and "Helicopter Combat" provide realistic simulation of a combat
environment.  These programs use validated threat data to evaluate the 
effectiveness of individual weapons or an integrated weapons system.  The 
easy-to-utilize programs are already in use by the Army, Navy, Air Force, and
many prime defense contractors.  Evaluate your system on a DoD-accepted model.
For more information, contact ... ( name, address, contact person).

(end of excerpt from ad)

The ad doesn't really say how you run this simulation, but kind of implies 
you can actually test real electronic warfare equipment with it.  Needless to
say, an interesting issue is, how comprehensive or realistic is this "validated
(by whom? how?) threat data?"  I checked the bingo card with some interest.
And this ad is just one example of the genre - p. 92 of the same issue 
advertises a product called "SCRAMBLE! Full mission simulators," showing 
several high-resolution out-the-window flight simulator displays of aerial
combat.

-Jonathan Jacky, University of Washington

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
MIT Symposium on economic impact of military spending
</A>
</H3>
<address>
Richard A. Cowan 
&lt;<A HREF="mailto:COWAN@XX.LCS.MIT.EDU">
COWAN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Thu 25 Sep 86 17:42:50-EDT
</i><PRE>
To: [...] risks@CSL.SRI.COM [...]

[The following seminar, sponsored by MIT, may be of interest to RISKS Readers.]

     November Symposium: "What are the effects of military spending?"
                MIT Technology and Culture Seminar
                   Saturday, November 1, 1986
                    9am-3pm, MIT Room 26-100
Topics:

Bernard O'Keefe
  --Chairman of the Executive Committee, EG&amp;G, Inc.
"Are we focusing on the military confrontation with the USSR
 while ignoring the trade war with the Japanese?"

Seymour Melman, 
  --Professor of Industrial Engineering, Columbia University
"Do present rates of military spending make capital effectively
 available for civilian industry?"

Alice Tepper-Martin,
  --Executive Director, Council on Economic Priorities
"If military spending is "only" about six or seven percent of the
 GNP, why worry?"

Frederick Salvucci
  --Secretary of Transportation and Construction for Massachusetts
"Where will the funds for our national infrastructure come from?"

Barry Bluestone
  --Professor of Economics, Boston University
"The arms race and unemployment."

John Kenneth Galbraith
  --Professor of Economics, Harvard University
"Does the military-industrial complex really exist, and what is its impact?"

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
"Friendly" missiles and computer error -- more on the Exocet
</A>
</H3>
<address>
Rob MacLachlan 
&lt;<A HREF="mailto:RAM@C.CS.CMU.EDU">
RAM@C.CS.CMU.EDU
</A>&gt;
</address>
<i>
Thu, 25 Sep 1986  21:23 EDT
</i><PRE>
       
   [We have been around on this case in the past, with the "friendly" theory
    having been officially denied. This is the current item in my summary list:
       !!$ Sheffield sunk during Falklands war, 20 killed.  Call to London
           jammed antimissile defenses.  Exocet on same frequency.  
           [AP 16 May 86](SEN 11 3)                 
    However, there is enough new material in this message to go at it once
    again!  But, please reread <A HREF="/Risks/2.53.html">RISKS-2.53</A> before responding to this.  PGN]

    I recently read a book about electronic warfare which had some
things to say about the Falklands war incident of the sinking of the
Sheffield by an Exocet missile.  This has been attributed to a
"computer error" on the part of a computer which "thought the missile
was friendly."  My conclusions are that:
 1] Although a system involving a computer didn't do what what one
    might like it to do, I don't think that the failure can reasonably
    be called a "computer error".
 2] If the system had functioned in an ideal fashion, it would
    probably have had no effect on the outcome.

The chronology is roughly as follows:

The Sheffield was one of several ships on picket duty, preventing
anyone from sneaking up on the fleet.  It had all transmitters
(including radar) off because it was communicating with a satellite.

Two Argentinan planes were detected by another ship's radar.  They
first appeared a few miles out because they had previously been flying
too low to be detected.  The planes briefly activated their radars,
then turned around and went home.

Two minutes later a lookout on the Sheffield saw the missile's flare
approaching.  Four seconds later, the missile hit.  The ship eventually
sank, since salvage efforts were hindered by uncontrollable fires.

What actually happened is that the planes popped up so that the could
acquire targets on their radars, then launched Exocet missiles and
left.  (The Exocet is an example of a "Fire and Forget" weapon.  Moral
or not, they work.)  The British didn't recognize that they had been
attacked, since they believed that the Argentinans didn't know how to
use their Exocet missiles.

It is irrelevent that the Sheffield had its radar off, since the
missile skims just above the water, making it virtually undetectable
by radar.  For most of the flight, it proceeds by internal guidance,
emitting no telltale radar signals.  About 20 seconds before the end
of the flight, it turns on a terminal homing radar which guides it
directly to the target.  The Sheffield was equipped with an ESM
receiver, whose main purpose is to detect hostile radar transmissions.

The ESM receiver can be preset to sound an alarm when any of a small
number of characteristic radar signals are received.  Evidently the
Exocet homing radar was not among these presets, since there would
have been a warning 20 sec before impact.  In any case, the ESM
receiver didn't "think the missile was friendly", it just hadn't been
told it was hostile.  It should be noted that British ships which were
actually present in the Falklands were equipped with a shipboard
version of the Exocet.

If the failure was as deduced above, then the ESM receiver behaved
exactly as designed.  It is also hard to conceive of a design change
which would have changed the outcome.  The ESM receiver had no range
information, and thus was incapable of concluding "anything coming
toward me is hostile", even supposing the probably rather feeble
computer in the ESM receiver were cable of such intelligence.

In any case, it is basically irrelevant that the ESM receiver didn't
do what it might have done, since by 20 seconds before impact it was
too late.  The Sheffield had no "active kill" capability effective
against a missile.  Its anti-aircraft guns were incapable of shooting
down a tiny target skimming the water at near the speed of sound.

It is also poossible to cause a missile to miss by jamming its radar,
but the Sheffield's jamming equipment was old and oriented toward
jamming russian radars, rather than smart western radars which
wheren't even designed when the Sheffield was built.  The Exocet has a
large bag of tricks for defeating jammers, such as homing in on the
jamming signal.

In fact, the only effective defense against the Exocet which was
available was chaff: a rocket dispersed cloud of metalized plastic
threads which confuses radars.  To be effective, chaff must be
dispersed as soon as possible, preferably before the attack starts.
After the Sheffield, the British were familiar with the Argentinan
attack tactics, and could launch chaff as soon as they detected the
aircraft on their radars.  This defense was mostly effective.

Ultimately the only significant mistake was the belief that the
Argentinans wouldn't use Exocet missiles.  If this possibility was
seriously analysed, then the original attack might have been
recognized.  The British were wrong, and ended up learning the hard
way.  Surprise conclusion: mistakes can be deadly; mistakes in war are
usually deadly.

I think that the most significant "risk" revealed by this event is
tendency to attribute the failure of any system which includes a
computer (such as the British Navy) to "computer error".

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.66.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.68.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-54</DOCNO>
<DOCOLDNO>IA012-000123-B023-244</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.68.html 128.240.150.127 19970217005005 text/html 22729
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:48:24 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 68</TITLE>
<LINK REL="Prev" HREF="/Risks/3.67.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.69.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.67.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.69.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 68</H1>
<H2> Friday, 26 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
VDU risks -- Government changes its mind, perhaps 
</A>
<DD>
<A HREF="#subj1.1">
Stephen Page
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  "Drive by wire" systems 
</A>
<DD>
<A HREF="#subj2.1">
Charles R. Fry
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Viking Landers worked the first time and met the specs 
</A>
<DD>
<A HREF="#subj3.1">
Dave Benson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Unix breakins - secure networks 
</A>
<DD>
<A HREF="#subj4.1">
David C. Stewart
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Comment on the reaction to Brian's Breakin Tale 
</A>
<DD>
<A HREF="#subj5.1">
Dave Taylor
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Reliability, complexity, and confidence in SDI software 
</A>
<DD>
<A HREF="#subj6.1">
Bob Estell
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
VDU risks -- Government changes its mind, perhaps
</A>
</H3>
<address>
&lt;<A
HREF="mailto:sdpage%sevax.prg.oxford.ac.uk@Cs.Ucl.AC.UK">Stephen
Page &lt;sdpage%sevax.prg.oxford.ac.uk@Cs.Ucl.AC.UK&gt;</A>&gt;
</address>
<i>
Fri, 26 Sep 86 21:28:47 GMT
</i><PRE>

&gt;From "Computer News" no. 141 (September 25, 1986):

                    Executive does U-turn on VDU risk

The [UK] government's Health and Safety Executive is spending nearly 1.5m
pounds on research into the hazards of using VDUs -- just five months after
assuring users that there is no danger.

The Executive has commissioned five reports into the possible health problems
which may arise from working with VDUs.

The studies, which typically last three years, will look at topics such as
repetitive VDU work, discomfort and optimum rest periods. It has contracted the
work out to a number of universities at a cost of 475,000 pounds.

[...]

Earlier this year, the Executive issued a booklet aimed at dispelling fears
that VDU work can lead to health risks and denying that radiation from
terminals would lead to birth defects and miscarriages.

Part of the new research will look at the possible effects of VDU strain and
stress on pregnant women.

                       [Of course, the US Government had previously  
                        cancelled some ongoing work in this area!  PGN]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
"Drive by wire" systems
</A>
</H3>
<address>
Charles R. Fry 
&lt;<A HREF="mailto:Chucko@GODZILLA.SCH.Symbolics.COM">
Chucko@GODZILLA.SCH.Symbolics.COM
</A>&gt;
</address>
<i>
Tue, 23 Sep 86 08:59 PDT
</i><PRE>
To: risks@csl.sri.com

From Henry Spencer:

  Doug Wade notes:
    
    &gt;   My comment to this, is what if a 8G limit had been programmed into
    &gt; the plane (if it had been fly-by-wire)...
    
  My first reaction on this was that military aircraft, at least front-line
  combat types, obviously need a way to override such restrictions in crises,
  but civilian aircraft shouldn't.  Then I remembered the case of the 727 ...
  It would seem that even [commecial] airliners might need overrides.

The "drive-by-wire" features now appearing in some cars, ostensibly to make
them "safe to drive in all conditions," also seem to require overrides.  For
instance, the most common of these systems is anti-lock braking.  The first
such system available to the public, introduced by Audi on its original
Quattro, could be disabled by a switch on the dashboard.  Why?  Because
under some conditions (e.g.  on gravel roads) the best braking performance
is obtained when the wheels are locked.  This was especially important on
the Quattro, a street-legal rally car which was intended for high speed
driving on all types of roads.  (But as Detroit catches on, look for such
switches to disappear in order to design some cost out of the systems.)

Now several European manufacturers (Mercedes-Benz, BMW) are introducing cars
with "accelerative anti-skid systems," with no direct linkage between the
gas pedal and the throttle on the engine.  The intent is to prevent the
engine from seeing full throttle when it would just cause excessive
wheelspin, especially in slick, wintry conditions.  However, on rear wheel
drive cars (only!! -- don't try this with your Honda) such wheelspin can be
used to make the car turn more tightly than it would without, and I can
easily imagine circumstances in which this maneuver could save some lives.

No matter how many automated controls we install on cars (and airplanes)
to prevent operators from exceeding their vehicles' limits, there will
always be a need to allow the deliberate violation of these limits.  

  [Chuck added an aside on the value of high performance driving schools.]

	-- Chuck Fry 
	   Chucko@STONY-BROOK.SCRC.Symbolics.COM

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
 Viking Landers worked the first time and met the specs
</A>
</H3>
<address>
Dave Benson 
&lt;<A HREF="mailto:benson%wsu.csnet@CSNET-RELAY.ARPA">
benson%wsu.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Wed, 24 Sep 86 18:01:18 pdt
</i><PRE>

Both Viking Landers worked in their first (and only) operation.  The
pre-operation testing simply ups one's confidence that the actual
operation will be successful.  Since the Viking Landers were the
first man-made objects to land on Mars, Murphy's Law should suggest
to any engineer that perhaps something might have been overlooked.
In actual operation, nothing was.

Both Viking Mars shots had specifications
for the length of time they were to remain in operation.  While I
do not recall the time span, both exceeded the specification by years.
I do recall that JPL had to scrounge additional funds to keep the
data coming in from all the deep-space probes, including the Vikings,
as the deep space mechanisms were all working for far longer than expected.
	
Surely any engineered artifact which lasts for longer than its
design specification must be considered a success.  Nothing
lasts forever, especially that most fragile of all artifacts, software.
Thus the fact that the Viking 1 Lander software was scrambled beyond
recovery some 8 years after the Mars landing only reminds one that
the software is one of the components of an artifact likely to fail.
So I see nothing remarkable about this event, nor does it in any way
detract from judging both Viking Mars missions as unqualified engineering
successes.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Unix breakins - secure networks
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: 24 Sep 86 13:46:39 PDT (Wed)
From: "David C. Stewart" &lt;davest%tektronix.csnet@CSNET-RELAY.ARPA&gt;

	One of the observations that have been made in the wake of the
Stanford breakin is that Berkeley Unix encourages the assumption that
the network itself is secure when in fact, it is not difficult to imagine
someone tapping the ethernet cable and masquerading as a trusted host.

	I have been intrigued by work that has been going on at CMU to
support the ITC Distributed File System.  (In the following, Virtue is
the portion of the filesystem running on a workstation and Vice is
that part running on the file server.)

	The authentication and secure transmission functions are
	provided as part of a connection-based communication package,
	based on the remote procedure call paradigm.  At connection
	establishment time, Vice and Virture are viewed as mutually
	suspicious parties sharing a common encryption key.  This key
	is used in an authentication handshake, at the end of which
	each party is assured of the identity of the other.  The final
	phase of the handshake generates a session key which is used
	for encrypting all further communication on the connection.
	The use of per-session encryption keys reduces the risk of
	exposure of authentication keys. [1]

	The paper goes on to state that the authorization key may be
supplied by a password (that generates the key but is not sent along
the wire in cleartext) or may be on a user-supplied magnetic card.

	This is one of the few systems I have seen that does not trust
network peers implicitly.  A nice possibility when trying to reduce
the risks involved with network security.

Dave Stewart - Tektronix Unix Support - davest@tektronix.TEK.COM

[1] "The ITC Distributed File System: Principles and Design",
Operating Systems Review, 19, 5, p. 43.

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Comment on the reaction to Brian's Breakin Tale
</A>
</H3>
<address>
Dave Taylor 
&lt;<A HREF="mailto:taylor%hpldat@hplabs.HP.COM">
taylor%hpldat@hplabs.HP.COM
</A>&gt;
</address>
<i>
Fri, 26 Sep 86 17:55:53 PDT
</i><PRE>
Organization: Hewlett-Packard Laboratories, Unix Networking Group
Work-Phone-Number: +1 415 857-6887

I have to admit I am also rather shocked at the attitudes of most of the
people responding to Brian Reids' tale of the breakin at Stanford.  What
these respondents are ignoring is The Human Element.

Any system, however secure and well designed, is still limited by the
abilities, morals, ethics, and so on of the Humans that work with it.  Even
the best paper shredder, for example, or the best encryption algorithm, isn't
much good if the person who uses it doesn't care about security (so they shred
half the document and get bored, or use their husbands' first name as the
encryption key).

The point here isn't to trivialize this, but to consider and indeed, PLAN FOR
the human element.

I think we need to take a step back and think about it in this forum...

						-- Dave

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Reliability, complexity, and confidence in SDI software
</A>
</H3>
<address>
"ESTELL ROBERT G" 
&lt;<A HREF="mailto:estell@nwc-143b.ARPA">
estell@nwc-143b.ARPA
</A>&gt;
</address>
<i>
26 Sep 86 13:22:00 PST
</i><PRE>
To: "risks" &lt;risks@csl.sri.com&gt;
Reply-To: "ESTELL ROBERT G" &lt;estell@nwc-143b.ARPA&gt;

I apologize in advance for the length of this piece.  But it's briefer
than the growing list of claims and counter-claims, made by resepctable
folks, based on either/both sound theory or/and actual experience.  
And we're dealing with a critical question: 
	Can very large systems be reliable?


The "bathtub curve" for MECHANICAL "failures" has always made sense to me.
I've heard lectures about how software follows similar curves.  
But I've really been stumped by the notion that "software wears out."

I'd like to attempt to "bound the problem" so to speak.
SUPPOSE that we had a system composed of ten modules; and suppose that
each module had ten possible INTERNAL logical paths, albeit only one 
entry and only one exit.

 The MINIMUM number of logical paths through the system  is ten (10); 
 i.e., *IF* path #1 in module A INVARIABLY invokes path #1 in modules 
 B, C, ... J; and likewise, path #2 in A INVARIABLY invokes path #2 
 in B, C, ... J; etc. then there are only ten paths.
 NOTE I'm also assuming that the modules invariably run in alpahbetical
 order, always start with A, and always finish with J; and never fail
 or otherwise get interrupted.  [I'm trying to avoid nits.]
 Some residential wiring systems are so built; there are many switches
 and outlets on each circuit; but each circuit is an isolated loop to the 
 main "fuze" box; "fuzes" for the kitchen are independent of the den.

 The MAXIMUM number of logical paths through the system is ten billion 
 (10.E10); i.e., *IF* each module can take any one of its ten paths in 
 response to any one of the ten paths from any one of the other ten modules, 
 there are 10**10 possibilities.
 AGAIN assuming that the system always starts with A, runs in  order, etc.
 *IF SEQUENCE IS SIGNIFICANT, and if the starting point is random, THEN
 there are ten!10.E10 paths; i.e., ten factorial times ten billion, or
 36,288,000,000,000,000 possible paths in the system.
 
 Further, *IF INTERRUPTS* are allowed and are significant, then I can't
 compute the exact number of possible paths; but I can guarantee that it's
 &gt;MORE&gt; than 10!10.E10.

End of bounds.  The scope reaches from the trivial, to the impossible.

The GOAL of good engineering practices [for hardware, software, and firmware]
is to design and implement modules that control the possible paths; e.g.,
systems should *NOT* interact in every conceivable way.
It does NOT follow that the interactions should be so restricted that 
there are only ten paths through a ten module system.
BUT there is some reason to HOPE that systems may be so designed, in a tree
structure such that:

 a. AT EACH LEVEL, exactly one module will be "in control" at any instant; 
 b. and that each module will run independently of others at its level; 
 c. and that there are a finite [and reasonably small] number of levels.

In "Levels of Abstraction in Operating Systems", RIACS TR 84.5, Brown,
Denning, and Tichy describe 15 levels, reaching from circuits to shell;
applications sit at level 16.  If one must have a layered application,
then add layers 17, 18, et al.

I will conjecture that at levels 1 and 2 [registers, and instruction set],
there are only five possible states (each):
 (1) not running; 
 (2) running - cannot be interrupted;
 (3) running - but at a possible interrupt point;
 (4) interrupted; and
 (5) error.

I will further conjecture that the GOAL of writing modules at each of the
other layers, from O/S kernel, through user application packages, can
reasonably be to limit any one module to ten possible states.  NOTE that
purely "in line code" can perform numerous functions, without putting the
module in more than a few states.  [e.g., Running, Ready to run, Blocked,
Intrerrupted, Critical region, or Error.]

Such a system, comprised of say 15 applications layers, would assume maybe
290 possible states; that's the SUM of the number of possibilities at each
layer, given the path that WAS ACTUALLY TAKEN to reach each layer.

Yet the number of functions that such a system could perform is at least
the sum of all the functions of all the modules in it.  If you're willing
to risk some interaction, then you can start playing with PRODUCTS [vice
SUMS] of calling modules, called modules, etc.  EVEN SO, if the calling
module at layer "n" can assume half a dozen states, and the called module
at layer "n+1" can assume a similar number, then the possible states of
that pair are about 40; that's more than a dozen, but it's still managable.

In real life, both humans and computers deal with enormously complex systems
using similar schemes.  For instance, two popular parlor games: chess, and
contract bridge.  Each admits millions of possible scenarios.  But in each,
the number of possible sensible *NEXT plays* is confined by the present 
state of affairs.  So-called "look ahead" strategies grow very complex; 
but once a legal play has been made, there are again a small number of 
possible legal "next plays."

In bridge, for instance, at least 635,013,559,600 possible hands can be dealt,
to ONE player [combination of 52 things, 13 at a time].  That one hand does
not uniquely determine the contents of the other three hands.
Whether the hands interact is not a simple question in pure mathematics;
in many cases, they do; but in one unique case, they don't; 
e.g., if dealer gets all 4 aces, and all 4 kings, all 4 queens, and any
jack, then he bids 7 no trump; and it doesn't matter who else has what
else; it's an unbeatable bid.  [Non bridge players, accept both my word 
for it; and my apology for an obscure example.]

We've been playing bridge a lot longer than we've been writing large, real-
time software systems.  I'll conjecture that we don't know nearly as much
about "SDI class systems" as we do about the card game.
But in either case, if we aren't careful, the sheer magnitude of the
numbers can overwhelm us.

BOTTOM LINEs:

1. The curve for debugging software has a DOWNslope and length that is 
some function of the number of possible paths through the code.

2. Good software engineering practice says that one checks the design
before writing lots of code.  ["Some" may be necessary, but not "lots."]
*IF* errors show up in the design, fix them there.
*IF* the DESIGN itself is flawed, then change it.  [e.g., Rethink a design
that allows modules to interact geometrically.]

3. Confidence builds as one approaches the 90% [or other arbitrary level]
point in testing the number of possible paths.

4. The reason that we haven't built confidence in the past is that we've
often run thousands of hours, without knowing either:

 a. how many paths got tested; or
 b. how many paths remained untested.

5. INTERACTIONS do occur - even ones that aren't supposed to.
[Trivial example: My car's cooling and electrical systems are NOT supposed
to interact; and they don't - until the heater hose springs a leak, and
squirts coolant all over the distributor and sparkplugs.]
In "The Arbitration Problem", RIACS TR 85.12, Dennning shows that 
computers are fundamentally NOT ABSOLUTELY predictable; it may be that
an unstable state is triggered ONLY by timing idiosyncracies such as:
 At the same minor cycle of the clock, CPU #1 suffers a floating 
 underflow in the midst of a vector multiplication, AND CPU #2 takes an 
 I/O interrupt from a disk read error, while servicing a page fault.

6. Since interactions do occur, experiences that many have had with small
programs in a well-confined environment do *NOT* necessarily "scale up"
to apply to very large, real-time codes, that run on raw hardware in a
hostile [or just "random"] environment.  NOTE that I'm claiming that in
such a system, the O/S kernel is part of the real-time system.

7. The "problem space" we've been discussing is at least triangular. 
In one corner, there are assembly language monoliths, running on second-
generation computers, without hardware protection; such systems convince
Parnas that "SDI won't ever work."  Written that way, it won't.
[Important aside: It's one thing to argue that *if* SDI were built using
modern software techniques, it would work.  It's another thing to realize
that in DOD, some (not all) tactical systems run on ancient computers that 
cost more to maintain than they would to replace; and offer less power than 
a PC AT.  Such facts, known to Parnas, understandably color his thinking.]

In another corner, there are small [1000 or so lines] modules, running
in a controlled environment, that and have been "proven" to work.
Most of us doubt that such experience scales up to SDI sizes.

In another corner, there are 100,000 line systems that work, in real life,
but without formal proofs.  Probably built using good S/W Eng practices.

8. The KISS principle ["Keep It Simple, Stupid"] eliminates lots of problems.
Prof. Richard Sites, at UCSD in 1978, told of a talk given by Seymour Cray.  
In answer to audience questions about "how to make the circuits run at those
speeds", Cray explained that circuit paths were all of known, fixed lengths; 
and that all paths were terminated cleanly at both ends; and other 
"good EE practices" taught to undergrads.  Less successful builders were 
so wrapped up in megaFLOPS that they got careless.

We could do well to adopt Cray's philosophy for hardware as we build our
software; e.g., build "RISC" programs; write code that does only a few tasks,
but does them very well, both quickly and reliably.
Maybe that's one reason why UNIX systems are so portable, powerful, and
popular?  [Each module is simple; power comes from piping them together.]
NOTE that I'm claiming that "RISC" computer architecture is not new;
look at almost every machine that Cray has designed; instruction sets are
limited, and their implementation is superb.

Bob
For the record, I'm speaking "off the record" and expressing personal opinion.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.67.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.69.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-55</DOCNO>
<DOCOLDNO>IA012-000123-B023-276</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.69.html 128.240.150.127 19970217005027 text/html 21256
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:48:53 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 69</TITLE>
<LINK REL="Prev" HREF="/Risks/3.68.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.70.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.68.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.70.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 69</H1>
<H2> Sunday, 28 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Confidence in software via fault expectations 
</A>
<DD>
<A HREF="#subj1.1">
Dave Benson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  More on Stanford's UNIX breakins 
</A>
<DD>
<A HREF="#subj2.1">
John Shore
</A><br>
<A HREF="#subj2.2">
 Scott Preece
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  F-16 simulator 
</A>
<DD>
<A HREF="#subj3.1">
Stev Knowles
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Deliberate overrides? 
</A>
<DD>
<A HREF="#subj4.1">
Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Viking Landers -- correction to <A HREF="/Risks/3.68.html">RISKS-3.68</A> 
</A>
<DD>
<A HREF="#subj5.1">
Courtenay Footman
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
 Confidence in software via fault expectations
</A>
</H3>
<address>
Dave Benson 
&lt;<A HREF="mailto:benson%wsu.csnet@CSNET-RELAY.ARPA">
benson%wsu.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Sat, 27 Sep 86 18:14:48 pdt
</i><PRE>

A partial reply to Estell's nice piece on "Reliability, complexity
and confidence in SDI software" as well as other comments about
fault rates in large software:

(1)  The bathtub curve for reliability of engineered artifacts is usually
considered to be composed of three distinct phenomena,
	(i) The early failures caused by manufacturing defects,
	(ii) The "random" failures of components for "unknown" reasons
		(These may be judged as defects in the design, allowed
		 to lower the cost of the product),
	(iii) Wearout failures near the end of the product life.
Type (i) failures give the initial declining failure rates during "burn-in",
type (ii) failures during the useful product life, and type (iii) failures
occur at the design-life limit.  This bathtub curve is not applicable to
software since the usual definition of a large software product includes
many different releases. Perhaps a software product should be compared to
several different models of the same car, toaster, airplane, etc.  The
bathtub curve describes the sum of manufacturing defects, design defects,
and wear.  Software ordinarily has no manufacturing defects and the
usual way ordinary backups are done insures that most software does not
wear out before it becomes obsolete.  Perhaps the Viking 1 Lander software
failure could be classified as a "wearout" due to inadequate preventative
maintenance, but this seems to be streching a point.
	So software ordinarily fails from design defects and design defects
only.  These are considered so important that we classify such defects
into specification, design and implementation defects.  The point here is
that none of these are manufacturing or wear failures.

(2) The defect rate models for software all attempt to describe a process
of redesigning the software after the discovery of failures, repeatedly,
in a never-ending cycle of testing (either formally or via users discovering
problems) and "maintenance" (which is actually redesigning a new model of
the software upon discovery of problems--with so-called enhancements
thrown in to confuse the issues).  I shall now give a crude approximation
to all of these models.  Let all realize I have abstracted the essential
features of these models to the point of unusability in QA practice.  The
essense is enough to make my point.
	We assume that the original release of the software has a load of
N design defects and that defects are discovered and instantly and
flawlessly reworked with a rate constant, a, according to the formula
	R(t) = N*exp(-a*t)
where exp() is the exponential function, t is a measure of software use
(time, person-years, cpu cycles consumed, ...) and R(t) is the remaining
number of design faults in the reworked software.  This formula clearly
illustrates that for any t&gt;=0, if R(t) is not zero, then more faults
remain.  In words, some faults mean yet more faults.
	The more detailed versions of this essential idea do, approximately,
describe the process of removing faults from a continuing sequence of
releases of a software product.  Bev Littlewood has a nice survey of these,
together with some practical suggestions, in a recent IEEE Trans.
on Software Engineering--perhaps last Jan or Feb issue.  In any case, we
may see that the essential feature of "some faults imply more faults"
is used in practice to estimate remaining design fault loads in
software.  The models have this feature because this seems theoretically
sound and the actual data is not inconsistent with this class of models.
	
(3) If faults are not repaired when discovered, there is data suggesting
that software failures may be viewed as type (ii), supra:  Singpurwalla
and Crow have a nice paper suggesting that faults are evidenced as
failures with a periodicity sufficiently good to make interesting Fourier
analysis of the failure data.  We may take this as suggesting that some failures
imply more failures at regular times in the future.

(4) Good designs have few faults and evidence few failures.  In software
this means few releases are necessary to correct faults.  However, many
software products interact primarily with that most flexible of
io devices, people, People quickly adjust to the ideosyncracies and
failures of the software they use.  In my opinion, Unix (Reg. Trademark,
AT&amp;T) and derivatives is successful because its ideosyncracies and failures
are somehow "human", but not because of low failure rates.
	Good software designs start with a low initial number of faults.
Good design practices seem to lead to better software.  But one simply
requires more data than currently exists to say much definite about the
advantages of Ada vs. a more traditional practice.  Furthermore, new
software is likely to be "more complex" than old software--leading to
perhaps the same MTTF.  Highly reliable software appears to be engineered
in much the same manner as any other highly reliable engineered artifact:
By repeatedly designing similar artifacts, obtaining experience with
their use, and then redesigning anew.

(5) Thus many of us are extremely dubious about the claims made for SDI
(and thus its driving software).  Without the ability to test in actual
practice, there is no compelling reason to believe any claims made for
the reliability of the software.  This point has been made several times,
by several people, on RISKS and I'll not repeat the argument.  It seems
that the onus of compelling evidence lies with those who claim SDI "will
work."  So far I've found no evidence whatsoever to support the claim
that ANY new military software works in its first adversarial role:  i.e.,
in the face of enemy action or a good, determined, simulation thereof.
I'd appreciate reliable evidence for such.  The claim for 100,000 line
programs which work reliably requires supporting evidence.  I am perfectly
prepared to believe that the 28th yadbm (yet another data base manager)
works reliably.  I'm not prepared to simply accept such claims for
military software.  An example:  JSS is a C3I system for the defense of
North America against bomber attack.  JSS is currently receiving some kind
of "independent operational" test in Colorado.  Workers at Hughes kept
careful records of defect rates during development, and reported that
certain of the standard models alluded to above failed to predict
defect rates at the next step of in-house testing.  Will I ever be able
to learn what the results of the "independent operational" test are?
I doubt it.  All I might be able to learn is whether the US adopts the
system or not.  I'm highly dubious about the reliability of JSS, despite
the adoption of reasonably current SE practices.  And recall, JSS is
the nth yac3i.

(6) Controlling complexity is a wonderful idea.  But what does one
do in the face of a truely complex world, in which complex decisions
must be made?  One designs complex software.  Recall that the Enroute
Air Traffic Control System has so far exercised only a minute fraction
of all the paths through it, despite being installed at about 10 sites
for about 10 years.  At the current rate one might get to 90% path
coverage by the year 2200?  Yet every time you fly on a commercial
aircraft, you implicitly trust this system.  I suggest you trust it
because it has been used operationally for 10 years and the enroute
controllers view it as trustworthy.  The fault rate is low enough
and the controllers flexible enough and the enroute mid-air near
collision rate is low enough that everyone is satisfied enough.  No
mathematics and little statistics here--just actual operational
experience.

(7) Software types need to adopt rather more of a Missouri attitude:
Show me that it works.  Part of the problem is defining what "works"
means.  Thats what makes the Viking Lander experiences so compelling.
Everyone can easily agree that the software worked the only two
times it was called upon to land the craft.  One might think that
military software experiences should be equally compelling to the 
senses.  So consider the Navy's Aegis experiences...   The result of
actual data suggests that SDI software is unbuildable as a highly
reliable program.  I repeat my call for serious, professional
papers on military software which worked the first time.  So far I
can only conclude than none such exist.  Thereby I think I am
entitled to discount any claims for the "quality" of military
software in actual, operational practice.  The logical, rational
conclusion is that, with no data supporting claims for military
software working in first use, and only data such as the Sgt. York
and Aegis, SDI software will not work the first and only time
it might be called upon to function.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Re: Brian Reid's follow-up on Stanford's UNIX breakins
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: 28 Sep 86 10:51:16 EST (Sun)
From: John Shore &lt;epiwrl!shore@seismo.CSS.GOV&gt;

Brian is quite right.  The job of an engineer is to build systems that
people can trust.  By this criterion, there exist few software engineers.
                                                          js

</PRE>
<HR><H3><A NAME="subj2.2">
Follow-up on Stanford breakins: PLEA
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%ccvaxa@GSWD-VMS.ARPA">
preece%ccvaxa@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Fri, 26 Sep 86 10:26:33 cdt
</i><PRE>

Brian Reid speaks eloquently to important issues.  Virtually
everything he says in this note makes perfect sense and should be
taken to heart by everyone designing systems.  BUT...

What he says now is not exactly what he said the first time;
when, I assure him, some of us were listening.  His first note
did in fact attribute blame, to the networking code and to the
student involved (under the general rubric of 'wizards').

The designer of the gelignite-handled screwdriver has clearly got a
responsibility when the screwdriver is (incorrectly) used to pound on
something and explodes.  The designer has little responsibility when the
screwdriver is used (incorrectly and maliciously) as a sharp object to stab
a co-worker during a fight.  If the screwdriver is used to hit someone over
the head in a fight, and explodes, the responsibility is a lot more muddled.
It is not at all clear how far the designer's responsibility for protecting
us from mistakes extends to protecting us from temptation.

Is a car manufacturer morally liable for its cars being capable of going 120
mph, creating the potential for more serious accidents when they are used
inappropriately?  Is the manufacturer of autodial modems responsible because
they make it possible for system crackers to try many more phone numbers per
hour than manually dialled modems?

Had Brian made slightly less attempt to de-jargonize his original posting
and said ".rhosts" instead of "permission files", which could refer to quite
a few different things ina BSD system, I would have taken a different
impression of his complaint away from that original posting.  I agree
strongly that .rhosts files are a danger that administrators should be able
to turn off, preferably on a host by host basis.

It should still be noted that .rhosts files are there for a reason and that
that reason is perfectly valid and the provision of .rhosts capabilities
perfectly reasonable IN THE APPROPRIATE SITUATION.  A campus-wide network of
machines under diverse administrators may not be such a situation; I would
hate to see the capabilities taken out of the system simply because there
may be inappropriate situations.  Ftp and telnet are still provided as well
as the r-utilities.

As our moderator has said, fault rarely lies on one head.  I agree with
Brian that the designer (of systems OR screwdrivers) has a strong
responsibility to consider both unintentional and intentional misuses of her
systems and to watch for aspects of her designs that could raise the
consequences of such misuses.  The strongest responsibility is to make the
limits of appropriate use obvious to the user, by packaging, documentation,
and whatever other steps may be necessary.  If on mature reflection it still
seems likely the user will be unaware of the problem (who reads
documentation on a screwdriver), the designer has a moral obligation to seek
other means to avoid misuse.  Perhaps the explosive screwdriver should be
sold only with with a two-foot long handle, making it unsuitable for common
domestic use, or as a separately packaged replacement handle in a six-inch
thick lead box bedecked with scenes of mutilation.  If, however, the object
is the best or only solution to a particular problem (only a gelignite
screwdriver can remove red kryptonite screws from lead doorframes), it may
also be morally unacceptable to suppress the product simply because it may
have dangerous implications in the hands of the unwary.

Hey, surprise, there's no easy answer...

scott preece, gould/csd - urbana, uucp:	ihnp4!uiucdcs!ccvaxa!preece

       [Let me commend Brian once again for having performed a truly
        valuable service to the community.  (I notice his original message
        is reappearing in many places!)  I don't think we should expect him
        to try to respond to each such comment.  But -- given the ease with
        which system and network security can be broken -- we may see lots
        more of such analyses of OTHER breakins.  The sad part is that most
        of these vulnerabilities are well known in the security community,
        but few other people have yet been concerned enough to do anything,
        including most system developers.  The consensus among security folks 
        is that it will take a Chernobyl-like event in computer security
        before most people wake up.  PGN]

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
F-16 simulator
</A>
</H3>
<address>
Stev Knowles 
&lt;<A HREF="mailto:stev@BU-CS.BU.EDU">
stev@BU-CS.BU.EDU
</A>&gt;
</address>
<i>
Thu, 25 Sep 86 17:36:43 EDT
</i><PRE>

As I see it, you are all missing the point. A simulator *should* allow the
plane to land with the gear up. A simulator should allow it to release a
bomb in any position, *if the plane would*.  The simulator should not try
and stop the pilot from doing stupid things, it should react as the plane
would. *If the plane will not allow something*, then the simulation should
not allow it.

There is a difference. the *plane* should not allow a bomb to be detached if
it will damage the plane. *But if it does* the software should too.

stev knowles, boston university distributed systems group
CSNET: stev@bu-cs.CSNET  UUCP:...harvard!bu-cs!stev  BITNET:ccsk@bostonu.BITNET
  
</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Deliberate overrides?
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Sat, 27 Sep 1986  08:36 EDT
</i><PRE>

    From: Charles R. Fry &lt;Chucko at GODZILLA.SCH.Symbolics.COM&gt;
    No matter how many automated controls we install on cars (and airplanes)
    to prevent operators from exceeding their vehicles' limits, there will
    always be a need to allow the deliberate violation of these limits.  

This discussion about allowing overrides to programmed safety limits worries
me.  It is certainly true that there are instances in which the preservation
of life requires the operator to override these devices.  But these have to
be weighed against the situations in which a careless operator will go
beyond those limits when it is inappropriate.  I haven't heard much
discussion about that, and maybe it is because it is very difficult
(impossible?) for the safety machinery to tell when an operator is being
careless given the operative conditions at the time.

There is a tradeoff here that many have resolved categorically in favor of
people being able to override computers.  I think only competent and
sensible people people, under the right circumstances, should be able to do
so.  The problem is to find a mechanical system capable of making these
distinctions.  Thus, the comment that PGN omitted "[Chuck added an aside on
the value of high performance driving schools.]" was, in my view, crucial to
understanding the situation involved.  Maybe a partial solution would be to
allow only drivers who have passed courses at high performance driving
schools to override.

     Herb                             [Shades of Chernobyl!  PGN]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Viking Landers -- correction to <A HREF="/Risks/3.68.html">RISKS-3.68</A>
</A>
</H3>
<address>
Courtenay Footman
&lt;<A HREF="mailto:cpf@tcgould.tn.cornell.edu ">
cpf@tcgould.tn.cornell.edu 
</A>&gt;
</address>
<i>
Sun, 28 Sep 86 22:12:58 EDT
</i><PRE>

  &gt;Date: Wed, 24 Sep 86 18:01:18 pdt
  &gt;From: Dave Benson &lt;benson%wsu.csnet@CSNET-RELAY.ARPA&gt;
  &gt;                              ... Since the Viking Landers were the
  &gt;first man-made objects to land on Mars, ...

Actually, the first man-made object to land on Mars was a Russian craft that
sent about 30 seconds of carrier signal and then died.  Nobody knows exactly
what happened to it.

Courtenay Footman		ARPA:	cpf@lnsvax.tn.cornell.edu
Lab. of Nuclear Studies		Usenet:	cornell!lnsvax!cpf
Cornell University		Bitnet: cpf%lnsvax.tn.cornell.edu@WISCVM.BITNET

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.68.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.70.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-56</DOCNO>
<DOCOLDNO>IA012-000123-B023-294</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.70.html 128.240.150.127 19970217005042 text/html 13016
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:49:09 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 70</TITLE>
<LINK REL="Prev" HREF="/Risks/3.69.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.71.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.69.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.71.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 70</H1>
<H2> Monday, 29 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Deliberate overrides? 
</A>
<DD>
<A HREF="#subj1.1">
Scott E. Preece
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Multiple causes and where to place the "blame" 
</A>
<DD>
<A HREF="#subj2.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  The Art of "Science" and its Computers 
</A>
<DD>
<A HREF="#subj3.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  No-lock Brakes 
</A>
<DD>
<A HREF="#subj4.1">
Peter Ladkin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Sanity in Automating Keyword Abstracting 
</A>
<DD>
<A HREF="#subj5.1">
Brint Cooper
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  The Network Is Getting Old? 
</A>
<DD>
<A HREF="#subj6.1">
PGN
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Deliberate overrides?
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%ccvaxa@GSWD-VMS.ARPA">
preece%ccvaxa@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Mon, 29 Sep 86 09:17:27 cdt
</i><PRE>

  /**** ccvaxa:fa.risks / LIN@XX.LCS.MI /  2:47 am  Sep 29, 1986 ****/
  &gt; From: Charles R. Fry &lt;Chucko at GODZILLA.SCH.Symbolics.COM&gt;
  &gt; 
  &gt; &gt; No matter how many automated controls we install on cars (and airplanes) 
  &gt; &gt; to prevent operators from exceeding their vehicles' limits, there will
  &gt; &gt; always be a need to allow the deliberate violation of these limits.  

  &gt; From: LIN@XX.LCS.MIT.EDU
 
  &gt; This discussion about allowing overrides to programmed safety limits
  &gt; worries me.

One of the nice things about computer-driven controls, as opposed to
mechanical controls, is that they allow you to be less draconian in
specifying limits.  You don't have to build a bomb release that can never,
ever allow the pilot to drop a bomb while inverted; you can instead say "You
know, if I do what you've asked, the bomb is going to fall on the wing and
probably strip off your starboard control surfaces." and the pilot can say
"Yes, I know, do it anyway."  And by providing a (safety-covered and
hard-to-reach) button that says "Override control limits" you can even make
it possible for the pilot to say in advance that at this point she feels the
danger in overriding the controls is smaller than the danger in not
overriding the controls.

The reason we think it's reasonable to require automated controls to allow
exceptions is that we know the automated controls have allowed and
encouraged us to incorporate limits and we recognize that (1) those limits
may have erred on the side of normal safety, (2) since the systems are new,
the necessary operational envelope may not be known, and (3) the interaction
of the limits may create unanticipated problems.  Yes, users should be
allowed to override automated controls in almost all cases AND designers
should make very, very sure that the effort to override is proportional to
the danger of the override.  In many cases there should also be logging of
overrides, so that operators, maintainers, and designers have an opportunity
to notice that actual use seems to be violating the design assumptions.

I wonder how many readers of this list [NO, this is NOT a survey, DON'T
write to tell me] drive cars with manual transmissions precisely because
they want to be in control, want to know that doing x and y will result in
the car doing z, without any control system in the way to place limits on
actions or responses...

scott preece  gould/csd - urbana  uucp:	ihnp4!uiucdcs!ccvaxa!preece

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Multiple causes and where to place the "blame"
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Mon 29 Sep 86 21:36:04-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

Today's AP noted that the FAA may cite the pilot of the Grumman Yankee for
being in restricted airspace, at precisely the moment of the crash between
the Aeromexico jet and the Piper Archer (which was also in restricted
airspace), which distracted the air traffic controller from attending to the
jet and the Piper (the absence of whose altitude information was also a
factor) -- at precisely the time the crash occurred.  The controller did
tell the Grumman pilot that he was in restricted air space, but then granted
him permission to continue (and that negotiation took two precious minutes
away from his attention to the jet).

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
The Art of "Science" and its Computers 
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Mon 29 Sep 86 16:36:51-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

A computer of the AAAS sent out renewal bills for SCIENCE to some subscribers:
                 Subscription price $6647, Postage $732, 
                 Voluntary contribution $10, Total $5437.
The subscription price during 1986 was $60, and the accompanying letter from
the president of AAAS noted that inflation had required an increase.  A quite
amusing editorial by Daniel Koshland, Jr. in the 26 Sept 86 issue wondered 
whether any people would rush out to take advantage of the incorrect addition.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
No-lock Brakes
</A>
</H3>
<address>
Peter Ladkin
&lt;<A HREF="mailto:ladkin@kestrel.ARPA ">
ladkin@kestrel.ARPA 
</A>&gt;
</address>
<i>
Mon, 29 Sep 86 14:47:16 pdt
</i><PRE>

A minor correction to Chuck Fry's comments - the first anti-skid system on a
production car was installed on a Jensen (pre-dating the Jensen-Healey) in
the 60s. It was made by Lockheed, and derived directly from aircraft systems.

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
 Sanity in Automating Keyword Abstracting
</A>
</H3>
<address>
    Brint Cooper 
&lt;<A HREF="mailto:abc@BRL.ARPA">
abc@BRL.ARPA
</A>&gt;
</address>
<i>
Mon, 29 Sep 86 15:09:02 EDT
</i><PRE>

Here is an example of a risk associated with the use of computers.  The risk
is to the accurate dissemination of information and is caused by faulty
programming (programmers?).

Today, the BRL Librarian informed us that the Defense Technical Information
Center (DTIC, formerly known as DDC) now requires that the titles of our
technical reports (the principal products of a research lab such as the BRL)
be written so that the "keywords" are found in the first five words of the
title.

Thus, a report which formerly was titled "Communication Modeling in the
Artillery Control Experiment" with keywords "error control," "tactical
communications," "networks," and "modeling" would have to be titled
"Modeling, Tactical Communications, and Error Control Networks," thus
sounding like, as one chap here put it, "a four volume set by Harry van
Trees" instead of a 25 page report.

Exact text of our librarian's notice follows:

  &gt; We have been advised by DTIC that the titles of technical reports should
  &gt; be designed with the key words positioned in the first five words of the
  &gt; title. This is because only the first five words are used in a title
  &gt; search in the DTIC electronic data base DROLS.(DEFENSE RESEARCH ON LINE
  &gt; SYSTEM).  Important to know (and remember) is that articles are counted
  &gt; in those first five words.  Therefore a report entitled "A report of the
  &gt; effect........." will not have any key words picked up in a title
  &gt; search.  If you currently have a report in editing, we will review it
  &gt; and if it it does not comply with the DTIC recommendation we will advise
  &gt; you so that it can be reworked.  If you currently have a report under
  &gt; review or in writing you might like to think about a title change.
  &gt; Please give this widest possible dissemination.
  
Brint
ARPA:  abc@brl.arpa    UUCP:  ...{seismo,unc,,decvax,cbosgd}!brl-smoke!abc

Dr Brinton Cooper, U.S. Army Ballistic Research Laboratory
Attn: SLCBR-SE-C (Cooper),  Aberdeen Proving Ground, MD  21005-5066
Offc:    301-278-6883    AV:  298-6883     FTS: 939-6883   Home: 301-879-8927

               [ASK NOT WHAT YOUR COMPUTER CAN DO FOR YOU, 
                ASK WHAT YOU CAN DO FOR YOUR COMPUTER!  
                I started to add a diatribe, but gave up in annoyance.  PGN]

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
The Network Is Getting Old?
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Mon 29 Sep 86 09:50:22-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

There has been an enormous amount of difficulty in dealing with the ARPANET
since early September, perhaps related to the installation of new IMP
software and subsequent patches when the new release did not work properly.
I had devastating problems TELNETing from four different East-coast hosts to
three different SRI systems (irrespective of whether there was a gateway at
my end, and even with no loads on either system).  No answers have been
forthcoming from any of our gurus, so the problems remain pervasive and
painful.  I am also getting a rash of returned net-barfed RISKS mail (as
well as RISKS filling up peoples' directories when they go on vacation).
There are also local problems.  Last Friday a message from SRI-STRIPE to
SRI-CSL took 7 hours to be delivered, while TN and FTP between those two
machines worked fine.

  From: David L. Edwards &lt;DLE@SRI-STRIPE.ARPA&gt;
  It is becoming increasingly apparent that there are serious network
  problems.  There has been some discussion of this on the TCPIP forum.
  Network delays, failed connections, inability to make connections etc. are
  being reported by hosts all over the network.

  BillW has noticed that direct communications between SRI and SU are bad.  In
  your case there are one or two gateways involved in addition to the IMPs.
  The mailer recently reported 750+ attempted connections with 670+ failures.

Old age?  Software rot?  Incompatible changes to net software?  Saturation?
Who knows.  Stay tuned.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.69.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.71.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-57</DOCNO>
<DOCOLDNO>IA012-000123-B023-316</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.71.html 128.240.150.127 19970217005058 text/html 23806
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:49:25 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 71</TITLE>
<LINK REL="Prev" HREF="/Risks/3.70.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.72.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.70.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.72.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 71</H1>
<H2>Tuesday, 30 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Deliberate overrides? 
</A>
<DD>
<A HREF="#subj1.1">
Herb Lin
</A><br>
<A HREF="#subj1.2">
 Alan M. Marcum
</A><br>
<A HREF="#subj1.3">
 Eugene Miya
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  "Friendly" missiles and computer error - more on the Exocet 
</A>
<DD>
<A HREF="#subj2.1">
Robert Stroud
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: Reliability, complexity, and confidence in SDI 
</A>
<DD>
<A HREF="#subj3.1">
Michal Young
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  My understanding of "path" and "bathtub curve" 
</A>
<DD>
<A HREF="#subj4.1">
Bob Estell
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  More artificial than intelligent? (Autokeywords) 
</A>
<DD>
<A HREF="#subj5.1">
Bob Estell
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  A Viking lander query 
</A>
<DD>
<A HREF="#subj6.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Note on ARPANET congestion 
</A>
<DD>
<A HREF="#subj7.1">
Nancy Cassidy
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Indeed, the network is getting old 
</A>
<DD>
<A HREF="#subj8.1">
Jonathan Young
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj9">
  [MULTIPLE COPIES OF <A HREF="/Risks/3.70.html">RISKS-3.70</A>?  Some of you were lucky enough to get 4   |
</A>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj10">
   copies, 2 after last reboot!  Foonly Wizard David Poole strikes again?  /
</A>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj11">
   Retries may also be due to net delays, timeouts in ACKS?  Sorry.  PGN] /
</A>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Deliberate overrides?
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Tue, 30 Sep 1986  00:58 EDT
</i><PRE>

    From: Scott E. Preece &lt;preece%ccvaxa at GSWD-VMS.ARPA&gt;

    ... users should be allowed to override automated controls in almost 
    all cases AND designers should make very, very sure that the effort to
    override is proportional to the danger of the override.  

The problem rarely comes with a competent person who wants to
override.  The real problems are those people who are incompetent who
want to override.  I have faith (should I?) in pilots as a rule, so I
am generally in favor of manual overrides in airplanes.  I don't in
drivers as a rule, so I am much less favorbaly disposed to overrides
in autos.  I recognize that in some instances, the unavailability of
an override will cause difficulties for a competent operator, but it
will also surely prevent a number of difficulties for incompetent
operators, and on balance, I think that that trade is worth while in
autos, but probably not in planes.

</PRE>
<HR><H3><A NAME="subj1.2">
Deliberate Overrides - mechanical, even
</A>
</H3>
<address>
Alan M. Marcum, Consulting
&lt;<A HREF="mailto:marcum@Sun.COM ">
marcum@Sun.COM 
</A>&gt;
</address>
<i>
Tue, 30 Sep 86 09:56:41 PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

Though perhaps not strictly computer related, I thought the following
might be of interest to the Risks forum.

The Piper Arrow is a four-place, single-engine airplane with retractable
landing gear.  Piper has a wonderful airspeed switch in the landing
gear system which will automatically lower the gear if the airspeed is
too low.  One side effect of this is that during a low-speed climb, the
gear may drop (or might never come up during takeoff).  Climbing with
the gear down will seriously erode climb performance (up to 500 feet
per minute, with max. climb around 1000 fpm), just when you want
MAXIMUM climb performance!

To overcome this, Piper installed a "gear override" handle, which can
be latched in the OVERRIDE position.  Many, many Arrow pilots routinely
take off with the override engaged, to ensure that the gear retract
when the pilot wants them up.

Why did Piper install this mechanism?  The reason most often cited is
to help prevent gear-up landings.  It is interesting that a number of
Arrow pilots have landed gear-up, having forgotten to disengage the
override after having gotten into the habit of depending on it.

I've flown retractable singles built by Piper, Cessna, Beech, and
Mooney.  Piper is the only one with the airspeed override.  All
manufacturers, including Piper, have a warning horn which sounds if
power is reduced past some threshhold with the gear up, though.

What's the point?  In my opinion, the automatic system increases pilot
workload during a critical time (takeoff and initial climb).  It's not
something on which one should EVER depend: it might fail.  It's prone
to lower the gear at inopportune moments -- times a pilot would
absolutely never lower the gear; times when having the gear down is
seriously more dangerous than having the gear up (certain emergency
situations, for example).  And, you need the override.

Certainly, performance- or functionality-limiting devices can be
useful.  They must be thought through carefully, and considered as part
of the whole, rather than as an isolated system.

Alan M. Marcum				Sun Microsystems, Technical Consulting
...!{dual,decvax}!sun!nescorna!marcum	Mountain View, California

</PRE>
<HR><H3><A NAME="subj1.3">
Re: Deliberate overrides and multiple causes (blame)
</A>
</H3>
<address>
Eugene Miya
&lt;<A HREF="mailto:eugene@AMES-NAS.ARPA ">
eugene@AMES-NAS.ARPA 
</A>&gt;
</address>
<i>
30 Sep 1986 1330-PDT (Tuesday)
</i><PRE>

  From: "Scott E. Preece" &lt;preece%ccvaxa@GSWD-VMS.ARPA&gt;
  &gt;  /**** ccvaxa:fa.risks / LIN@XX.LCS.MI /  2:47 am  Sep 29, 1986 ****/
  &gt;  &gt; From: Charles R. Fry &lt;Chucko at GODZILLA.SCH.Symbolics.COM&gt; [...]
  &gt;  &gt; From: LIN@XX.LCS.MIT.EDU [...]

Just a point of information: the Soviets just announced that they planned
to get rid of reactor control rod overrides, and that one manual override
at TMI accentuated the problem ("But overall system worked" summarizing
the pro-nuclear viewpoint).

Is it possible to write a rule without exception?

  &gt;"You know, if I do what you've asked, the bomb is going to fall on the wing 
  &gt; and probably strip off your starboard control surfaces."
  &gt;"Yes, I know, do it anyway."

Yes, I can see this happening, but it reminds me of the film Dark Star.

"Talk to the bomb . . . about phenomenology....."

--eugene miya,   NASA Ames Research Center,   eugene@ames-aurora.ARPA

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Re: "Friendly" missiles and computer error - more on the Exocet
</A>
</H3>
<address>
Robert Stroud 
&lt;<A HREF="mailto:robert%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK">
robert%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Tue, 30 Sep 86 14:43:24 gmt
</i><PRE>

There is a very interesting BBC TV documentary in the Horizon series called
"In the wake of HMS Sheffield" which is well worth seeing if you get the
chance. It discusses the failures in technology during the Falklands war
and the lessons which have been learnt from them, and includes interviews
with participants on both sides.

Naturally the fate of HMS Sheffield features prominently, and the chronology
given by Rob MacLachlan matches the program in most respects. However, I'm
afraid it says nothing about the Exocet homing signal being friendly - I was
specifically looking out for this. Instead, according to the documentary, the
device which should have detected the homing signal is situated next to the
satellite transmission device and was simply swamped by the signal from a
telephone call to London in progress at the time - this backs up Peter's
definitive account.

A couple of other points from the documentary are worth mentioning. Chaff
was indeed effective in helping one ship avoid an Exocet (I forget which one)
but it is by no means fool proof. The fuse needs to be set manually on deck
and must be exact, taking into account lots of factors like wind direction,
ship's course, distance from missile, etc. If you get it wrong, the distraction
comes too early or too late. There was a nice piece of computer graphics
showing the difference half a second could make - needless to say, they are
working on an automatic fuse!

The Argentinian planes were able to avoid radar detection using a technique
called "pecking the lobes". Basically they exploit the shape of the radar
cone and the curvature of the earth by flying level until they detect
a radar signal, then losing height and repeating the process. As Rob said,
they only need to rise up high enough to be detected at the last minute
when they fire the Exocets and turn for home - even this trace would only
be visible very briefly on the radar display and could easily be missed.
Thereafter the Exocets are silent until the last few seconds when they
lock onto the target to make last minute course corrections.

This problem has been dealt with by building radar devices that can be used
from helicopters several thousand feet up so they can see further over the
horizon.

There was also a discussion about whether it would be feasible to install
anti-missile weapons in cargo ships such as the Atlantic Conveyor (sunk
twice by the Argentinians with Exocets who mistook it for one of the aircraft
carriers). Apparently, installing a weapon would be possible, but to be
effective it would need all the command &amp; control computer systems as well to
keep track of everything else that was going on, and that would not be
feasible.

Robert Stroud, Computing Laboratory, University of Newcastle upon Tyne.

ARPA robert%cheviot.newcastle@cs.ucl.ac.uk (or ucl-cs.ARPA)
UUCP ...!ukc!cheviot!robert

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: Reliability, complexity, and confidence in SDI software
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Mon, 29 Sep 86 22:57:46 -0800
From: Michal Young &lt;young@ICSC.UCI.EDU&gt;

Bob's message, and some of the replies, seem to be using the term `path' in
a sense I am unfamiliar with, since they refer to (large but) finite numbers
of paths in software.  If software contains loops, isn't the number of paths
infinite?  And therefore, after any finite amount of use, isn't the percentage
of paths tested actually zero?  If there is another commonly accepted
meaning of `path' through a piece of software, please fill me in on it.

I have a similar problem with the term `state.'  It seems to be used to
refer to major states like `ready to run' and `running', whereas a fault may
be sensitive to smaller-grain state like `i = 0 and j &gt; 999'.  It may be 
possible to design software to have a small number of major states, but 
the number of possible data+control states of any useful program is very
large indeed.

&gt;    BOTTOM LINEs:
&gt;
&gt;    1. The curve for debugging software has a DOWNslope and length that is 
&gt;    some function of the number of possible paths through the code.
&gt;    ... 
&gt;    3. Confidence builds as one approaches the 90% [or other arbitrary level]
&gt;    point in testing the number of possible paths.
&gt;
&gt;    4. The reason that we haven't built confidence in the past is that we've
&gt;    often run thousands of hours, without knowing either:
&gt;
&gt;     a. how many paths got tested; or
&gt;     b. how many paths remained untested.

By the terminology I am familiar with, 3 is "never" and 4(b) is 
"an infinite number" for every useful piece of software, always.

--Michal Young,   UC Irvine,   young@ics.uci.edu

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
My understanding of "path" and "bathtub curve"
</A>
</H3>
<address>
"ESTELL ROBERT G" 
&lt;<A HREF="mailto:estell@nwc-143b.ARPA">
estell@nwc-143b.ARPA
</A>&gt;
</address>
<i>
30 Sep 86 09:04:00 PST
</i><PRE>
To: "young" &lt;young@icsc.uci.edu&gt;
cc: risks@csl.sri.com

I don't claim to use "path" in a way that may be common in graduate courses
in software engineering.  My use is based on the highway map analog; e.g.,
there are many paths through the LA freeway system that one might take
from Irvine to Mammoth on a ski weekend.  One can drive any of the paths
any number of times [loops?]; for lack of good all-way interchanges, some
paths might not work well [design errors?]; because of temporary traffic 
congestion, some paths might be troublesome on some days [data sensitivity?].

I agree that software *is* sensitive to "minor" state conditions; e.g., loop
counts of "zero" and "n+1" [where "n" was the intended limit] are notorious.
I contend that it *should NOT* be; i.e., that proper design and testing can
reduce such errors to a tolerable range.  A goal of good software design is 
to construct "modules" whose internal states are insensitive to all legal
arguments, and whose entry code screens out all illegal arguments; at least
that's my personal understanding of one [of several] key benefits of "data 
hiding" and "defensive programming."

Another respondent disputed the "downslope" claim, because his experience
was that the error rate degenerates to some constant level.  Well, all the
bathtubs I've seen do have bottoms.  One can expect some non-zero number of
bugs to persist; let's only hope that it's tolerably low - lower than during
"alpha testing."  Finally, if some "new release" goes badly sour [e.g., the
"new" ARPANET s/w?]  because it tries to "add on" [vice "design in?"] new
features, maybe that's the equivalent to the "wear out" upslope in mechanical 
designs.  That may be what we've seen with some older operating systems that 
tried to "add on" time sharing, security, or multi-processor logic.

Bob

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
More artificial than intelligent? (Autokeywords)
</A>
</H3>
<address>
"ESTELL ROBERT G" 
&lt;<A HREF="mailto:estell@nwc-143b.ARPA">
estell@nwc-143b.ARPA
</A>&gt;
</address>
<i>
30 Sep 86 10:14:00 PST
</i><PRE>
To: "risks" &lt;risks@csl.sri.com&gt;

Computer titles on documents are going to take over. Don't fight it.
It could be worse; they might have to be "bar coded."  
Instead, just use "human" sub-titles; e.g.
ANTLERS, TREETOPS, MYSTERY; (or "Who Goosed the Moose?")

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
A Viking lander query
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Tue 30 Sep 86 20:26:06-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

Is there a RISKS reader who can report on what the Viking lander software
really did?  Was it used for landing? or just for communication and control
of onboard equipment?  "Working the first time" would be much more
impressive if it were for landing, whereas the rest is more easily testable
on the ground.

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Note on ARPANET congestion
</A>
</H3>
<address>
Nancy Cassidy 
&lt;<A HREF="mailto:ncassidy@ccm.bbn.com">
ncassidy@ccm.bbn.com
</A>&gt;
</address>
<i>
Mon, 22 Sep 86 12:22:13 EDT
</i><PRE>

=====================================================================
Report on Investigation Request #: IR86-0051-ARPANET-SY   Report #: 1
Date of Report:  9/22/86                                  Priority: 2
=====================================================================
Reporting:  open
=====================================================================

IR Title:  ARPANET congestion

Summary of Problem:
------------------

ARPANET  users  are experiencing unacceptable delays caused by network
congestion.   This  problem  is  believed  to  be  attributed  to  the
network's  dramatic  increase  in traffic at the rate of 33% in recent
months.  However, additional bandwidth and PSNs have not been added to
the network to sufficiently support this constant increase in traffic.
As a result, severe congestion causes users to experience long delays.

Another problem exists for users on MILNET who must access the  BBNNET
(especially  users  on DDN1 and DDN2).  Currently, there is no gateway
between the MILNET and BBNNET.  Instead, traffic  passes  through  the
ARPANET  to  an  ARPANET  Gateway  in  order to reach the BBNNET.  The
critical congestion problems the ARPANET is experiencing causes TELNET
and FTP connections to time out and mail messages from MILNET hosts to
take up to 2-3 days to be delivered to BBNNET hosts.

One other result of network  congestion  is  the  Monitoring  Center's
ability  to  effectively  monitor operations.  The number of traps and
status messages has increased proportionately to the severity  of  the
congestion.   This  dramatic  increase in network messages received by
the MC consumes CPU space and slows down C/70 performance to the point
where it affects monitoring and control of the network.

        [Further reporting and recommendations truncated...]

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
Indeed, the network is getting old
</A>
</H3>
<address>
Jonathan Young 
&lt;<A HREF="mailto:young-jonathan@YALE.ARPA">
young-jonathan@YALE.ARPA
</A>&gt;
</address>
<i>
Tue, 30 Sep 86 12:59:47 edt
</i><PRE>
To: Neumann@CSL.SRI.COM

Here at Yale we have been aware of two problems:  host tables are
overflowing and mail is bouncing.  Actually, we think that SENDMAIL
connections (more often from BSD4.3 machines) are timing out and retrying.
This has resulted in dozens of copies of certain messages.  I enclose a copy
of a message from our network administrator.

I'm very surprised that others haven't commented about the virtual
unavailability of the ARPANET.  On the other hand, Yale's connection is via
a 9600 BAUD LINE to Harvard.  Sigh.
                                        Jonathan (YOUNG-JONATHAN@YALE.ARPA)

              [Is that anywhere near the 50 YARD LINE?  (rELIability!)  PGN]

  From: Morrow Long &lt;long@YALE.ARPA&gt;
  To: department
  Subject: ARPAnet mail problems

  We began to see a large problem with repeating incoming arpanet mail
  messages in August (when cheops was still yale.arpa - the mail name host) -
  especially in the department bboard where a MIT site was flooding our
  newsgroups and bulletin boards with the mail internet bulletin board
  messages.  After christening Yale-Eli as Yale.ARPA (a dedicated SMTP mail
  server) we have continued to experience the problem with repeating messages
  emanating from some hosts.

  From statistics we have gathered on the problem we have noticed that many of
  the problem hosts are running 4.3bsd.  Our problem may not be due to 4.3bsd
  TCP/IP (nor Sendmail/SMTP) but may be brought on by problems with arpanet
  congestion/delays wrecking session protocols.

  To alleviate and eventually rectify the problem we have taken the following
  steps:

  1. We have notified the administrators of the remote sites to
     remove the repeating messages from their spool queues.

  2. We are tracing Sendmail/SMTP debugging messages to	session
     logfiles to capture maximum information.

  3. Luis has agreed to act as moderator for one of the most troublesome
     groups ('apollo@yale'), screening out duplicates before reposting them
     to the world.

  4. A 'sweep' daemon has been created and installed on Eli to check for
     duplicate messages to bboards and mailing list in the mail queue and
     remove them for exact matches on Message-ID, sender and subject.  At least
     one copy is always allowed through.  Even this drastic program will allow
     repeat messages if they arrive outside of the queue sweep window for
     duplicates.

  5. We will be investigating the 4.3bsd Unix sendmail program for
     incompatibilities with our SMTP servers.

				H. Morrow Long, Computing Facility

       [This is just the tip of the iceberg on reports and messages.  The
        TCP-IP BBOARD IS OVERFLOWING.  All sorts of contributing factors are
        being discussed.  Dramatic increase in net traffic, total saturation of
        the IMPs, hosts that stick with 4.2bsd instead of 4.3, weak gateways,
        mail distributions to multiple users at the same site, etc.  Who knows?
        No one yet.  The total collapse of the ARPANET on 27 Oct 1980 was only 
        for four hours or so, and has not happened since; this fiasco has been 
        going on for at least three weeks, and the network seems to be
        rotting completely.

        So, if you got this far in the issue, and are getting a private
        copy of RISKS, please let me know if your site now has a BBOARD or
        redistribution and you can live without a private copy of RISKS.
        (Each time I suggest that I actually do get a few willing people.)
        PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.70.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.72.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-58</DOCNO>
<DOCOLDNO>IA012-000123-B023-338</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.72.html 128.240.150.127 19970217005114 text/html 15771
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:49:41 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 72</TITLE>
<LINK REL="Prev" HREF="/Risks/3.71.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.73.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.71.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.73.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 72</H1>
<H2>Wednesday, 1 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Viking Lander 
</A>
<DD>
<A HREF="#subj1.1">
Nancy Leveson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Deliberate override 
</A>
<DD>
<A HREF="#subj2.1">
George Adams
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Overriding overrides 
</A>
<DD>
<A HREF="#subj3.1">
Peter Ladkin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  A propos landing gear 
</A>
<DD>
<A HREF="#subj4.1">
Peter Ladkin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Paths in Testing 
</A>
<DD>
<A HREF="#subj5.1">
Mark S. Day
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Confidence in software via fault expectations (<A HREF="/Risks/3.69.html">RISKS-3.69</A>) 
</A>
<DD>
<A HREF="#subj6.1">
Darrel VanBuer
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Viking Lander
</A>
</H3>
<address>
&lt;<A HREF="mailto:leveson@sei.cmu.edu">
leveson@sei.cmu.edu
</A>&gt;
</address>
<i>
Wed, 1 Oct 86 11:44:21 edt
</i><PRE>

I have spoken to a man who was involved in the building of the Viking
Lander software.  He told me that the on-board software consisted of
three basic functions:  terminal descent software, some transmission
of data, and a sequence of operations while the lander was on the ground.
The Honeywell computer had a memory of about 18,000 24-bit words.  It
was programmed in assembly language.  There was an interrupt-driven
executive program.  What is interesting is his claim that there were
several bugs in the software.  There just were not any bugs that caused
an abort of the mission or other serious consequences.  The programs
were, in fact, overloaded on the way to Mars because of discovery of
some problems.  This man claims that there was a great unwillingness 
to admit that there were bugs in the software and so some remain 
undocumented or difficult to find in the documentation.  But the software
was NOT bug-free.

There was also ground system software which was responsible for
auxiliary computations such as pointing the antenna.  

One of the reasons I have come into contact with this software is that
I have been asked to participate in a fault tolerance experiment which
will use the Viking lander as an example.  In examining the software
in the light of using it for such an experiment, Janet Dunham at RTI
has found it not to be adequate in its present form because it is too
small and straightforward.  In fact, she is presently writing a 
specification of the terminal descent portion which adds complexity to
the problem to make it more interesting and complex for the experiment
(she is afraid that there just will not be enough errors made given the
original problem).  She says that the navigation portion of the 
terminal descent software (which is the most complex part) is only 
about 100 lines of Ada code.  

The point to remember is that not all software is alike.
Small, straightforward problems with very little complexity in the logic
(e.g., just a series of mathematical equations) may not say much about
the reliability of large, complex systems.  We know that scaling-up
is a very serious problem in software engineering.  In fact, it has been
suggested that small vs. large software projects have very few similarities.
It should also be noted that the avionics part of this relatively small 
and relatively simple software system cost $18,000,000 to build.  Although
the 18,000 words of memory were overloaded a few times, the amount of 
money spent per line of code was extremely high.  

I worry when anecdotal evidence about one software project is used as
"proof" about what will happen with general software projects.  There 
just are too many independent variables and factors to do this with 
confidence.  And, in fact, we do not even know for sure what the important
variables are.

     Nancy Leveson
     Info. &amp; Computer Science Dept.
     University of California, Irvine

     (arpanet address, for angry replies, is nancy@ics.uci.edu despite 
      evidence above to the contrary)

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Deliberate override
</A>
</H3>
<address>
George Adams 
&lt;<A HREF="mailto:gba@riacs.edu">
gba@riacs.edu
</A>&gt;
</address>
<i>
Wed, 1 Oct 86 10:18:43 pdt
</i><PRE>

   Even automobiles might appropriately have overrides of automated
controls, and even of automated safety systems.  I have only read about
the following automobile item and wish I had the opportunity to verify
it, but it seems reasonable.
   Consider the anti-lock braking systems now becoming more widely
available in automobiles.  The driver can apply a constant input to the
brake pedal, but modulated braking forces are applied at the wheels so
that the wheels do not lock.  Many have probably seen the ad on tele-
vision in which the car with anti-lock brakes sucessfully negotiates the
turn on wet pavement while coming to a rapid stop without skidding out of
control.  Yet, perhaps such vehicles should have a switch to disable
anti-lock and allow conventional braking.  Imaging trying to stop quickly
with anti-lock brakes on a gravel road.
   Even if an incompetent driver forgot to enable the system on hard
pavement, performance would be no worse than now common.  Without the
switch a competent driver might hit that cow instead of stopping in time.
   Regarding aircraft, a report on the midair collision involving the
Aero Mexico flight said that the flight crew applied thrust reversers
after the collision.  This seems like a creative response, and one that
might easily be disallowed in a more automated aircraft in which a
check for weight on extended landing gear was a prerequisite for thrust
reversal.  While thrust reversal had no benefit for the Aero Mexico
flight itself, perhaps it reduced impact speed and consequently reduced
the extent of damage on the ground.
   A vehicle and its operator are a system.  By automating vehicle systems
we can adapt operator workload to better match the capabilities of human
beings and make it possible for an operator to do a better job.  We can
also automate to limit operator options for coping with non-routine
situations and impede rapid operator override, thereby making a more
expert system and also a less generally capable one.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Overriding overrides
</A>
</H3>
<address>
Peter Ladkin
&lt;<A HREF="mailto:ladkin@kestrel.ARPA ">
ladkin@kestrel.ARPA 
</A>&gt;
</address>
<i>
Wed, 1 Oct 86 17:08:22 pdt
</i><PRE>

An example of a deliberate override that led to disaster:
An Eastern Airlines 727 crashed in Pennsylvania with considerable
loss of life, when the pilots were completing an approach in
instrument conditions (ground fog), 1000 feet lower than they
should have been at that stage.
They overrode the altitude alert system when it gave warning.

Peter Ladkin

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
A propos landing gear
</A>
</H3>
<address>
Peter Ladkin
&lt;<A HREF="mailto:ladkin@kestrel.ARPA ">
ladkin@kestrel.ARPA 
</A>&gt;
</address>
<i>
Wed, 1 Oct 86 16:55:14 pdt
</i><PRE>

Alan Marcum's comment on gear overrides in the Arrow reminded me
of a recent incident in my flying club (and his, too).
The Beech Duchess, a light training twin, has an override that
maintains the landing gear *down* while there is weight on the
wheels, ostensibly to prevent the pilot from retracting the
gear while on the ground (this is a problem that Beech has in
some of its airplanes, since they chose to use a non-standard
location for gear and flap switches, encouraging a pilot to
mistake one for the other).
Pilots can get into the habit of *retracting* the gear before
takeoff, secure in the knowledge that it will remain down
until weight is lifted off the wheels, whence it will commence
retracting. This has the major advantages that it's one less
thing to do during takeoff, allowing more concentration on
flying, and the gear is retracted at the earliest possible
moment, allowing maximum climb performance, which is important
in case an engine fails at this critical stage.
Can anyone guess the disadvantage of this procedure yet?

Our club pilot, on his ATP check ride, with an FAA inspector
aboard, suffered nosewheel collapse on take-off, and dinged
the nose, and both props, necessitating an expensive repair
and engine rebuild. Thankfully, all walked away unharmed.
It was a windy day.

It is popularly supposed that the premature retraction technique
was used, and a gust of wind near rotation speed caused the weight
to be lifted off the nosewheel. When the plane settled, the 
retraction had activated, and the lock had disengaged, 
allowing the weight to collapse the nosewheel.

Both pilots assure that the gear switch was in the down position,
contrary to the popular supposition.

All gear systems in the aircraft were functioning normally when
tested after the accident.

The relevance to Risks? The system is simple, and understood in
its entirety by all competent users. The technique of
premature retraction has advantages. It's not clear that a
gedankenexperiment could predict the disadvantage.

Peter Ladkin

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Paths in Testing
</A>
</H3>
<address>
Mark S. Day 
&lt;<A HREF="mailto:MDAY@XX.LCS.MIT.EDU">
MDAY@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Wed 1 Oct 86 11:57:32-EDT
</i><PRE>
To: RISKS@CSL.SRI.COM
In-Reply-To: &lt;8610010515.AA25307@ATHENA&gt;
Message-ID: &lt;12243347691.50.MDAY@XX.LCS.MIT.EDU&gt;

"Paths" as used in the discussion of "path coverage" are probably
intended to be what are called "basis paths."  A piece of code with
loops can indeed have an infinite number of paths, but every path is a
linear combination of a much smaller set of paths.  Testing that
covers every basis path and also tests each loop using "engineer's
induction" ("zero, one, two, three... good enough for me") is
significantly better than random testing to "see what breaks" and much
more feasible than trying to test all the combinations of basis paths.
The McCabe or cyclomatic complexity metric defines the number of basis
paths through a piece of code; see

   T.J. McCabe, "A Complexity Measure", IEEE Transactions on Software
   Engineering SE-2, 4 (Dec 1976) pp. 308-320.

A quick approximation of McCabe complexity is that straight-line code
has a complexity of 1 (obviously, I guess) and most control statements
(if-then, if-then-else, while, repeat, for...) each add 1 to the
complexity.  An n-way case statement adds n-1 paths to the "straight"
path, so it adds n-1 to the complexity.  This approximation only applies
to code with no gotos.

The IEEE Computer Society puts out a tutorial volume called
"Structured Testing" that includes the previously-cited paper and a
number of other related articles, including a heuristic for using the
McCabe complexity to select test paths.

--Mark

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Re: Confidence in software via fault expectations (<A HREF="/Risks/3.69.html">RISKS-3.69</A>)
</A>
</H3>
<address>
Darrel VanBuer
&lt;<A HREF="mailto:hplabs!sdcrdcf!darrelj@ucbvax.Berkeley.EDU ">
hplabs!sdcrdcf!darrelj@ucbvax.Berkeley.EDU 
</A>&gt;
</address>
<i>
Tue, 30 Sep 86 18:37:27 pdt
</i><PRE>

  &gt;From: Dave Benson &lt;benson%wsu.csnet@CSNET-RELAY.ARPA&gt;
  &gt;...  Software ordinarily has no manufacturing defects and the
  &gt;usual way ordinary backups are done insures that most software does not
  &gt;wear out before it becomes obsolete. 

The thing is software DOES wear out in the sense that it loses its ability
to function because the world continues to change around it (maybe a bit
because the pattern of bits does NOT wear out): e.g. operating systems which
have gone psychotic because the number of bits used to represent a date
because compatible hardware has continued to run far longer than designers
of software and hardware anticipate (how many IBM-360 programs will correctly
handle the fact that year 2100 is NOT a leap year, but still be running inside
some emulation/automatic retranslation) or financial software unable to deal
with 1000 fold inflation because all the numbers overflow...

Darrel J. Van Buer, PhD,  System Development Corp.,  2525 Colorado Ave
Santa Monica, CA 90406  (213)820-4111 x5449             /     !sdcrdcf!darrelj
...{allegra,burdvax,cbosgd,hplabs,ihnp4,orstcs,sdcsvax,ucla-cs,akgua}
                                            
       [This one is getting to be like the "YES, VIRGINIA, THERE IS A
        SANTA CLAUS" letter that used to appear each year in the Herald
        Tribune.  But I keep reprinting the recurrences because some of 
        you still don't believe it.  There is no sanity clause.  PGN]

End of RISKS-FORUM Digest
************************
-------


<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.71.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.73.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-59</DOCNO>
<DOCOLDNO>IA012-000123-B023-360</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.73.html 128.240.150.127 19970217005126 text/html 14323
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:49:55 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 73</TITLE>
<LINK REL="Prev" HREF="/Risks/3.72.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.74.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.72.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.74.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 73</H1>
<H2> Thursday, 2 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Lessons from Viking Lander software 
</A>
<DD>
<A HREF="#subj1.1">
Bob Estell
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Software wears out? 
</A>
<DD>
<A HREF="#subj2.1">
Rob Austein
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Wrongful eviction through computer error 
</A>
<DD>
<A HREF="#subj3.1">
Bill Janssen
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Deliberate override 
</A>
<DD>
<A HREF="#subj4.1">
Herb Lin
</A><br>
<A HREF="#subj4.2">
 Ray Chen
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Re: Piper Arrow Gear Override 
</A>
<DD>
<A HREF="#subj5.1">
Douglas Adams
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Undesirable breakins and causes 
</A>
<DD>
<A HREF="#subj6.1">
Ian Davis
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Lessons from Viking Lander software
</A>
</H3>
<address>
"ESTELL ROBERT G" 
&lt;<A HREF="mailto:estell@nwc-143b.ARPA">
estell@nwc-143b.ARPA
</A>&gt;
</address>
<i>
2 Oct 86 12:18:00 PST
</i><PRE>
To: "risks" &lt;risks@csl.sri.com&gt;

Angry replies? Never!  To the contrary, Thank you, Nancy.
In the last few weeks in the "pages" of this journal, we have established:
1. Software cannot be perfect, because it is designed by fallible humans.
2. Hardware cannot be absolutely predictable, adding to the software woes.
3. Very concise programs, though not "perfect" nevertheless have sometimes
   run well enough to be considered "successful" - even on a "first chance."
   [Other than during simulated tests.]

It's time to move forward.  I concur heartily with Nancy's point that what
applies to small programs may not scale up to large programs.  It's worth
noting that in her last message, Nancy alluded to the Viking Lander as a
"small" program, with "only" 18,000 words in assembly!  How many of you
agree that we might use orders of magnitude to distinguish sizes? e.g., 
 IF xx,000 = small, then x,000 = tiny, and x00 = toy; and xxx,000 = large,
 and x,000,000 = very large; etc.
 Maybe it then follows that xx,000,000 is "possible?"  [SDI size?]
 Maybe not.  Depends a LOT on HOW it's designed, coded, and tested.

I'd like those more knowledgable than I to address some of the following;
maybe the software engineering journal is a better forum; maybe conclusions
could be reprinted in RISKS.

 a. What are the measures of "acceptability?"
    [Analogy: In baseball, the 1.000 batting average [perfection] is never
    possible - except in trivial circumstances, then only with luck.
    A century of experience says that .3xx is outstanding.  Likewise, the
    1.000 fielding average is "impossible."  But anything less than .9xx
    is terrible.
    What are similar failure rates for software?  Where should we set our
    expectations?  Where is the "point of diminishing returns?"
    I'm not at all clear on just HOW to set these expectations.  
    [On a saturated UNIVAC 1110 a few years ago, we were suffering up to
    3 crashes a day; we "engineered" that down to less than 3 per month.
    Did that improvement make us just average, or much better?]

 b. *IF* it's true that "small" programs can run "acceptably" albeit NOT 
    perfectly, and that "very large" programs probably cannot (?), then
    why don't we get serious about building very large programs as orderly
    structures of small modules?  (Where "small" may mean xx,000 lines.)
    At the moment, it seems to me that we're caught on the horns of a 
    dilemma of our own making; the "idealists" among us are saying that
    very large systems cannot be perfect, hence should not be pursued;
    the "realists" among us are saying that the present status of large, 
    real-time systems is a disaster; the "analysts" among us are saying 
    that there seems to be no good formula for success, yet; and the 
    "pragmatists" among us are saying that we can make SOME worthwhile
    improvements in the status quo, and thus we should.
    ALL FOUR VIEWPOINTS ARE "CORRECT."
    Isn't it time now for all of us to start "groping" forward together?

Bob

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Software wears out?
</A>
</H3>
<address>
Rob Austein 
&lt;<A HREF="mailto:SRA@XX.LCS.MIT.EDU">
SRA@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Thu, 2 Oct 1986  00:47 EDT
</i><PRE>

The other sense in which software "wears out" is that people lose
their ability to maintain it.  I recently had to work on a mailer
daemon that is about 15 years old.  Fine code, possibly one of the
best mailers ever written (certainly for its day), coded according to
good programming practices -- for the early 1970s.  I almost went nuts
trying to modify it, people just don't think that way anymore (you
know, labels every ten instructions, GOTOs everywhere...).

I was the person modifying the code because everybody else had better sense
than to even try.  At this point I can say with a fair amount of certainty
that -nobody- really understands that program anymore, although the people
who installed the various features (if still alive) usually remember having
done so within a month or so of being asked.  And this is a fairly small
program compared to stuff done out in the Real World.

--Rob &lt;BUG-COMSAT@MC.LCS.MIT.EDU&gt;

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Wrongful eviction through computer error
</A>
</H3>
<address>
Bill Janssen 
&lt;<A HREF="mailto:janssen@mcc.com">
janssen@mcc.com
</A>&gt;
</address>
<i>
Thu, 2 Oct 86 19:10:10 CDT
</i><PRE>
To: risks@sri-csl.arpa

An interesting thing happened to me last month.  I got home on the 5th of
September to find an eviction notice on my living room floor.  Something
about not paying my rent.  Well, I gathered up the checks and went over to
the office.  Turns out the problem was that I had already paid for October,
as well as September, and the apartment management folks had just switched
to a new computer system! There must have been a line in it something like

	if (last_month_paid_for != this_month
	    AND day == trigger_day_for_eviction)

		issue_eviction_notice();

According to some of the office staff, 11 other people had already
been in with similar complaints.
                                
Bill Janssen, MCC Software Technology, 9430 Research Blvd, Austin, Texas  78759
 UUCP:  {ihnp4,seismo,harvard,gatech,pyramid}!ut-sally!im4u!milano!janssen

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Deliberate override
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Thu, 2 Oct 1986  08:26 EDT
</i><PRE>

    From: George Adams &lt;gba at riacs.edu&gt;
       Even if an incompetent driver forgot to enable the system on hard
    pavement, performance would be no worse than now common.  Without the
    switch a competent driver might hit that cow instead of stopping in time.

But you are assuming that the average level of competence remains the
same in the presence of the automatic system.  For tasks on which you
can enforce some high level of performance (such as flying), this may
be valid.  But you can't do it for drivers.  It is entirely possible
that drivers will come to RELY on the automated system, perhaps even
without knowing it; their abilities will decline, but they will still
feel capable of overriding the system.  That's a recipe for disaster.

I'm not arguing that overrides should be forbidden.  I'm saying that
there's a trade-off that has to be addressed before they are installed.

</PRE>
<HR><H3><A NAME="subj4.2">
Deliberate Overrides
</A>
</H3>
<address>
Ray Chen 
&lt;<A HREF="mailto:chen%gt-stratus%gatech.csnet@CSNET-RELAY.ARPA">
chen%gt-stratus%gatech.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Wed, 1 Oct 86 02:37:57 EDT
</i><PRE>
Organization: The Clouds Project, School of ICS, Georgia Tech

Manual overrides are a nice idea, but chances are they'll be needed
most during a sudden emergency when there isn't time to think about,
much less trigger any kind of safety override.

How would you like to have to trigger a safety override while powering
into a corner trying to avoid an accident?  Ugh.

Personally, I don't see any way around it.  Total control after
all is just that -- the ability to specify exactly what the machine is
going to do, even if it's beyond the normal performance envelope.
Saftey restrictions on the other hand are designed to keep you
from exceeding the performance envelope.

There's an inherent contradiction in the two objectives which can't be
neatly resolved unless the safety system and the user/operator are
always in perfect agreement on the limits of the performance envelope
in every possible situation.

I think we have a trade-off here.
                                        	Ray Chen
uucp:	...!{akgua,decvax,hplabs,ihnp4,linus,seismo}!gatech!chen
CSNet:	chen@GATech	 ARPA:  chen%GATech.CSNET@CSNet-Relay.ARPA

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Re: Piper Arrow Gear Override
</A>
</H3>
<address>
Adams Douglas
&lt;<A HREF="mailto:crash!pnet01!adamsd@nosc.ARPA ">
crash!pnet01!adamsd@nosc.ARPA 
</A>&gt;
</address>
<i>
Thu, 2 Oct 86 08:59:20 PDT
</i><PRE>

Piper could also have installed the override system because some old lawsuit or
other related to a gear-up landing would have caused their insurance rates to
go through the roof if they didn't implement some kind of 'fix' that they
could point to.

Incidentally, I don't advocate it, the problem of leaving the override on could
be solved by having a flashing panel light next to the other two gear-status
lights on the glareshield such as OVERRIDE ENGAGED. But that would simply add
to the pilot's cockpit stimuli--which is never a good idea during takeoff.

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Undesirable breakins and causes
</A>
</H3>
<address>
Ian Davis 
&lt;<A HREF="mailto:ijdavis%watdaisy.waterloo.edu@CSNET-RELAY.ARPA">
ijdavis%watdaisy.waterloo.edu@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Thu, 2 Oct 86 17:32:33 edt
</i><PRE>
Organization: U. of Waterloo, Ontario

Has anyone suggested that hardware can also seriously undermine security? [YES]
I tend to work from home and thus communicate via a modem, and have always
logged off by merely switching from data to voice on my modem, which both
drops the line, and hangs up the phone for me...  unfortunately (and initially
unbeknown to me) at least one of the modems that answers incoming calls from
me (at a deliberately unspecified site) was hit by a current surge during a
recent lightning storm, and now no longer drops the communication line to
the CPU when I drop my line to it. This has disasterous consequences since
the next caller to use this recieving modem finds themselves logged into my
account with totally unrestricted access to the system.  Fortunately most
users are honest and promptly sign off, but the risks are very real.

The moral for those of you who are concerned, is that one should always
log off from an operating system before dropping communication lines,
and that one should log back on as soon as possible if the line is dropped
accidentally.
                                        Ian Davis

                [We have been around that one several times now, although 
                 lightning hitting the modem is a new wrinkle!  PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.72.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.74.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-60</DOCNO>
<DOCOLDNO>IA012-000123-B023-386</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.74.html 128.240.150.127 19970217005143 text/html 25905
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:50:08 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 74</TITLE>
<LINK REL="Prev" HREF="/Risks/3.73.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.75.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.73.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.75.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 74</H1>
<H2> Friday, 3 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Opinions vs. Facts in RISKS Reports (re Aviation Accidents) 
</A>
<DD>
<A HREF="#subj1.1">
Danny Cohen
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Mathematical checking of programs (quoting Tony Hoare) 
</A>
<DD>
<A HREF="#subj2.1">
Niall Mansfield
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Risks of maintaining computer timestamps revisited [<A HREF="/Risks/3.57.html">RISKS-3.57</A>] 
</A>
<DD>
<A HREF="#subj3.1">
Ian Davis
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Keyword indexing in automated catalogs 
</A>
<DD>
<A HREF="#subj4.1">
Betsy Hanes Perry
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Re: Viking Landers -- correction 
</A>
<DD>
<A HREF="#subj5.1">
Scott Preece
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Re: Confidence in software via fault expectations 
</A>
<DD>
<A HREF="#subj6.1">
Scott Preece
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Overrides and tradeoffs 
</A>
<DD>
<A HREF="#subj7.1">
Jerry Leichter
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Re: Deliberate overrides 
</A>
<DD>
<A HREF="#subj8.1">
Brint Cooper
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj9">
  Re: idiot-proof cars (risks-3.68) 
</A>
<DD>
<A HREF="#subj9.1">
Col. G. L. Sicherman
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Opinions vs. Facts in RISKS Reports (re Aviation Accidents)
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
3 Oct 1986 09:18:27 PDT
</i><PRE>
From: COHEN@B.ISI.EDU
To: Peter G. Neumann &lt;Neumann@CSL.SRI.COM&gt;
ReSent-To: RISKS@CSL.SRI.COM

    Opinions vs. Facts in RISKS Reports (re Aviation Accidents)
    -----------------------------------------------------------

Everyone is entitled to opinions and to facts.  Keeping the two
distinguishly separated is the basis of good reporting -- including
the reports/contributions to RISKS.

The RISKS readers are best served by being able to tell one from
the other, and to tell what is based on opinions/rumors and what on
facts.  Two examples follow.


In <A HREF="/Risks/3.27.html">RISKS-3.27</A> Stephen Little reported about "one major accident in which
the pilot followed the drill for a specific failure, as practiced on the
simulator, only to crash because a critical common-mode feature of the
system was neither understood, or incorporated in the simulation."

Since this is a very important evidence of "major accident" (with
possible/probable loss of hundreds of lives) I tried to follow up on it
and offered to pursue this report.  

The best way to verify such a report is by a reference to the official
NTSB (National Transportation Safety Board) accident investigation
report.  Therefore, I have volunteered to pursue this reference myself
if anyone could give me details like the date (approximately), place
(country, for example), or the make and type of the aircraft.

My plea for this information appeared in <A HREF="/Risks/3.34.html">RISKS-3.34</A>, on 8/9/1986.

In response, one RISKS reader provided me with a pointer to what he
vaguely remembered to be such a case.  After pursuing the original
report we both found that the pilot (Capt. John Perkins, of United
Airlines) claimed that [computer based] simulator training helped him
and his crew to survive a windshear encounter (not the kind of story
the RISKS community finds to be of interest).

       (The long discussion about the F-16 does not relate to this
	topic since it was concentrated on what the simulator software
	should do and what the aircraft software should do, rather than
	on the fidelity of the simulator and on its training value).

If the original report about that computer-induced major accident is
based on facts -- let's find them, we tried but did not succeed.
If it is based or rumors -- let's say so explicitly.


A more recent RISKS (3.72) has another report, this time by a pilot,
Peter Ladkin, who also provides the place and the make and type of the
aircraft (just as I asked for).  His report says:

      "	An example of a deliberate override that led to disaster:
	An Eastern Airlines 727 crashed in Pennsylvania with considerable
	loss of life, when the pilots were completing an approach in
	instrument conditions (ground fog), 1000 feet lower than they
	should have been at that stage.
	They overrode the altitude alert system when it gave warning.  "

I found it very interesting.  The mention of the aircraft type and the
location are helpful hints for pursuing such accidents.

However, I failed to locate any information about that "Eastern
Airlines 727 [which] crashed in Pennsylvania".

I (and Eastern Airlines, too) know of only two losses of Eastern
Airlines 727's -- neither in Pennsylvania.  One in JFK to (windshear)
and one in La Paz, Bolivia (flying into a mountain, in IFR conditions).

However, I know of the 9/11/1974 Eastern Airline crash of a DC-9 in
Charlotte, North Carolina -- which, I guess, is what Peter Ladkin's
report is about.  This guess may be wrong.

  I APOLOGIZE TO PETER LADKIN IF I DID NOT GUESS THE RIGHT ACCIDENT.

According to the NTSB accident report (NTSB-AAR-75-9) about the DC-9 in
Charlotte: "The probable cause of the accident was the flightcrew's lack
of altitude awareness at critical points during the approach due to poor
cockpit discipline in that the crew did not follow predescribed
procedure."  [They were too low, and too fast.]

The report also mentions that "The flightcrew was engaged in
conversations not pertinent to the operation of the aircraft.  These
conversations covered a number of subjects, from politics to used cars,
and both crew members expressed strong views and mild aggravation
concerning the subjects discussed.  The Safety Board believes that these
conversations were distractive and reflected a casual mood and a lax
cockpit atmosphere, which continued throughout the reminder of the
approach and which contributed to the accident."

What also contributed to the accident is that "the captain did not make
the required callout at the FAF [Final Approach Fix], which should have
included the altitude (above field elevation)".  They also did not make
other mandatory callouts.

Other possible contributing factors was a confusion between QNE and QFE
altitudes (the former is above sea level, and the latter above the field
elevation).  [This may be the 1,000' confusion mentioned in Peter
Ladkin's report.]

"The terrain warning alert sounded at 1,000 feet above the ground but
was not heeded by the flightcrew" (which is typical to many airline
pilots who regard this signal more of nuisance than a warning).

Question: What did Ladkin mean by "An example of a deliberate override
          that led to disaster: ..... They overrode the altitude alert 
          system when it gave warning" ?

According to the NTSB they just did not pay attention to it.  According
to the Ladkin report they DELIBERATELY OVERRODE it, which implies
explicit taking some positive action to override it.  It is hard to
substantiate this suggestion.

Not paying attention is not a "deliberate override" as promised in the
first line of the Ladkin report, just as flying under VFR conditions
into the ground is not "a deliberate override of the visual cues" -- it
is a poor practice.  (The only thing DELIBERATE in that cockpit was the
discussion of used cars!)

Does this example contribute to the RISKS discussion about "deliberate
override"?


In summary: Starting from wrong "facts" based on third hand vague
            recollections is not always the best way to develop theories.

Again, the RISKS readers are best served by more accurate reporting.
They deserve it.

							Danny Cohen.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
       Mathematical checking of programs (quoting Tony Hoare)
</A>
</H3>
<address>
          Niall Mansfield  
&lt;<A HREF="mailto:MANSFIEL%DHDEMBL5.BITNET@WISCVM.WISC.EDU">
MANSFIEL%DHDEMBL5.BITNET@WISCVM.WISC.EDU
</A>&gt;
</address>
<i>
Thu 2 Oct 86 11:53:55 N
</i><PRE>

In "New Scientist", 18-Sep-86, C.A.R. Hoare discusses mathematical
techniques for improving the reliability of programs, especially
life-critical ones.  The following somewhat arbitrary excerpts (quoted
without permission) include some interesting ideas:

  But computers are beginning to play an increasing role in "life-critical
  applications", situations where the correction of errors on discovery is not
  an acceptable option - for example, in control of industrial processes,
  nuclear reactors, weapons systems, oil rigs, aero engines and railway
  signalling.  The engineers in charge of such projects are naturally worried
  about the correctness of the programs performing these tasks, and they have
  suggested several expedients for tackling the problem.  Let me give some
  examples of four proposed methods.
  
  The first method is the simplest.  I illustrate it with a story.  When
  Brunel's ship the SS Great Britain was launched into the River Thames, it
  made such a splash that several spectators on the opposite bank were
  drowned.  Nowadays, engineers reduce the force of entry into the water by
  rope tethers which are designed to break at carefully calculated intervals.
  
  When the first computer came into operation in the Mathematish Centrum in
  Amsterdam, one of the first tasks was to calculate the appropriate intervals
  and breaking strains of these tethers.  In order to ensure the correctness
  of the program which did the calculations, the programmers were invited to
  watch the launching from the first row of the ceremonial viewing stand set
  up on the opposite bank.  They accepted and they survived.
  
  ... [1.5 pages omitted]
  
  I therefore suggest that we should explore an additional method, which
  promises to increase the reliability of programs.  The same method has
  assisted the reliability of designs in other branches of engineering, namely
  the use of mathematics to calculate the parameters and check, the soundness
  of a design before passing it for construction and installation.
  
  Alan Turing first made this suggestion some 40 years ago; it was put into
  practice, on occasion, by the other great pioneer of computing, John von
  Neumann.  Shigeru Igarashi and Bob Floyd revived the idea some 20 years ago,
  providing the groundwork for a wide and deep research movement aimed at
  developing the relevant mathematical techniques.  Wirth, Dijkstra, Jones,
  Gries and many others, (including me) have made significant contributions.
  Yet, as far as I know, no one has ever checked a single safety-critical
  program using the available mathematical methods.  What is more, I have met
  several programmers and managers at various levels of a safety-critical
  project who have never even heard of the possibility that you can establish
  the total correctness of computer programs by the normal mathematical
  techniques of modelling, calculation and proof.
  
  Such total ignorance would seem willful, and perhaps it is.  People working
  on safety-critical projects carry a heavy responsibility.  If they ever get
  to hear of a method which might lead to an improvement in reliability, they
  are obliged to investigate it in depth.  This would give them no time to
  complete their current projects on schedule and within budget.  I think that
  this is the reason why no industry and no profession has ever voluntarily
  and spontaneously developed or adopted an effective and relevant code of
  safe practice.  Even voluntary codes are established only in the face of
  some kind of external pressure or threat, arising from public disquiet,
  fostered by journals and newspapers and taken up by politicians.
  
  A mathematical proof is, technically, a completely reliable method of
  ensuring the correctness of programs, but this method could never be
  effective in practice unless it is accompanied by the appropriate attitudes
  and managerial techniques.  These techniques are in fact based on the same
  ideas that have been used effectively in the past.
  
  It is not practical or desirable to punish errors in programming by instant
  death.  Nevertheless, programmers must stop regarding error as an inevitable
  feature of their daily lives.  Like surgeons or airline pilots, they must
  feel a personal commitment to adopt techniques that eliminate error and to
  feel the appropriate shame and resolution to improve when they fail.  In a
  safety-critical project, every failure should be investigated by an
  impartial enquiry, with powers to name the programmer responsible, and
  forbid that person any further employment on safety-critical work.  In cases
  of proven negligence, criminal sanctions should not be ruled out.  In other
  engineering disciplines, these measures have led to marked improvement in
  personal and professional responsibility, and in public safety.  There is
  not reason why programmers should be granted further immunity...
  
  ... [1 page, to end of article, omitted]
  
</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Risks of maintaining computer timestamps revisited [<A HREF="/Risks/3.57.html">RISKS-3.57</A>]
</A>
</H3>
<address>
Ian Davis 
&lt;<A HREF="mailto:ijdavis%watdaisy.waterloo.edu@CSNET-RELAY.ARPA">
ijdavis%watdaisy.waterloo.edu@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Wed, 1 Oct 86 17:47:29 edt
</i><PRE>

CP-6 has a further problem when first loaded that was encountered recently
at Wilfrid Laurier University.  A check is made to ensure that front end
processors (FEP's) are up and running, but not that they contain the correct
software... the consequence in W.L.U's case was that after loading version
C01 for testing and then rebooting C00 software they left C01 software in
the FEP's.  Unfortunately, this resulted (for whatever reason) in disk
record writes being interpreted as disk record deletes.  The problem became
apparent when using the editor which performs direct disk updates... but its
severity was not at first appreciated... the system was brought down very
rapidly when it was....  Ian Davis.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Keyword indexing in automated catalogs
</A>
</H3>
<address>
Betsy Hanes Perry 
&lt;<A HREF="mailto:betsy%dartmouth.edu@CSNET-RELAY.ARPA">
betsy%dartmouth.edu@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Wed, 1 Oct 86 10:40:39 edt
</i><PRE>

The recent notice about title-indexing (article titles must include all
important article keywords in their first five words) struck a real chord in
me.  My current job is maintaining and updating Dartmouth College's
automated card catalog.

We have a database of over 800,000 records, all completely free-text
searchable (EVERY WORD in every record is indexed).  We are beginning to
suffer storage limitations, and are exploring our options.  However, if we
tried to suggest anything so restrictive as "five keywords per title", we'd
have a revolution on our hands.
 
The instance cited seems to me to be a clear example of shaping the
task to suit the tools at hand.  Somebody out there ought to be ashamed
of him/herself.  At the very least, the notice explaining why articles'
titles must be rewritten should have been
 
  1.  Extremely apologetic    and
  2.  Should have given a time by which this temporary limitation
      would no longer apply.
 
As it stands, the system sounds as if it is going to be less useful
than some of the available conventional journal indexes -- what 
incentive does this give for using it?
 
Tsk, tsk.

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Re: Viking Landers -- correction 
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%ccvaxa@GSWD-VMS.ARPA">
preece%ccvaxa@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Thu, 2 Oct 86 09:33:19 cdt
</i><PRE>

&gt; From: leveson@sei.cmu.edu
&gt; Small, straightforward problems with very little complexity in the
&gt; logic (e.g., just a series of mathematical equations) may not say much
&gt; about the reliability of large, complex systems.

And there, of course, lies the heart of the structured programming
movement.  You improve reliability by reducing the complexity of
program logic.  You turn a large, complex system into a small,
straightforward system by building it in layers, each of which
makes use of primitives defined in the layer below.

The reason it may not be as effective as many have hoped is
that even simple, straightforward programs often turn out to
have bugs...

scott preece, gould/csd - urbana, uucp:	ihnp4!uiucdcs!ccvaxa!preece

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Re: Confidence in software via fault expectations 
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%ccvaxa@GSWD-VMS.ARPA">
preece%ccvaxa@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Thu, 2 Oct 86 09:25:04 cdt
</i><PRE>

&gt; From: hplabs!sdcrdcf!darrelj@ucbvax.Berkeley.EDU (Darrel VanBuer)

&gt; The thing is software DOES wear out in the sense that it loses its
&gt; ability to function because the world continues to change around it...
----------
That's like saying "People do live forever in the sense that some of their
atoms linger."  The sense you depend on is not in the words you use.

"Becoming obsolete" is NOT the same thing as "wearing out."  The word "wear"
is in there for a reason.  Software does not suffer wear (though storage
media do).  The only exception I can think of would be demonstration
packages that self-destruct after a set number of uses.

Words are important; if you smear their meaning, you lose the ability to say
exactly what you mean.  This is a risk the computing profession has
contributed to disproportionately.

scott preece

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
 Overrides and tradeoffs
</A>
</H3>
<address>

&lt;<A HREF="mailto:LEICHTER-JERRY@YALE.ARPA">
LEICHTER-JERRY@YALE.ARPA
</A>&gt;
</address>
<i>
3 OCT 1986 13:26:54 EST
</i><PRE>

The recent discussions on manual overrides for airplane landing gear and car
brakes have all been ignoring a fundamental issue:  To compute the expected
cost/risk of having/not having an automated system, you need more than just a
few gedanken experiments; you need some estimates of the probabilities of
various situations, and, in each of those situations, the expected costs of
using or not using the automatic systems.

Here's a simple, well-known example:  Some people claim they don't wear seat
belts because, in an accident, they might be trapped in a burning car, or one
sinking into a lake.  Is this a valid objection?  Certainly; it COULD happen.
But the reality is that such accidents are extremely rare, while accidents in
which seat belts contribute positively are quite common.  So, on balance, the
best you can do is wear seat belts.  Of course, if you are in some very spe-
cial situation - doing a stunt that involves driving a car slowly across a
narrow, swaying bridge over a lake, for example - the general statistics fail
and you might properly come to a different conclusion.

In the United States, how many people regularly drive on gravel roads?  Per-
haps for those relatively few who do, an override for the automatic brake
system, or even a car WITHOUT such a system might make sense.  Perhaps the
costs for all those people who almost never drive on gravel roads can be shown
to be trivial.  There certainly ARE costs; every additional part adds cost,
weight, something that can break; plus, there's another decision the driver
might not want to be burdened with.  And there are "external" costs:  An
uncontrolled, skidding car could easily injure someone besides the driver who
chose to override the ABS.

Accidents in general are fairly low-probability events.  As such, they have to
be reasoned about carefully - our intuitions on such events are usually based
on too little data to be worth much.  Also, since we have little direct expe-
rience, we are more likely to let emotional factors color our thinking.  The
thought of being trapped in a burning or sinking car is very disturbing to
most people, so they weight such accidents much more heavily than their actual
probability of occurrence merits.

It's also worth remembering another interesting statistic (I wish I knew a
reference):  When asked, something like 80% of American male drivers assert
that their driving abilities are "above average".  Given such a population
of users, there are risks in providing overrides of safety systems.

							-- Jerry

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
 Re: Deliberate overrides
</A>
</H3>
<address>
    Brint Cooper 
&lt;<A HREF="mailto:abc@BRL.ARPA">
abc@BRL.ARPA
</A>&gt;
</address>
<i>
Fri, 3 Oct 86 13:53:54 EDT
</i><PRE>

&gt; .....  Yet, perhaps such vehicles should have a switch to disable
&gt; anti-lock and allow conventional braking.  Imaging trying to stop quickly
&gt; with anti-lock brakes on a gravel road...

But the whole point of anti-lock brakes is to avoid skidding when traction
is lost.  If the vehicle skids, it'll hit the cow.  Overrides, as has been
said before, allow incompetent operators to substitute their opinions for
facts.
                                        Brint

</PRE>
<A NAME="subj9"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj9.1">
Re: idiot-proof cars (risks-3.68)
</A>
</H3>
<address>
"Col. G. L. Sicherman" 
&lt;<A HREF="mailto:colonel%buffalo.csnet@CSNET-RELAY.ARPA">
colonel%buffalo.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Mon, 29 Sep 86 09:15:13 EDT
</i><PRE>
To: risks@csl.sri.com

Chuck Fry's argument for override provisions in automated controls on cars
makes a lot of sense.  Frankly, though, I'd rather see as few new automatic
controls as we can manage with.  I live in the Buffalo area--heavy industry
with cobwebs on it--and people here are driving cars that ought to have been
junked last year.

Airplanes get first-class maintenance, or at least second-class.  With cars
it's different; when something breaks, many people just can't afford to have
it fixed.  The simpler a car's design, the longer a poor man can keep it
running safely.

Maybe I'm being cynical, but I believe that so simple an improvement as
putting brake lights on rear windshields will prevent far more accidents
than any amount of intermediary computerization.

     [Since deregulation, you might be surprised that the airlines like
      everyone else believe in cutting expenses to the bone.  Maintenance
      may or may not be what it was.  I have seen several reports that it
      is not, although it is certainly nowhere near so bad as with autos.  PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.73.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.75.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-61</DOCNO>
<DOCOLDNO>IA012-000123-B023-402</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.75.html 128.240.150.127 19970217005157 text/html 19002
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:50:24 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 75</TITLE>
<LINK REL="Prev" HREF="/Risks/3.74.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.76.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.74.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.76.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 75</H1>
<H2>Saturday, 4 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
re: Estell on Viking (<A HREF="/Risks/3.73.html">RISKS-3.73</A>) 
</A>
<DD>
<A HREF="#subj1.1">
David Parnas
</A><br>
<A HREF="#subj1.2">
 Dave Benson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Software becomes obsolete, but does not wear out 
</A>
<DD>
<A HREF="#subj2.1">
Dave Benson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  The fallacy of independence 
</A>
<DD>
<A HREF="#subj3.1">
Dave Benson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: Paths in Testing (<A HREF="/Risks/3.72.html">RISKS-3:72</A>) 
</A>
<DD>
<A HREF="#subj4.1">
Chuck Youman
</A><br>
<A HREF="#subj4.2">
 Mark Day
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Mathematical checking of programs (quoting Tony Hoare) 
</A>
<DD>
<A HREF="#subj5.1">
Henry Spencer
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
re: Estell on Viking (<A HREF="/Risks/3.73.html">RISKS-3.73</A>)
</A>
</H3>
<address>
&lt;<A HREF="mailto: parnas%qucis.BITNET@WISCVM.WISC.EDU">
 parnas%qucis.BITNET@WISCVM.WISC.EDU
</A>&gt;
</address>
<i>
Fri, 3 Oct 86 15:33:03 EDT
</i><PRE>
ReSent-To: RISKS@CSL.SRI.COM

Robert Estell's contribution perpetuates two serious myths about the
discussion on Viking and other software.

(1) That any of the discussants is expecting perfection.

Perfectionists do not use the net.  In fact, the only computer
scientist I know who could be called a perfectionist does not use
computers.  Most of us know that computer systems, like other human
artifacts, will never be perfect.  Our concern is with establishing
confidence that the system is free of unacceptable or catastrophic
errors.  This we can do with many other engineering products.  Only
software products regularly carry disclaimers instead of limited
warranties.  That is not because they are the only products that are
imperfect.  It is because we have so little confidence that they are
free of catastrophic flaws.

(2) That size is a good measure of the difficulty of a problem.

There are big programs solving dull but easy problems.  Small
programs occasionally solve very hard problems.  The size
and irregularity of the problem state space, and how well
we know that state space determine, in large part the
complexity of the problem.  The size of the program is often
determined by the simplicity of the programmer.

In spite of Nancy's help, we don't know much from this forum
about what the Viking software actually did.  It seems clear that
most of the software could have been, and was, used before the
flight.  Whether the descent software could have been used
depends on what it did.  At 100 lines one would expect that
it did not do much.

        We all know that programs can work acceptably well.  We
use them and accept what they do.  We also know that failures
are not catastrophic and that these programs failed many times
before they became reliable enough to be useful.  If we had
been in a situation in which those failures were unacceptable
we would have found another way to solve the real problem.

</PRE>
<HR><H3><A NAME="subj1.2">
 Viking Lander, once again.
</A>
</H3>
<address>
Dave Benson 
&lt;<A HREF="mailto:benson%wsu.csnet@CSNET-RELAY.ARPA">
benson%wsu.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Fri, 3 Oct 86 17:57:52 pdt
</i><PRE>

I repeat some quotations from Bonnie A. Claussen's paper:
	The unprecented success of the Viking mission was due in part to
        the ability of the flight software to operate in an AUTONOMOUS
        and ERROR FREE manner. ...  Upon separation from the Orbiter the
        Viking Lander, under AUTONOMOUS	software control, deorbits, enters
        the Martian atmosphere, and performs a soft landing on the surface.
        [CAPS added for emphasis.]
Since the up-link was only capable of 4 bits/sec and the light-speed signal
requires about 14 minutes for a round-trip to Mars, manifestly the software
carried out these control functions without human assistance.

 &gt;I worry when anecdotal evidence about one software project is used as
 &gt;"proof" about what will happen with general software projects.
 &gt;     Nancy Leveson

I concur.  But the Viking Lander experience does give a compelling example
that autonomous software can be made to work under certain circumstances.
Thus a claim that &lt;&lt;all&gt;&gt; autonomous software fails in its first
operational experience is in contradiction to the facts.

For amusement, assume that this experience scales linearly with project size.
(I assure everyone on RISKS that data suggests a diseconomy of scale--thus
larger projects require a more than linear increase of effort to obtain
the same reliability.)  Now the Viking Lander required 135 engineer-years
for about 18000 words of software.  Suppose each line of Ada represents
about 5 words of software.  Thus, if you'll agree with these assumptions,
each 40 lines of Ada requite 1.5 engineer-years to produce "equally well
tested" code.  So a 4 million line Ada program requires 150,000 engineer
years of effort.  Assuming a 1500 engineer level of effort, that's
one century to write and test the code.
	Of course, to be "equally well tested" would require far more effort
than that, from the diseconomies of scale.  This is the proper lesson
to draw from the Viking Lander experience.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Software becomes obsolete, but does not wear out
</A>
</H3>
<address>
Dave Benson 
&lt;<A HREF="mailto:benson%wsu.csnet@CSNET-RELAY.ARPA">
benson%wsu.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Fri, 3 Oct 86 18:33:12 pdt
</i><PRE>

ob'so.lete.  Abbr. obs.  Of a type or fashion no longer current; out of
date; as, an obsolete machine.

ob'so.les''cent.  Going out of use; becoming obsolete.

wear v.t. ... 6. To use up by wearing (sense 1); as, to wear out a dress;
hence, to consume or cause to deteriorate by use, esp. personal use;
as, the lugage is worn. 7. To impair, waste, or diminish, by continual
attrition, scraping, or the like; as, the rocks are worn by water;
hence, to exhaust or lessen the strength of; fatigue; weary; use up;
as, to be worn with desease. 8. To cause or make by friction or wasting;
as, to wear a channel or hole.
wear v.i. ... 4. To be wasted, consumed, or diminished, by use; to
suffer injury, loss, or extinction, by use or time;-- often with
&lt;out&gt;, &lt;off&gt;, &lt;on&gt;, etc.; as the day has worn on.

Software, like any artifact, becomes obsolete over time.  The changing
informational environment about the software drives it to obsolesence.
It becomes unmaintainable, not from wear, but because the expertise
required has become dissipated.  Recall that nobody knows how to
make a Stradivarius violin anymore, either.

I agree with the causes of software obsolescence, but strongly recommend that
we use the customary meanings of words in the dictionary so that
we understand one-another and so that non-software-types can somewhat
understand us as well.  Thus: software may become obsolete from many
causes, some of which are understood.  But software ordinarily does not
wear out and never, never rots.  [...]

There is a reason for precise technical terms.  In other disciplines words
are coined, just to avoid the overloading and potential resultant
misunderstanding.  I recommend that we attempt this, but suggest looking
in the dictionary first.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
 The fallacy of independence
</A>
</H3>
<address>
Dave Benson 
&lt;<A HREF="mailto:benson%wsu.csnet@CSNET-RELAY.ARPA">
benson%wsu.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Sat, 4 Oct 86 22:21:39 pdt
</i><PRE>

A RISKS contribution suggests that since we can engineer good 100,000
statement software, the means to make good 1,000,000 statement software
is to produce 10 smaller packages and hook these 10 together.
	Such a claim makes the assumption that the informational
environment of the total software is such that the total software system
can be decomposed into 10 nearly independent parts, which communicate with
one another along well-understood interfaces.
	The key is the claim that the interfaces are well-understood.
Software is an example of an extremely complex artifact, a class of artifacts
which we understand poorly--for otherwise we wouldn't call them complex.
In smaller programs we repeatedly see that the interfaces are not
well-understood until the program is available for experimentation.  Even
then, our everyday experiences with software demonstrate again and again
that what we had assumed about the program behavior does not match the
reality of actual experience.  Thus we discover that the interfaces are
not well-understood.
	Example:  Virtual storage managers in operating systems provide
a superficially simple interface to the hardware  and the rest of the
operating system.  The interface to the user program is the essense of
simplicity--complete transparency.  Now the earliest virtual storage managers
were the essense of simplicity.  So nothing could go wrong, right?  Wrong.
The interaction of user virtual storage requests, the operating system
scheduler, and the virtual storage manager led to thrashing--slowing
performance to a crawl, at best.  Upon OBSERVING this phenomenon, theories
were developed and better, more complex, algorithms were installed.  But
this phenomenon was not predicted a priori.
	The essential point is that even the cleanest design may fail in
actual engineering practice until it is tried in the fully operational
environment for which it was intended.  In software engineering we only
have confidence in a design if it is similar to a previous, successful
design.  But that is just like any other engineering practice.  The
intuition and insight of a Roebling (Brooklyn Bridge, 1883) is rare
in any engineering field.  Most of us are good copiers, making local
improvements to a  design already shown to be successful.
	The corollary is that it is wrong to assume the near-independence
of components until this near-independence has been abundantly shown in
practice and theory.
	Example:  The division of the frontend of a compiler into lexical
and syntactic parsing components which interact in well-understood ways
has an excellent underlying theory and works well in practice.  Thus it is
common to teach this practice and theory, since post-facto it is a
workable engineering design of nearly-independent components.
	By all means color me realist.  Also color me existentialist.
What works is that which works, not what we might hope or dream or
imagine works.  The near-independence of software components is an
aspect which is proved in practice to be a near-independence of
components.  As there is no "software decomposition theorem" which provides
a general framework for that elusive quality, near-independence, we
cannot assume that 10 good parts will actually form a cohesive, practical
reliable whole.  In each separate design, then, the value of the whole
system can only be demonstrated by the use of the whole system.
	Thus I claim it is a fallacy to assert independence, or even
near-independence, for any division of the work within a system until
this has been conclusively demonstrated.  I further claim, with ample
historical precedent, that the reliability of a system is only poorly
correlated with the reliability of its parts.  Without a specific design
one can say nothing in general.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: Paths in Testing (<A HREF="/Risks/3.72.html">RISKS-3:72</A>)
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Fri, 03 Oct 86 16:46:13 -0500
From: Chuck Youman &lt;m14817@mitre.ARPA&gt;

A comment on basis paths.

There was a paper on "Evaluating Software Testing Strategies" presented
by Richard Selby at the 9th Annual NASA Goddard Software Engineering Workshop
that compared the strategies of code reading, functional testing, and 
structural testing in three aspects of software testing.  One of the
conclusions I recall is that structural testing was not as effective
as the other two methods at detecting omission faults and control faults.

The conference proceedings are report SEL-84-004 and can be obtained from Frank
E. McGarry, Code 552, NASA/GSFC, Greenbelt, MD 20771.
 
Charles Youman (youman@mitre.arpa)

</PRE>
<HR><H3><A NAME="subj4.2">
Re: Paths in Testing (<A HREF="/Risks/3.72.html">RISKS-3:72</A>)
</A>
</H3>
<address>
Mark S. Day 
&lt;<A HREF="mailto:MDAY@XX.LCS.MIT.EDU">
MDAY@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Sat 4 Oct 86 14:03:24-EDT
</i><PRE>
To: m14817@MITRE.ARPA
cc: risks@CSL.SRI.COM

It's reasonably well known that structural (path-based) testing is
poor at detecting faults of omission.  Correspondingly, functional
testing is poor at detecting faults on "extra" paths that are present
in the implementation (for optimization of common cases, for example)
but are not "visible" in a functional spec of the module.  The conclusion
to draw is that proper testing requires a combination of "external" testing
(treating the module as a black box and examining its input/output structure)
and "internal" testing (examining the contents of the module).  

--Mark

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Mathematical checking of programs (quoting Tony Hoare)
</A>
</H3>
<address>
&lt;<A HREF="mailto:decvax!utzoo!henry@ucbvax.Berkeley.EDU">
decvax!utzoo!henry@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Sat, 4 Oct 86 21:12:13 edt
</i><PRE>

I agree with much of the quoted discussion from Hoare, including the
obvious desirability of rather heavier use of mathematical analysis
of safety-critical programs.  I do have one quibble with some of his
comments, though:

&gt;   ... never even heard of the possibility that you can establish
&gt;   the total correctness of computer programs by the normal mathematical
&gt;   techniques of modelling, calculation and proof. ...
&gt;   A mathematical proof is, technically, a completely reliable method of
&gt;   ensuring the correctness of programs, but this method could never be
&gt;   effective in practice unless it is accompanied by the appropriate attitudes
&gt;   and managerial techniques. ...

I think talk of "total correctness" and "complete reliability" shows excess
enthusiasm rather than realistic appreciation of the situation.  Considering
the number of errors that have been found in the small programs used as
published examples of "proven correctness", wariness is indicated.  Another
cautionary tale is the current debate about the validity of the Rourke/Rego
proof of the Poincare conjecture.  As I understand it -- it's not an area
I know much about -- the proof is long, complex, and sketchy, and nobody
is sure whether or not to believe it.  And this is a case where the specs
for the problem are very simple and obviously "right".  Mathematical proof
has its own feet of clay.  If one defines "effective in practice" to imply
complete confidence in the results, then I would not fly on an airliner
whose flight-control software was written by a team making such claims.
Complete confidence in provably fallible techniques worsens risks rather
than reducing them.

(The apocryphal comment of the aeronautical structure engineer looking
at his competitor's aircraft:  "Fly in it?  I wouldn't even walk under it!")

On the other hand, if one defines "effective in practice" to mean "useful
in finding errors, and valuable in increasing one's confidence of their
absence", I wholeheartedly agree.  One should not throw out the baby with
the bathwater.  If one sets aside the arrogant propaganda of the proof-
of-correctness faction, there is much of value there.  To borrow from the
theme of a PhD thesis here some years ago, proving programs INcorrect is
much easier than proving them correct, and is very useful even if it isn't
the Nirvana of "total correctness".  The mental discipline imposed on program
creation (defining loop invariants, etc.) is also important.

				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.74.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.76.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-62</DOCNO>
<DOCOLDNO>IA012-000123-B023-422</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.76.html 128.240.150.127 19970217005229 text/html 12543
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:50:38 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 76</TITLE>
<LINK REL="Prev" HREF="/Risks/3.75.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.77.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.75.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.77.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 76</H1>
<H2> Sunday, 5 September 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Obsolescence vs wearing out (<A HREF="/Risks/3.75.html">RISKS-3.75</A>) 
</A>
<DD>
<A HREF="#subj1.1">
Jerome H. Saltzer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Cars, computers and unexpected interactions 
</A>
<DD>
<A HREF="#subj2.1">
Mike McLaughlin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: Mathematical checking of programs (quoting Tony Hoare) 
</A>
<DD>
<A HREF="#subj3.1">
Matthew Wiener
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  "Total correctness", "complete reliability" (<A HREF="/Risks/3.75.html">RISKS-3.75</A>) 
</A>
<DD>
<A HREF="#subj4.1">
Bard Bloom
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Obsolescence vs wearing out (<A HREF="/Risks/3.75.html">RISKS-3.75</A>)
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
Sun, 5 Oct 86 14:14:16 EDT
</i><PRE>
From: Jerome H. Saltzer &lt;Saltzer@ATHENA.MIT.EDU&gt;

Dave Benson nicely identifies a distinction between becoming obsolete
and wearing out, and argues that only the former applies to software.

There is another effect that isn't exactly captured by the words "to
become obsolete."  A high quality piece of software, carefully
designed and debugged by an expert, is turned over to a less-skillful
operations team which installs it, runs it--and adds minor field
modifications.  As time goes on users of the software notice that it
is no longer bug-free, because the less-skillful modifiers have been
screwing it up.  It isn't appropriate to say that the software became
obsolete; if it hadn't been tinkered with then the term obsolete
might apply.

Since most software does get modified to meet changing conditions of
use, and often those modifications are not done by the original
implementation team, this effect is quite common.  If the effect goes
on long enough, it may be necessary to commission a new
implementation, almost as if the original implementation had worn out.

Some people have in mind the impairment and diminished usability
caused by this effect when they use words like "wears out" or "rots".
I guess we need a plain English word for it so that neophytes won't
think that computers that haven't been oiled properly rub too hard
on the bits.
						Jerry

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Cars, computers and unexpected interactions
</A>
</H3>
<address>
Mike McLaughlin
&lt;<A HREF="mailto:mikemcl@nrl-csr ">
mikemcl@nrl-csr 
</A>&gt;
</address>
<i>
Sun, 5 Oct 86 16:33:33 edt
</i><PRE>

1.  I have a 1983 Ford with "Cruise Control"
2.  I have had a CB in it from the day it was picked up (7/3/83)
until the day the CB was stolen (10/2/86).  No problems.
3.  I put a new, more sophisticated CB in it on 10/4.  New CB has an
SWR (Standing Wave Ratio) meter, and an "Antenna Warning" light.  Both
intended to help tune antenna system, and ensure crummy antenna connections
don't cause loss of signal strength - or excessive reflection of trans-
mitted signal.
4.  SWR of 1.0 is perfect, and impossible.  SWR of 1.5 is good.  SWR of
2.0 is poor.  SWR &gt; 3.0 UNSAT!
5.  New CB installed with only trivial cursing and sweating.  Tuned up just
fine.  Car drove fine (as before).
6.  Rains came.  SWR &gt; 3.0.  Probable cause bad antenna connections/cable,
getting soaked.  Cruise control acted up.  Wonder why?
7.  Car baked in sun.  SWR &lt; 2.0.  Cruise control OK.
8.  This morning, car wet from heavy dew.  SWR &gt; 3.0.  Cruise control cuts
out when microphone is keyed.  Every time.
9.  Car dries out, SWR &lt; 2.0, Cruise control not affected.
10. SWR ratio must have varied with moisture on old set, same as new. 
Never had problem before... but did re-route the power cables to new set,
more "neatly" than before, i.e., more jammed up behind instrument panel.
Conclusion:  New CB/re-routed wiring somehow interacts with "Cruise Control"
micro, causing it to kick out when SWR is high.  At least it "fails safe."
N.B.: I don't usually drive in rain with cruise control on, but do use it w
whenever safe to do so - saves gas on level-ish interstates. - Mike

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: Mathematical checking of programs (quoting Tony Hoare)
</A>
</H3>
<address>
Matthew P Wiener
&lt;<A HREF="mailto:weemba@brahms.Berkeley.EDU ">
weemba@brahms.Berkeley.EDU 
</A>&gt;
</address>
<i>
Sun, 5 Oct 86 16:00:39 pdt
</i><PRE>

In response to utzoo!henry (Henry Spencer):

&gt;&gt;   A mathematical proof is, technically, a completely reliable method of
&gt;&gt;   ensuring the correctness of programs, ...     [from a Hoare quotation]

&gt;I think talk of "total correctness" and "complete reliability" shows excess
&gt;enthusiasm rather than realistic appreciation of the situation.

Agreed.

Henry then compares this notion of proof with the Rourke-Rego "proof" of
the Poincare conjecture, whose status currently is unknown.  And as Henry
says, in mathematics
&gt;the specs for the problem are very simple and obviously "right".

I must take exception to this comparison.

Mathematics, believe it or not, works under the Hundredth Monkey Phen-
omenon.  Programs do not.

Let me explain.

Proofs in mathematics (at least at the cutting edge) deal with inher-
ently complicated mentally defined objects.  It takes a while to get
your mind in sync with whatever it is you are studying.  Details and
(not always elementary) claims are left to the reader.  The field,
already huge beyond comprehension, would sink under its own weight
otherwise.

New and difficult proofs, like that of Rourke-Rego, take their time to
sink into the mathematical community's collective consciousness.  But
once they do, a new level of confidence and ability is reached, and the
proofs become accessible.

The above is not possible with programs.  At some point, every detail
must be given, somewhere.  There is no reason why a proof-checker could
not be used to check for correctness, matching pre-and-post assertions
with each statement.

So, where do "proven" programs fall down?

First, there really are the incorrect proofs.  But I believe this can
be cured.  (Of course, relying on a proof-checker could be risky if
*that* program has bugs.  But surely that is a low enough operation to
get right.  [And now a new {recursive} nightmare comes to mind.])

Second, compilers and hardware do not always match the programmer's
intent.  Hidden pointer nonsense, erroneous implementations of math-
ematical functions, silent truncation of overflows, etc. cannot be
checked for unless the programmer is aware of such glitches.

Third, the outside world need not match the programmer's intent eith-
er.  The beginning assignment of input, and the final interpretation
of output is outside the program's proof's scope.  GIGO, as we all
know.

Fourth, the theoretical process being used may be incorrect or just
inappropriate in a particular situation.  One can give your numerical
analysis routines a proof that they do what is wanted, and build your
aircraft or nuclear reactor or what have you with a new false confi-
dence, despite the fact that the case at hand is subject to numerical
instability or similar problems.

So in summary, a program and its proof are meaningful relative to each
other, and nothing else.  I would hate to think of the consequences if
someone forgot this when implementing SDI, say.

ucbvax!brahms!weemba	Matthew P Wiener/UCB Math Dept/Berkeley CA 94720

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
"Total correctness", "complete reliability" (<A HREF="/Risks/3.75.html">RISKS-3.75</A>)
</A>
</H3>
<address>
Bard Bloom 
&lt;<A HREF="mailto:bard@THEORY.LCS.MIT.EDU">
bard@THEORY.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Sun, 5 Oct 86 10:48:52 edt
</i><PRE>

&gt;From: decvax!utzoo!henry@ucbvax.Berkeley.EDU
&gt;I think talk of "total correctness" and "complete reliability" shows excess
&gt;enthusiasm rather than realistic appreciation of the situation...

"Total correctness", at least, is a technical term in program verification.
"Partial correctness" means that the program does the correct thing iff it
terminates (i.e., the program that never terminates is partially correct).
Total correctness is, partial correctness together with termination.  
All of these terms really mean "meets the mathematical specification".

  
  &gt;Another cautionary tale is the current debate about the validity of the
  &gt;Rourke/Rego proof of the Poincare conjecture.  As I understand it -- it's
  &gt;not an area I know much about -- the proof is long, complex, and sketchy,
  &gt;and nobody is sure whether or not to believe it.  And this is a case
  &gt;where the specs for the problem are very simple and obviously "right".

The proofs of program correctness are (supposed to be) checked by machines.
There's been a lot of work done (and even a little success, I think) in getting
proof techniques that can be checked automatically, and even ways of getting
the machine to do a lot of the drudgework in converting a human-style proof
into a machine one.  Of course, you have to check the proof-checker...

As I understand the area of correctness proofs, there are two major problems:
 
1) Program specifications (especially complicated ones) rarely specify what you
want the program to do.  Not a whole lot program verification can do about
this.

2) It is very hard to prove a program correct.  Loop invariants, for example,
are rather hard to come up with.  Once you have the proof, it's easy to check.

  &gt; To borrow from the theme of a PhD thesis here some years ago, proving
  &gt; programs INcorrect is much easier than proving them correct,

I agree.  The rumor around here is, the best use of program-proving techniques
is in finding bugs.

-- Bard Bloom

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.75.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.77.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-63</DOCNO>
<DOCOLDNO>IA012-000123-B023-440</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.77.html 128.240.150.127 19970217005303 text/html 16138
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:51:21 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 77</TITLE>
<LINK REL="Prev" HREF="/Risks/3.76.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.78.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.76.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.78.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 77</H1>
<H2> Wednesday, 8 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Evaluating software risks 
</A>
<DD>
<A HREF="#subj1.1">
Brian Randell
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Misapplication of hardware reliability models 
</A>
<DD>
<A HREF="#subj2.1">
Nancy Leveson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Deliberate overrides? 
</A>
<DD>
<A HREF="#subj3.1">
Mark Brader
</A><br>
<A HREF="#subj3.2">
 Ephraim
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Trusting-infallible-machines Stonehenge anecdote 
</A>
<DD>
<A HREF="#subj4.1">
Mark Brader
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  [More Aviation Hearsay?] 
</A>
<DD>
<A HREF="#subj5.1">
C Lewis
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Evaluating software risks
</A>
</H3>
<address>
Brian Randell 
&lt;<A HREF="mailto:brian%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK">
brian%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Mon, 6 Oct 86 18:59:47 gmt
</i><PRE>

In the article by Nancy Leveson in RISKS 3.72, she mentioned that a
task based on the Viking Landing software was going to be used as the
basis for some experiments on concerning fault tolerant software.
There have in fact been a number of carefully cntrolled experiments
aimed at assessing the possible cost-effectiveness of fault tolerance
in software.
I then read the Tony Hoare quote in RISKS 3.74, bemoaning the fact
that formal verification had not been used in any safety-critical
software. Offhand, I do not know of any similar controlled experiments
being performed on the cost-effectiveness of formal verification.
Indeed, it strikes me that it would be very interesting if the planned
experiments that Nancy refers to were to cover various verification and
testing, as well as, fault tolerance experiments. Ideally risks should
be quantified, so claimed remedies should be the subject of experimental
evaluation, as well as eloquent pleading.

Brian Randell - Computing Laboratory, University of Newcastle upon Tyne

  ARPA  : brian%cheviot.newcastle@ucl-cs.arpa
  UUCP  : &lt;UK&gt;!ukc!cheviot!brian
  JANET : brian@uk.ac.newcastle.cheviot

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Misapplication of hardware reliability models
</A>
</H3>
<address>
&lt;<A HREF="mailto:leveson@sei.cmu.edu">
leveson@sei.cmu.edu
</A>&gt;
</address>
<i>
Mon, 6 Oct 86 11:39:34 edt
</i><PRE>



There has been some discussion on Risks lately about the application of
hardware reliability models to software.  The purpose of
such models is to make predictions.  The accuracy of a prediction based upon
a mathematical model depends on whether the assumptions of the underlying
model fit the situation in which it is being applied.  Hardware reliability
models make such assumptions as:
   1) component failures will occur independently in independent
      replicated units
   2) the behavior of a physical component can be predicted from data
      gathered from observations of other components that are assumed
      to be similar.
   3) the design of the system is free from faults.


None of these seem to apply to software.  Attempting to come up with some
strange meaning of "wear out" so that the models can be applied to software
is begging the question.  We know that "wear out" AS IS MEANT IN THESE
MODELS does not apply to software.  Therefore, the results of applying
the models to software may be inaccurate.  The burden of proof is in
showing that the assumptions apply as originally conceived in the models.
As an example, trying to fit software to "bathtub curve" models (which 
were built by observing hardware) would seem to be a less fruitful 
line of endeavor than attempting to build models from what we observe 
about software.

    Nancy Leveson
    Info. &amp; Computer Science
    University of California, Irvine

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Deliberate overrides?
</A>
</H3>
<address>
&lt;<A HREF="mailto:decvax!utzoo!dciem!msb@ucbvax.Berkeley.EDU">
decvax!utzoo!dciem!msb@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Wed, 8 Oct 86 04:32:03 edt
</i><PRE>

City buses on this continent generally have rear or center exit doors
interlocked with the brakes, so that once the exit doors are opened,
the bus cannot be moved until they are closed.  An excellent safety
feature, yes?  You'd never move a bus while someone might be getting off...

Well, one day a few years ago, on the Toronto transit system, the exit
doors of a bus popped open spontaneously due to a malfunction in their
control system, and stayed open.  The bus was on a level crossing, and
was full, and a few seconds later the barriers started lowering as a
train approached.  The collision was frightful.

In the investigation it turned out that the buses were fitted with a
control to override the interlock, but it was in a concealed location
(for maintenance access only) and drivers were not trained in its use.
Needless to say this was promptly changed.

On the other hand, I could also cite several instances in the well-
documented history of British railway accident investigations where
both drivers and signalmen* were provided with overrides to be used
only in case of equipment malfunction, and did not believe their
equipment, and used the overrides to cause accidents.

*They WERE all men in those days.  I don't know what the modern word is.

The moral seems to be that overrides are indeed a good thing to have,
but you have to be very sure that the user knows when to use them.
And if the engineer or programmer isn't the one training the users,
this can be rather difficult.

By the way, those reading this somewhere else than on Usenet may be
interested to know that people who use an interface called Pnews to
post Usenet articles are asked:

   This program posts news to many hundreds of machines throughout the world.
   Are you absolutely sure that you want to do this? [ny]

This comes up before the message is entered; afterwards, the question

   Send, abort, edit, or list?

is asked, so the initial question is not the only chance to abort.
In effect, the extra initial confirmation asks users to override a
safety feature on every normal invocation of the program.  Is this useful?
    [ANSWER TO MARK PLEASE, NOT RISKS ON THIS QUESTION.]
Mark Brader, utzoo!dciem!msb

</PRE>
<HR><H3><A NAME="subj3.2">
Re: Overrides and tradeoffs (Risks 3.74)
</A>
</H3>
<address>

&lt;<A HREF="mailto:decvax!wanginst!wang!ephraim@ucbvax.Berkeley.EDU">
decvax!wanginst!wang!ephraim@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Tue, 7 Oct 86 20:20:56 edt
</i><PRE>

In Risks 3.74, Jerry Leichter writes:
&gt;Accidents in general are fairly low-probability events.  As such, they have to
&gt;be reasoned about carefully - our intuitions on such events are usually based
&gt;on too little data to be worth much.  Also, since we have little direct expe-
&gt;rience, we are more likely to let emotional factors color our thinking.  The
&gt;thought of being trapped in a burning or sinking car is very disturbing to
&gt;most people, so they weight such accidents much more heavily than their actual
&gt;probability of occurrence merits.
 
An interesting article on this topic (perception of risk) appeared in
Scientific American a few years back.  To summarize, small non-zero risks
have much more emotional weight than they "deserve" (statistically, that
is).  Large variations in the middle of the scale have less effect than
they deserve.  Memory fails me on how risk at the other end of the scale
(near certainty) is perceived.

Personally, I find the thought of being sent through the windshield at
least as disturbing as (and much more likely than) being trapped in a burning
car.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Trusting-infallible-machines Stonehenge anecdote
</A>
</H3>
<address>
&lt;<A HREF="mailto:decvax!utzoo!dciem!msb@ucbvax.Berkeley.EDU">
decvax!utzoo!dciem!msb@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Wed, 8 Oct 86 05:14:25 edt
</i><PRE>

In the 1973 book "Beyond Stonehenge", Gerald S. Hawkins is telling about
the digitization of the layout of Stonehenge from new aerial photos ...

    Back at the laboratory, two pictures, red and green, are
    projected.  The operator looks through special glasses.
    A miniature Stonehenge sits there in the machine, three-
    dimensional, vividly real.  A small white spot moves in
    the machine, controlled by hand dials.  It can be moved
    along the ground; up ... down ...   The machine reads height
    of stone or height of ground above datum.  The method is
    accurate, absolute, unambiguous, mechanically final.  The
    details are safely left with the engineer.

    When I saw the first photogrammetic plan I was puzzled.
    The number of stones was wrong.  There was an extra stone
    mapped in the bluestone horseshoe.

    I raised the question with Mr. Herschel.  The engineers put
    the film back in the infallible machine and redid the mea-
    surements.

    Apologies!

    The object was not a stone.  It was human.

    The error was excusable and quite understandable.  There was
    a gentleman, a sightseer (bald-headed), who happened to
    stand in a gap in the line of bluestones at the instant of
    the click-click of the passing plane.  His shadow was like
    that of a stone; his head top looked like polished dolerite.
    "Vertical object, height 5 ft 10 ins", recorded the machine.

Mark Brader

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
[More Aviation Hearsay?]
</A>
</H3>
<address>
&lt;<A HREF="mailto:mnetor!spectrix!clewis@seismo.CSS.GOV">
mnetor!spectrix!clewis@seismo.CSS.GOV
</A>&gt;
</address>
<i>

</i><PRE>
Date: Wed Oct  8 12:04:57 1986
ReSent-To: RISKS@CSL.SRI.COM

I understand and appreciate your comments in the mod.risks about nth party/
hearsay stuff.  But, from the examples you gave, in case you are really
looking for some aviation accidents partially due to obedience to the
"book", here are two - both commercial accidents at Toronto International
(Now Pearson International).  Both from MOT (then DOT) accident 
investigations:

About 15 years ago a Air Canada DC-8 was coming in for a landing.  At
60 feet above the runway, the pilot asked the co-pilot to "arm" the spoilers.
The co-pilot goofed and fired them.  The aircraft dropped abruptly onto
the runway, pulling about 4 G's on impact.  At which point one of the
engine/pylon assembly tore away from the wing - this was an aircraft 
defect because the engines were supposed to withstand this impact - a 
6 G impact is supposed to shear the mounting pins.  Not aware of this 
fact, the pilot performed what the book told him to do - go around for 
another try.  He only made it halfway around - the pylon had tore away 
a portion of the fuel tank and the aircraft caught fire and crashed in
a farmer's field killing all aboard.

In retrospect, the pilot should have stayed on the ground, contrary
to the book.  Many would have survived the fire on the ground.  However, 
it was difficult to see how the flight crew could have realized that
the aircraft was damaged as it was in the short time that they had to
decide.  The spoiler arming system was altered to make this more unlikely.

The second incident was about 8 years ago - on a Air Canada DC-9 taking
off.  During take off one of the tires blew throwing rubber fragments
through one of the engines.  One of these fragments damaged a temperature
sensor in the engine, causing an "engine fire" indication to come on in
the cockpit.  The pilot did what the book said, "abort takeoff", even
though he was beyond the safe stopping point.  The aircraft slid off the
end of the runway and into the now infamous 50 foot deep gully between
the runway and the 401 highway.  The fuselage broke in 2 places, causing
one death and several broken bones and minor back injuries.

In retrospect, if the pilot had not aborted takeoff, he would have been
able to take off successfully and come around for reasonably safe landing,
saving the aircraft and preventing any injuries.  However, there was 
absolutely no way that they could have determined that the engine was not 
on fire.  

Results: 
    - in spite of the findings, I seem to remember that the pilot was 
      suspended for some time.
    - Recommendations:
        - filling in the gully - not done
	- cutting grooves in the runways for improved braking - not done yet,
	  but the media is still pushing the MOT.  (I'm neutral on this one,
	  the MOT has some good reasons for not doing it)
	- cleaning the tarmac of burned rubber - only done once if I recall
	  correctly.

As a counter example, I offer you another:

It had become common practise for twin-otter pilots to place the props
in full reverse pitch while landing, instants before actually touching down.
This had the effect of shortening the landing run considerably over the
already short runs (twin-otter is STOL).  However, due to a number of
accidents being traced to pilots doing this too soon - eg: 50 feet up,
the aircraft manufacturer then modified the aircraft so as to prevent
reverse pitch unless the aircraft was actually on the ground.

(The above, however, is from a newspaper, and would bear closer research).

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.76.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.78.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-64</DOCNO>
<DOCOLDNO>IA012-000123-B024-29</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.78.html 128.240.150.127 19970217005320 text/html 23369
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:51:46 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 78</TITLE>
<LINK REL="Prev" HREF="/Risks/3.77.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.79.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.77.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.79.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 78</H1>
<H2> Thursday, 9 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
On models, methods, and results 
</A>
<DD>
<A HREF="#subj1.1">
Bob Estell
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Fault tolerance vs. verification experiments 
</A>
<DD>
<A HREF="#subj2.1">
Nancy Leveson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  The second Tomahawk failure 
</A>
<DD>
<A HREF="#subj3.1">
PGNeumann
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: Overrides and tradeoffs 
</A>
<DD>
<A HREF="#subj4.1">
Eugene Miya
</A><br>
<A HREF="#subj4.2">
 Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Software getting old 
</A>
<DD>
<A HREF="#subj5.1">
Ady Wiernik
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Rebuttal -- Software CAN Wear Out! 
</A>
<DD>
<A HREF="#subj6.1">
George Cole
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  "Obsolescence" and "wearing out" as software terms 
</A>
<DD>
<A HREF="#subj7.1">
Dave Benson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Obsolesence and maintenance - interesting non-software anecdote 
</A>
<DD>
<A HREF="#subj8.1">
Jon Jacky
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj9">
  FAA - Plans to replace unused computers with new ones 
</A>
<DD>
<A HREF="#subj9.1">
 McCullough
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
On models, methods, and results
</A>
</H3>
<address>
"ESTELL ROBERT G" 
&lt;<A HREF="mailto:estell@nwc-143b.ARPA">
estell@nwc-143b.ARPA
</A>&gt;
</address>
<i>
7 Oct 86 08:14:00 PST
</i><PRE>
To: "risks" &lt;risks@csl.sri.com&gt;

Models, methods, and conclusions are, as Dave Benson pointed out,
connected.  Let's explore that more.

There are at least two kinds of models: formal [well understood mathematics],
and informal [heuristics or things that just work];
and there are two approaches to model usage: induction, and deduction;
finally, a model can be in the mind of the user implicitly, or explicitly.

I'll conjecture that there is a high positive correlation between "informal,
induction, and implicitly", and an even higher positive correlation between
"formal, deduction, and explicitly."  Though a good logician would wince at
the notion of "deduction from heurustics" I'll further conjecture that all
eight cells in this model are populated,  albeit unevenly.
I'm not arguing that this SHOULD be so; I'm NOT saying HOW computer models
should work; I'm just admitting how lots of people do think.  In many cases
"fuzzy" [or worse] is a good description; e.g., we "prove" things based on a
few examples, or the opinion of an authority, often in the absence of good
scientific theory, or verifiable facts.
Humans satisfice a lot; even those of us who are primarily analytical are
also a bit pragmatic; we rarely "undo" something that works just because
we don't understand it completely.  ["Undo" means "retract" and forfeit the
benefits.  Often we do "dissect" after the fact, for better understanding.]

I think that some of the back-and-forth in RISKS is between groups at ends
of a spectrum; at  one end, there are those who are using informal models, 
based on experience, who induce conclusions from them implicitly; 
at the other end, are those who yearn for formal models, with results 
deduced explicitly.  My sympathies lie with both groups; I too yearn to
understand; but IF forced to choose, I'd take results now, and wait for 
understanding.  But in fact, I believe there's a middle course; I believe we
are already achieving some successes; and I believe we have some under-
standing.  I personally have experienced much bettter results using high
order languages, and writing modular code; hence my "understanding" that 
these techniques are "better" than some alternatives.  If this experience
is not universal, that's no surprise either.  The most intricate piece of 
code I ever wrote was an I/O driver that ran in diagnostic mode; it was very
short, more or less modular, and in a mixture of assembly and machine
language.  It solved a problem so poorly understood that I got more credit
for the project than I probably deserved.

As others in RISKS have pointed out, we need to take some care with words;
else, we'll lose the ability to understand each other.  OKAY, it was a mistake
to ever think that anyone thought that "SDI software had to be perfect."
I think that agreement represents enormous progress.   Thank you.

Now, can we proceed to define "acceptable."  [Or other terms.]
Can we begin to use some numbers?  Can we remember that Hamming was
right: "The purpose of computing is insight, not numbers."  But can we have
some numbers anyhow, just to help us understand?

Another analogy may help.  In baseball, a pitcher is credited with a "perfect
game" if no opposing batter reaches first base safely.  He doesn't have to
strike out all 27 batters; or retire each with only one pitch, by getting them
to hit easy pop-ups, or easy grounders.  [NOTE that real purists could argue
endlessly about these two cases; which is better? striking out all 27, which
requires at least 81 pitches? or retiring all 27 with only 27 pitches?]
If the definition of "perfect" is arbitrary, it doesn't matter too much, since
there are so few perfect games.  Wins, strike-outs, earned run average,
and other metrics usually help us decide who the great pitchers are.

One case we've been discussing [Viking Lander] seems to indicate that the
software was "successful" while admitting that it had flaws.  Without some
metrics, we'll rehash our differing opinions endlessly.

One closing thought about models.  It's a fact that induction is always at
risk of being upset by "the next case."  It's also true that deduction is not
able to prove anything outside the scope of the axiom set on which it is
based.  At their extremes, the one is fragile; the other, sterile.
Life should be both robust and fertile; it's more fun that way.
A judicious blending of the analytical and the practical can give us some 
clues to how near the extremes we're operating. 
                                                       Bob

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Fault tolerance vs. verification experiments
</A>
</H3>
<address>
&lt;<A HREF="mailto:leveson@sei.cmu.edu">
leveson@sei.cmu.edu
</A>&gt;
</address>
<i>
Thu, 9 Oct 86 11:06:38 edt
</i><PRE>

In Brian Randell's message (RISKS 3.77) he says:

  &gt;Indeed, it strikes me that it would be very interesting if the planned
  &gt;experiments that Nancy refers to were to cover various verification and
  &gt;testing, as well as, fault tolerance experiments. 

We are indeed doing this in the latest of our series of experiments on 
software fault tolerance (the first of which was reported in TSE in
January and the second, which involves fault detection, is currently being
written up -- both were done jointly with John Knight at the University of
Virginia). The experiment in question, which is being conducted by Tim 
Shimeall (a Ph.D. student of mine at UCI) includes comparison of 
software fault tolerance methods with various expensive validation methods 
including sophisticated testing and code reading by hierarchical abstraction.  

We would like to include formal verification also, but have not found 
funding and other support for this yet.  John McHugh at RTI may join in 
the experiment by providing versions of the program using IBM Clean Room 
techniques (a form of formal verification along with software reliability 
growth modeling is used in the development of the programs), but again 
we have not yet found funding.

The programs involve a battle management application (the Viking
problem did not turn out to be appropriate) which is based on a real
program developed by TRW (who are partially funding the experiment).
Twenty versions of the program are currently under development, and we 
should be able to report some results by next summer. 

       Nancy Leveson
       Info. &amp; Computer Science Dept.
       University of California, Irvine

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
The second Tomahawk failure
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Thu 9 Oct 86 15:25:40-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

Apparently the second Tomahawk test failure was due to a bit dropped
by the hardware, resulting in the accidental triggering of the ABORT
sequence.  (Readers may recall that a parachute opened and the missile
landed safely.)

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: Overrides and tradeoffs (Jerry Leichter)
</A>
</H3>
<address>
Eugene Miya
&lt;<A HREF="mailto:eugene@AMES-NAS.ARPA ">
eugene@AMES-NAS.ARPA 
</A>&gt;
</address>
<i>
7 Oct 1986 1247-PDT (Tuesday)
</i><PRE>

I would just like to point out the software engineering dilemma
of dealing with RISKy systems.

&gt; Subject:  Overrides and tradeoffs
&gt; Accidents in general are fairly low-probability events.

This is in inverse proportion to the use of effort taught by the 90-10
rule (90% of the time is used by 10% of the code and other variants).
RISKs are a case where the remaining 10% taking the other 90% of the time.
Perhaps, we should think about the 10% first (error handling)
and worry about the high probability events last?  I don't know,
but I did give an example earlier where this was the case.

--eugene miya,   NASA Ames Research Center
  {hplabs,hao,dual,ihnp4,decwrl,allegra,tektronix,menlo70}!ames!aurora!eugene

</PRE>
<HR><H3><A NAME="subj4.2">
Overrides and tradeoffs (Risks 3.74)
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Thu, 9 Oct 1986  09:06 EDT
</i><PRE>

The best work I have seen on this stuff is work by Kahneman and
Tversky, who identify two "heuristics" that people use to estimate
probability -- availability and representativeness.  Availability is
the ease with which one can remember a particular event, so that if
you have direct experience with something, it is more salient in your
mind, and thus you think that it is more likely.  Representativeness
is using the extent to which the features of a particular situation
match your general conception of a class of situations to determine
the probability that the situation is a member of the class, rather
than using other kinds of information to make that judgment.

A great review book on the subject is "Judgment under Uncertainty:
Heuristics and Biases", edited by Daniel Kahneman, Paul Slovic, Amos
Tversky, Cambridge University Press, 1982.

Availability and representativeness explain A LOT!

   [NOTE:  By the way, speaking of REPRESENTATIVEness, Herb has
   accepted a full-time position for one year as a "Congressional
   Fellow", sponsored by the American Association for the Advancement
   of Science.  (The purpose of the Congressional Fellowships is to take
   professional scientists, engineers, social scientists, etc. and expose
   them to the policy-making process and in turn contribute some
   scientific expertise to the decision-making process.)  It seems to me
   wonderful that he is willing to spend a year in such an enterprise.
   We expect Herb to continue participating in RISKS as an integral part
   of his job, and hope to have some inside RISKS SCOOPS.  Perhaps RISKS
   can even have an impact on Congress!  PGN]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Software getting old
</A>
</H3>
<address>
  Ady Wiernik   
&lt;<A HREF="mailto:ady%taurus.BITNET@WISCVM.WISC.EDU">
ady%taurus.BITNET@WISCVM.WISC.EDU
</A>&gt;
</address>
<i>
Sun, 5 Oct 86 16:02:07 -0300
</i><PRE>

     It has recently been suggested in this forum that software whose
environment changed over time (requiring a change in the functional
specification) might become "old" and "rotten".  One example given was
that of financial software which can't handle high inflation rate
(having insufficient number of digits in various total fields).

     Well, here in Israel we have already gone through two currency
changes: in 1977 we changed the currency from Lira to Shekel (which
was 10 Liras) and in 1985 we change it again from Shekel to new Shekel
(which equals 1000 old Shekels).  These changes affected every piece
of financial software in the market, and before each change there was
usually a period in which financial software had to be adjusted to
have more digits in total fields.  In addition to this, we had gone
from an inflation rate whose peak was 21% per month to an average 2%
per month inflation.

     Most packages survived the changeovers rather easily.

     The morale of this is - even if the environment changes
drasticly, software doesn't have to die. It all depends on how much
you are willing to pay the physicians (maintenance programmers).  Only
the software which was bad to start with (i.e. didn't sell well) will
die due to natural selection.

     Ady Wiernik     Tel-Aviv Univ.

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Rebuttal -- Software CAN Wear Out!
</A>
</H3>
<address>
&lt;<A HREF="mailto:Cole.pa@Xerox.COM">
Cole.pa@Xerox.COM
</A>&gt;
</address>
<i>
9 Oct 86 10:05 PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

	Software as it exists in the programming language (and mathematical
statements) is theoretically perfect -- a Platonian Ideal. Yet there is
a risk that the software as it is embedded in the hardware will become
distorted and "worn out".  Between background radiation, hardware
failures causing bit-changes (the resistor lets too little or too much
current through, causing a "1" to be read as a "0"), and people-caused
hardware failures (bent pins, crimped cables, etc.), there is the chance
for distortions in the software. In "Bad Bits" (Scientific American,
Feb. 1980, p. 70, there is a reference to radiation failures
(presumably from background radiation) causing random failures -- I
believe the figures are 3,000 / million hours of operation in a 256k
charge-coupled device. 
	I do not think these sources are currently a major part of the
"software failures" in the industry; design, specification and
maintenance problems seem to be far more prevalent because of the lack
of attention paid to human engineering problems -- the basic presumption
seems to be that Murphy's Law doesn't hold for people (programmers or
administrators or scientists). As computing machines become more "dense"
though, this real possibility of unpredictable failure ought to be
considered. 

George S. Cole, Esq.
GCole@sushi.stanford.edu
 	 
</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
"Obsolescence" and "wearing out" as software terms
</A>
</H3>
<address>
Dave Benson 
&lt;<A HREF="mailto:benson%wsu.csnet@CSNET-RELAY.ARPA">
benson%wsu.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Mon, 6 Oct 86 18:16:53 pdt
</i><PRE>

JS&gt;Dave Benson nicely identifies a distinction between becoming obsolete
JS&gt;and wearing out, and argues that only the former applies to software.

Thank you, but not quite. Software can also become lost, dissipate or
in various other manners disappear.  One way this might occur is if
all copies of the media containing the bit patterns wore out.  But in
most situations, software becomes obsolete before it disappears.

JS&gt;There is another effect that isn't exactly captured by the words "to
JS&gt;become obsolete." ...
JS&gt;Some people have in mind the impairment and diminished usability
JS&gt;caused by this effect when they use words like "wears out" or "rots".

Software suffers so-called "maintenance" since the changing requirements
require modification.  This is common enough in other engineered
artifacts:  Coal-fired steam plants have exhaust scrubbers added, etc.
The diminished usability in software is caused by the rapidly changing
external conditions, thus "obsolesence" is an entirely appropriate term.
The fact that the re-engineering of the artifact in the attempt to
keep the artifact current is poorly done only causes the artifact to
become obsolete more rapidly than if the re-engineering was done well.

JS&gt;I guess we need a plain English word for it so that neophytes won't
JS&gt;think that computers that haven't been oiled properly rub too hard
JS&gt;on the bits.
How about "obsolete"?  Here are some examples.  Some are fact, others
fiction, still others opinion.  Decide whether the word fits before
coining a new term.

  Arpanet is rapidly showing its obsolescence
  under the dramatically increased traffic.  While the obsolescence of the
  Enroute Air Traffic Control System is appearent to the controllers, it
  is judged that providing computers with 3 times current speed will keep
  the system operational until the year 2010.  The financial transaction
  system of the Bank of Calichusetts is showing its obsolesence by the
  large losses to so-called computer criminals.  Unix will be obsolete by
  the year 2010, then being replaced by Yaos, which is currently in
  advanced engineering at the Yet Another Company.  The LGP-30 is an
  obsolete computer.  Sage is an obsolete software system. SDI sofware
  will be obsolete before it is written.
  
I shudder at the thought that this may become so popular that the gerund
"obsolescing" will appear on RISKS.

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
Obsolesence and maintenance - interesting non-software anecdote
</A>
</H3>
<address>
Jon Jacky
&lt;<A HREF="mailto:jon@june.cs.washington.edu ">
jon@june.cs.washington.edu 
</A>&gt;
</address>
<i>
Tue, 7 Oct 86 22:49:43 PDT
</i><PRE>

Hammersmith Hospital, in London England, closed down its research
cyclotron last year. The cyclotron was the first ever to be dedicated
to medical research and applications (mostly, production of
radioactive tracer chemicals and treatment of cancer with neutron
beams), and began running in 1955.  According to one of the physicists
on the staff, who gave a seminar at the University of Washington
yesterday, an important factor in the decision to close the facility
was that the original designer is scheduled to retire this year, and
he is the one person who really understands how to keep it going and
modify it.  England's Medical Research Council (or MRC, sort of like
NIH in this country) is building a replacement cyclotron at a
different site at the cost of many millions of pounds.

-Jonathan Jacky
University of Washington
       [There is of course an analagous problem in software.  PGN]

</PRE>
<A NAME="subj9"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj9.1">
FAA - Plans to replace unused computers with new ones
</A>
</H3>
<address>
&lt;<A HREF="mailto:mccullough.pa@Xerox.COM">
mccullough.pa@Xerox.COM
</A>&gt;
</address>
<i>
Tue, 7 Oct 86 11:32:07 PDT
</i><PRE>
{AP News Wire, 6-Oct-86, 9:59}

   Federal officials say a problem-plagued air traffic control system installed
 at many U.S. airports four years ago probably will be replaced before most of
 the equipment is ever used. The multimillion-dollar system was supposed to
 make radar screens clearer, help track aircraft that do not carry radar signal
 equipment and otherwise relieve some of the load on the existing system. But
 engineers have been stumped by programming problems that have rendered the
 Sensor Receiver and Processor System, or SRAPS, virtually useless, the Orange
 County Register reported Saturday. The agency expects a new $500 million
 Westinghouse system ordered 2 1/2 years ago to arrive in November or December
 for testing, said FAA engineer Marty Pozesky. Known as the ASR-9, for Airport
 Surveillance Radar, the system will affect virtually every U.S. airport, he
 said, adding it may be four years before it is fully operating. The SRAPS
 computers were purchased in 1981 from the now-defunct Sperry Univac
 Information Storage Systems. Researchers at the successor company, Sperry,
 have continued to seek a solution to the software problems, but no longer have
 the help of FAA engineers, Pozesky said. "We don't think the solution will be
 there, so we have really stopped searching," Pozesky said by telephone
 Saturday from his Silver Spring, Md., home.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.77.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.79.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-65</DOCNO>
<DOCOLDNO>IA012-000123-B024-52</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.79.html 128.240.150.127 19970217005337 text/html 12152
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:52:05 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 79</TITLE>
<LINK REL="Prev" HREF="/Risks/3.78.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.80.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.78.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.80.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 79</H1>
<H2> Sunday, 12 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
China Air incident... the real story 
</A>
<DD>
<A HREF="#subj1.1">
Peter G. Trei
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Air-Traffic Control Spoof 
</A>
<DD>
<A HREF="#subj2.1">
Peter G. Neumann
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Aviation Accidents and Following Procedures (<A HREF="/Risks/3.77.html">RISKS-3.77</A>) 
</A>
<DD>
<A HREF="#subj3.1">
Matthew Waugh
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  DC-9 crash again 
</A>
<DD>
<A HREF="#subj4.1">
Peter Ladkin
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
China Air incident... the real story
</A>
</H3>
<address>
Peter G. Trei 
&lt;<A HREF="mailto:OC.TREI@CU20B.COLUMBIA.EDU">
OC.TREI@CU20B.COLUMBIA.EDU
</A>&gt;
</address>
<i>
Mon 13 Oct 86 01:04:22-EDT
</i><PRE>
To: risks@CSL.SRI.COM

Excerpted from 'Tumbledown Jumbo', an article in the Oct 86' issue of
FLYING magazine, concerning the China Airlines 006 incident of Feb 86.

ellipses and contractions in [square brackets] are mine.
...
   At one point the autothrottle brought the engines back to about zero 
thrust. ...as the throttles came forward again, the number-four engine did
not respond. The flight engineer ... told the captain that the engine 
had flamed out.
   Maximum restart altitude is 30,000 feet [the plane started at 41,000].
The captain told the first officer to request a lower altitude. He then
told the engineer to attempt a relight, even though the plane ... was still
at 41,000. The restart attempt was unsuccessful.
   The captain ... released the speed and altitude hold on the autopilot. The
autopilot was now programmed to maintain pitch attitude and ground track. The
airplane continued to lose speed gradually ... and the captain eventually 
disengaged the autopilot completely and pushed the nose down.
   At the same moment, the airplane yawed and rolled to the right. The
captain's attitude indicator appeared to tumble [as did two backups].
The airplane had now entered the clouds. At the same time ... the other three
engines quit.

[paragraph omitted, describing speed varying between Mach .92 and 80 knots,
as crew attempts recovery under up to 5G accelerations.]

   After ... more than two minutes, the 747 emerged from the clouds at 11,000
feet and the captain was able to level it by outside reference. Coincidentally,
he felt that the attitude indicators 'came back in' at this point. [engines
1,2, &amp; 3 restart themselves, and 4 responds to a checklist restart].

Initially the captain decided to continue ... [but it was noticed
that] the landing gear was down and one hydraulic system had lost all
its fluid. ... the captain decided to land at San Francicso. The plane operated
normally during descent, approach and landing.

    [Later analysis showed that engine four had NOT flamed out, but just stuck
at low thrust due to a worn part. The others were also responding to the 
throttles very slowly, a common problem at 41,000 feet. The NTSB inquiry
concluded that...] the captain had become so preoccupied with the dwindling
airspeed that he failed to note that the autopilot, which relied on ailerons
only, not the rudder, to maintain heading, was using the maximum left control-
wheel deflection available to it to overcome the thrust asymmetry due to the 
hung outboard engine. When the right wing nevertheless began to drop, ...
the captain didn't notice the bank on the attitude indicator ... . When he
did notice it, he refused to believe what he saw. At this point, ... the
upset had begun and the captain and first officer were both spatially 
disorientated.

[...]

    Once the erroneous diagnosis of a flameout had been announced, ... the
captain placed excessive reliance on the autopilot.... When he finally
disengaged it, and put himself 'back into the feedback loop' it was at a
critical moment, and he could not adjust quickly enough to the unexpected
combination of control feel and instrument indications to prevent the upset.

END OF QUOTATIONS.

     The rest of the article is devoted to RISKS-style analysis of use
of automatic systems. To give a more down-to-earth (pun intended)
analogy, suppose your car was equipped with an AI 'drivers assistant',
which handled all normal highway driving. Suppose further, at night,
with you drowsy and at 60 mph, the right front wheel blows out. The AI
blasts the horn to alert you, and applies substantial left torque to
the steering wheel to keep it straight. You realize your in trouble,
grab the wheel, and turn off the AI. The wheel immediatally jumps out
of your hands to the right (you didn't know how much torque the AI was
applying), and the car swerves off the road...

    The use of automated systems to handle routine operations of critical
systems, with dangerous situations suddenly dumped in the hands of human
operators, presents a new Risk... that they may not fully understand the
ramifications of the problem during the critical transition time.

    A co-worker of mine who has worked in both the Navy and civilian
nuclear programs tells me that Navy reactor systems are designed to keep
humans in the loop. The only thing the automated systems can do without
a person is 'scram' or shut down the reactor. Changes in power level,
opening and shutting valves, pulling control rods, operating pumps, etc,
must be performed by the people constantly tending the reactor. Thus, the
system cant very easily spring surprises on the operators.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Air-Traffic Control Spoof
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Sat 11 Oct 86 20:03:57-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

Some of you may have missed a recent set of rather serious breaches of the
integrity of the air-traffic control system.  It is another important
instance of a masquerading spoof attack typified by the Captain Midnight
case (although via voice rather than digital signals).  [Again note the
October 86 issue of Mother Jones noting similar vulnerabilities and the ease
of performing attacks.]

  Washington Post, 8 October 1986

  MIAMI -- A radio operator with a ``bizarre sense of humor'' is posing as
  an air traffic controller and transmitting potentially dangerous flight
  instructions to airliners, and pilots have been warned about it, an
  Federal Aviation Administration spokesman said.  Two fake transmissions
  have occurred in the last week, and one caused a premature descent, said
  Jack Barker of the FAA's southern region in Atlanta.  ``There have been no
  dangerous incidents, but the potential for danger is there.  It's more an
  annoyance than a safety problem,'' Barker said from an FAA meeting in
  Washington.  Barker said the operator uses two frequencies that air
  traffic controllers use to tell pilots how to approach Miami International
  Airport.  The transmissions began Sept. 25, and the last was Friday [3
  Oct], he said.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Aviation Accidents and Following Procedures (<A HREF="/Risks/3.77.html">RISKS-3.77</A>)
</A>
</H3>
<address>
&lt;<A HREF="mailto:ihnp4!houxm!mtuxo!pegasus!phoenix!poseidon!popeye!naples!mjw@ucbvax.Berkeley.EDU">
ihnp4!houxm!mtuxo!pegasus!phoenix!poseidon!popeye!naples!mjw@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Fri, 10 Oct 86 11:50:44 PDT
</i><PRE>

The accident report involving a British Airways 737 at Manchester Airport
was released recently. The aircraft suffered an engine compressor failure on
take-off. The aircraft instruments indicated something else (I'm a little
hazy about exactly what, I think it was a tire burst), and standard
operating procedure was to turn clear of the runway, basically I believe to
clear the runway for other traffic. This the pilots did, bringing the wind,
which had been dead ahead to blow from the now burning engine and wing, onto
the fuselage. Multiple lives were lost, etc.

It would appear from this that had the pilots performed an abort and
maintained the runway, all that would be required for safety reasons, the
deaths could have been reduced or avoided. However the operating procedure,
for operational (not safety) reasons mandated otherwise and worsened an
otherwise pretty terrible situation.

UUCP   : {ihnp4|mtuxo}!naples!mjw	Matthew Waugh
ATTMAIL: attmail!mjw			AT&amp;T IS, Lincroft, N.J.
                        		Telephone : (201) 576-3362

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
DC-9 crash again
</A>
</H3>
<address>
Peter Ladkin
&lt;<A HREF="mailto:ladkin@kestrel.ARPA ">
ladkin@kestrel.ARPA 
</A>&gt;
</address>
<i>
Fri, 10 Oct 86 14:50:49 pdt
</i><PRE>

Danny Cohen's point about accuracy is well taken. The incident I was trying
to refer to was the crash of Eastern 212, a DC-9, in Charlotte, N.C. I
apologise to Risks readers for not confirming this before posting.

Danny and I have exchanged letters on the issue of *deliberate override*.
Danny considers the action of turning off the LAAS to be both non-deliberate
and not an override.  I still consider it both deliberate and an override.
It seems to hinge on whether habitual actions can be described as
deliberate, and on whether not following prescribed procedure upon receipt
of a warning can be considered an override.

Peter Ladkin

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.78.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.80.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-66</DOCNO>
<DOCOLDNO>IA012-000123-B024-76</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.80.html 128.240.150.127 19970217005352 text/html 16380
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:52:19 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 80</TITLE>
<LINK REL="Prev" HREF="/Risks/3.79.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.81.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.79.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.81.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 80</H1>
<H2>Wednesday, 15 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
US Navy reactors 
</A>
<DD>
<A HREF="#subj1.1">
Henry Spencer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Data Protection Act Risks 
</A>
<DD>
<A HREF="#subj2.1">
Lindsay F. Marshall
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Is Bours(e)in on the Menu? 
</A>
<DD>
<A HREF="#subj3.1">
Martin Minow
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: Software Wears Out 
</A>
<DD>
<A HREF="#subj4.1">
anonymous
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
US Navy reactors
</A>
</H3>
<address>
Henry Spencer
&lt;<A HREF="mailto:decvax!utzoo!henry@ucbvax.Berkeley.EDU ">
decvax!utzoo!henry@ucbvax.Berkeley.EDU 
</A>&gt;
</address>
<i>
Tue, 14 Oct 86 17:56:36 edt
</i><PRE>

  &gt;     A co-worker of mine who has worked in both the Navy and civilian
  &gt; nuclear programs tells me that Navy reactor systems are designed to keep
  &gt; humans in the loop. The only thing the automated systems can do without
  &gt; a person is 'scram' or shut down the reactor...  Thus, the
  &gt; system can't very easily spring surprises on the operators.

A probable contributing factor here is that the US Navy's submarine people
do not trust automation at all in crucial roles.  For example, US subs have
no autopilots, even though they spend most of their time at constant speed
and depth.  They are "flown" manually at all times.  This is not so much a
matter of keeping the operators alert and informed as it is a matter of
complete distrust of complexity and automation in submarines.  This is a
significant constraint on submarine design, in fact.  Modern subs generally
have a fairly symmetrical set of vertical and horizontal fins at the tail.
Looked at from behind, it's a cross shape.  There would be advantages to
using an X shape instead, just shifting the whole cluster 45 degrees:  this
would permit grounding the sub on the bottom without damage to the bottom
fin, and would permit docking against a straight dock without worries about
banging one of the horizontal fins against the dock.  The US Navy does not
think highly of the idea, because it would require a mixing box of some kind
(which could be purely mechanical!) to turn the horizontal and vertical
control inputs into rudder/elevator motion.  That's how deep the distrust of
complexity runs.  I'm not surprised that they have manually- controlled
reactors.

The USN also has an outstanding reactor safety record -- no big accidents,
no serious radiation releases -- with a stable of reactors comparable in
numbers (although not in output) to the entire US nuclear-power industry.
They are very fussy about materials, assembly, and operator training.

				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

   [Intriguing.  I have frequently heard it said -- by Nancy Leveson and
    others -- that the nuclear power technology is so sensitive that they
    feel they cannot afford to use computers!  PGN]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Data Protection Act Risks  
</A>
</H3>
<address>
"Lindsay F. Marshall" 
&lt;<A HREF="mailto:lindsay%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK">
lindsay%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Wed, 15 Oct 86 14:36:27 gmt
</i><PRE>

Police find a Catch 22 for data victim - From The Guardian

The police are ready to challenge the new right to compensation guaranteed
by the Data Protection Act to people injured through the passing of
inaccurate information.  Hertfordshire Police, which wrongly suggested to
Tayside Regional Council that a woman it was considering appointing had a
criminal record, has denied that the woman has any claim to compensation.
Under the Data Protection Act, all agencies - including the police - which
hold information electronically are liable to damage claims for any harm
which inaccuracies create for people on their records.  But Hertfordshire
Police has produced a Catch 22 defence.  In a letter to the woman's
solicitor, the force suggests that the woman has no claim to compensation.
The police now conceded that the woman does not have a criminal record but
go on to argue that she is therefore not on their records.  As she is not a
"data subject" she cannot be eligible for compensation.

Mr.  Eric Howe, the Data Registrar, said yesterday that he would resist such
an interpretation of the act.  One problem for the woman, Mrs Anne Trotter,
of Kirriemuir, Tayside, would be the cost of the court action.  There is no
legal aid in such cases.  The Data Registrar can initiate criminal
prosecutions but cannot sponsor civil actions.  The case would cost over
1,000 pounds.

The mistake happened earlier this year.  Tayside Regional Council social
work department, which was considering appointing Mrs.  Trotter to a special
fostering programme for delinquent teenagers, followed the recommended
procedure of checking the criminal records of its applicants.  The authority
wrote to the police in Hertfordshire, where Mrs Trotter had lived for a
period, and was informed that two separate sets of "convictions are recorded
against Anne Trotter, who appears identical with the applicant." They
involved thefts in Newcastle upon Tyne in 1942 and theft and false pretences
in Newcastle in 1947.

Anne Trotter's maiden name was Lawson until she married in 1954.  In 1942
she was 15 years old and was still at school in Arbroath.  The police were
given her maiden name.  Mrs Trotter was so upset by the incident that she
decided to drop her application and take up a temporary teaching post.  She
asked the social services department for a copy of the police letter and,
unusually, was given one.  The right of access to such letters does not come
into force until November next year.  

Later, after hearing about the Data Protection Act, she took it to a
solicitor in Dundee.  He wrote to the Hertfordshire Police on July 3 asking
for compensation.  The police replied on July 8, denying responsibility.
The force said its letter had only said the Newcastle offender "appears
identical with the applicant."  The letter went on to claim: "The fact of
the matter is that your client is not a data subject within the terms of the
Data Protection Act as it is now clear ...  that no records are held in
respect of your client."

Mr Kevin Veal, the solicitor, sent a second letter which said: "It seems
to use that insufficient care was given to the issue.  For example, it
must have been obvious to anyone compiling the report that a young girl
born in 1927 under the name of Lawson could not have been convicted
under the name of Trotter in 1942. 

The case is made more complicated by the fact that the police supplied
the information on April 21 but the compensation provisions of the act
only came into force on May 11.  There was no retraction, however, until
July 8 and no attempt by the police in the letters to use the May 11
date as the reason for not providing compensation. 

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Is Bours(e)in on the Menu?   
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
15-Oct-1986 1530
</i><PRE>
From: minow%regent.DEC@decwrl.DEC.COM
                  (Martin Minow, DECtalk Engineering ML3-1/U47 223-9922)

                BEAR MARKET MEANS BARGAIN FOR DINERS
                          By Paul Lewis

    (reprinted without permission from the New York Times News Service)

PARIS - The two hungry diners sat down, turned expectantly to a flickering
computer screen on a nearby stand and began studying the latest quotations.
The news seemed ominous.  Making money would not be easy in today's luncheon
market.

The scene was La Connivence, a small new bistro-style restaurant at 6 Rue
Feydeau, a stone's throw from the Paris Bourse, or stock exchange.  As with
stocks on the exchange, the laws of supply and demand determine the price
diners at La Connivence pay for a meal.  (The name, La Connivence, means
complicity, with the slightly shady overtones appropriate for a gambling den
of sorts.)

As patrons place their orders in the austere ground-floor dining room, one
of the owners, Jean-Claude Trastour, enters them into a computer which
promptly adjusts the menu prices to reflect demand.  Popular dishes, like
popular stocks, go up in price while less popular ones decline.

Timorous diners may choose to pay the quoted price for a dish at the
moment they order it.  That is called eating on the march comptant, or
cash market.  If the price rises while these diners are tucking in, they
have done very well for themselves.  If the price falls, they get
indigestion.  It is the safe way to eat - safe and dull.

More adventurous folks play the futures market, the march a terme,
agreeing to pay the price quoted when they call for the check at the end
of their meal.  Naturally, they hope the price will have fallen by that
fateful moment.  But hopes may be dashed by a flurry of buying, and the
price may easily shoot up.  Worse indigestion.

The newly seated diners began preparing their gambling strategy by reading
the trends.  They saw that the prices of several dishes had already fallen
by close to 6 francs--the limit for price changes up or down in any one
eating-trading session.  (A dollar is worth about 7 francs.)  That left
little room for further decline.  There would be no point in ordering any of
those dishes, no matter how delectable--unless, of course, the diner was
more interested in eating than in successful speculation.

The computer screen flashed chute du filet mignon, indicating that the price
of that choice steak had already fallen 5 francs, to 50 francs a serving.  A
veal casserole with herbs had slipped 4 francs, to 48 francs.
 A rack of lamb chops for two, down 10 francs, was priced to sell for
110 francs a serving.  As for the haddock, the computer reported a
"sharp fall" of 5 francs a portion, to 57 francs.

Other dishes were doing better.  The screen showed that a "stampede" of
orders for lotte had pushed the price of that pleasant Mediterranean
fish up 4 francs to 62 francs a portion, making it an interesting
speculation.  If diners played the forward market, the price might be
substantially lower when the time came to pay; of course, it could still
rise another 2 francs before reaching the 6 francs ceiling.

Occasionally, a diner's greed is outweighed by the thought of what he would
have to eat to turn a profit.  An example: "Victorious advance of the
stuffed pigs' trotter," the computer flashed, marking it up 5 francs, to 43
francs.  Surely it could only fall.  But a lunch of pigs' feet?

In the end, the diners chose a conservative strategy, ordering the special
of the day, saddle of lamb, on the marche a terme.  The lamb was trading at
39 francs a portion; up a modest 2 francs for the day thus far.

The check arrived for the conservative diners: 228 francs for two, which is
pretty good by Paris standards since it included a bottle of Beaujolais, a
cheese-filled ravioli from the French Alps for a starter, homemade apple
tart, and coffee.  But the roast saddle of lamb stood at 38 francs, only a
meager 1 franc cheaper than when it was ordered.  Down the street, the
Bourse was having one of its best days ever.

      [Inside tip: Sell-SHORT-Ribs, Buy-LONGustine.  Bon appetit!  Pierre]

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: Software Wears Out
</A>
</H3>
<address>
Anonymous
&lt;<A HREF="mailto:[...] ">
[...] 
</A>&gt;
</address>
<i>
Mon, 13 Oct 86 08:15:06 [...]
</i><PRE>

       [I have been rejecting almost all messages on this subject, in that
       (1) the topic was not converging, and (2) the discussion might better
       belong in SOFT-ENG@MIT-XX.  But this somewhat historical note seems
       worth including -- along with this note explaining that I have been
       throttling other contributions.  PGN]

I have to remain anonymous because my management lives in fear that someone
who works for them may post something dumb.  Herewith, I justify their most
morbid fears.

The comments on software "wearing out" vs. becoming obsolete seem to me to
be dancing around the issue.  L.A. Belady and M.M. Lehman addressed this
matter in a seminal paper: "Programming System Dynamics, or the Meta-dynamics 
of Systems in Maintenance and Growth" (IBM Research, RC 3546, Sept 17, 1971).

The authors maintain that systems do have a "lifetime," and so in that
sense, they may be supposed to wear out, although they do not use that term;
nor do they say that software becomes obsolete.  Instead, their measure is
entropy.  When the programming system's entropy is low, its ability to do
"work" on its environment is high, and vice-versa.

A system at release, or shortly thereafter, possesses low entropy.
Maintenance and enhancement over time increase the entropy until the
marginal cost of the next required set of fixes and/or enhancements
approaches, say, the amounts expended on the system up to that point.
Entropy is then high, and the system may be said to be "worn out."

This is at best a poor precis of a very elegant paper; the gentle reader is
referred to the original for a deeper insight into the reasons why software
wears out.

   [Among all the complaints that software is static and -- in never changing 
    -- should not be said to "wear out", we note that it is often NOT static,
    which is of course a large part of the problem.  In the other hand one 
    might say that the INTERFACE wears out rather than the software.  But
    let us not quibble on this one any more.  PGN]
 
</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.79.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.81.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-67</DOCNO>
<DOCOLDNO>IA012-000123-B024-105</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.81.html 128.240.150.127 19970217005407 text/html 17941
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:52:34 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 81</TITLE>
<LINK REL="Prev" HREF="/Risks/3.80.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.82.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.80.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.82.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 81</H1>
<H2> Sunday, 19 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
System effectiveness is NOT a constant! 
</A>
<DD>
<A HREF="#subj1.1">
anonymous
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Aircraft self-awareness 
</A>
<DD>
<A HREF="#subj2.1">
Scott Preece
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: US Navy reactors 
</A>
<DD>
<A HREF="#subj3.1">
Brint Cooper
</A><br>
<A HREF="#subj3.2">
 Eugene Miya
</A><br>
<A HREF="#subj3.3">
 Stephen C Woods
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Editorial on SDI 
</A>
<DD>
<A HREF="#subj4.1">
Michael L. Scott
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
System effectiveness is NOT a constant!
</A>
</H3>
<address>
&lt;<A HREF="mailto:[anonymous]">
[anonymous]
</A>&gt;
</address>
<i>
16 Oct 86 20:03:00 [...]
</i><PRE>
To: risks@sri-csl

There seems to be a tendency in the current SDI debate to fall into an old 
engineering fallacy: that systems scale up linearly.  Everyone seems to avoid
this trap when talking about cost and effort--it seems to be well accepted 
that a 10-million line program is much harder than 10 1-million line programs--
but (most) people are *not* avoiding the trap when they speak of SDI's 
effectiveness.  A recurrent argument seems to be that "SDI will be 80% 
[to use a number currently being bandied about] effective against a Soviet
attack of N missiles; thus the Soviets would have to build and launch 5N 
missiles in order to have N missiles reach their targets, which would be
economically ruinous."  The implicit assumption is that if SDI is x% 
effective against N, it will continue to be x% effective against N'.  This
is fallacious unless x is very close to 0 or 100%.  Assuming 80% effectiveness
and 1000 missiles, SDI stops 800.  Using the reasoning above, against 2000 
missiles, SDI would stop 1600; but this cannot be so.  If 1000 missiles 
strains the system to the point that it can only stop 800, why would anyone 
think it could stop more when the number of missiles and decoys is doubled, 
straining the system's ability to identify, track, and destroy missiles at 
least twice as much?  Or to put it another way, if SDI could stop 1600 out 
of 2000, shouldn't it be able to stop 1600 out of, say, 1800 (1800 is surely 
an easier problem than 2000!).  Or turn the argument around: if SDI can stop 
800 out of 1000--80% effectiveness--does this mean it can stop only 80 out 
of a 100-missile attack?  Or 8 out of a 10-missile attack?

When anyone says that SDI will have such-and-such effectiveness, they
must be made to state the assumptions used to calculate that effectiveness.  
Otherwise the numbers are meaningless.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Aircraft self-awareness
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%ccvaxa@GSWD-VMS.ARPA">
preece%ccvaxa@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Tue, 14 Oct 86 10:15:09 cdt
</i><PRE>

A lot of recent RISKS messages have discussed one kind or another of
aircraft accident.  Many of the reports have included things like "The pilot
thought [X] but in fact [Y]" or "[X] occurred, though the indications were
that [Y] had occurred" or "[X], though there was no way for the flight crew
to know that".

So, what's going on in the area of improving flight crew/control system
awareness of the state of basic external structures?  Is anyone considering
whether the FAA should require external cameras or periscopes so that (for
instance) the pilot could find out that her entire vertical stabilizer had
fallen off or her starboard outboard engine exploded?

While there are many cases where the pilot would not, in any case, have time
to check, there are also cases like the Japan Airlines crash where the plane
stayed up for some time but the pilot had no way to determine the gross
condition of the control surfaces.  Some reports have said that that plane
might have been saved if the pilot had known what he had to compensate for.

Given that we are depending more and more on automated controls, should we
be spending more effort on sensors that can determine more basic kinds of
information?  Should the control surfaces be instrumented so that the flight
controls can tell the captain "Oh, the starboard outboard engine is no
longer on its pylon and the outer flaps on that wing seem to be missing." as
opposed to current systems just recognizing the effects of that loss and
trying to compensate, with the risk that the operator will be unaware of the
magnitude of that compensation and forced to guess at the state of the
aircraft by observing what the control system is doing to deal with the
effects of that state ("Oh, I'm having to turn the rudder vigorously to port
to maintain my heading; can't say why.").

scott preece, gould/csd - urbana
uucp:	ihnp4!uiucdcs!ccvaxa!preece

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
 Re: US Navy reactors
</A>
</H3>
<address>
    Brint Cooper 
&lt;<A HREF="mailto:abc@BRL.ARPA">
abc@BRL.ARPA
</A>&gt;
</address>
<i>
Thu, 16 Oct 86 8:33:57 EDT
</i><PRE>

Henry Spencer writes:
&gt; A probable contributing factor here is that the US Navy's submarine people
&gt; do not trust automation at all in crucial roles...  That's how deep the 
&gt; distrust of complexity runs.  I'm not surprised that they have manually- 
&gt; controlled reactors.
Then, he observes:
&gt; The USN also has an outstanding reactor safety record -- no big accidents,
&gt; no serious radiation releases -- with a stable of reactors comparable in
&gt; numbers (although not in output) to the entire US nuclear-power industry.
&gt; They are very fussy about materials, assembly, and operator training.

Perhaps we should suspect that the safety record follows directly from
the suspicion?
                                        Brint

</PRE>
<HR><H3><A NAME="subj3.2">
RE: Reactors of the USN
</A>
</H3>
<address>
Eugene Miya
&lt;<A HREF="mailto:eugene@AMES-NAS.ARPA ">
eugene@AMES-NAS.ARPA 
</A>&gt;
</address>
<i>
Thu, 16 Oct 86 09:14:52 pdt
</i><PRE>

I generally concur with Henry Spencer's accessment.  The USN is very
conservative about its use of proven technologies and reliability
(also notice all new Navy jets have two engines [exclude older
A-4, A-7, and F-8s]).  But, while the Navy's record is certainly
outstanding, I must point out there is a question about "no big accidents."

One of the major contending theories on the loss of the USS Thresher in 1964
was sudden loss of reactor power.  We will never really if this is the
case, but it cannot ignored.

Excellent reading about the safety record, the conservativitism, and the
development of the nuclear navy is found in the 700+ page unauthorized
biography of Rickover.

--eugene

</PRE>
<HR><H3><A NAME="subj3.3">
       US Navy reactors [<A HREF="/Risks/3.80.html">RISKS-3.80</A> DIGEST]
</A>
</H3>
<address>
          Stephen C Woods 
&lt;<A HREF="mailto:scw@LOCUS.UCLA.EDU">
scw@LOCUS.UCLA.EDU
</A>&gt;
</address>
<i>
Fri, 17 Oct 86 11:43:17 PDT
</i><PRE>

There is another factor to consider here, redundancy.  Submariners are ALL
cross trained EXTENSIVELY (the ideal is that everyone can do everything,
usually they come fairly close to the ideal).

    Why, you may ask, does the Navy go to such lengths?  The answer is
fairly simple; these are WARSHIPS, they need to be able to function even
after suffering SEVERE damage and heavy casualties.  Just for normal day to
day operations there are at least 2 people for every job (watch on and watch
off), usually there are 3, often there are 4 or more.

The following from net.aviation may be of interest to you. (ESP the
quote).  You may be interested in the whole discussion there.     [scw]

&gt;From: wanttaja@ssc-vax.UUCP (Ronald J Wanttaja)
&gt;Newsgroups: net.aviation
&gt;Subject: Re: Problems with flying by the book (a pithy comment)
&gt;Date: 14 Oct 86 15:58:15 GMT
&gt;Organization: Boeing Aerospace Co., Seattle, WA

&gt;&gt; I understand and appreciate your comments in the mod.risks about nth party/
&gt;&gt; hearsay stuff.  But, from the examples you gave, in case you are really
&gt;&gt; looking for some aviation accidents partially due to obedience to the
&gt;&gt; "book", here are two - both commercial accidents at Toronto International
&gt;&gt; (Now Pearson International).  Both from MOT (then DOT) accident 
&gt;&gt; investigations:
&gt; 
  [...]

&gt;"Rule books are paper:  They will not cushion a sudden meeting of stone and
&gt; metal."
&gt;				     - Earnest K. Gann

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Editorial on SDI
</A>
</H3>
<address>
Michael L. Scott
&lt;<A HREF="mailto:scott@rochester.arpa ">
scott@rochester.arpa 
</A>&gt;
</address>
<i>
Sat, 18 Oct 86 17:51:36 edt
</i><PRE>

The following is an op-ed piece that I wrote for the Rochester, NY,
DEMOCRAT AND CHRONICLE.  It appeared on page 4A on September 29, 1986.

        'STAR WARS' CAN'T SUCCEED AS SHIELD, HAS OFFENSIVE CAPABILITY
   
        Can the Strategic Defense Initiative succeed?  The  answer  depends
   critically  on what you mean by success.  Unfortunately, the public per-
   ception of the purpose of SDI differs dramatically from the actual goals
   of the program.
   
        In his original "Star Wars" speech, President  Reagan  called  upon
   the   scientific   community  to  make  nuclear  weapons  "impotent  and
   obsolete."  He has maintained ever since that this is the SDI  goal:  to
   develop an impenetrable defensive shield that would protect the American
   population from attack.  With such a shield in place,  nuclear  missiles
   would  be useless, and both the United States and the Soviet Union could
   disarm.
   
        Can such a shield be built?  The most qualified minds in the  coun-
   try  say  "no."   In  an  unprecedented  move, over 6,500 scientists and
   engineers at the nation's research Universities have signed a  statement
   indicating  that "Anti-ballistic missile defense of sufficient reliabil-
   ity to defend the population of  the  United  States  against  a  Soviet
   attack  is  not  technically  feasible."  The signatures were drawn from
   over 110 campuses in 41 states, and include 15 Nobel Laureates  in  Phy-
   sics and Chemistry, and 57% of the combined faculties of the top 20 Phy-
   sics departments in the country.  Given the usual  political  apathy  of
   scientists and engineers, these numbers are absolutely staggering.
   
        The obstacles to population defense include a vast array  of  prob-
   lems  in physics, optics, astronautics, computer science, economics, and
   logistics.  Some of these problems can be solved with  adequate  funding
   for  research;  others  cannot.  Consider the single subject of software
   for "Star Wars" computers.  As a researcher in parallel and  distributed
   computing, I am in a position to speak on this subject with considerable
   confidence.  The computer programs for  population  defense  would  span
   thousands  of  computers  all  over the planet and in space.  They would
   constitute the single largest software system ever  written.   There  is
   absolutely  no  way  we  could ever be sure that the software would work
   correctly.
   
        Why not?  To  begin  with,  we  cannot  anticipate  every  possible
   scenario  in  a  Soviet  attack.   Human commanders cope with unexpected
   situations by drawing on their experience, their common sense, and their
   knack for military tactics.  Computers have no such abilities.  They can
   only deal with situations they were programmed  in  advance  to  expect.
   Before  we can even start to write the programs for "Star Wars," we must
   predict every situation that might arise and  every  trick  the  Soviets
   might pull.  Would you bet the future of the United States that the Rus-
   sians won't think of ANYTHING we haven't thought of first?
   
        Even if we could specify exactly what we want the computers to  do,
   the  task  of translating that specification into flawless computer pro-
   grams would be beyond our capabilities for many,  many  years,  possibly
   forever.   Current and projected techniques for testing and quality con-
   trol may reduce the number of  flaws  in  large  computer  systems,  but
   actual  use  under  real-life  conditions  will  always  uncover further
   "bugs."  (For details on the software problem, see  Dr.  David  Parnas's
   article  in the October 1985 issue of AMERICAN SCIENTIST.)  The only way
   to gain real confidence in "Star Wars" software would be to try  it  out
   in full-scale nuclear combat.  Such testing is clearly not an option.
   
        But if effective population  defense  is  impossible,  why  are  we
   spending  billions  of dollars on SDI, and why are the Russians so upset
   about it?  The answer is remarkably simple: because  population  defense
   is  not  the goal of SDI.  The kinetic and directed energy devices being
   developed for the "Star Wars" program will have a  tremendous  range  of
   uses  in  offensive  weapons and in increasing the survivability of U.S.
   land-based missiles.  The Soviets fear "Star Wars" for its  first-strike
   capabilities.   To make nuclear weapons impotent and obsolete, SDI would
   have to be perfect.  To shoot down Soviet  satellites,  to  thin  out  a
   pre-emptive  strike  on  U.S.  missile  fields, or to develop exotic new
   weapons for the conventional battlefield, SDI will only need to  succeed
   on a much more modest level.
   
        By focusing public attention on the unattainable goal of population
   defense,  the Administration has managed to avoid discussion of the more
   practical,  immediate  consequences  of  SDI  research.    The   weapons
   developed  for  "Star Wars" will have a profound impact on both our war-
   fighting strategy and our treaty obligations.  That impact should be the
   subject  of public and Congressional debate.  By pretending to develop a
   defensive shield, the President has  fooled  the  American  people  into
   funding  a program that is far less clear-cut and benign.  In effect, he
   has sold a system we cannot build in order to build a system  he  cannot
   sell.
   
   BYLINE:
       Michael L. Scott is an Assistant Professor of Computer Science at
       the University of Rochester.  His article was co-signed by 10 other
       faculty members  [almost the entire department]  and 36 doctoral
       students and researchers.  The views expressed should not be regarded
       as the official position of the University of Rochester or of its
       Computer Science Department.
   
           [We haven't had any RISKS mention of this topic in a long time. 
           Perhaps it is time to dust it off again in the light of Reykjavik.
           The nature of the offensive capability is not a new issue, but is
           clearly an enormous potential RISK -- at least in the eyes of the 
           Soviets.  However, subsequent discussion on that issue probably
           belongs on ARMS-D.  Let's once again try to stick to issues 
           relevant to computers and related technologies.  PGN]
   
</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.80.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.82.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-68</DOCNO>
<DOCOLDNO>IA012-000125-B043-155</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.82.html 128.240.150.127 19970217005436 text/html 14082
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:52:48 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 82</TITLE>
<LINK REL="Prev" HREF="/Risks/3.81.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.83.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.81.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.83.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 82</H1>
<H2> Monday, 20 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
NASDAQ computer crashes 
</A>
<DD>
<A HREF="#subj1.1">
Jerry Leichter
</A><br>
<A HREF="#subj1.2">
 Vint Cerf
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Sensors on aircraft 
</A>
<DD>
<A HREF="#subj2.1">
Art Evans
</A><br>
<A HREF="#subj2.2">
 Henry Spencer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Loss of the USS Thresher 
</A>
<DD>
<A HREF="#subj3.1">
John Allred
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: US Navy reactors 
</A>
<DD>
<A HREF="#subj4.1">
Henry Spencer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Risks from Expert Articles 
</A>
<DD>
<A HREF="#subj5.1">
Andy Freeman
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
 NASDAQ computer crashes
</A>
</H3>
<address>

&lt;<A HREF="mailto:LEICHTER-JERRY@YALE.ARPA">
LEICHTER-JERRY@YALE.ARPA
</A>&gt;
</address>
<i>
20 OCT 1986 11:09:46 EST
</i><PRE>

         OTC stock market - Computer problems snag trading

Computer problems halted trading for about three hours throughout the day
Thursday [16 October 1986] in over-the-counter stocks listed through the
National Association of Securities Dealers Automatic Quotation system.
Craig Thompson, manager of marketing information for the National
Association of Securities Dealers, said the system was shut down from about
11:05 a.m. to 2 p.m. EDT, then five minutes before the 4 p.m. closing due to
a breakdown of equipment at its computer operations center in Trumbull,
Conn.  The exact nature of the problem had not been determined, Thompson
said.  "We don't think it will effect tomorrow's business as we hope it will
be corrected by then," Thompson said.
                                            {AP News Wire, 16-Oct-86, 16:48}

</PRE>
<HR><H3><A NAME="subj1.2">
NASDAQ computer crashes 
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
20 Oct 1986 06:47-EDT
</i><PRE>
From: CERF@A.ISI.EDU
To: risks@CSL.SRI.COM

Since so much of Wall Street operation is heavily dependent on automation
and communication, it would be very interesting to know more about the
causes and nature of the failure and how dealers/users coped with the
outage.  Obviously, neither Wall Street nor the economy collapsed, but it
might be instructive to know whether the ability to accommodate the failure
was a function of the length of the outage (how close to disaster did we
actually approach?  How much longer an outage could have been sustained
without permanent damage?).
                                        Vint Cerf

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Sensors on aircraft
</A>
</H3>
<address>
"Art Evans" 
&lt;<A HREF="mailto:Evans@TL-20B.ARPA">
Evans@TL-20B.ARPA
</A>&gt;
</address>
<i>
Mon 20 Oct 86 13:11:56-EDT
</i><PRE>
To: Risks@CSL.SRI.COM

It's all well and good to propose a sensor that reports, "the left engine
isn't there," or, "the left ailerons are gone," or whatever.  But, how is
the sensor to work?  That is, just what do you propose to sense?  Sure, you
and I can look at the left wing and decide immediately, but what is the
sensor to do?  Moreover, how do you propose checking the reliability of a
sensor that, in the nature of things, almost never does anything?  I think
these are hard problems.

As for the JAL 747 disaster -- the flight crew knew precisely what the
problem was: With the loss of all three (or was it four?) hydraulic systems,
they had no control whatsoever over any control services.  They may not have
known what caused the problem, but they were all too aware of the effects.

Aviation Week published the transcript of the cockpit voice recorder not too
long after the accident, and it is the most terrifying such transcript I've
ever read.  The flight crew were dead, and they knew it.  They were still
flying around, but they were in effect test pilots in a new kind of aircraft
no one had ever thought much about before.  Their problem was simple:
control pitch attitude (nose up or down) with power, and control direction
with differential power (more power on one side than the other).  Well,
maybe with plenty of time to experiment someone might learn to fly a 747
that way.  They tried, as long as they could, but they just weren't able to
hack it.  Most power adjustments produced oscillations in attitude that they
were unable to damp out.  Finally, it got away from them in a way they
couldn't recover from, and they went down.  A brave attempt at the probably
impossible.

Art Evans

</PRE>
<HR><H3><A NAME="subj2.2">
Aircraft self-awareness (Sensors on aircraft)
</A>
</H3>
<address>
Henry Spencer
&lt;<A HREF="mailto:decvax!utzoo!henry@ucbvax.Berkeley.EDU ">
decvax!utzoo!henry@ucbvax.Berkeley.EDU 
</A>&gt;
</address>
<i>
Mon, 20 Oct 86 22:00:32 edt
</i><PRE>

I believe some of the DC-10 engineers proposed during development that it
should have a set of video cameras viewing things like the wings and tail,
so that the flight crew could get a look at the situation if they really
needed to.  (This is not as good as having it automatically brought to
their attention, but many classes of problems would come to their attention
quickly anyway...)  The proposal was rejected, I believe on grounds of cost
and weight.

In fairness, the only DC-10 crash I remember offhand where this might have
helped was the Chicago engine-separation one, and it's not clear that the
crew had time to study the problem.  I don't know what the proposal had
in the way of monitors, but for sheer reasons of panel space I suspect it
would have been a switchable monitor rather than a bank of screens showing
all views continuously.  That crash happened fast; I doubt that information
not available at a glance would have helped.

				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
 Loss of the USS Thresher
</A>
</H3>
<address>
    John Allred 
&lt;<A HREF="mailto:jallred@labs-b.bbn.com">
jallred@labs-b.bbn.com
</A>&gt;
</address>
<i>
Mon, 20 Oct 86 13:31:40 EDT
</i><PRE>

Thresher, according to the information I received while serving on submarines, 
was lost due to a catastrophic failure of a main sea water valve and/or pipe, 
causing the flooding of a major compartment.  The cause of the sinking was 
reported by the mother ship during the boat's sea trials.  Scorpion, on the 
other hand, had no observer present.  No reason of loss has been given to the 
public.

The loss of reactor power, in and of itself, should not have caused the loss of
the Thresher.  Boats are usually trimmed to be neutrally bouyant, so the loss
of motiviation should not be fatal.

Does anyone out in netland have access to the report of the Thresher's loss?
It would be good to hear the true story.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
 Re: US Navy reactors
</A>
</H3>
<address>
Henry Spencer
&lt;<A HREF="mailto:decvax!utzoo!henry@ucbvax.Berkeley.EDU ">
decvax!utzoo!henry@ucbvax.Berkeley.EDU 
</A>&gt;
</address>
<i>
Mon, 20 Oct 86 22:00:42 edt
</i><PRE>

Brint Cooper suggests that the USN's excellent reactor safety record might
stem from their deep distrust of automatic equipment.  Personally, I think
the connection is indirect.  It's not at all obvious that manually-run
reactors are safer than partly-automated ones.  Humans are better at coping
with unforeseen situations, *if* they truly understand the equipment they
are controlling.  If they're just being used as organic servomechanisms,
then they are less reliable than automatic equipment, which does not get
tired or bored (when things are going well) or frightened or tense (when
they aren't).  I suspect the USN reactor technicians have a pretty good
understanding of their hardware, given the general atmosphere of great care
surrounding USN reactors.  However, servomechanisms are probably still
safer when the problems have, in fact, been foreseen accurately.  This is
likely to be the case for the majority of problems.

The indirect connection I see is the obvious one:  distrust breeds caution.
Whether or not manually-operated reactors are safer than semiautomated ones,
*any* equipment clearly is going to be safer when elaborate care is taken
in materials, assembly, testing, crew training, and maintenance.  A high-
quality reactor run by carefully-trained humans is clearly safer than a
slipshod one run by rusty machinery.

Eugene Miya notes that there is some doubt about the reactor being blameless
in the loss of the Thresher.  True; I should have noted that.

Steve Woods notes:

&gt; There is another factor to consider here, redundancy [cross-training] ...
&gt; ... these are WARSHIPS, they need to be able to function even
&gt; after suffering SEVERE damage and heavy casualties...

While I tend to agree that cross-training is a good idea, it's actually
not clear that the USN has thought this one through, for submarines in
particular.  It's not obvious to me that there is any likelihood of severe
damage and heavy casualties in a nuclear sub without catastrophic hull
damage as well.  Nuclear subs generally do not have internal pressure
bulkheads, as I recall, because there isn't enough buoyancy reserve for
the sub to survive with a flooded section anyway.  This means that a
serious hull breach is quickly fatal.

				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Risks from Expert Articles
</A>
</H3>
<address>
Andy Freeman 
&lt;<A HREF="mailto:ANDY@Sushi.Stanford.EDU">
ANDY@Sushi.Stanford.EDU
</A>&gt;
</address>
<i>
Mon 20 Oct 86 11:45:32-PDT
</i><PRE>
To: RISKS@CSL.SRI.COM

Scott@rochester.arpa (Michael L. Scott) wrote the following in <A HREF="/Risks/3.81.html">RISKS-3.81</A>:

        Why not?  To  begin  with,  we  cannot  anticipate  every  possible
   scenario  in  a  Soviet  attack.   Human commanders cope with unexpected
   situations by drawing on their experience, their common sense, and their
   knack for military tactics.  Computers have no such abilities.  They can
   only deal with situations they were programmed  in  advance  to  expect.

Dr. Scott obviously doesn't write very interesting programs. :-)

Operating systems, compilers, editors, mailers, etc. all receive input
that their designers/authors didn't know about exactly.  Some people
believe that computer reasoning is inherently less powerful than human
reasoning, but it hasn't been proven yet.

Most op-ed pieces written by experts (on any subject, supporting any
position) simplify things so far that they're actually incorrect.  The
public may be ignorant, but they aren't stupid.  Don't lie to them.
(This is one of the risks of experts.)

It can be argued that SDI isn't understood well enough for humans to make
the correct decisions (assuming super-speed people), let alone for them to
be programmed.  That's a different argument, and Dr. Scott is (presumably)
unqualified to give an expert opinion.  His expertise does apply to the "can
SDI decision be programmed correctly?"  question, which he spends just one
paragraph on.
                                                       -andy

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.81.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.83.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-69</DOCNO>
<DOCOLDNO>IA012-000125-B043-178</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.83.html 128.240.150.127 19970217005503 text/html 17301
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:53:31 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 83</TITLE>
<LINK REL="Prev" HREF="/Risks/3.82.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.84.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.82.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.84.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 83</H1>
<H2> Tuesday, 21 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Risks from Expert Articles 
</A>
<DD>
<A HREF="#subj1.1">
David Parnas
</A><br>
<A HREF="#subj1.2">
 Herb Lin
</A><br>
<A HREF="#subj1.3">
 Andy Freeman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Loss of Nuclear Submarine Scorpion 
</A>
<DD>
<A HREF="#subj2.1">
Donald W. Coley
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Staffing Nuclear Submarines 
</A>
<DD>
<A HREF="#subj3.1">
Martin Minow
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  An SDI Debate from the Past 
</A>
<DD>
<A HREF="#subj4.1">
Ken Dymond
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  System effectiveness is non-linear 
</A>
<DD>
<A HREF="#subj5.1">
Dave Benson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Stealth vs Air Traffic Control 
</A>
<DD>
<A HREF="#subj6.1">
Schuster via Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Missing engines &amp; volcano alarms 
</A>
<DD>
<A HREF="#subj7.1">
Martin Ewing
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Re: Risks from Expert Articles (<A HREF="/Risks/3.82.html">RISKS-3.82</A>)
</A>
</H3>
<address>
&lt;<A HREF="mailto: parnas%qucis.BITNET@WISCVM.WISC.EDU">
 parnas%qucis.BITNET@WISCVM.WISC.EDU
</A>&gt;
</address>
<i>
Tue, 21 Oct 86 09:39:51 EDT
</i><PRE>

   Andy Freeman criticizes the following by Michael L.  Scott, "Computers have
no such abilities.  They can only deal with situations they were programmed
in advance to expect."  He writes, "Dr.  Scott obviously doesn't write very
interesting programs.  :-) Operating systems, compilers, editors, mailers,
etc. all receive input that their designers/authors didn't know about
exactly.  "

   Scott's statement is not refuted by Freeman's.  Scott said that the
computer had to have been programmed, in advance, to deal with a situation.
Freeman said that sometimes the programmer did not expect what happened.
Scott made a statement about the computer.  Freeman's statement was about
the programmer.  Except for the anthropomorphic terms in which it is
couched, Scott's statement is obviously correct.

   It appears to me that Freeman considers a program interesting only if we
don't know what the program is supposed to do or what it does.  My
engineering education taught me that the first job of an engineer is to find
out what problem he is supposed to solve.  Then he must design a system
whose limits are well understood.  In Freeman's terminology, it is the job
of the software engineer to rid the world of interesting programs.

   Reliable compilers, editors, etc., (of which there are few) are all
designed on the basis of a definition of the class of inputs that they are
to process.  We cannot identify the actual indvidual inputs, but we must be
able to define the class of possible inputs if we are to talk about
trustworthiness or reliability.  In fact, to talk about reliability we need
to know, not just the set of possible inputs, but the statistical
distribution of those inputs.

Dave Parnas

</PRE>
<HR><H3><A NAME="subj1.2">
Risks from Expert Articles
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Tue, 21 Oct 1986  09:16 EDT
</i><PRE>

    From: Andy Freeman &lt;ANDY at Sushi.Stanford.EDU&gt;

    Operating systems, compilers, editors, mailers, etc. all receive input
    that their designers/authors didn't know about exactly.  

When was the last time you used a mailer, operating system, compiler,
etc.. that you trusted to work *exactly* as documented on all kinds of
input?  (If you have, pls share it with the rest of us!)

    It can be argued that SDI isn't understood well enough for humans to make
    the correct decisions (assuming super-speed people), let alone for them to
    be programmed.  That's a different argument, and Dr. Scott is (presumably)
    unqualified to give an expert opinion.  His expertise does apply
    to the "can
    SDI decision be programmed correctly?"  question, which he spends just one
    paragraph on.

You are essentially assuming away the essence of the problem by
asserting that the specs for the programs involved are not part of the
programming problem.  You can certainly SAY that, but that's too
narrow a definition in my view.

</PRE>
<HR><H3><A NAME="subj1.3">
Re: Risks from Expert Articles
</A>
</H3>
<address>
Andy Freeman 
&lt;<A HREF="mailto:ANDY@Sushi.Stanford.EDU">
ANDY@Sushi.Stanford.EDU
</A>&gt;
</address>
<i>
Tue 21 Oct 86 14:40:48-PDT
</i><PRE>
To: LIN@XX.LCS.MIT.EDU
cc: RISKS@CSL.SRI.COM

Herb Lin writes:

    When was the last time you used a mailer, operating system, compiler,
    etc.. that you trusted to work *exactly* as documented on all kinds of
    input?  (If you have, pls share it with the rest of us!)

The programs I use profit me, that is, their benefits to me exceed
their costs.  The latter includes their failures (as well as mine).  A
similar metric applies to weapons in general, including SDI.  (Machine
guns jam too, but I'd rather have one than a sword in most battle
conditions.  The latter are, for the most obsolete, but there aren't
perfect defenses against them.)

Lin continued with:

    You are essentially assuming away the essence of the problem by
    asserting that the specs for the programs involved are not part of the
    programming problem.  You can certainly SAY that, but that's too
    narrow a definition in my view.

Sorry, I was unclear.  Specification and implementation are related,
but they aren't the same.  There are specs that can't be implemented
acceptably (as opposed to perfectly).  Some specs can't be implemented
acceptably in some technologies, but can in others.  (This can be
context dependent.)  Dr. Scott's expertise applies to the question of
whether a given spec can be programmed acceptably, not whether there
is an spec that can be implemented acceptably.  Much of the spec,
including the interesting parts of the definition of "acceptable", is
outside CS, and (presumably) Dr. Scott's expertise.

Another danger (apart from simplification to incorrectness) of expert
opinion articles is unwarranted claims of expertise.  Dr. Scott
(presumably) has no expertise in directed energy weapons yet he claims
that they can be used against cities and missiles in silos.  Both
proponents and opponents of SDI usually agree that it doesn't deal
with cruise missiles.  If you can kill missiles in silos and attack
cities, cruise missiles are easy.

-andy

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Loss of Nuclear Submarine Scorpion
</A>
</H3>
<address>
Donald W. Coley 
&lt;<A HREF="mailto:coley@SCRC-VALLECITO.ARPA">
coley@SCRC-VALLECITO.ARPA
</A>&gt;
</address>
<i>
Tue, 21 Oct 86 12:38 EDT
</i><PRE>
To: RISKS@CSL.SRI.COM

This is in response to John Allred's comments about the loss of both the
Thresher and the Scorpion (<A HREF="/Risks/3.82.html">RISKS-3.82</A>).

    Date:     Mon, 20 Oct 86 13:31:40 EDT
    From:     John Allred &lt;jallred@labs-b.bbn.com&gt;
    Subject:  Loss of the USS Thresher

    Thresher, according to the information I received while serving
    on submarines, was lost due to a catastrophic failure of a main
    sea water valve and/or pipe, causing the flooding of a major
    compartment.  The cause of the sinking was reported by the mother
    ship during the boat's sea trials.

Just to confirm what John stated, fracture of a hull-penetration fitting, at
the weld between the flange and the pipe, quickly flooded the engineering
spaces.  The sinking had nothing to do with the reactor.

    Scorpion, on the other hand, had no observer present.  No reason
    of loss has been given to the public.

Scorpion was in very high speed transit, westbound in one of the submarine
transit lanes, when she struck a previously uncharted undersea mountain.
The speed of the collision was "in excess of forty miles per hour" (probably
closer to sixty).  It was the very high speed that had rendered her
(acoustically) blind; unable to see the obstacle in her path.  True, no
observer was present, but a lot of people did get to hear the result.  The
"days spent searching for the lost sub" were just to avoid revealing how
accurate our tracking capabilities were.  All the Navy brass knew within the
hour, exactly what had happened and exactly where.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Staffing Nuclear Submarines
</A>
</H3>
<address>
Martin Minow, DECtalk Engineering ML3-1/U47 223-9922
&lt;<A HREF="mailto:minow%regent.DEC@decwrl.DEC.COM  ">
minow%regent.DEC@decwrl.DEC.COM  
</A>&gt;
</address>
<i>
21-Oct-1986 1457
</i><PRE>

Disclaimer: a few months ago, my knuckles were rapped when I incorrectly
cited a study on airline safety.  Please be warned that I know absolutely
nothing about nuclear submarines and am using the ongoing discussion about
automatic controls for nuclear reactors (on submarines) only as a starting
place for a wider discussion.

From the discussion on Risks it seems that, while automatic controls may do
a satisfactory job of running the reactor in normal circumstances, people
will still be needed to run the reactor when the automatic controls
malfunction.

Adding automatic controls adds weight (and probably noise), making the
ship less effective. 

Adding automatic controls to a nuclear submarine's reactor frees personnel
for other tasks.  But, there isn't much else for them to do (they can hardly
chip rust on the deck), so they'll get bored and lose their "combat
readiness."

Relying on totally manual control keeps the crew alert and aware of the
action of the reactor.  It also keeps them busy. 

In other words -- and I think this is directly relevant to Risks -- there
are times when external factors make it unwise to automate a task, even
when it can easily be done. 

Martin

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
An SDI Debate from the Past
</A>
</H3>
<address>
"DYMOND, KEN" 
&lt;<A HREF="mailto:dymond@nbs-vms.ARPA">
dymond@nbs-vms.ARPA
</A>&gt;
</address>
<i>
21 Oct 86 11:03:00 EDT
</i><PRE>
To: "risks" &lt;risks@csl.sri.com&gt;

While looking something up in Martin Shooman's book on software 
engineering yesterday, I came across the following footnote (p.495):

    Alan Kaplan, the editor of Modern Data magazine, posed the question,
    "Is the ABM system capable of being practically implemented or is
    it beyond our current state-of-the-art ?"  The replies to this
    question were printed in the January and April 1970 issues of the
    magazine.  John S. Foster, director of the Office of Defense
    Research and Engineering, led the proponents, and Daniel D.
    McCracken, chairman of Computer Professionals against ABM, led
    the opposition.

It's startling that the very question that so interests us today was
put 15 or so years ago; to make it the exact question, all you have
to do is change the 3 letters of the acronym.  And this was 3 (?)
generations ago in computer hardware terms (LSI, VLSI, VHSIC ?) and
some indeterminate time in terms of software engineering (I can't
think of anything so clear-cut as circuit size to mark progress in
software).  International politics, however, seems not to have
changed much at all.

I'll try to track down those articles (Modern Data no longer exists
having become Mini-Micro Systems in 1976), but in the meantime can anyone 
shed light on this debate from the dim past ?

(BTW, Shooman comments "Technical and political considerations were finally
separated, and diplomatic success caused an abrupt termination of the
project." p. 498)

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
System effectiveness is non-linear
</A>
</H3>
<address>
Dave Benson 
&lt;<A HREF="mailto:benson%wsu.csnet@CSNET-RELAY.ARPA">
benson%wsu.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Mon, 20 Oct 86 16:01:06 pdt
</i><PRE>

I agree with Anon that overall system effectiveness is non-linear:

  &gt;If 1000 missiles strains the system to the point that it can only
  &gt;stop 800, why would anyone think it could stop more when the number of
  &gt;missiles and decoys is doubled, straining the system's ability to
  &gt;identify, track, and destroy missiles at least twice as much?

The more reasonable (and conservative) assumption is that the SDI system
would stop ZERO missles when faced with, say, 2000 targets.  Case in
point is revision n of the US Navy Aegis system -- seems that being
designed to track a maximum of (17) targets,  when there are (18)
targets the computer software crashed.

Any engineered artifact has design limits.  When stressed beyond those
limits, it fails.  We understand this for civil engineering artifacts,
such as bridges.  Clearly this is not well understood for software
engineering artifacts.

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
 Missing engines &amp; volcano alarms
</A>
</H3>
<address>
Martin Ewing
&lt;<A HREF="mailto:    mse%Phobos.Caltech.Edu@DEImos.Caltech.Edu ">
    mse%Phobos.Caltech.Edu@DEImos.Caltech.Edu 
</A>&gt;
</address>
<i>
Tue, 21 Oct 86 13:41:58 PDT
</i><PRE>
To:       risks%Phobos.Caltech.Edu@DEImos.Caltech.Edu

We visited New Zealand a few years ago and went to the major skiing area
on the North Island (the name escapes me).  It is built on the slopes of
an active volcano.  There were prominent warnings for skiers of what to
do in case of an eruption alarm.  (Head for a nearby ridge.  Don't try
to outrun the likely mud/ash slide coming down the hill.) 

How do they get the alarm?  There is an instrument hut at the lip of the
crater connected to park headquarters by a cable.  The instruments
measure some parameter(s) or other.  (heat, acceleration, pressure, ?) 
When something crosses a threshold, the warning alarms on the ski slopes 
are set off automatically. 

In fact, someone admitted, what would probably happen is that the
explosion would destroy the hut and cut the cable.  Loss of signal is
probably as good a diagnostic as anything else.

I can imagine a display on the DC-10 instrument panel inscribed with the
outline of the aircraft.  Little red lights come on when you lose
continuity on a wire to an engine, aileron, etc. - like what happens
when you leave your door open on a Honda Civic.  What you do with this
data is another matter. 

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.82.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.84.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-70</DOCNO>
<DOCOLDNO>IA012-000125-B043-206</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.84.html 128.240.150.127 19970217005522 text/html 27851
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:53:50 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 84</TITLE>
<LINK REL="Prev" HREF="/Risks/3.83.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.85.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.83.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.85.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 84</H1>
<H2> Tuesday, 22 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Risks of using an automatic dialer 
</A>
<DD>
<A HREF="#subj1.1">
Bill Keefe
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Re: Missing engines &amp; volcano alarms 
</A>
<DD>
<A HREF="#subj2.1">
Eugene Miya
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  False premise ==&gt; untrustworthy conclusions 
</A>
<DD>
<A HREF="#subj3.1">
Martin Harriman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  USN Automated Reactors 
</A>
<DD>
<A HREF="#subj4.1">
Dan C Duval
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Keep It Simple as applied to commercial nuclear power generation       
</A>
<DD>
<A HREF="#subj5.1">
Martin Harriman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Works as Documented 
</A>
<DD>
<A HREF="#subj6.1">
Martin Minow
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Re:  Editorial on SDI 
</A>
<DD>
<A HREF="#subj7.1">
Michael L. Scott
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Risks from Expert Articles 
</A>
<DD>
<A HREF="#subj8.1">
Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj9">
  Stealth vs. ATC / SDI Impossibility? / Missing Engines ? 
</A>
<DD>
<A HREF="#subj9.1">
Douglas Humphrey
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Risks of using an automatic dialer
</A>
</H3>
<address>
Bill Keefe
&lt;<A HREF="mailto:keefe%milrat.DEC@decwrl.DEC.COM  ">
keefe%milrat.DEC@decwrl.DEC.COM  
</A>&gt;
</address>
<i>
Wednesday, 22 Oct 1986 10:09:16-PDT
</i><PRE>

I wonder if it's significant that they are willing to talk about payment 
for aggravation but not for lost business.  Unfortunately, it was not 
reported whether the failure was due to a hardware or software problem.

  Computerized Sales Call Gets Stuck, Ties Up Phone for Three Days
  
     GREENWICH, Conn. (AP) - A shipping broker who does all his work
  on the phone says he lost at least one deal because a computerized
  sales pitch called him nearly every two minutes for 72 hours, tying
  up his lines.
     The voice-activated computer message bedeviling Joern Repenning
  was shut off Monday after he had complained to New York Telephone's
  annoyance bureau, the Better Business Bureau, AT&amp;T, police and the
  state attorney general.
     The problem was in a computer at Integrated Resources Equity
  Corp. in Stamford, said William Banks, an employee of the company.
  The repeated calls blocked all other incoming calls to Repenning's
  office with a busy signal.
     ``There was no way we could conduct business,'' Repenning said.
  ``We can't shut off our telephone. That's our business.''
     He said he lost at least one deal because he could not reply by a
  certain deadline on a shipping-cargo transaction.
     Integrated is willing to talk with Repenning about payment for
  aggravation he suffered, Banks said.
  
</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Re: Missing engines &amp; volcano alarms
</A>
</H3>
<address>
Eugene Miya
&lt;<A HREF="mailto:eugene@AMES-NAS.ARPA ">
eugene@AMES-NAS.ARPA 
</A>&gt;
</address>
<i>
Wed, 22 Oct 86 09:34:51 pdt
</i><PRE>

Martin Ewing gives an example of "absence of signal" as an indication
that something maybe wrong.  He concludes by precisely indicating
the problem but glossing over with "What you do with this data is another
matter."  This last statement is unacceptable completion of the argument
for aircraft manufacturers.

This is precisely the problem with planes, spacecraft, and other
highly constained systems.  How do we adequately know something,
almost as bad: how do we know our instrument is not malfunctioning?
Do we perenially "tap" the instrument?  Designers of aircraft prefer
"indicator/effector" systems, not to put just "indicators" into planes.
"Great, my wings fell off" so what are you going to do?

There is a wind tunnel across the street from where I lunch.  This tunnel
has a set of sensor wires which enter a plate.  This struck me as
the nerve system of the wind tunnel when I first saw it.  How inadequate
this appears.  The metal hull of the tunnel isn't a sensory tool like
our skin (able to sense, heat, pressure, and other things to a much better
precision).  Some day perhaps.

On the posting on the safety of Stealth aircraft: I was visiting a friend
on the day of the recent non-crash of the non-existent F-19.  We were
assured (not-assured?) by authorities [friend lived within a few miles
of the non-site] that, since the non-F-19 only flew at night, it ALWAYS
flew with a radar detectable chase plane (not a non-plane).

--eugene miya

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
 False premise ==&gt; untrustworthy conclusions
</A>
</H3>
<address>
    Martin Harriman 
&lt;<A HREF="mailto:"SRUCAD::MARTIN%sc.intel.com"@CSNET-RELAY.ARPA">
"SRUCAD::MARTIN%sc.intel.com"@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Wed, 22 Oct 86 14:54 PDT
</i><PRE>

There seems to be a misconception floating around in RISKS regarding the
degree of automation in Navy and civilian nuclear reactors.  Civilian
reactors are not significantly different than Navy reactors in this
respect; both types of reactors have a single form of automated control.
Both Navy (propulsion) and civilian (electric power generation) reactors
have a reactor protection system--a system rather like a circuit breaker
that automatically shuts the reactor down if some parameters (such as
reaction level or temperature) exceed defined limits.  If you've ever
seen the reactor jargon "scram" or "trip" (as in, "we had three unplanned
trips this year"), that's what is being referred to.

Everything else is manual, in either system.

At least in civilian systems, this system is tested regularly (planned
trips), and the reactor's responses noted.  I am not sure if the Navy
has planned trips; I know they have unplanned trips often enough to annoy
the reactor operators (the scram alarm is a *very* loud klaxon in a
*very* small compartment).

Reactors are not a good paradigm for a debate on the risks of automated
controls.  Arguments based on the safety record of one class of reactors
versus another will miss the point; the reactors differ in many interesting
respects (training, discipline, nature of the task, ...), but the nature of
the control mechanisms is not one of them.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
USN Automated Reactors
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: 21 Oct 86 12:10:27 PDT (Tue)
From: Dan C Duval &lt;dand@tekigm.TEK.CSNET&gt;

Arguments over whether the Navy's choice to NOT use automated safety
systems on USN reactors are overlooking one major point, in that the
choice of using or not using any safety equipment of any kind also has
to meet a weight/benefit tradeoff.

If you design a reactor with built-in automated safety features, you
have the weight of the reactor, the weight (and bulk) of the safety
systems, the reactor operators (and the systems to support them, such
as galleys, bunk space, food stores, etc), and the personnel to maintain
the safety equipment (with support for them as well).

A "manual" reactor requires only the reactor and the operators (plus
their support).

Adding the automated safety gear adds weight, requiring a larger boat,
a larger power plant, more support for boat and crew, etc, all for no
added war-fighting capability. Meantime, adding training to a human
being does not add appreciable weight to the human being, nor require
further support systems.

Though this weight consideration is paramount for subs, it holds as well
for surface ships (or "targets", as my ex-submariner buddy calls them.)
Thus, I think the argument that the USN doesn't trust automation is
weakened, since the USN also has other things to worry about than just
the automated safety vs non-automated safety tradeoff.

This weight-consideration argument also has some bearing on the aircraft
sensor question. More weight in sensors means a larger plane, more systems
that can break, more potential for overlooking problems during maintenance,
and more ways to confuse the flight crew. (Scenario: Crew cannot see wing
to tell if engine has fallen off, but sensor says it has; did it fall off
or did the sensor fail? Did anyone ever see the movie where the flight crew
shut down their last remaining engine because coffee, spilled into the control
panel, caused the "Engine Fire" warning to sound? So we have sensors to
check the sensors, to check those sensors, etc.)

Dan C Duval, Tektronix, Inc
uucp: tektronix!tekigm!dand

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
 Keep It Simple as applied to commercial nuclear power generation
</A>
</H3>
<address>
    "Martin Harriman" 
&lt;<A HREF="mailto:"SRUCAD::MARTIN"@sc.intel.com">
"SRUCAD::MARTIN"@sc.intel.com
</A>&gt;
</address>
<i>
Fri, 17 Oct 86 17:05 PDT
</i><PRE>

I think it might be rather amusing if the nuclear power generating plants
in the US were all run by some (reasonably competent) admiral.  Oh well...

The nuclear power (design) industry--the folks who design the nuclear
steam supply systems and their controls--uses a very similar approach to
that used in the Navy.  The automated controls on the reactors I am
familiar with are limited to the reactor protective systems--the system(s)
that detect a fault condition, and trip the reactor.  These systems are
kept very simple (on the same principle as keeping a circuit breaker as
simple as possible for the job it does).

Control of reaction rate and profile is accomplished through manual adjustments
of the control rods and the water chemistry.

The reliability of this system (and its safety) depends on the quality of the
reactor operator (that is, the power company operating the reactor).  One of
the more encouraging signs in recent years has been the NRC's willingness to
suspend the operating licenses of operators who have poor safety records:
the TVA suspension is the most obvious.

  --Martin Harriman, Intel Santa Cruz

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Works as Documented
</A>
</H3>
<address>
Martin Minow, DECtalk Engineering ML3-1/U47 223-9922
&lt;<A HREF="mailto:minow%regent.DEC@decwrl.DEC.COM  ">
minow%regent.DEC@decwrl.DEC.COM  
</A>&gt;
</address>
<i>
22-Oct-1986 0842
</i><PRE>

&gt; When was the last time you used a mailer, operating system, compiler,
&gt; etc.. that you trusted to work *exactly* as documented on all kinds of
&gt; input?  (If you have, pls share it with the rest of us!)

The problem is not that the software (etc.) works as documented, but
whether it works as we *expect* it to.

This distinction has wider applicability.  We *expect* SDI to protect us
from a Russian missile attack.  SDI is *documented* to protect some large
percentage of our missiles from a Russian missile attack. 

Martin.

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Re:  Editorial on SDI
</A>
</H3>
<address>
&lt;<A HREF="mailto:scott@rochester.arpa">
scott@rochester.arpa
</A>&gt;
</address>
<i>
Wed, 22 Oct 86 11:51:50 edt
</i><PRE>
Cc: scott@rochester.arpa

<A HREF="/Risks/3.82.html">RISKS-3.82</A> contains a response from Andy Freeman to an editorial
I posted to <A HREF="/Risks/3.81.html">RISKS-3.81</A>.  Andy and I have also exchanged a fair amount
of personal correspondence in the past couple of days.  In that
correspondence he maintains that I have disguised a political argument
as expert opinion.  This from his posting to RISKS:

&gt; Most op-ed pieces written by experts (on any subject, supporting any
&gt; position) simplify things so far that they're actually incorrect.  The
&gt; public may be ignorant, but they aren't stupid.  Don't lie to them.
&gt; (This is one of the risks of experts.)

I do not believe that I have oversimplified anything.  I certainly haven't
lied to anybody (let's not get personal here, ok?).

When technical arguments disagree with government policy, it is standard
practice to dismiss those arguments as "purely political."  Almost everything
that a citizen says or does in a democratic society has political overtones,
but those overtones do not in and of themselves diminish the technical
validity of an argument.  "The emperor has no clothes!" can be regarded
as a highly political statement.  It is also technically accurate.

In my original editorial, I declared that we could not be certain that
the software developed for SDI would work correctly, 1) because we don't
know what 'correctly' means, and 2) because even if we did, we wouldn't
be able to capture that meaning in a computer program with absolute
certainty.  Andy takes issue with point 1).  My words on the subject:

   &gt; Human commanders cope with unexpected situations by drawing on their
   &gt; experience, their common sense, and their knack for military
   &gt; tactics.  Computers have no such abilities.  They can only deal with
   &gt; situations they were programmed in advance to expect.

This is the statement Andy feels is 'actually incorrect'.  His words:

&gt; Operating systems, compilers, editors, mailers, etc. all receive input
&gt; that their designers/authors didn't know about exactly.  Some people
&gt; believe that computer reasoning is inherently less powerful than human
&gt; reasoning, but it hasn't been proven yet....
&gt;
&gt; It can be argued that SDI isn't understood well enough for humans to
&gt; make the correct decisions (assuming super-speed people), let alone
&gt; for them to be programmed.  That's a different argument and Dr. Scott
&gt; is (presumably) unqualified to give an expert opinion.

Very true, the designers of everyday programs don't know about their
input *exactly*, but they *are* able to come up with complete
characterizations of valid inputs.  That is what counts.  The "inputs"
to SDI include virtually anything the Soviets can do on the planet or
in outer space.  It does not require an expert to realize that there is
no way to characterize the set of all such actions.  A command interpreter
is free to respond "invalid input; try again"; SDI is not.

I stand by the technical content of my article: SDI cannot provide
an impenetrable population defense.  Impenetrability requires certainty,
and that we can never provide.  Though the White House has kept
debate alive in the minds of the public, it is really not an issue
among the technically literate.  Almost no one with scientific credentials
is wiling to maintain that SDI can defend the American population
against nuclear weapons.  There are individuals, of course (Edward Teller
springs to mind), but in light of the evidence I must admit to a personal
tendency to doubt their personal or scientific judgment.  Certainly
there is no groundswell of qualified support to match the incredible
numbers of top-notch physicists, engineers, and computer scientists
who have publically declared that population defense is a myth.

What we do see are large numbers of individuals who believe that the
SDI program should continue for reasons *other* than perfect population
defense.  It is possible to make a very good case for developing
directed energy and kinetic weapons to keep the U.S. up-to-date in
military technology and to enhance our defensive capabilities.

My editorial is not anti-SDI; it is anti-falsity in advertising.
Those who oppose SDI will oppose it however it is sold.  Those who
support it will find it very tempting to allow the "right" ends to
be achieved (with incredible budgets) through deceptive means, but
that is not how a democracy is supposed to work.  Let the public know
what SDI is all about, and let us debate it for what it is.

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
Risks from Expert Articles
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Tue, 21 Oct 1986  22:43 EDT
</i><PRE>

    LIN@XX.LCS.MIT.EDU (Herb?) writes:
        When was the last time you used a mailer, operating system, compiler,
        etc.. that you trusted to work *exactly* as documented on all kinds of
        input?  (If you have, pls share it with the rest of us!)

    From: Andy Freeman &lt;ANDY at Sushi.Stanford.EDU&gt;
    The programs I use profit me, that is, their benefits to me exceed
    their costs.  The latter includes their failures (as well as mine).  A
    similar metric applies to weapons in general, including SDI.

But you can bound the costs of using a faulty mailer.  You can't with
missile defense for population.

    Dr. Scott's expertise applies to the question of
    whether a given spec can be programmed acceptably, not whether there
    is an spec that can be implemented acceptably.  Much of the spec,
    including the interesting parts of the definition of "acceptable", is
    outside CS, and (presumably) Dr. Scott's expertise.

Are you saying that computer scientists should not be calling attention to
the problem of writing specifications?  Or that they have no expertise in
knowing the consequences of faulty specs?  I think quite the contrary --
computer scientists know, probably better than anyone else, how important
the specs are to a functional program.  I agree that CS background does not
grant people particular knowledge about which specs are proper, but in my
view CS people are entirely proper to holler about lousy specs and what
would happen if they were bad.

    Another danger (apart from simplification to incorrectness) of expert
    opinion articles is unwarranted claims of expertise.  Dr. Scott
    (presumably) has no expertise in directed energy weapons yet he claims
    that they can be used against cities and missles in silos.  

Reports that space-based lasers can be used against cities were
recently published, and a fairly simple order of magnitude calculation
that anyone can do with sophomore physics suggests that city attack
with lasers is at least plausible.  You're right about silos.

    Both proponents and opponents of SDI usually agree that it doesn't 
    deal with cruise missles.  If you can kill missles in silos and attack
    cities, cruise missles are easy.

Hardly.  The problem with cruise missiles is finding the damn things.
Cities and silos are EASY to find.

</PRE>
<A NAME="subj9"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj9.1">
Stealth vs. ATC / SDI Impossibility? / Missing Engines ?
</A>
</H3>
<address>
Douglas Humphrey 
&lt;<A HREF="mailto:deh@eneevax.umd.edu">
deh@eneevax.umd.edu
</A>&gt;
</address>
<i>
Wed, 22 Oct 86 12:52:44 EDT
</i><PRE>

This is kind of a grab bag of responses to the last RISKS. 

Stealth vs. ATC - The general public does not seem to know a lot about the
Air Traffic Control system and how it works. In controlled airspace such as
around large airports, a Terminal Control Area (TCA) is defined into which
only aircraft equipped with a Transponder may traverse. In reality, the
rules and flavors concerned with this whole process are very complex and
aren't needed here. If you are really interested, go to Ground School.  The
transponder replies to the interrogation of the ATC radar providing at least
a bright radar image, and in more sophisticated systems the call sign of the
aircraft, heading, altitude, etc. Thus, the concept of Stealth vs. ATC is
not real. If the stealth aircraft is flying under Positive Control of ATC,
then it will have the transponder. If it does not have one, then it better
stay out of busy places or it is illegal and the pilot sure as hell will
have his ticket pulled.

    [Peter Ladkin also responded on this point.  However, if the stealth
     plane is foreign/unfriendly/hostile/sabotage-minded/..., and NOT flying
     under postive control of ATC, then this argument does not hold.  PGN]

SDI Impossibility?  - I have a good background in physics, computing
(software and vlsi hardware) and a lot of DEW (Directed Energy Weapons), and
I have yet to hear ANYONE explain WHY SDI is impossible. I hear all this
about the complexity of the software, but I used to be part of a group that
supported a software system of over 20 million lines of code, and it rarely
had problems. Admittedly, we wrote simulators for a lot of the load since we
did not want to try experimental code out on the production machines, but we
never had a simulator fail to correctly simulate the situation. There were
over 100 programmers supporting this stuff, and it was properly managed and
it all worked well.  Is someone suggesting that the incoming target stream
can not be simulated ?  Why not ? We do it now on launch profile simulations
involving the DEW (Distant Early Warning) network and a lot of other sensor
systems.  Is someone suggesting that PENAIDS (Penetration Aids) can not be
simulated ?  Why not ? We do it now also. Worst case studies just treat all
of the PENAIDS as valid targets. If you can intercept THAT mess, then you
can stop anything !

I get the feeling that people are assuming that the SDI software is going
to be one long chunk of code running on one machine and that if it ever
sees anything that is not what it expects its going to do a HALT and 
stop the entire process. Wrong. I wouldn't build a game that way, much less
something like SDI ?

So. The Challenge. People out there who think it is Impossible, please
identify what is impossible. Pointing systems ? Target acquisition ?
Target Classification ? Target descrimination ? Destruction of the targets ?
Nobody is saying that it is easy. Nobody is saying that our current level
of technology is capable of doing it all perfectly. But it sure isn't
(in my opinion) impossible. 

   [We've gone around on this one before.  DEH's message is somewhat fatuous,
    but needs a serious response.  Before responding further, make sure you
    have read the Parnas Papers from American Scientist, Sept-Oct 1985, also
    reprinted in ACM Software Engineering Notes October 1985, and the
    Communications of the ACM, December 1985.  But remember that we never seem
    to converge in these discussions.  Parnas does not PROVE that SDI is
    IMPOSSIBLE.  He gives some good reasons to worry about the software.  No
    one else can prove that it CAN BE IMPLEMENTED to satisfy rigorous
    requirements for reliability, safety, security, nonspoofability, etc.,
    under all possible attack modes and environmental circumstances -- even
    with full-scale deployment in real combat.  Especially when operating
    under stressed conditions, things often fail for perverse reasons not
    sufficiently anticipated.  (That should be particularly clear to long-time
    readers of RISKS.)  Think about OVERALL SYSTEM TESTING in the absence of 
    live combat as one problem, among others.  Remember, this Forum exists as
    part of a social process, and contributions according to the masthead
    guidelines are welcome.  But SDI debates seem to degenerate repeatedly 
    into what seems like religious wars.  So bear with me if I try to
    close the Pandora's box that I have again reopened.  I would like to see
    some intelligent open discussion relating to computers and related
    technologies in SDI, but perhaps that is a futile wish.  But once again,
    much discussion has taken place before, on both RISKS and ARMS-D.  New
    RISKS participants might want to check back issues.  See the summary issues
    at the end of Volumes 1 and 2 noted above, and the end of Volume 3 --
    which will happen soon.  Computer relevance to RISKS, else to ARMS-D.  PGN]
    
Missing Engines  - In most aircraft the loss of a major component of
the control system is pretty obvious, generally announced by an abrupt
change in the flight characteristics of the aircraft. Same would go for
the loss of an engine. I am not sure why a pilot would need a video monitor
to tell him that Number 2 just fell off the wing, or that he no longer
has a left horizontal stabilizer. He will no doubt understand this by the
way the aircraft is acting. Most pilots have a good understanding of
Why they are flying and How, and are able to discern the condition of their
aircraft from how it behaves. Certainly I know of Airline pilots who have 
been able to tell by the handling of a DC-9 that a cargo door was partially
open, even though the indicator in the cockpit said it was closed.

   [See above note from Dan Duval.]

I might mention that the landing gear might be a good place for some sort of 
camera system. Pilots get rather paranoid about the state of the landing
gear when they fail to get 3 green lights up in the cockpit.

Doug Humphrey
Digital Express Inc.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.83.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.85.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-71</DOCNO>
<DOCOLDNO>IA012-000125-B043-224</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.85.html 128.240.150.127 19970217005533 text/html 16546
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:54:04 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 85</TITLE>
<LINK REL="Prev" HREF="/Risks/3.84.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.86.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.84.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.86.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 85</H1>
<H2> Thursday, 23 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
On the Risk of Discussing SDI 
</A>
<DD>
<A HREF="#subj1.1">
Craig Milo Rogers
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  SDI Impossibility 
</A>
<DD>
<A HREF="#subj2.1">
Douglas Humphrey
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Swedish Vulnerability Board Report on Complex System Vulnerabilities      
</A>
<DD>
<A HREF="#subj3.1">
Chuck Youman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: Thresher 
</A>
<DD>
<A HREF="#subj4.1">
David Feldman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Stealth and ATC 
</A>
<DD>
<A HREF="#subj5.1">
Dan Melson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Inoperative components 
</A>
<DD>
<A HREF="#subj6.1">
Peter Ladkin
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
On the Risk of Discussing SDI
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
23 Oct 1986 17:52:08 PDT
</i><PRE>
From: Craig Milo Rogers  &lt;ROGERS@B.ISI.EDU&gt;
To: Risks@CSL.SRI.COM

	The moderator recently requested intelligent open discussion
relating to computers and related technologies in SDI.  I believe that
there has instead been too much discussion of computers and SDI.

	The hardware and software issues raised by Parnas and others are
interesting.  They are complex, they defy simple quantification, and they
relate directly to the work of many of the readers of this digest.

	Yet, there are much simpler and more easily discussed problems with
SDI.  SDI provides minimal protection to Europe.  SDI does not appear to
provide protection against nuclear weapons launched at the US from off-shore
submarines.  Bombs can be smuggled into the US via, say, Canada, and
reassembled in the hearts of our cities.  Clearly, if you heed these
arguments, SDI in no way makes nuclear waepons "impotent and obsolete".

	By focusing our attention and that of the general public on
computer-related SDI arguments, we run the *risk* of diverting attention
from more important issues.  We as computer technologists are raising the
(weak, esoteric) issues with which we are familiar, when we as intelligent,
informed citizens should be raising more general questions (perhaps
precisely because we *are* less familiar with them).

	There is a risk in introducing computers into a discussion in
which they are not really relevant.  It is not enough to be able to
discuss an issue intelligently.  One must also know when it is
intelligent to raise the issue in the first place.  (By the way, it is
not clear to me that this message qualifies, either).

					Craig Milo Rogers

    [This issue reaches a relative high mark for noninclusion of messages,
     as I have omitted several on this topic.  However, this one gets accepted
     -- because it is sound, objective, and coherent, and does not violate
     the other requirements.  I have stated before that it is impossible to
     draw a line around "computer relevance".  Craig's point is well taken.   
     By the way, I squelched the discussions between Michael Scott and 
     Andy Freeman (plus a comment from Herb Lin) which were getting to
     third-order arguments and re-reinterpretations.  (Both of the main
     participants still feel they have further clarifications to make.)
     However, I urge you all to take more care in your INITIAL statements.
     That can do wonders at staving off lengthy iterations.  PGN]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
SDI Impossibility
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Thu, 23 Oct 1986  08:47 EDT
</i><PRE>

    From: Douglas Humphrey &lt;deh at eneevax.umd.edu&gt;
    SDI Impossibility?  - I have a good background in physics, computing
    (software and vlsi hardware) and a lot of DEW (Directed Energy
    Weapons), and I have yet to hear ANYONE explain WHY SDI is impossible.

Tell us what you mean by SDI, and it can be explained or not.  Every
technical analyst believes it is possible to build something that will
destroy some missiles.  No analyst believes it is possible to build
something that will destroy all missiles.  The question is whether or not
the ability to destroy some missiles is worth what you must pay to get it.

    I hear all this about the complexity of the software, but I used to be
    part of a group that supported a software system of over 20 million
    lines of code, and it rarely had problems. 

But it sometimes did.  How much would you have been willing to bet that the
problems would not arise at critical times when you could not do debugging?

    we wrote simulators for a lot of the load since we did not want to try
    experimental code out on the production machines, but we never had a
    simulator fail to correctly simulate the situation. 

I'll bet you didn't simulate something with which you had no experience.  To
judge what it means to have a simulator run correctly means that you have
some way of judging its correctness.  No one has such experience with a real
nuclear war.

    There were over 100 programmers supporting this stuff, and it was
    properly managed and it all worked well.

Given the current estimates of SDI software size, the total programming team
might be an order of magnitude bigger.  100 programmers would be tiny.

    Is someone suggesting that the incoming target stream can not be
    simulated ?  Why not ? We do it now on launch profile simulations
    involving the DEW (Distant Early Warning) network and a lot of other
    sensor systems.

But ballistic missile attacks would be straightforward now, because
there are no defenses.  If you assume that the Soviets do nothing
differently, then maybe you could (though I personally doubt that).
But the Soviets will react, and what gives you the confidence that you
can predict their new tactics?

    Is someone suggesting that PENAIDS (Penetration Aids) can not be
    simulated ?  Why not ? We do it now also.

Penaids that we know about we can simulate.  Penaids that we don't
know about we can't.

    Worst case studies just treat all of the PENAIDS as valid targets. If
    you can intercept THAT mess, then you can stop anything !

But you can't. Current threat cloud estimates range from a low of
30,000 to a high of a few million.  If you spend enough money, you
might be able to kill everything, but it seems unlikely that you can
kill them all with just a few thousand platforms in 20 minutes.

    I get the feeling that people are assuming that the SDI software is
    going to be one long chunk of code running on one machine and that if
    it ever sees anything that is not what it expects its going to do a
    HALT and stop the entire process.

No critic has said this.  The fear is that it will do something that
it should not do, of which halting could be one thing.  The problem is
that you can't predict what that thing will be.

    So. The Challenge. People out there who think it is Impossible, please
    identify what is impossible. Pointing systems ? Target acquisition ?
    Target Classification ? Target descrimination ? Destruction of the
    targets ?

The hard thing is not any of these, and it illustrates the primary issue in
software as well.  The hard thing is knowing what the Soviets will do; that
places the specification of requirements of our software in their hands, and
they are unlikely to tell us what they will do.  You've mentioned essentially 
the analog of implementation details -- serious, complicated, hard, maybe
(or maybe not) impossible.  But that's assuming a cooperative opponent.

It seems that the real question on which we disagree is one raised by the
recent discussion of Scott's editorial and Freeman's response.  Computer
programs handle a variety of inputs, even if we can't specify in precise
detail the exact sequence of bits that are input.  However, our ability to
write computer programs that do this is dependent on our ability to formulate 
general rules that characterize the essential features and regularities in the
bit stream.  That is one reason why writing compilers is easier than writing
automatic translators from English to French; rules for computer languages
are easy, rules for natural language are hard (and maybe impossible).

Similarly, all military systems function in unknown environments, i.e.,
environments that cannot be specified down to the last detail.  When these
systems function as expected, the system designers must have correctly
predicted the essential features of the operating environment -- you could
say that they have been able to formulate general rules that characterize
the essential features and regularities of the environment.

Critics of SDI have no faith that it is possible to capture the
essential features of ALL possible Soviet responses to SDI.  As a
non-critic of SDI, do you think we can?  Or do you think that this
criterion is too strong?

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Swedish Vulnerability Board Report on Complex System Vulnerabilities
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Thu, 23 Oct 86 13:52:32 -0400
From: Chuck Youman &lt;m14817@mitre.ARPA&gt;

The October issue of Signal magazine contains an article by Thomas Osvald on
"Computers, Vulnerability and Security in Sweden."  It describes a number of
projects carried out by the Swedish Vulnerability Board.  Of particular
interest to RISKS is a project that addressed the vulnerability problems
associated with the complexity of EDP systems.  Mr. Osvald writes:

 &gt; A system becomes too complex when nobody can intellectually 
 &gt; understand and comprehend it.  Thus, a company will not change a
 &gt; system because secondary effects cannot be foreseen.  The board
 &gt; concluded that one of the problems of conventional, administrative,
 &gt; complex systems is that it is difficult or even impossible to 
 &gt; change these systems in an orderly, controlled way.  On the other
 &gt; hand, there is a rapid increase in the change rate in our society
 &gt; in general and a correspondingly increasing demand for flexibility
 &gt; in information systems.

 &gt; Therefore, it must be accepted that programs are for standard or
 &gt; nonrecurrent use with an ever shorter life expectancy.  However,
 &gt; data that are the raw material of information will not change as
 &gt; quickly as the processing rules.  Data are therefore the resource 
 &gt; that has to be cultivated, protected, tended, preserved, and developed.
 &gt; This approach supports recent developments of systems design methods,
 &gt; such as fourth generation languages, data dictionaries, and data base
 &gt; techniques.

Unfortunately, the article does not include a bibliography.  Does 
anyone out in RISKS-land know if a English translation of this report
exists?

Charles Youman (youman@mitre.arpa)

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: Thresher
</A>
</H3>
<address>
David Feldman 
&lt;<A HREF="mailto:feldman%dartmouth.edu@CSNET-RELAY.ARPA">
feldman%dartmouth.edu@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Wed, 22 Oct 86 02:34:25 edt
</i><PRE>

      A friend of my dad's who served in the submarine service once told me his
"version" of the events on the Thresher:
      Water had gotten into a compartment (or at least onto a sensor) in the
reactor unit, and that caused the reactor to scram. (According to him, this
type of shutdown is unconditional and irreversible on USN subs).  When the
ballast tanks were blown, for some reason the delivery pressure of the air that
cleared the ballast tanks came in higher than normal, and caused a greater
temperature drop at the valves.  The valves froze open, allowing all of the air
to escape, leaving the Thresher defenseless.
   Note: this is second hand from one submarine officer.
   Dave Feldman
   feldman@dartvax.edu

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Stealth and ATC
</A>
</H3>
<address>
Dan Melson
&lt;<A HREF="mailto:crash!pnet01!dm@nosc.ARPA ">
crash!pnet01!dm@nosc.ARPA 
</A>&gt;
</address>
<i>
Thu, 23 Oct 86 01:03:13 PDT
</i><PRE>

If it exists, they are hardly going to put it into heavily travelled airspace
over high population areas, where everybody can see it.

As for radar signatures, civilian ATC relies upon a mode 3/a transponder, and
targets are generated on our PVD's (primarily) as a result of that.  If they
want the aircraft visible to civil radar, they simply turn the transponder on.

(There are large areas of restricted airspace and MOA's (Military Operations
Areas) where the military does it's own operations without hindering civil
ATC, and if it exists, would guess that most stealth flights are within
such areas)

The above information is non-classified, freely available to any private
pilot.
                                                DM

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Inoperative components
</A>
</H3>
<address>
Peter Ladkin
&lt;<A HREF="mailto:ladkin@kestrel.ARPA ">
ladkin@kestrel.ARPA 
</A>&gt;
</address>
<i>
Thu, 23 Oct 86 18:28:22 pdt
</i><PRE>

Doug Humphrey wonders whether aircraft need cockpit warnings to tell of
major failure modes. The answer seems to be yes.  Multi-engine aircraft
instructors will tell you that a common occurrence with simulated engine
failures in multi-engine aircraft is for the student to feather the prop on
the good engine. The NTSB notes that this happens for real, too.

Peter Ladkin
ladkin@kestrel.arpa

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.84.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.86.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-72</DOCNO>
<DOCOLDNO>IA012-000125-B043-242</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.86.html 128.240.150.127 19970217005546 text/html 15923
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:54:16 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 86</TITLE>
<LINK REL="Prev" HREF="/Risks/3.85.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.87.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.85.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.87.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 86</H1>
<H2> Sunday, 26 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Addition to Census of Uncensored Sensors 
</A>
<DD>
<A HREF="#subj1.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Military vs. civilian automatic control systems 
</A>
<DD>
<A HREF="#subj2.1">
Will Martin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re:  System effectiveness is non-linear 
</A>
<DD>
<A HREF="#subj3.1">
Scott E. Preece
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  SDI assumptions 
</A>
<DD>
<A HREF="#subj4.1">
Daniel M. Frank
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  SDI impossibility 
</A>
<DD>
<A HREF="#subj5.1">
David Chase
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Editorial on SDI 
</A>
<DD>
<A HREF="#subj6.1">
Henry Spencer plus quote from David Parnas
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Addition to Census of Uncensored Sensors
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Sun 26 Oct 86 15:37:02-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

On 23 October 1986, a United Airlines Boeing 727 jet (UA 616, on a 20-minute
flight from San Francisco Airport to San Jose) had the nose-gear indicator
light stay on after takeoff, suggesting that the landing gear might not have
retracted.  The plane landed again at SFO at 7:48 AM (8 minutes after
takeoff).  The problem was later attributed to a malfunctioning nose gear
indicator.  [Source: San Francisco Chronicle, 24 October 1986, p. 30]

This is another example for the discussion on the risks of using sensors to
detect aircraft behavior.  Yes, if someone worries about this problem in
advance, it is always possible to have redundant sensors and redundant
indicators.  (This is done in SRI's SIFT system [Software Implemented Fault
Tolerance], a prototype flight-control system running at NASA Langley AFB.)
The cost of that must be compared with the resulting costs.  The total cost
of even an 8-minute aborted flight (including fuel, landing fees, and delays
-- with requeuing for takeoff) is nontrivial.  There are of course all sorts
of hidden costs in delays, such as the costs to passengers, and snowball
effects if such a delay exhausts the pilot's flying time for the month and
requires the location of another pilot! (That actually happened to me
once...)

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
 Military vs. civilian automatic control systems
</A>
</H3>
<address>
    Will Martin -- AMXAL-RI 
&lt;<A HREF="mailto:wmartin@ALMSA-1.ARPA">
wmartin@ALMSA-1.ARPA
</A>&gt;
</address>
<i>
Fri, 24 Oct 86 15:11:22 CDT
</i><PRE>

Many good points have been brought out about the rationale for the Navy not
having more automation in submarine control systems, including those on the
nuclear reactors. I think that a particular aspect of this needs emphasis.
There is a major difference in the basic concepts behind military systems
vs. civilian implementations -- the mission may be more important than human
life in the military environment, but never so in civilian situations. (Also
the completion of the mission may be more important than the preservation of
property or things in the military.)

It may well be necessary to "tie down the safety valve" to prevent a reactor
scram on a submarine in order that the vessel complete the mission or action
in progress, even if the inevitable result is death by radiation poisoning
of the crew, or some fraction of them, or the destruction of the reactor or
the vessel itself after it has completed the action it is required to
perform. In a civilian situation, this is never true -- the production of
electricity from reactor "X" can never be more important than the safety of
the population around or even the operators of that reactor. (We ignore here
the statistical probablity that the shutdown of reactor "X" will trigger a
cascade of blackouts which will eventually result in some number of deaths
due to related factors -- patients on the operating table, people trapped in
elevators, etc.) In fact, the value of the reactor itself is more important
than its continuing production of electricity -- it will be shut down to
prevent faulty operation causing damage to itself. For a military device,
completion of the wartime mission or task is often more important than the
continued safety or preservation of the device itself.

In the light of this, it is reasonable to expect relatively elaborate,
"idiot-proof", overriding automatic control systems in civilian
installations, and the absence of such in military versions of similar
devices (or perhaps the military system will have some for use only in
peacetime or training situations, which can be switched off in wartime).  It
may be necesary to operate devices "outside their envelopes" or to violate
various guidelines regarding safety in wartime missions. Also, of course,
military systems should continue to be at least somewhat usable even after
they have suffered damage and elaborate safety systems are merely something
more that will be liable to damage in combat. It is not acceptable to have
your power source turn off in the middle of a battle because a minor and
easily-controlled fire burned nothing vital but only some part of an
automatic safety system control circuit; it would be reasonable for a
civilian reactor to shut down given the exact same situation.

Note please that I am not saying that wartime operation would routinely be
done with a complete disregard for safety or that every mission is more
important than the lives of the people carrying it out. But there will be
certain exceptional circumstances where the missions are that important,
where the sacrifice of some lives (and certainly some amount of property) is
necessary for the achievement of larger goals. The military systems have to
support both routine operation and these rare exceptions.

Will Martin

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re:  System effectiveness is non-linear
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%mycroft@GSWD-VMS.ARPA">
preece%mycroft@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Thu, 23 Oct 86 13:52:06 CDT
</i><PRE>

Dave Benson argues that it is more reasonable and conservative to assume
that an overloaded system will fail entirely than to assume it will either
perform at its design limit but no more or perform above its design limit.

That's unarguably the conservative assumption.  I would deny that ANY
assumption was reasonable, given only a performance ceiling and the
knowledge that performance demand will exceed that ceiling.  It is obvious
that the system could be designed to perform in any of the suggested ways
when unable to cope with load.  Suggesting one response or another is simply
expressing an opinion of the designers' competence rather than any realistic
assessment of the risks of SDI.  Given that neither the design nor the
designers are determined yet, this is a silly exercise.

scott preece, gould/csd - urbana, uucp:	ihnp4!uiucdcs!ccvaxa!preece
arpa:	preece@gswd-vms

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
SDI assumptions
</A>
</H3>
<address>
Daniel M. Frank
&lt;<A HREF="mailto:prairie!dan@rsch.wisc.edu ">
prairie!dan@rsch.wisc.edu 
</A>&gt;
</address>
<i>
25 Oct 86 20:35:15 GMT
</i><PRE>
Organization: Prairie Computing, Madison, Wisconsin

   It seems to me that much of the discussion of SDI possibilities and
risks has gone on without stating the writers' assumptions about the
control systems to be used in any deployed strategic defense system.

   Is it presumed that SD will sit around waiting for trouble, detect
it, fight the war, and then send the survivors an electronic mail message 
giving kill statistics and performance data?  Much of the concern over
"perfection" in SDI seems to revolve around this model (aside from the
legitimate observation that there is no such thing as a leakproof
defense).  Arguments have raged over whether software can be adaptable
enough to deal with unforseen attack strategies, and so forth.

   I think that if automatic systems of that sort were advisable or achievable,
we could phase out air traffic controllers, and leave the job to computers.
Wars, even technological ones, will still be fought by men, with computers
acting to coordinate communications, acquire and analyze target data, and
control the mechanics of weapons system control.  These tasks are formidable, 
and I make no judgement on which are achievable, and within what limits.

   Both sides of the SDI debate have tended to use unrealistic models of
technological warfare, the proponents to sell their program, the opponents
to brand it as unachievable.  The dialogue would be better served by
agreeing on a model, or set of models, and debating the feasability of
software systems for implementing them.

    Dan Frank,  uucp: ... uwvax!prairie!dan,  arpa: dan%caseus@spool.wisc.edu

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
 SDI impossibility
</A>
</H3>
<address>
David Chase 
&lt;<A HREF="mailto:rbbb@rice.edu">
rbbb@rice.edu
</A>&gt;
</address>
<i>
Sat, 25 Oct 86 13:54:36 CDT
</i><PRE>
To: risks@csl.sri.com

I don't know terribly much about the physics involved, and I am not
convinced that it is impossible to build a system that will shoot down most
of the incoming missiles (or seem likely enough to do so that the enemy is
less likely to try an attack, which is effective), but people seem to forget
another thing; SDI should ONLY shoot down incoming missiles.  This system
has to tread the fine line between not missing missiles and not hitting
non-missiles.

I admit that we will have many more opportunities to evaluate its behavior
on passenger airplanes, the moon, large meteors and lightning bolts than on
incoming missiles, but we eventually have to let the thing go more or less
on its own and hope that there are no disasters.  How effective will it be
on missiles once it has been programmed not to attack non-targets?  To
avoid disasters, it seems that we will have to publish its criteria for
deciding between targets and non-targets (how much is an international
incident worth?  One vaporized weather satellite, maybe?  If I were the
other side, you can be sure that I would begin to try queer styles of
launching my peaceful stuff to see how we responded).

I think solving both problems is what makes the software hard; it's easy
to shoot everything if you have enough guns.  We could always put
truckloads of beach sand into low orbit.

David

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
  Editorial on SDI
</A>
</H3>
<address>
&lt;<A HREF="mailto:decvax!utzoo!henry@ucbvax.Berkeley.EDU">
decvax!utzoo!henry@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Fri, 24 Oct 86 00:31:56 edt
</i><PRE>

   &gt; ... The signatures were drawn from over 110 campuses in 41 states, and 
   &gt; include 15 Nobel Laureates in Physics and Chemistry, and 57% of the 
   &gt; combined faculties of the top 20 Physics departments in the country...

Hmmm.  If a group of aerospace and laser engineers were to express an
opinion on, say, the mass of the neutrino, physicists would ridicule them.
But when Nobel Laureates in Physics and Chemistry express an opinion on a
problem of engineering, well, *that's* impressive.

NONSENSE.

Dave Parnas, on the other hand, actually *is* an expert on the subject he
has been expressing doubts about (the software problem).  Although I'm not
sure I agree with everything he says, I give his views a *lot* more credence
than the people mentioned above.

				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

   [I could have been a little more precise in my comment on Douglas 
    Humphrey's message in <A HREF="/Risks/3.84.html">RISKS-3.84</A>.  I said that Dave Parnas "does not
    PROVE that SDI is IMPOSSIBLE."  By my curious emphasis, I meant to imply
    that Dave never even tried to prove impossibility.  He said that the SDI
    software system would be untrustworthy.  "..we will never be able to
    believe with any confidence that we have succeeded.  We won't have any
    way of knowing whether or not SDI has succeeded."

    Because Dave's comments really add significantly to this discussion -- and 
    because Henry set me up -- let me quote an excerpt from a private note
    from Dave.  PGN]

      "SDIO's own report to congress quotes President Reagan about its
      goals.  It says it is going to make nuclear weapons impotent and
      obsolete.  They claim to be able to end the fear of nuclear weapons.
      They can do neither of these things unless they can make a trustworthy
      software system, one that we can rely upon.  Without that, neither
      side will give up their offensive weapons.

      "In short, the SDI software is not impossible, but ending the
      fear of nuclear weapons that way is."   [David Parnas]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.85.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.87.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-73</DOCNO>
<DOCOLDNO>IA012-000125-B043-256</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.87.html 128.240.150.127 19970217005600 text/html 7326
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:54:32 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 87</TITLE>
<LINK REL="Prev" HREF="/Risks/3.86.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.88.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.86.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.88.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 87</H1>
<H2> Sunday, 26 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
System Overload 
</A>
<DD>
<A HREF="#subj1.1">
Mike McLaughlin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Information Overload 
</A>
<DD>
<A HREF="#subj2.1">
Mike McLaughlin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  SDI assumptions 
</A>
<DD>
<A HREF="#subj3.1">
Herb Lin
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
System Overload
</A>
</H3>
<address>
Mike McLaughlin
&lt;<A HREF="mailto:mikemcl@nrl-csr ">
mikemcl@nrl-csr 
</A>&gt;
</address>
<i>
Sun, 26 Oct 86 21:13:56 est
</i><PRE>

Back in Systems 001 I was taught that an overloaded system, be it a reactor 
control or SDI, failed due to overload in the following manners:
	1.  Sacrificed quality of work.
	2.  Sacrificed throughput rate.
	3.  Failed catastrophically (crashed).
	4.  Any combination of the above.  
Can a given system be designed to fail in a _chosen_ manner, so that it does
not crash - i.e. "graceful degradation."  Of course.  I see no reason why new
systems cannot do the same - at least in regard to the overload portion of the
problem. - mikemcl@nrl-csr.arpa

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Information Overload
</A>
</H3>
<address>
Mike McLaughlin
&lt;<A HREF="mailto:mikemcl@nrl-csr ">
mikemcl@nrl-csr 
</A>&gt;
</address>
<i>
Sun, 26 Oct 86 21:39:26 est
</i><PRE>

Undoubtedly we can load sensors on a system until it will no longer fly,
move, fight, or whatever due to the number of sensors.  Airplane cockpits
already provide more information than pilots can handle.  Combat sensor
systems provide more data than battle-managers can handle.  On the early
space flights we even instrumented the astronauts themselves -- in a manner
that should not be discussed on a family forum.  There seems little point
in providing a cockpit display of the pilot's rectal temperature; but on the
ground someone cared.

One of the functions being performed by computers today is to filter the
information, so that the system operator sees relevant data.  One of the
tough parts is to decide what is relevant.  I submit that "operator
assistant" computers deserve special care in design and testing.  They seem
to be used where lives are at stake, and where data is available.  Relying
on the computer to decide what is "relevant" in a given situation is fraught
with risk.  Relying on a human to decide in advance of the situation is not
much better.

Another area of concern is the "transition" problem discussed in previous
issues.  I don't know that Navy Propulsion reactors are under-computerized
deliberately, accidentally or at all.  Having been a watch officer in the
Navy and having lived through a number of unexpected emergencies I can
personally attest to the seriousness of the "transition" problem - even
without computers.  To be awakened from sleep with alarm bells ringing and
bullhorns blaring "FIRE, FIRE, FIRE IN NUMBER TWO MAGAZINE!" - and then be
standing dressed, over the magazine, and in charge of the situation in less
than 60 seconds is quite an experience.  That I am here to recognize the
problem is due to excellent train- ing of the entire crew, not to any
specific actions on my part.  Frankly, I just "went automatic" and shook
after it was over, not during.  I suspect that any pilot, truck driver,
policeman, etc. could tell a dozen similar tales.
 
I'm not proposing any answers - except for extreme care.  

	- mikemcl@nrl-csr.arpa

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
SDI assumptions
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Sun, 26 Oct 1986  23:48 EST
</i><PRE>

    From: prairie!dan at rsch.wisc.edu (Daniel M. Frank)

    Much of the concern over "perfection" in SDI seems to revolve around
    this model (aside from the legitimate observation that there is no such
    thing as a leakproof defense).

I've said it before, but it bears repeating; no critic has ever said
SDI software must be perfect.  The only ones who say this are the
pro-SDI people who are criticizing the critics.

    The [SDI] dialogue would be better served by agreeing on a model, or set
    of models, and debating the feasability of software systems for
    implementing them.

Having a "set of models" means that those models share certain
characteristics.  There is one major characteristic that all SDI
software will share: we will never be able to test SDI software --
whatever its precise nature -- under realistic conditions.  Then the
relevant question is "What can we infer about software that cannot be
tested under realistic conditions?"

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.86.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.88.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-74</DOCNO>
<DOCOLDNO>IA012-000125-B043-285</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.88.html 128.240.150.127 19970217005615 text/html 17847
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:54:42 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 88</TITLE>
<LINK REL="Prev" HREF="/Risks/3.87.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.89.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.87.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.89.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 88</H1>
<H2> Monday, 27 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
SDI, Missing engines, feeping creatureism in consumer products 
</A>
<DD>
<A HREF="#subj1.1">
Roy Smith
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  More aircraft instrumentation 
</A>
<DD>
<A HREF="#subj2.1">
John Allred
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: Military vs. civilian automatic control systems 
</A>
<DD>
<A HREF="#subj3.1">
Eugene Miya
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Perfection 
</A>
<DD>
<A HREF="#subj4.1">
Douglas Humphrey
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Shipboard anecdotes 
</A>
<DD>
<A HREF="#subj5.1">
Mike McLaughlin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  RISKS UNDIGESTIFIER on UNIX 
</A>
<DD>
<A HREF="#subj6.1">
John Romine
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
SDI, Missing engines, feeping creatureism in consumer products
</A>
</H3>
<address>
Roy Smith
&lt;<A HREF="mailto:cmcl2!phri!roy@seismo.CSS.GOV ">
cmcl2!phri!roy@seismo.CSS.GOV 
</A>&gt;
</address>
<i>
Thu, 23 Oct 86 15:28:25 edt
</i><PRE>

	This message is a potpouri of several random thoughts that I've had
in the past few days.  The first two are apropos to recent topics on RISKS,
the last is new material.

	Re: SDI and unexpected inputs.  I have a friend who works for the
Army Night Vision Lab (I'm not sure that's actually the correct name).
They work on "find the tank in the jungle at night" problems.  He once
described a program that looks for tanks in a battlefield -- the first
thing it does is find the horizon and concentrate on the area below (i.e.
the ground).  My first thought was "what happens when they start dropping
tanks by parachute?"

	Re: Planes loosing engines.  I gather than in many of the cases of
planes having gross defects (i.e. a control surface torn off), the
situation was at least meta-stable until the pilot tried to do something
(i.e. turned off the auto-pilot to take control).  I'm just guessing, but
it seems that a chase plane could take off and intercept the damaged plane
to make a visual inspection of its exterior quickly enough to be of some
use.  Am I being naive to think that this would be 1) practical and 2) of
any use?  Is it done already?

	Re: feeping creatureism.  There is an annoying trend towards
computerizing things that just don't need computerization.  Even worse is
the urge to make things *seem* computerized when the microprocessor in them
does nothing more than scan for switch closures on the control panel and
run a simple timer.  I recently bought an air conditioner -- it doesn't
have a control panel, it has a "command center".  It has the same controls
(on/off, etc) as any other air-conditioner, but the panel is made up to
look like some sort of computerized gizmo.  My new electric dryer is the
same way -- it's got "electronic drying", which means is it has a
thermostat is the exhaust vent just like my mother's old mechanical-timer
model.  Speaking of my mother, she just bought a new car and hasn't figured
out how the radio works yet because the familiar volume and tuning knobs
aren't there any more.

	So, how does all this tie in with COMPUTER RISKS?  Take the dryer;
by making it appear that there is some kind of computerized system
monitoring and controlling the drying process, the consumer is duped into
believing that his dryer is somehow better than the old ones.  He doesn't
really understand *why* it is better, but since it computerized, it *must*
be better, right?  Likewise with the car radio.  While it may be true that
digitally synthesized tuning is better than mechanical variable capacitors,
(let's not start arguing about *that*) there was nothing wrong with the
user interface (2 knobs to turn, maybe some pre-set pushbuttons).  While
the real advantage of the new radio over the old is the PLL instead of the
variable cap, the *percieved* advantage is the "tune-up/tune-down" buttons
instead of the tuning knob to turn.  In fact, the new-fangled user
interface is no better than the old one, and may in fact be worse.

Roy Smith, {allegra,philabs}!phri!roy
System Administrator, Public Health Research Institute
455 First Avenue, New York, NY 10016

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
 more aircraft instrumentation
</A>
</H3>
<address>
    John Allred 
&lt;<A HREF="mailto:jallred@labs-b.bbn.com">
jallred@labs-b.bbn.com
</A>&gt;
</address>
<i>
Mon, 27 Oct 86 10:35:39 EST
</i><PRE>

Doug Humphrey asks:
   " ...  I am not sure why a pilot would need a video monitor to tell him that
     Number 2 just fell off the wing, ... .  He will no doubt understand this
     by the way the aircraft is acting."

A perfect example of why a pilot could use a monitor is the American Airline
DC-10 crash at O'hare.  The pilots knew they had lost power on the engine.
However, they had no way of knowing that they had physically lost the engine
(because you can't see the engines from the DC-10 cockpit.)  Upon detecting
that they had lost power in one engine, the pilots went exactly by the book -
they changed the airspeed to best-2-engine-climb speed.  Unfortunately, when
the engine fell off the wing, it also ripped out some hydraulic lines in the
wing, which were holding the slats (high lift devices on the leading edge of
the wing) extended.  With the slats retracted, the stall speed of the damaged
wing was *above* best-2-engine-climb speed.  So, one wing stalled, the other
kept generating lift, and the plane rolled over.

It should also be noted that pilots in simulators, when given the exact same
situation, were able to save the aircraft when they knew that they had
physically lost the engine, while pilots that did not know uniformly failed to
save the aircraft.

Doug is correct in stating that a pilot should be able to understand if he's
lost something important.  However, that understanding could come too late, or
in and of itself be fatal.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: Military vs. civilian automatic control systems
</A>
</H3>
<address>
Eugene Miya
&lt;<A HREF="mailto:eugene@AMES-NAS.ARPA ">
eugene@AMES-NAS.ARPA 
</A>&gt;
</address>
<i>
Mon, 27 Oct 86 09:04:33 pst
</i><PRE>

I basically agree with Will's thesis about missions, but I don't
the difference is that simple (binary).  Two years ago, an F-8 Crusader
(single engine Navy fighter, older) lost power over San Diego.  The
pilot had time to eject, but before doing so, he tried to avoid hitting
buildings in the Serrento Valley area.  (True he might have misjudged
prior to ejection, but the plane did come down in a parking lot
and not the nearby electronics buildings.)  Many pilots have faced this
dilemma in the past: including civilian pilots (do I kill several hundred
people on the ground in addition to the passengers I have just killed?).
I think this also goes for civilian rescue missions.  Ford' Mayeguez (sp)
mission in 1975 cost more Marine lives than civilians rescued.  True
we will never know the real political consequences of not rescueing
(liberals: "we would have negiotated release," conservatives: "they would
have died"), but my point is many of the fundamental types of systems
are no different in the civilian or military sphere, and that there is
overlap (with tricky trade offs) with military operations.

--eugene miya

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Perfection
</A>
</H3>
<address>
Douglas Humphrey 
&lt;<A HREF="mailto:deh@eneevax.umd.edu">
deh@eneevax.umd.edu
</A>&gt;
</address>
<i>
Mon, 27 Oct 86 02:52:25 EST
</i><PRE>


To LIN : In response to a message, you state that none of the anti-SDI
         folk ever stated that the software had to be perfect. I have 
         heard constantly in both the widely read (Washington Post) and
         limited (?) distribution industry media (Aviation Leak and 
         Space Mythology) SDI critics that contect that it must be perfect
         or it is useless. I don't beleive this, and I would hope you
         don't either, but saying that the whole must be perfect 
         certainly implies that the parts must be perfect. (Opps. contend..)

         About failure modes in software systems, yes, it is possible to
         design fault tolerant and fault permissive systems. Systems that 
         have a know 'prefered failure mode'. Example, hardened underground
         facilities, I have been told (no references here) are not designed
         to withstand forces equaly throughout the structure. That would mean
         that when the structure finaly failed under load, there would be no 
         reliable way to project where the failure would happen. Better to 
         design with structural over load failure in mind and specificaly
         designate one area as the failure area, and then take withever
         measures one can (air/water tight bulkheads, etc.) in that
         area since you now have a high degree of confidence that the failure
         will happen where you want it, and are ready for it.
         Software can be designed the same way by dealing not only with the 
         quantity (targets) by the quality of targets (destinations) and 
         selectivly 'failing' on those which are the least important.

         I would guess that a catostrophic failure would be the one
         to avoid, even of the system decided that it was time to reboot, 
         clearing target tracking data since some of it was detected as 
         bad. The system might then let through whatever was locked at the
         time of the failure, but at least it would resume defense rather than 
         either crash outright, or get into a position where its target load
         started to effect its real time processing and maybe preventing it
         from reacting well enough to to its job. 

         Hey ! If we get flaming about this much deeper, we should all start 
         submitting bills to SDIO.......

Doug

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Shipboard anecdotes           [marginally relevant but intersting]
</A>
</H3>
<address>
Mike McLaughlin
&lt;<A HREF="mailto:mikemcl@nrl-csr ">
mikemcl@nrl-csr 
</A>&gt;
</address>
<i>
Mon, 27 Oct 86 13:05:11 est
</i><PRE>
Cc: neumann@csl

Two anecdotes about shipboard emergencies.  
	In that fire, one sailor did think about what was happening, and
ran aft as fast as his little legs would carry him.  A _giant_ Chief Gunner's
Mate named Mills grabbed him, pointed him back to his battle station, and
said something like "Son, you better get to your battlestation.  When a 
destroyer has a fire in a magazine, you just can't run far enough!"

	In another emergency that was really too complex to explain on
Risks, I _really_ went automatic.  I had far more charge of the situation, 
and far more depended on my own actions.  Simply put, the USS Saratoga was
about to run over us, and we had lost control of our rudders.  I did the
requisite things, and am here to tell about it.  But _during_ the experience
I was "out of body" - Some part of me was floating above and behind me, 
watching me give orders &amp; do things, sort of supervising/monitoring me,
but not interfering.  I have no recollection of the situation from my
body's eyes and ears once the situation developed.  All of my quite 
detailed memory is from that viewpoint floating up in the aft port 
quarter of the pilothouse.  I must have done good, because everybody said so, 
from the skipper down to the real authorities, the mess cooks.  I have to 
conclude that I had been so thoroughly trained that I was operating on a 
learned-reflex basis, leaving my conscious mind free to observe.  I don't
know if we can use that somehow in designing "operator assistants" or not.

	- Mike

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
RISKS UNDIGESTIFIER
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Mon, 27 Oct 86 10:24:41 -0800
From: John Romine &lt;jromine@nrtc-gremlin&gt;

If you have the MH Message Handler (a user agent for UNIX) you have the
"burst" command which seems to work just fine on Risks digests.  MH is
now distributed as user-contributed software on the 4.3BSD tape, and is
available for anonymous ftp from the host louie.udel.edu.  Also, you can
get a magtape copy for $75 from the University of California, Irvine.
I've included the release announcement below.

/JLR

    A new release of the UCI version of the Rand Message Handling (MH)
    system is available for distribution.  This release of MH is called 

				  MH 6.5

    There are a lot of changes between MH.6 and MH 6.5; a lot of
    performance enhancements were made, there's also a lot of support
    for distributed mail (personal mail and bulletin bboards).

    Here are the details:  

	- MH is in the public-domain
	- MH runs on a number of versions of UNIX (4.[123]BSD, V7, SYS5, and
	  related variants, e.g., HPUX) [sorry, no support for SYS3.]
	- MH runs on top of a number of mail transport systems
	  (MMDF-{I,II}, SendMail, stand-alone (with UUCP support))

    Although MH is not "supported" per se, it does have a bug-reporting
    address, Bug-MH@ICS.UCI.EDU.  Bug reports (and fixes) are welcome, by
    the way.  There are also two ARPA Internet discussion groups:
    MH-Users@ICS.UCI.EDU and MH-Workers@ICS.UCI.EDU (somewhat analogous in
    charter to Info-UNIX and UNIX-Wizards).  

    There are two ways to get a distribution:

    1.  If you can FTP to the ARPA Internet, use anonymous FTP to
    louie.udel.edu [10.0.0.96] and retrieve the file portal/mh-6.tar.
    This is a tar image (approx 4MB).  The file portal/mh-6.tar.C is
    the tar image after being run through the compact program (approx
    2.3MB).  The file portal/mh-6.tar.Z is the tar image after being run
    through the compress program (approx 1.5MB).

    2.  You can send $75 to the address below.  This covers the cost of
    a magtape, handling, and shipping.  In addition, you'll get a
    laser-printed hard-copy of the entire MH documentation set.  Be sure
    to include your USPS address with your check.  Checks should be made
    payable to 

		Regents of the University of California

    and must be drawn on U.S.  funds.  It's also a good idea (though not
    mandatory) to send a computer mail message to "Bug-MH@ICS.UCI.EDU" when
    you send your check via USPS to ensure minimal turn-around time.
    The distribution address is:  

	Support Group 
	Attn: MH distribution
	Department of Information and Computer Science
	University of California, Irvine
	Irvine, CA  92717

	714/856-7553

    Sadly, if you just want the hard-copies of the documentation, you
    still have to pay the $75.00.  The tar image has the documentation
    source (the manual is in roff format, but the rest are in TeX
    format).  

/mtr

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.87.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.89.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-75</DOCNO>
<DOCOLDNO>IA012-000125-B043-306</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.89.html 128.240.150.127 19970217005636 text/html 15458
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:54:58 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 89</TITLE>
<LINK REL="Prev" HREF="/Risks/3.88.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.90.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.88.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.90.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 89</H1>
<H2> Tuesday, 28 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Airplanes and risks 
</A>
<DD>
<A HREF="#subj1.1">
Alan Wexelblat
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  TSE, Air Canada 
</A>
<DD>
<A HREF="#subj2.1">
Matthew Kruk
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Big Bang 
</A>
<DD>
<A HREF="#subj3.1">
Robert Stroud
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Physicists on SDI and engineering.. 
</A>
<DD>
<A HREF="#subj4.1">
Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  ABM, SDI, and Freeman Dyson 
</A>
<DD>
<A HREF="#subj5.1">
Peter Denning
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Airplanes and risks
</A>
</H3>
<address>
Alan Wexelblat 
&lt;<A HREF="mailto:wex@mcc.com">
wex@mcc.com
</A>&gt;
</address>
<i>
Tue, 28 Oct 86 11:23:52 CST
</i><PRE>

Today's paper has a couple of airplane-related items that got me to thinking.

One item is a story on how the FAA is going to adopt strict rules for small
aircraft in busy airspaces and establish a system to find an punish pilots
who violate these rules.  The question this brought to mind is: is this the
right approach for the FAA's problem?  How about for computer systems?  Can
(or should) we manipulate the user so that he uses the system the way we
designers intended it to be used?  Is training the answer (as suggested by
the Navy emergency stories)?

The next item is an analysis of the emergency aboard the Thai jet.  Apparently
the fault is similar to the one that doomed the JAL 747 that crashed recently
in Japan.  The factor that made the difference -- according to Hiroshi Fujiwara
who is deputy chief investigator of Japan's Aviation Accident Investigation
Commission -- was that the Thai Airbus A-300 retained hydraulic control of
the flaps and rudder on the tail.

Both the 747 and the A-300 have triply-redundant hydraulic systems, but on the
747 all three pass through the rear bulkhead in the same opening.  Thus all
three were ruptured at once.  On the A-300 there are three separate
openings and while two of the systems were ruptured in the Thai jet, the
third remained usable.

The related question is: can we make use of this feature in computer systems
(hardware or software)?  That is, if a program has three ways of doing
something can we isolate them so that a bug somewhere doesn't simultaneously
cripple all three?  Can we (given needs like security) separate computer
hardware so that it is much more difficult to simultaneously destroy primary
and backup hardware?

Comments and discussion welcomed.

Alan Wexelblat
ARPA: WEX@MCC.ARPA or WEX@MCC.COM
UUCP: {seismo, harvard, gatech, pyramid, &amp;c.}!ut-sally!im4u!milano!wex

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
TSE, Air Canada
</A>
</H3>
<address>
&lt;<A HREF="mailto:Matthew_Kruk%UBC.MAILNET@MIT-MULTICS.ARPA">
Matthew_Kruk%UBC.MAILNET@MIT-MULTICS.ARPA
</A>&gt;
</address>
<i>
Mon, 27 Oct 86 10:46:30 PST
</i><PRE>
ReSent-To: RISKS@CSL.SRI.COM

No doubt you will hear more about these items from better informed sources. I
merely heard brief summaries on the morning news today (Monday, 27th).

1. The Toronto Stock Exchange computer went down for about 5 minutes this
   morning. No cause given (yet).

2. A fire in a building, which houses the main computer (reservations?) of Air
   Canada, in Montreal. An Air Canada official cannot predict the effect on
   people holding advance registration. Damage cost estimates run in the
   millions.

Presumably there will be more information in tonight's paper. I'll try to get
a summary out as soon as I can.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Big Bang                        [Also noted by Martin Minow.  Thanks.]
</A>
</H3>
<address>
Robert Stroud 
&lt;<A HREF="mailto:robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Tue, 28 Oct 86 19:42:40 gmt
</i><PRE>

Yesterday, October 27th, was the day of the Big Bang in the City - a revolution
in the way in which the Stock Exchange is organised. Basically, three things
happened - the market was opened to foreigners, the distinction between jobbers
(who trade on their own account) and brokers (who buy and sell on behalf of
clients) was abolished (thereby introducing potential conflicts of interest
and necessitating the erection of so-called Chinese Walls to prevent this),
and finally, guaranteed minimum commissions were removed, making things much
more competitive. Wall Street went through something like this on May Day a few
years ago.

Anyway, these three changes led to the introduction of new computing systems
developed in something of a rush to meet yesterday's deadline. Most
important of these was the Stock Exchange Automated Quotation system (SEAQ)
which several companies had to switch to by default at the last minute when
they realised that their in-house systems would not be working in time. SEAQ
provides information over the Topic network to 10,000 terminals about share
prices - dealing is still done manually (at least until next year) although
the SEAQ system is supposed to be updated continuously to reflect the
trading.

There was a full-scale rehearsal last week when the Stock Exchange opened on
a Saturday for the first time in its history. Not everything went smoothly
and there were complaints about prices not being updated for as long as 20
minutes, making it possible to buy at one price and simultaneously sell at
another.  However, as late as Sunday afternoon, the chairman of the Stock
Exchange Council was defiantly challenging anyone to demonstrate that this
was still a problem.

Well, I'm sure that RISKS readers can guess what happened on Monday morning.
The system lasted half an hour before it broke down at 8.30am! Although it
was later up and running, and the problem was with the antiquated Topic
network rather than the SEAQ system itself, there are fears that it could
happen again under crisis. Apparently, this failure was caused by curiosity
- everybody wanted to try out the new system at once, and it couldn't cope.

Curiosity is an interesting example of human behaviour causing a computer
system to fail. I believe the telephone companies have a similar problem on
Mother's Day when the pattern of usage is abnormal.

Another example of human behaviour has been the reaction of the dealers to
the new system, to some extent invalidating the whole concept. Only time
will tell whether this is just suspicion of a new technology or a real
problem. However, at present the dealers are rather wary and are therefore
only offering small deals on the system (up to 1000 shares) so that the big
deals (100,000) are still negotiated over the telephone. This is partly a
defensive move because the system is (rightly or wrongly) perceived as being
slow, making it possible to offer unrealistic prices not in line with the
market - the real market is off the screen. Equally, some market makers "are
playing complicated games to test their competitors and this is likely to
become a feature of the new markets".  One dealer has even gone so far as to
describe the SEAQ terminals as "useless".  [This paragraph extracted from an
interesting article in today's Times entitled "New screens 'fail to catch
full deals'" by Richard Thomas]

Naturally, there has been a wealth of material about all this in the media
recently, and today, all the papers are competing with each other for puns
on Big Bang! When the dust settles on this most public of failures, RISKS
archaeologists will have plenty of relics to excavate. Here is one of the
more technical articles, reproduced without permission from today's Times,
(28th October p.21)

Robert Stroud,
Computing Laboratory,
University of Newcastle upon Tyne.

ARPA robert%cheviot.newcastle@ucl-cs.ARPA (or cs.ucl.ac.uk if you trust domains!)
UUCP ...!ukc!cheviot!robert

             ++++++++++++++++++++++++++++++++++++++++

            "Big Bang shambles as computer breaks down - 
             Goodison blames Topic subscriber's curiosity"

by Michael Clark

(c) Times Newpapers PLC

Yesterday's disastrous debut for the Stock Exchange Automatic Quotations
system was a prime example of Murphy's Law: "If something can go wrong, it
will". But the problems encountered by dealers on the trading floor stemmed
from technical problems at Topic, the Stock Exchange's own tried-and-tested
screen-based information system.

Topic went off the air at 8.30am - a crucial time for traders hoping to
establish the price of stocks ahead of the official start of dealings at 9am
- and stayed down for more than an hour, apart from one intermission. The
break also resulted in all operations on SEAQ being suspended for the same
period.

Stock Exchange officials blamed a breakdown in the link between Topic and
SEAQ.  Market-makers feed their prices into the SEAQ computer which are then
updated and displayed on the 10,000 Topic terminals situated in the City
offices of brokers and fund managers.

Sir Nicholas Goodison, chairman of the Stock Exchange Council, described
Topic as the world's eye on the market and said that although it had enjoyed
a high level of reliability, it was six years old and considered fairly
antiquated by today's standards.

A Stock Exchange spokesman quickly blamed curiosity for the failure: "The
system cannot handle all the Topic sets being used at the same time."

Topic was operating at maximum capacity yesterday, receiving 12,000 page
requests a minute, or 200 per second. [SEAQ itself is designed to handle 40
transactions per second, but the maximum demand yesterday was 22 per
second.] Sir Nicholas said that the system had suffered a small setback
which had been put right. He said that Topic had been overwhelmed by the
number of page changes which, normally, it would not have to cope with. Most
of it was simply curiosity by subscribers.

"If you want to put a monkey, or a dodo in a zoo, everyone will want to look
at it on the first day," he said.

But it is still possible the breakdown could happen again. SEAQ encourages
dealers and fund managers to use its screens more and a sudden surge of
business may overload Topic.

The Stock Exchange's technical officers say there are only a few adjustments
that can be made to Topic. One may be to introduce an automatically
triggered queuing system which limits the number of subscribers using the
system at any one time.  But many dealers fear this could lose them
business.

Meanwhile, there were still complaints from market makers about the time it
took for a price change to appear on Topic after dealing. There were reports
of delays up to one hour. Sir Nicholas said these would be checked but still
blamed market makers' own internal systems for the delay.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Physicists on SDI and engineering..
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Mon, 27 Oct 1986  20:01 EST
</i><PRE>

    From: decvax!utzoo!henry at ucbvax.Berkeley.EDU

    Hmmm.  If a group of aerospace and laser engineers were to express an
    opinion on, say, the mass of the neutrino, physicists would ridicule them.
    But when Nobel Laureates in Physics and Chemistry express an opinion on a
    problem of engineering, well, *that's* impressive.

I simply point out that the Manhattan Project was run by a bunch of
physicists.  The H bomb was transformed from an 80 ton clunker to a
practical device by physicists.  These were "mere" engineering
problems too.

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
ABM, SDI, and Freeman Dyson
</A>
</H3>
<address>
Peter Denning 
&lt;<A HREF="mailto:pjd@riacs.edu">
pjd@riacs.edu
</A>&gt;
</address>
<i>
Tue, 28 Oct 86 11:10:29 pst
</i><PRE>
To: neumann@sri-csl
ReSent-To: RISKS@CSL.SRI.COM

In RISKS 3.83, Ken Dymond noted that the ABM (anti ballistic missile
system) debate of the early 1970s is similar to the SDI debate of the
mid 1980s, and asked for sources that might shed light on the past
debate.  Here's one source known to me:

Chapter 7 in Freeman Dyson's WEAPONS AND HOPE is an excellent analysis
of the ABM debate.  He compares that debate with the ``star wars''
debate and finds both similarities and differences.  He sees a role
for (nonnuclear) ABM systems in a nuclear-free world, and expresses
the hope that the ABM debate will one day be reopened.  In contrast,
he considers ``star wars'' a technical folly, for reasons having
little to do with the reliability of the software systems.

Peter Denning

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.88.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.90.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-76</DOCNO>
<DOCOLDNO>IA012-000125-B043-326</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.90.html 128.240.150.127 19970217005650 text/html 20696
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:55:19 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 90</TITLE>
<LINK REL="Prev" HREF="/Risks/3.89.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.91.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.89.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.91.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 90</H1>
<H2> Thursday, 30 October 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Anti Skid Brakes 
</A>
<DD>
<A HREF="#subj1.1">
Paul Schauble
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  The Mother's Day Myth, and "Old Reliable" 
</A>
<DD>
<A HREF="#subj2.1">
Jerome H. Saltzer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Collision avoidance systems 
</A>
<DD>
<A HREF="#subj3.1">
John Larson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Crime and punishment 
</A>
<DD>
<A HREF="#subj4.1">
Peter Ladkin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Air Canada 
</A>
<DD>
<A HREF="#subj5.1">
Matthew Kruk
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  (Voting) Machine Politics 
</A>
<DD>
<A HREF="#subj6.1">
Mike McLaughlin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Computer RISKS in "Ticker-Tape Parades" 
</A>
<DD>
<A HREF="#subj7.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  SDI vs. Social Security 
</A>
<DD>
<A HREF="#subj8.1">
Scott Guthery
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj9">
  SDI Impossibility? 
</A>
<DD>
<A HREF="#subj9.1">
Scott Dorsey
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj10">
  Feeping Creaturism 
</A>
<DD>
<A HREF="#subj10.1">
Charley Wingate
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
 Anti Skid Brakes
</A>
</H3>
<address>
 Paul Schauble 
&lt;<A HREF="mailto:Schauble@MIT-MULTICS.ARPA">
Schauble@MIT-MULTICS.ARPA
</A>&gt;
</address>
<i>
Thu, 30 Oct 86 04:44 EST
</i><PRE>
To:  Risks@CSL.SRI.COM

    In view of the recent discussion on Anti-Skid Brakes and their
overrides, I thought I would post this item.  It is by John Dinkel's
column in the October 1986 issue of Road &amp; Track and describes his and
other race drivers experience with the Anti-skid Braking System (ABS) on
a Corvette.

          - - - - - - - - - - - - - - - - - - - -

During a recent test session at Sears Point International Raceway, the
Bakeracing Corvette drivers were treated to a couple of graphic
demonstrations of the differences between ABS and non-ABS braking.
Coming down the Carousel, a long sweeping, downhill left-hander, team
leader Kim Baker found himself running a bit fast for the wet track
conditions.  Rather than drive off the track, Kim locked the brakes and
put the car into a harmless spin.  Surprise.  This time it wasn't
totally harmless.  Once the car stopped sliding sideways, the ABS caused
the Vette to steer in the direction in which the front wheels were
aimed.  In this instance the ABS allowed the car to take a wider than
expected arc, and Kim and the Corvette found themselves rolling gently
into the tire wall on the outside of the turn.  No harm except for
embarrassment on Kim's part, but this incident certainly pointed out one
of the differences between spinning a car with and without ABS.

That wasn't the only difference.  I listened intently as two of out
drivers complained of lack of braking and a soft pedal as they applied
the brakes at the top of the Carousel.  Having just finished driving
several laps following a discussion with John Powell, owner of one of
the other Corvette teams and an experienced driver training instructor,
about ABS versus non-ABS race track driving, I knew what the problem
was.  Coming up to the braking point at the entrance to the Carousel, a
car gets light as it crests a hill.  If you apply ABS brakes at the
instant, the ABS senses loss of traction or a low-coefficient [of
friction] surface and releases pressure to one or more wheels that it
thinks is trying to lock.  The ABS brain has been fooled by the car
losing download over that crest, and it can take up to half a second for
the system to recover and allow full braking force after the wheel loads
return to normal.  What does the driver sense during that half second
besides panic?  A soft pedal and longer than expected braking distances.
The solution?  Simple.  Initiate your braking right before the car gets
light or wait until the wheels are fully loaded again after the crest.
Exercise either of these two options and you'll never know that the car
is equipped with ABS except for the added security it affords when you
hot foot it into a corner and discover that you can still steer into the
turn despite having the brakes "locked".  And, as we discovered at
Portland, a Corvette with ABS and drive rings around the competition on
a wet track.

           - - - - - - - - - - - - - - - - - - - -

It's noteworthy that some racing teams are experimenting with computer
controlled cars.  The suspension, braking, steering, and engine
parameters under direct control of an on-board computer that is
programmed for the specific race and track being driven.  So far, such a
car has not run in competition.  However, as Risks readers know,
computer controlled engines and transmissions are almost commonplace.  I
expect to see the car with the computer controlled suspension in
competition in 1987.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
The Mother's Day Myth, and "Old Reliable"
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
Tue, 28 Oct 86 23:11:16 EST
</i><PRE>
From: Jerome H. Saltzer &lt;Saltzer@ATHENA.MIT.EDU&gt;

From Robert Stroud's piece on SEAQ. . .  (<A HREF="/Risks/3.89.html">RISKS-3.89</A>)

&gt; Curiosity is an interesting example of human behaviour causing a
&gt; computer system to fail. I believe the telephone companies have a
&gt; similar problem on Mother's Day when the pattern of usage is abnormal.

Workers in the New England Toll Switching Center here in Cambridge
tell visitors on guided tours (that is the best I can do for a
reference; sorry) that their busiest day for long distance calls is
the Monday after Thanksgiving.  The explanation they give is that the
Friday after Thanksgiving is the first real Christmas shopping day,
because so many people have or take that day off.  All the retailers
in New England study the pattern of sales on Friday and Saturday,
ponder it on Sunday, and spend Monday morning on the telephone to
their suppliers trying frantically to get their hands on more of
whatever seems to be selling well this year.

That one falls in the category of hard-to-imagine-in-advance-but-
easy-to-explain-in-retrospect system problems.

The Michael Clark article quoted by Stroud contains a comment that
is eyebrow-raising from the point of view of RISKS:

&gt; . . . said that although it had enjoyed a high level of reliability,
&gt; it was six years old and considered fairly antiquated by today's
&gt; standards.

I wonder who it is that considers that system as antiquated?  Another
perspective says that a complex system that has been running for six
years is just beginning to be seasoned enough that its users can have
some confidence in it.  People who have work to do (as compared with
computer scientists, who users perceive as mostly interfering with
people trying to get work done) know that in many cases the most
effective system is one that has just become obsolete.  The tinkerers
move on to the shiny new system and leave the old one alone; it
becomes extraordinarily stable and its customers usually love it.

					Jerry Saltzer

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Collision avoidance systems
</A>
</H3>
<address>
&lt;<A HREF="mailto:jlarson.pa@Xerox.COM">
jlarson.pa@Xerox.COM
</A>&gt;
</address>
<i>
Wed, 29 Oct 86 11:29:49 PST
</i><PRE>
To: RISKS@CSL.SRI.COM

There was a rather distressing article about collision avoidance systems
in the San Jose Mercury News recently (Sun, 26 Oct).  According to the
article the FAA nixed a workable collision avoidance system designed by
Honeywell 11 years ago because it competed with an in house collision
avoidance system they were developing.  This was done in spite of
several studies showing that the Honeywell system would be better than
the FAA system.  The Honeywell system would have cost $14,000 per
comercial airline and was projected to be cost reduced to about $1000
making it affordable for most aircraft.  

They also quoted a former FAA official to the effect that the FAA was
partly responsible for the loss of over 700 lives due to collisions
because of their failure to go ahead with the Honeywell system. 

The FAA is finally almost ready with their own version of a collision
avoidance system (apparently needs another year of testing), but it will
cost a lot more than the original Honeywell system ($40-70K) and has
problems with clouds and bad weather.  It also apparently can't be made
as cheap as the original Honeywell system ($5,000 or so) so it will
probably not be used much except in commercial aircraft.

Does anyone know more about this issue ?  I'm particulary interested in
technical details about the Honeywell and the FAA systems.  

John

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Crime and punishment
</A>
</H3>
<address>
Peter Ladkin
&lt;<A HREF="mailto:ladkin@kestrel.ARPA ">
ladkin@kestrel.ARPA 
</A>&gt;
</address>
<i>
Tue, 28 Oct 86 18:34:59 pst
</i><PRE>

Alan Wexelblatt asks:

  [...] the FAA is going to adopt strict rules for small aircraft in busy
  airspaces and establish a system to find and punish pilots who violate
  these rules.  The question this brought to mind is: is this the right
approach for the FAA's problem?

These rules are already in existence, and so are the punitive
practices. Neither can stop mistakes, as in the Cerritos
airspace violation by the Archer. They are even less effective
against deliberate violators, who turn off their transponders.

  How about for computer systems? [..] Is training the answer [..] ?

Maybe to avoid mistakes, as in rm *, but not for deliberate violators.
The late-70s Berkeley Unix cracker was known, and wouldn't stop.
I believe that the Computer Center tried to hire him to turn his
talents to useful purposes - which didn't work.
Eventually the police went around to arrest him, which seemed
to work (he was a young middle-class teenager).
So training wasn't the answer, but sufficiently severe punishment
was, in this case. Not that I advocate this approach.

Peter Ladkin

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Air Canada
</A>
</H3>
<address>
&lt;<A HREF="mailto:Matthew_Kruk%UBC.MAILNET@umix.cc.umich.edu">
Matthew_Kruk%UBC.MAILNET@umix.cc.umich.edu
</A>&gt;
</address>
<i>
Wed, 29 Oct 86 08:37:58 PST
</i><PRE>
ReSent-To: RISKS@CSL.SRI.COM

Apparently this was not the main computer system but a (reservations) backup
system. The "stupidity" of this situation is that, according to news
reports, major building damage (currently estimated at greater than $10
million) might have been avoided had their been a sprinkler system. I would
be interested in knowing how it came to be decided to have a "backup system"
located in such a building and if there was additional data security
measures were taken by Air Canada (initial newspaper reports seem to imply
that there were none). Perhaps Risks readers in eastern Canada might be able
to shed more light on this.

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
(Voting) Machine Politics
</A>
</H3>
<address>
Mike McLaughlin
&lt;<A HREF="mailto:mikemcl@nrl-csr ">
mikemcl@nrl-csr 
</A>&gt;
</address>
<i>
Wed, 29 Oct 86 16:11:13 est
</i><PRE>

See DATAMATION, 1 Nov 86, Vol 32 No 21, "Machine Politics" beginning on
page 54.  Good article by John W. Verity.  Quotes Deloris J. Davisson of
Emerald Software &amp; Consulting, Inc., Terre Haute, Ind., and of Ancilla
Domini College.  If anyone knows Ms. Davisson, request she be invited to
contribute to Risks.

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Computer RISKS in "Ticker-Tape Parades"
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Thu 30 Oct 86 03:01:32-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

Mets fans were treated to an interesting new form of computer risk on
Tuesday.  An estimated 2.2 million people turned out for the parade to honor
the Mets, so clearly more paper had to be found to dump on the people in
keeping with New York's tradition of a ticker-tape parade.  The solution was
to use computer printout as well as ticker-tape, including huge volumes of
billing reports, shipping orders, and stock records.  Thus, we ask our New
York RISKers whether they picked up any interesting print-out that might
have been a violation of privacy.  Scavenging dumpsters is an old art, but
having possibly sensitive printouts raining down on you is a new twist.

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
 SDI vs. Social Security
</A>
</H3>
<address>
&lt;<A HREF="mailto:    "guthery%ascvx5.asc@slb-test.CSNET" ">
    "guthery%ascvx5.asc@slb-test.CSNET" 
</A>&gt;
</address>
<i>
Tue, 28 Oct 86 07:37 EDT
</i><PRE>

When I think about the risks of computerization, I'm much more afraid
of the Social Security System than I am of SDI.  We know computers
hitched to things-that-go-boom are dangerous so we watch them carefully 
as we build them and as we use them.  But computers hitched to paper?
Who really cares?  If it issues a check that's too small or a report
that's fallacious, it's the recipient's problem to make it right. Right?

In other words, if the builders and maintainers of the system have vested
interests in the correctness of the system it is more likely to be correct
than if they don't.  Said another way, it is always the "users" who are
ultimately in charge of ... not responsible for, mind you ... debugging 
the program. Things get fun when the only means a "user" has to debug 
the system is a bureaucratic hole to yell into.

But beyond these mild inconveniences to that lowest of all computer life,
there is a more ominous shadow on the horizon.  We are bringing into being
very large systems whose behavior we don't understand yet which are woven 
into the fabric of our daily life.  I don't mean we don't understand the
line that says multiply hours worked by hourly pay.  I mean we aren't in
control of it or its destiny.  We can't describe its global behavior.  We 
change it but we don't know where its evolutionary path is leading.

("Well, son, it started out as a computer program but we just kinda lost
track of it.  Now it's kinda like the law of gravity.  We take it as
given and just try to work with it or work around it.")

What do we know about scaling up and evolving software?  Are there any 
empirical studies of the evolution of large code bodies (5+ million lines, 
10+ years)? Do we know how to engineer global behavior from local function?  
How do we recover functional descriptions and domain-specific knowledge from
large, mature software systems?  

Software productivity always seems to mean bringing more code into being 
quickly.  Yet the problem I fear is that there is too much code of unknown 
quality and function scattered everywhere and then forgotten.

I suggest that we already have many of the problems that the SDI critics
call out ... only in a more innocuous form.  Cancer kills just as surely 
as a bullet but it's a hell of lot harder death.  We all seem to be sitting 
around smoking cigarettes and worrying about being shot.

</PRE>
<A NAME="subj9"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj9.1">
SDI Impossibility?
</A>
</H3>
<address>
Scott Dorsey 
&lt;<A HREF="mailto:kludge%gitpyr%gatech.csnet@CSNET-RELAY.ARPA">
kludge%gitpyr%gatech.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Mon, 27 Oct 86 18:36:49 est
</i><PRE>

&gt;      "In short, the SDI software is not impossible, but ending the
&gt;      fear of nuclear weapons that way is."   [David Parnas]  (<A HREF="/Risks/3.86.html">RISKS-3.86</A>)

    Is such reliable software impossible?  In 1967, a conference on
computer systems in space contained a paper certifying that the software
required for the Apollo missions was so complex and hard to certify that
it would never work.  Maybe at the time it was true.  And it was certainly
true that it did not work the first time.  The point that I am making is
that no one can really forsee how far software engineering technology will
advance in the next few years, and how far simulation technology will
advance.  Is it worth spending money for something that may not work?
In my (* opinion *) it is always worth spending money on pure research,
but my position is a bit biased.

Scott Dorsey
ICS Programming Lab (Where old terminals go to die),  Rich 110,
    Georgia Institute of Technology, Box 36681, Atlanta, Georgia 30332
    ...!{akgua,allegra,amd,hplabs,ihnp4,seismo,ut-ngp}!gatech!gitpyr!kludge

</PRE>
<A NAME="subj10"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj10.1">
Feeping Creaturism
</A>
</H3>
<address>
Charley Wingate 
&lt;<A HREF="mailto:mangoe@mimsy.umd.edu">
mangoe@mimsy.umd.edu
</A>&gt;
</address>
<i>
Tue, 28 Oct 86 22:42:02 EST
</i><PRE>
Organization: University of Maryland, Dept. of Computer Sci.

(Follow-up to Roy Smith)

This gratuitous computerization also has the obvious risk of introducing a
useless level of unreliability in the system without much gain in
performance.  This is especially a problem for consumer products, where the
electronics are in a far from ideal environment, and which are modularized
to the point of guaranteeing a world tantalum shortage in the not-too-distant
future :-).

C. Wingate

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.89.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.91.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-77</DOCNO>
<DOCOLDNO>IA012-000125-B044-11</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/3.02.html 128.240.150.127 19970217005704 text/html 5862
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:55:32 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 3: Issue 2</TITLE>
<LINK REL="Prev" HREF="/Risks/3.01.html">
<LINK REL="Up" HREF="/Risks/index.3.html">
<LINK REL="Next" HREF="/Risks/3.03.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/3.01.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.03.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 3: Issue 2</H1>
<H2> Thursday, 5 June 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Are SDI Software predictions biased by old tactical software? 
</A>
<DD>
<A HREF="#subj1.1">
Herb Lin
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Are SDI Software predictions biased by old tactical software?
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Thu, 5 Jun 1986  01:58 EDT
</i><PRE>

      [Since Herb was evidently up late, since I was up late also, and
       since distribution of this message may stave off many overlapping
       responses to Bob Estell and prompt many rebuttals as well, it seems
       appropriate to distribute this response from Herb Lin as a special
       one-message issue that you can read along with RISKS-3.1.  SDI is 
       probably one of the most significant debate subjects of our lifetimes
       and deserves thorough coverage.  Yes, it does mix politics and
       technology.  It must.  There is simply no other way.  So, don't be
       UP IN ARMS-D.  But let us keep any subsequent discussion cogent and
       sensible.  PGN]

    From: &lt;estell at nwc-143b&gt;

    ...  At some personal risk, let me say at the outset that SDI, 
    as ballyhooed in the popular press, may never work - certainly not in this 
    decade.  

It is NOT the public press that says that SDI will create a perfect
defense.  It is the President and the Secretary of Defense.

    ...Similarly, one errant 
    SDI computer need not fail the entire network - anymore than one failing
    IMP need crash the entire ARPANET.

That point has never been made by the critics either.  The problem is
not that it WOULD, but that some fundamental design error COULD fail
the entire system.  There is no way to rule that out.

    ...  A missile defense is worth having if it is good 
    enough to save only 5% of the USA population in an all-out nuclear attack.

Not necessarily true.  If having a defense that can kill 5% of the
current Soviet threat prompts the Soviets to increase their missile
force by a factor of two, we are not better off, and the missile
defense would not be worth having.

   That shield might save 75% of the population in a terrorist attack, launched
   by an irresponsible source; this is far more likely than a saturation attack
   by a well armed power like the USSR.  

Where will the Libyans get even one ICBM?  Besides, we NOW have the
capability to build defenses against one missile aimed against the US,
and we don't need SDI for that.  We solved that problem in the 1960's.
The hard problem is the saturation attack.

    ... I am saying that if we don't try, we won't progress...

    But my point is that we must not shun the challenge to TRY to improve the
    software in the field, and the tools used to design and build and test it.

But if trying makes war and nuclear buildups more likely, then we may
not progress either.  Actions are taken or not taken in a context;
most responsible critics of SDI argue that there is a downside to
"just doing research".  That downside has to be evaluated.

Specifically, a system that works with questionable reliability or
effectiveness is most useful in the aftermath of a thinned-out
retaliatory blow, i.e., one that most closely resembles your terrorist
attack.  Thus, it is not unreasonable to interpret the building of
defenses as an offensive act.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/3.01.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.3.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/3.03.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-78</DOCNO>
<DOCOLDNO>IA012-000125-B044-52</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/5/index.html 128.240.150.127 19970217005738 text/html 76126
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:55:49 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Index to Volume 5</TITLE>
<LINK REL="Pref" HREF="/Risks/4/index.html">
<LINK REL="Next" HREF="/Risks/6/index.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4/index.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Volume" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/6/index.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Volume" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Index to Volume 5</H1>
<H2> Thursday, 31 December 1987 </H2>
<H3>Forum on Risks to the Public in Computers and Related Systems</H3>
<I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------">
<DL>
<DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.1.html">Volume 5 Issue 1 (6 Jun 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.1.html#subj1">  [There was no RISKS 5.1.  Sorry.]</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.2.html">Volume 5 Issue 2 (12 Jun 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.2.html#subj1">  Three gremlins on the loose: nukes, sharks, enlightened rockets (Dave Platt)</A>
<LI><A HREF="/Risks/5.2.html#subj2">  Yet another air-traffic-controller foul-up (Roy Smith)</A>
<LI><A HREF="/Risks/5.2.html#subj3">  National Crime Information Center access (PGN)</A>
<LI><A HREF="/Risks/5.2.html#subj4">  Yes, Virginia, There Are Software Problems (Nick Condyles)</A>
<LI><A HREF="/Risks/5.2.html#subj5">  Heisenbugs; Also, Risks of Supercomputers (Eugene Miya)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.3.html">Volume 5 Issue 3 (19 Jun 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.3.html#subj1">  Australian ATM troubles... (David Purdue, Dave Horsfall, John Colville)</A>
<LI><A HREF="/Risks/5.3.html#subj2">  Not paying by Access can ruin your credit limit! (Mike Bell)</A>
<LI><A HREF="/Risks/5.3.html#subj3">  Ex-Directory [Arrested by unwristed phone mumbers!] (Brian Randell)</A>
<LI><A HREF="/Risks/5.3.html#subj4">  Risks of Computerized Airport Gate Signs (Chuck Weinstock)</A>
<LI><A HREF="/Risks/5.3.html#subj5">  DMV Computer Changes Names (John Mulhollen)</A>
<LI><A HREF="/Risks/5.3.html#subj6">  UHB demonstrator flight aborted by software error (Kenneth R. Jongsma)</A>
<LI><A HREF="/Risks/5.3.html#subj7">  Aircraft Transponders and Errors in Setting Codes (Joe Morris, Paul Suhler)</A>
<LI><A HREF="/Risks/5.3.html#subj8">  On the bright side, at least my computer still works... (Jon Jacky)</A>
<LI><A HREF="/Risks/5.3.html#subj9">  Human Factors and Risks (Lindsay F. Marshall)</A>
<LI><A HREF="/Risks/5.3.html#subj10">  Re: Risks of so-called ``computer addiction'' (John Mackin)</A>
<LI><A HREF="/Risks/5.3.html#subj11">  Directions and Implications of Advanced Computing (Douglas Schuler)</A>
<LI><A HREF="/Risks/5.3.html#subj12">  Software Risk Management (Dolores Wallace)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.4.html">Volume 5 Issue 4 (24 Jun 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.4.html#subj1">  Immoderation and Nonmoderation (PGN)</A>
<LI><A HREF="/Risks/5.4.html#subj2">  A Passive-Aggressive User Interface -- U.Iowa telephone tidbits (Ray Ford)</A>
<LI><A HREF="/Risks/5.4.html#subj3">  Bogus ROOT domain server on ARPAnet (Paul Richards via Robert Lenoil)</A>
<LI><A HREF="/Risks/5.4.html#subj4">  Printer raises utility false alarm (A. Harry Williams)</A>
<LI><A HREF="/Risks/5.4.html#subj5">  New VAX UNIX file system disk purge runs amok     (Mike Accetta via Chris Koenigsberg)          [SEN 12 3 through RISKS-5.4]
</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.5.html">Volume 5 Issue 5 (26 Jun 87           )</A>
<DD><UL>
<LI><A HREF="/Risks/5.5.html#subj1">  Re: Immoderation and Nonmoderation (Joe Buck, Roy Smith)</A>
<LI><A HREF="/Risks/5.5.html#subj2">  "Computer woes hit air traffic" (Alex Jenkins)</A>
<LI><A HREF="/Risks/5.5.html#subj3">  BBC documentary filming causes Library of Congress computer crashes    (Howard C. Berkowitz via Mark Brader)                              
</A>
<LI><A HREF="/Risks/5.5.html#subj4">  Running out of gas could be hazardous! (Steve McLafferty)</A>
<LI><A HREF="/Risks/5.5.html#subj5">  NASA Safety Reporting System (Eugene Miya)</A>
<LI><A HREF="/Risks/5.5.html#subj6">  EGP madness (David Chase, Dave Mills [2])</A>
<LI><A HREF="/Risks/5.5.html#subj7">  FCC Information Tax -- Risks of Networking (Steve Schultz)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.6.html">Volume 5 Issue 6 (26 Jun 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.6.html#subj1">  Hardware vs Software Battles (Mark Brader, Guest RISKS Editor)</A>
<LI><A HREF="/Risks/5.6.html#subj2">  What the world needs now ... (Jonathan D. Trudel, Rick Lahrson,    WIlliam Swan, Karen M. Davis, Henri J. Socha, Stuart D. Gathman,
    Peter DaSilva, The Sentinel, David Phillip Oster)
</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.7.html">Volume 5 Issue 7 (5 Jul 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.7.html#subj1">  Actual stock price change fails sanity check (Mark Brader)</A>
<LI><A HREF="/Risks/5.7.html#subj2">  PacBell service "glitch" (Walt Thode)</A>
<LI><A HREF="/Risks/5.7.html#subj3">  NASA Safety Reporting System (Jim Olsen)</A>
<LI><A HREF="/Risks/5.7.html#subj4">  "Information Tax" -- Risks of nonsense (Joseph I. Pallas)</A>
<LI><A HREF="/Risks/5.7.html#subj5">  "Computer woes hit air traffic" (Davis)</A>
<LI><A HREF="/Risks/5.7.html#subj6">  Re: Aircraft Transponders and O'Hare AIRMISS</A>
<LI><A HREF="/Risks/5.7.html#subj7">  Phone Company Billing Blunder (Steve Thompson)</A>
<LI><A HREF="/Risks/5.7.html#subj8">  Relaxed DOD Rules? (Dennis Hamilton)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.8.html">Volume 5 Issue 8 (7 Jul 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.8.html#subj1">  Erasing Ford (and other) car computers (Shaun Stine)</A>
<LI><A HREF="/Risks/5.8.html#subj2">  7 Inmates Escape; Computer Blamed! (PGN)</A>
<LI><A HREF="/Risks/5.8.html#subj3">  Hardware failures (Don Chiasson)</A>
<LI><A HREF="/Risks/5.8.html#subj4">  Liability of Expert System Developers (Benjamin I Olasov via Martin Minow)</A>
<LI><A HREF="/Risks/5.8.html#subj5">  PC's and Ad-Hoc Distributed DB's (Amos Shapir)</A>
<LI><A HREF="/Risks/5.8.html#subj6">  Risks of proposed FCC ruling (Keith F. Lynch)</A>
<LI><A HREF="/Risks/5.8.html#subj7">  RISKS in "Balance of Power" (Heikki Pesonen)</A>
<LI><A HREF="/Risks/5.8.html#subj8">  Re: Aviation Safety Reporting System (Doug Pardee)</A>
<LI><A HREF="/Risks/5.8.html#subj9">  A computer RISK in need of a name... (Jerry Leichter)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.9.html">Volume 5 Issue 9 (9 Jul 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.9.html#subj1">  BIG RED, ICEPICK, etc. (David Purdue)</A>
<LI><A HREF="/Risks/5.9.html#subj2">  Air Traffic (out-of?) Control (PGN)</A>
<LI><A HREF="/Risks/5.9.html#subj3">  Cause of the Mysterious Bay Area Rapid Transit Power Outage Identified (PGN)</A>
<LI><A HREF="/Risks/5.9.html#subj4">  Sprint access code penetration (Geof Cooper)</A>
<LI><A HREF="/Risks/5.9.html#subj5">  Eraser's edge (Martin Harriman)</A>
<LI><A HREF="/Risks/5.9.html#subj6">  Hardware/software interaction RISK (Alan Wexelblat)</A>
<LI><A HREF="/Risks/5.9.html#subj7">  How to (or how not to) speed up your computer! (Willie Smith)</A>
<LI><A HREF="/Risks/5.9.html#subj8">  Re: Aviation Safety Reporting System (Jim Olsen, Henry Spencer)</A>
<LI><A HREF="/Risks/5.9.html#subj9">  Re: RISKS in "Balance of Power" (Eugene Miya, Hugh Pritchard)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.10.html">Volume 5 Issue 10 (9 Jul 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.10.html#subj1">  Firebird computer story (Paul Kalapathy)</A>
<LI><A HREF="/Risks/5.10.html#subj2">  COMPUTER CLUBS FOOT (Anthony A. Datri)</A>
<LI><A HREF="/Risks/5.10.html#subj3">  Re: 7 Inmates Escape; Computer Blamed! (James Lujan)</A>
<LI><A HREF="/Risks/5.10.html#subj4">  Sprint access code penetration (catching the baddie) (Darrell Long)</A>
<LI><A HREF="/Risks/5.10.html#subj5">  US Sprint and free long distance (Eric N Starkman, Edward J Cetron)</A>
<LI><A HREF="/Risks/5.10.html#subj6">  RE: BIG RED (Eugene Miya)</A>
<LI><A HREF="/Risks/5.10.html#subj7">  Risks of battery disconnections (Steve Mahan)</A>
<LI><A HREF="/Risks/5.10.html#subj8">  Japanese simulation design (Sean Malloy)</A>
<LI><A HREF="/Risks/5.10.html#subj9">  Hardware failures and proofs of correctness (Rob Aitken, Michael K. Smith)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.11.html">Volume 5 Issue 11 (12 Jul 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.11.html#subj1">  Old News from New Olds: Check that Backup! (Fleischmann)</A>
<LI><A HREF="/Risks/5.11.html#subj2">  Auto Computers (Tony Siegman)</A>
<LI><A HREF="/Risks/5.11.html#subj3">  Re: Liability of Expert Systems Developers (George Cross)</A>
<LI><A HREF="/Risks/5.11.html#subj4">  Re: Hardware failures (Sam Crowley)</A>
<LI><A HREF="/Risks/5.11.html#subj5">  Hardware/software interaction RISK (Robert Weiss)</A>
<LI><A HREF="/Risks/5.11.html#subj6">  More on Risks in "Balance of Power" (Heikki Pesonen)</A>
<LI><A HREF="/Risks/5.11.html#subj7">  Re: Sprint access code penetration (John Gilmore)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.12.html">Volume 5 Issue 12 (16 Jul 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.12.html#subj1">  Another computer-related prison escape (Andrew Klossner)</A>
<LI><A HREF="/Risks/5.12.html#subj2">  New York Public Library computer loses thousands of book references (PGN)</A>
<LI><A HREF="/Risks/5.12.html#subj3">  Risks of being a hacker (PGN)</A>
<LI><A HREF="/Risks/5.12.html#subj4">  Re: Old News from New Olds: Check that Backup! (Henry Spencer)</A>
<LI><A HREF="/Risks/5.12.html#subj5">  Tax fraud by tax collectors (Jerry Harper)</A>
<LI><A HREF="/Risks/5.12.html#subj6">  Re: Hardware faults and complete testing (Richard S. D'Ippolito)</A>
<LI><A HREF="/Risks/5.12.html#subj7">  Re: Sprint Access Penetration (Dan Graifer)</A>
<LI><A HREF="/Risks/5.12.html#subj8">  Phone access charges (Leff)</A>
<LI><A HREF="/Risks/5.12.html#subj9">  Risks in Fiction [Book Report] (Martin Minow)</A>
<LI><A HREF="/Risks/5.12.html#subj10">  The Other Perspective? (Baldwin)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.13.html">Volume 5 Issue 13 (20 Jul 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.13.html#subj1">  Re: Another computer-related prison escape (Alan J Rosenthal)</A>
<LI><A HREF="/Risks/5.13.html#subj2">  Credit card risks (David 'Witt' Wittenberg)</A>
<LI><A HREF="/Risks/5.13.html#subj3">  The latest in Do-It-Yourself manuals (Andrew Scott Beals)</A>
<LI><A HREF="/Risks/5.13.html#subj4">  Re: Robocop review (Eugene Miya)</A>
<LI><A HREF="/Risks/5.13.html#subj5">  Robocop and following instructions (Brian Gordon)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.14.html">Volume 5 Issue 14 (22 Jul 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.14.html#subj1">  FAA absolves Delta in 2 close calls, ATC problems blamed in one (PGN)</A>
<LI><A HREF="/Risks/5.14.html#subj2">  Origin of term "intelligent machine" (Jon Jacky)</A>
<LI><A HREF="/Risks/5.14.html#subj3">  robocop (Lou Steinberg)</A>
<LI><A HREF="/Risks/5.14.html#subj4">  Nuclear power plants (Alex Bangs, Nancy Leveson)</A>
<LI><A HREF="/Risks/5.14.html#subj5">  Reminder about alarms (Eugene Miya)</A>
<LI><A HREF="/Risks/5.14.html#subj6">  FCC computer fees (Alex Bangs)</A>
<LI><A HREF="/Risks/5.14.html#subj7">  Risks of exporting technology (Clint Wong)</A>
<LI><A HREF="/Risks/5.14.html#subj8">  Electronic Cash Registers (William Daul)</A>
<LI><A HREF="/Risks/5.14.html#subj9">  Brief book review of the Hacker's Handbook (John Gilmore)</A>
<LI><A HREF="/Risks/5.14.html#subj10">  Re: Credit card risks (Amos Shapir)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.15.html">Volume 5 Issue 15 (23 Jul 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.15.html#subj1">  Access by 'hackers' to computer not criminal (Robert Stroud)</A>
<LI><A HREF="/Risks/5.15.html#subj2">  On expecting the unexpected in nuclear power plants (David Chase)</A>
<LI><A HREF="/Risks/5.15.html#subj3">  Risks of Nuclear Power (Mark S. Day) </A>
<LI><A HREF="/Risks/5.15.html#subj4">  Chernobyl predecessors? (Henry Spencer)</A>
<LI><A HREF="/Risks/5.15.html#subj5">  Who's responsible - ATC or pilots (Andy Freeman)</A>
<LI><A HREF="/Risks/5.15.html#subj6">  "Intelligent" control (Alex Bangs)</A>
<LI><A HREF="/Risks/5.15.html#subj7">  Taxes and who pays them (William L. Rupp)</A>
<LI><A HREF="/Risks/5.15.html#subj8">  Computer Know Thine Enemy; Reactor control-room design (Eugene Miya)</A>
<LI><A HREF="/Risks/5.15.html#subj9">  Medical computer risks? (Prentiss Riddle)</A>
<LI><A HREF="/Risks/5.15.html#subj10">  Electronic cash registers (Michael Scott)</A>
<LI><A HREF="/Risks/5.15.html#subj11">  Re: Credit card risks  (Michael Wagner)</A>
<LI><A HREF="/Risks/5.15.html#subj12">  Re: "The Other Perspective?" (Baldwin)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.16.html">Volume 5 Issue 16 (25 Jul 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.16.html#subj1">  $23 million computer banking snafu (Rodney Hoffman)</A>
<LI><A HREF="/Risks/5.16.html#subj2">  Computer crime, etc. (Matthew Kruk, PGN)</A>
<LI><A HREF="/Risks/5.16.html#subj3">  Reactor control-room design and public awareness (Robert Cohen)</A>
<LI><A HREF="/Risks/5.16.html#subj4">  Computerized Tollbooths Debut in PA (Chris Koenigsberg)</A>
<LI><A HREF="/Risks/5.16.html#subj5">  Re: ATC Responsibilities (Alan M. Marcum)</A>
<LI><A HREF="/Risks/5.16.html#subj6">  Air traffic control and collision avoidance (Willis Ware)</A>
<LI><A HREF="/Risks/5.16.html#subj7">  Risks of computerizing data bases (Tom Benson)</A>
<LI><A HREF="/Risks/5.16.html#subj8">  Re: electronic cash registers and wrong prices     (Brent, Brian R. Lair, Will Martin, Mark Fulk)
</A>
<LI><A HREF="/Risks/5.16.html#subj9">  Taxes and who pays them (Rick Busdiecker, Andrew Klossner)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.17.html">Volume 5 Issue 17 (26 Jul 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.17.html#subj1">  Re: Separation of Duties and Computer Security (Ted Lee)</A>
<LI><A HREF="/Risks/5.17.html#subj2">  Re: Robocop (Zalman Stern)</A>
<LI><A HREF="/Risks/5.17.html#subj3">  Re: B of A's computer problems (Bob Larson)</A>
<LI><A HREF="/Risks/5.17.html#subj4">  Nuclear power plant monitoring and engineering (Leff)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.18.html">Volume 5 Issue 18 (27 Jul 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.18.html#subj1">  Its Barcode is NOT worse than its Byte; Rooting for AT&amp;T PC truffles     (Elizabeth Zwicky)
</A>
<LI><A HREF="/Risks/5.18.html#subj2">  Too much security? (Richard Schooler)</A>
<LI><A HREF="/Risks/5.18.html#subj3">  "Hacker Program" -- PC Prankster (Sam Rebelsky)</A>
<LI><A HREF="/Risks/5.18.html#subj4">  Pittsburgh credit card hackers (Chris Koenigsberg)</A>
<LI><A HREF="/Risks/5.18.html#subj5">  Hacking and Criminal Offenses (David Sherman)</A>
<LI><A HREF="/Risks/5.18.html#subj6">  911 Surprises (Paul Fuqua)</A>
<LI><A HREF="/Risks/5.18.html#subj7">  Re: Taxes and who pays them (Craig E W)</A>
<LI><A HREF="/Risks/5.18.html#subj8">  Statistics as a Fancy Name for Ignorance (Mark S. Day)</A>
<LI><A HREF="/Risks/5.18.html#subj9">  Supermarkets (Chris Koenigsberg, Jon Mauney)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.19.html">Volume 5 Issue 19 (29 Jul 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.19.html#subj1">  Automating Air Travel (Dan Graifer)</A>
<LI><A HREF="/Risks/5.19.html#subj2">  Responsibilities of the pilots and the traffic controllers (Nathan Meyers)</A>
<LI><A HREF="/Risks/5.19.html#subj3">  Flippin' statistics (Joe Morris)</A>
<LI><A HREF="/Risks/5.19.html#subj4">  Nuclear power safety and intelligent control (Rich Kulawiec)</A>
<LI><A HREF="/Risks/5.19.html#subj5">  Single-pipe failures (Kenneth Ng)</A>
<LI><A HREF="/Risks/5.19.html#subj6">  Hacking and Criminal Offenses (SEG)</A>
<LI><A HREF="/Risks/5.19.html#subj7">  Passwords and telephone numbers (Jonathan Thornburg)</A>
<LI><A HREF="/Risks/5.19.html#subj8">  Separation of duties and "2-man control" (Patrick D. Farrell)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.20.html">Volume 5 Issue 20 (30 Jul 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.20.html#subj1">  Lack of sanity at the IRS (Victor S. Miller)</A>
<LI><A HREF="/Risks/5.20.html#subj2">  Hot Stuff (Burch Seymour)</A>
<LI><A HREF="/Risks/5.20.html#subj3">  Re: Nuclear power plant monitoring and engineering (Brian Douglass)</A>
<LI><A HREF="/Risks/5.20.html#subj4">  Re: Credit card risks (Ross Patterson)</A>
<LI><A HREF="/Risks/5.20.html#subj5">  Re: Passwords and telephone numbers (Brian Randell, Keith F. Lynch)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.21.html">Volume 5 Issue 21 (1 Aug 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.21.html#subj1">  Macaquepit Monkey Business on 747 (PGN)</A>
<LI><A HREF="/Risks/5.21.html#subj2">  Re: IRS Sanity Checks (Willis Ware, Joseph Beckman)</A>
<LI><A HREF="/Risks/5.21.html#subj3">  Re: Telephone access cards (Willis Ware, Robert Hartman)</A>
<LI><A HREF="/Risks/5.21.html#subj4">  Re: Origin of term "artificial intelligence" (Dave Benson)</A>
<LI><A HREF="/Risks/5.21.html#subj5">  FDA opportunity for system safety person (Frank Houston)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.22.html">Volume 5 Issue 22 (3 Aug 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.22.html#subj1">  Home of IBM computers succumbs to telephone computer up-down-upgrade (PGN)</A>
<LI><A HREF="/Risks/5.22.html#subj2">  Re: IRS Sanity Checks (Jerome H. Saltzer)</A>
<LI><A HREF="/Risks/5.22.html#subj3">  Re: Monkey business (clarification) (PGN)</A>
<LI><A HREF="/Risks/5.22.html#subj4">  Computer (claustro)phobia (Kent Paul Dolan)</A>
<LI><A HREF="/Risks/5.22.html#subj5">  Security-induced RISK (Alan Wexelblat)</A>
<LI><A HREF="/Risks/5.22.html#subj6">  Another ATM story (Jeffrey Mogul)</A>
<LI><A HREF="/Risks/5.22.html#subj7">  SDI is feasible (Walt Thode)</A>
<LI><A HREF="/Risks/5.22.html#subj8">  Publicized Risks (Henry Spencer)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.23.html">Volume 5 Issue 23 (4 Aug 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.23.html#subj1">  Article on "Computer (In)security" (Jim Horning)</A>
<LI><A HREF="/Risks/5.23.html#subj2">  DC sends bad tax bill to the *WRONG* citizen (Joe Morris)</A>
<LI><A HREF="/Risks/5.23.html#subj3">  New Report on SDI Feasibility (Mark S. Day)</A>
<LI><A HREF="/Risks/5.23.html#subj4">  Railway automation (Stephen Colwill)</A>
<LI><A HREF="/Risks/5.23.html#subj5">  Faults in 911 system caused by software bug? (Jim Purtilo)</A>
<LI><A HREF="/Risks/5.23.html#subj6">  Re: Macaqueswain steering (PGN)</A>
<LI><A HREF="/Risks/5.23.html#subj7">  PIN-demonium (Curtis C. Galloway)</A>
<LI><A HREF="/Risks/5.23.html#subj8">  Factory automation and risks to jobs (James H. Coombs)</A>
<LI><A HREF="/Risks/5.23.html#subj9">  Nukes vs Coal (Tom Athanasiou) [and why is this message in RISKS?  PGN]</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.24.html">Volume 5 Issue 24 (6 Aug 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.24.html#subj1">  Another animal story (Bill Pase)</A>
<LI><A HREF="/Risks/5.24.html#subj2">  Re: Security-induced RISK (Henry Spencer)</A>
<LI><A HREF="/Risks/5.24.html#subj3">  Re: Factory automation and risks to jobs -- "apparently" not (Randall Davis)</A>
<LI><A HREF="/Risks/5.24.html#subj4">  Railway automation (Scott E. Preece)</A>
<LI><A HREF="/Risks/5.24.html#subj5">  Nuclear generated electrical power and RISKS (Dave Benson)</A>
<LI><A HREF="/Risks/5.24.html#subj6">  PIN money? (BJORNDKG)</A>
<LI><A HREF="/Risks/5.24.html#subj7">  Re: Another ATM story (Scott Nelson)</A>
<LI><A HREF="/Risks/5.24.html#subj8">  Computer `assumes' the worst in billing for hotel phone calls (Bruce Forstall)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.25.html">Volume 5 Issue 25 (9 Aug 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.25.html#subj1">  Computer Error Opened Flood Gates of Alta Dam  (Haavard Hegna)</A>
<LI><A HREF="/Risks/5.25.html#subj2">  Heating up planning discussions ... (Robert Slade)</A>
<LI><A HREF="/Risks/5.25.html#subj3">  Re: Faults in 911 system caused by software bug? (Paul Garnet)</A>
<LI><A HREF="/Risks/5.25.html#subj4">  "It must work, the contract says so" (Henry Spencer)</A>
<LI><A HREF="/Risks/5.25.html#subj5">  Separation of Duty and Computer Systems (Howard Israel)</A>
<LI><A HREF="/Risks/5.25.html#subj6">  Optical Disks Raising Old Legal Issue (Leff)</A>
<LI><A HREF="/Risks/5.25.html#subj7">  AAAS Colloquium Notice (Stan Rifkin)</A>
<LI><A HREF="/Risks/5.25.html#subj8">  Secrecy About Risks of Secrecy Vulnerabilities and Attacks? (Peter J. Denning)</A>
<LI><A HREF="/Risks/5.25.html#subj9">  Another electronic mail risk (Doug Mosher)</A>
<LI><A HREF="/Risks/5.25.html#subj10">  Risks TO computer users (US Sprint) (James H. Coombs)</A>
<LI><A HREF="/Risks/5.25.html#subj11">  Computer Safety and System Safety (Al Watters)</A>
<LI><A HREF="/Risks/5.25.html#subj12">  Computers in nuclear power plants (Frederick Wamsley)</A>
<LI><A HREF="/Risks/5.25.html#subj13">  Autoteller problems (Alex Colvin)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.26.html">Volume 5 Issue 26 (11 Aug 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.26.html#subj1">  Secrecy About Risks of Secrecy (Jerome H. Saltzer, Maj. Doug Hardie)</A>
<LI><A HREF="/Risks/5.26.html#subj2">  Separation of Duty and Computer Systems (Willis Ware)</A>
<LI><A HREF="/Risks/5.26.html#subj3">  NASA Computers Not All Wet (Mike McLaughlin)</A>
<LI><A HREF="/Risks/5.26.html#subj4">  Computer Error Opened Flood Gates of Alta Dam (Henry Spencer, Amos Shapir)</A>
<LI><A HREF="/Risks/5.26.html#subj5">  Re: Another electronic mail risk (Prentiss Riddle)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.27.html">Volume 5 Issue 27 (11 Aug 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.27.html#subj1">  Re: Secrecy About Risks of Secrecy (Jerome H. Saltzer)</A>
<LI><A HREF="/Risks/5.27.html#subj2">  "Mustn't tire the computer!" (A. N. Walker)</A>
<LI><A HREF="/Risks/5.27.html#subj3">  Automated environmental control RISKS (Joe Morris)</A>
<LI><A HREF="/Risks/5.27.html#subj4">  Social Security Inside Scoop (Lance Keigwin via Martin Minow)</A>
<LI><A HREF="/Risks/5.27.html#subj5">  Fire protection in the computer room (Dave Curry)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.28.html">Volume 5 Issue 28 (12 Aug 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.28.html#subj1">  Certification of software engineers (Nancy Leveson)</A>
<LI><A HREF="/Risks/5.28.html#subj2">  Re: Secrecy About Risks of Secrecy     (Maj. Doug Hardie, Russell Williams, Jeff Putnam)
</A>
<LI><A HREF="/Risks/5.28.html#subj3">  Eliminating the Need for Passwords (Lee Hasiuk)</A>
<LI><A HREF="/Risks/5.28.html#subj4">  Re: Risks of automating production (Richard A. Cowan, James H. Coombs)</A>
<LI><A HREF="/Risks/5.28.html#subj5">  'Mustn't tire the computer!' (Scott E. Preece, Rick Kuhn)</A>
<LI><A HREF="/Risks/5.28.html#subj6">  Re: NASA wet computers (Eugene Miya)</A>
<LI><A HREF="/Risks/5.28.html#subj7">  Halon (Dave Platt, Steve Conklin, Jack Ostroff, LT Scott Norton, Scott Preece)</A>
<LI><A HREF="/Risks/5.28.html#subj8">  Railway automation (Stephen Colwill)</A>
<LI><A HREF="/Risks/5.28.html#subj9">  Employment opportunities at MITRE (Marshall D. Abrams)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.29.html">Volume 5 Issue 29 (15 Aug 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.29.html#subj1">  RISKS submissions (PGN)</A>
<LI><A HREF="/Risks/5.29.html#subj2">  Lack of user training = legal liability? --    Computer SNAFU Ruled a Rights Violation (Rodney Hoffman)
</A>
<LI><A HREF="/Risks/5.29.html#subj3">  London Docklands Light Railway (Mark Brader)</A>
<LI><A HREF="/Risks/5.29.html#subj4">  Software and system safety (Nancy Leveson)</A>
<LI><A HREF="/Risks/5.29.html#subj5">  New safety MIL-STD (Nancy Leveson)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.30.html">Volume 5 Issue 30 (19 Aug 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.30.html#subj1">  Role of NISAC in Reporting Vulnerabilities (Bruce N. Baker)</A>
<LI><A HREF="/Risks/5.30.html#subj2">  Indemnification of ATC manufacturers (Bill Buckley)</A>
<LI><A HREF="/Risks/5.30.html#subj3">  Bank Computers and flagging (Joseph I. Herman)</A>
<LI><A HREF="/Risks/5.30.html#subj4">  Re: Certifying Software Engineers (Mark Weiser, Nancy Leveson)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.31.html">Volume 5 Issue 31 (21 Aug 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.31.html#subj1">  "Computer Failed to Warn Jet Crew" (PGN)</A>
<LI><A HREF="/Risks/5.31.html#subj2">  Risks to Privacy (Jerome H. Saltzer)</A>
<LI><A HREF="/Risks/5.31.html#subj3">  ATM features (Jack Holleran)</A>
<LI><A HREF="/Risks/5.31.html#subj4">  Licensing software engineers (Frank Houston, Dave Benson)</A>
<LI><A HREF="/Risks/5.31.html#subj5">  Re: Risks of automating production (Henry Spencer)</A>
<LI><A HREF="/Risks/5.31.html#subj6">  Re: Automated environment control (Robert Stanley, Brian Douglass)</A>
<LI><A HREF="/Risks/5.31.html#subj7">  Trusting Computers (Marcus Hall)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.32.html">Volume 5 Issue 32 (4 Sep 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.32.html#subj1">  Honda eschews computers for new 4-wheel steering system (Roy Smith)</A>
<LI><A HREF="/Risks/5.32.html#subj2">  Another Trojan Horse? (Brian Tompsett)</A>
<LI><A HREF="/Risks/5.32.html#subj3">  Transatlantic Flights at Risk from Computer (Daniel Karrenberg)</A>
<LI><A HREF="/Risks/5.32.html#subj4">  Re: "Computer Failed to Warn Jet Crew" (Mark Ethan Smith)</A>
<LI><A HREF="/Risks/5.32.html#subj5">  Delta-Continental Near-Miss</A>
<LI><A HREF="/Risks/5.32.html#subj6">  Decomposing Software (Charles Gard)</A>
<LI><A HREF="/Risks/5.32.html#subj7">  Why the Phalanx Didn't Fire (IEEE Spectrum Reference) (Eugene Miya)</A>
<LI><A HREF="/Risks/5.32.html#subj8">  Cheap modems and other delights (Steve Leon via bobmon)</A>
<LI><A HREF="/Risks/5.32.html#subj9">  Reach out, touch someone (Michael Sclafani)</A>
<LI><A HREF="/Risks/5.32.html#subj10">  SDI event (Gary Chapman)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.33.html">Volume 5 Issue 33 (4 Sep 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.33.html#subj1">  How to Beat the Spanish telephone system (Lindsay F. Marshall)</A>
<LI><A HREF="/Risks/5.33.html#subj2">  Re: Automated control stability and sabotage (Amos Shapir)</A>
<LI><A HREF="/Risks/5.33.html#subj3">  Crisis in the Service Bay (Mark Brader)</A>
<LI><A HREF="/Risks/5.33.html#subj4">  Who is responsible for safety? (Nancy Leveson)</A>
<LI><A HREF="/Risks/5.33.html#subj5">  Certification of Software Engineers     (Brian Tompsett, Richard Neitzel, Wilson H. Bent)
</A>
<LI><A HREF="/Risks/5.33.html#subj6">  Irish Tax Swindle (John Murray)</A>
<LI><A HREF="/Risks/5.33.html#subj7">  Pogo Wins a Free Lunch -- Costs and Liability in Good Systems (Hal Guthery)</A>
<LI><A HREF="/Risks/5.33.html#subj8">  Re: Bank Computers and flagging (Bill Fisher)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.34.html">Volume 5 Issue 34 (7 Sep 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.34.html#subj1">  Dutch Police Hampered By Faulty Computer System (Patrick van Kleef)</A>
<LI><A HREF="/Risks/5.34.html#subj2">  Computer Psychosis (Bill McGarry)</A>
<LI><A HREF="/Risks/5.34.html#subj3">  Risks and people (Alan Wexelblat)</A>
<LI><A HREF="/Risks/5.34.html#subj4">  The influence of RISKS on car design? (Danny Cohen)</A>
<LI><A HREF="/Risks/5.34.html#subj5">  Reach out, touch someone (Scott E. Preece)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.35.html">Volume 5 Issue 35 (10 Sep 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.35.html#subj1">  Drugs, DES, and the criminal world (Jerry Leichter)</A>
<LI><A HREF="/Risks/5.35.html#subj2">  More on the Irish Tax Swindle (Jerry Harper)</A>
<LI><A HREF="/Risks/5.35.html#subj3">  Costs and Liability in Good Systems (David Collier-Brown)</A>
<LI><A HREF="/Risks/5.35.html#subj4">  Re: The influence of RISKS on car design? (Benjamin Thompson)</A>
<LI><A HREF="/Risks/5.35.html#subj5">  Re: Computer Syndrome; Dutch Crime Computer (Brian Douglass)</A>
<LI><A HREF="/Risks/5.35.html#subj6">  Reach out, touch someone (Brad Miller, Richard Kovalcik, Jr., Curtis Abbott)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.36.html">Volume 5 Issue 36 (13 Sep 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.36.html#subj1">  Australian Bank Bungles Foreign Exchange Deal (Ken Ross)</A>
<LI><A HREF="/Risks/5.36.html#subj2">  Computer misses the bus (Doug Barry)</A>
<LI><A HREF="/Risks/5.36.html#subj3">  Quite a dish subverts Playboy channel (PGN)</A>
<LI><A HREF="/Risks/5.36.html#subj4">  "Software Glitch Shuts Down Phones in Minneapolis" (Alan)</A>
<LI><A HREF="/Risks/5.36.html#subj5">  Computer Syndrome (Mark Jackson, Simson L. Garfinkel)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.37.html">Volume 5 Issue 37 (18 Sep 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.37.html#subj1">  Another prison inmate spoofs computer, this one gains freedom (Bill Weisman)</A>
<LI><A HREF="/Risks/5.37.html#subj2">  detroit flaps flap (Barry Nelson)</A>
<LI><A HREF="/Risks/5.37.html#subj3">  AT&amp;T Computers (PGN)</A>
<LI><A HREF="/Risks/5.37.html#subj4">  Hackers enter nasa computers (Mike Linnig)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.38.html">Volume 5 Issue 38 (24 Sep 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.38.html#subj1">  Computer crash causes ATC delay (Dave Horsfall)</A>
<LI><A HREF="/Risks/5.38.html#subj2">  Risks TO Computers: Man Shoots Computer! (Martin Minow)</A>
<LI><A HREF="/Risks/5.38.html#subj3">  An Aporkriffle Tail? (Zeke via Martin Minow) (also noted by others)</A>
<LI><A HREF="/Risks/5.38.html#subj4">  The naming of names  (Dave Horsfall)</A>
<LI><A HREF="/Risks/5.38.html#subj5">  Aliases, SINs and Taxes (Robert Aitken)</A>
<LI><A HREF="/Risks/5.38.html#subj6">  Risks in the Misuse of Databases (Cliff Jones)</A>
<LI><A HREF="/Risks/5.38.html#subj7">  Sprint Sues Hackers (Dan Epstein)</A>
<LI><A HREF="/Risks/5.38.html#subj8">  Re: Reach out, touch someone (Bob English)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.39.html">Volume 5 Issue 39 (26 Sep 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.39.html#subj1">  Another Australian ATM Card Snatch (Dave Horsfall)</A>
<LI><A HREF="/Risks/5.39.html#subj2">  AT&amp;T Computers Penetrated (Joe Morris)   </A>
<LI><A HREF="/Risks/5.39.html#subj3">  On-line Robotic Repair of Software (Maj. Doug Hardie)</A>
<LI><A HREF="/Risks/5.39.html#subj4">  Re: An Aporkriffle Tail (Michael Wagner)</A>
<LI><A HREF="/Risks/5.39.html#subj5">  Risks in the Misuse of Databases? (Brint Cooper)</A>
<LI><A HREF="/Risks/5.39.html#subj6">  SDI Simulation (Steve Schlesinger)</A>
<LI><A HREF="/Risks/5.39.html#subj7">  Ethical dilemmas and all that... (Herb Lin)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.40.html">Volume 5 Issue 40 (28 Sep 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.40.html#subj1">  Yet another "hackers break MILNET" story (Jon Jacky)</A>
<LI><A HREF="/Risks/5.40.html#subj2">  Military role for software sabotage cited ... (Jon Jacky)</A>
<LI><A HREF="/Risks/5.40.html#subj3">  $80,000 bank computing error reported in 'Ann Landers' (Jon Jacky)</A>
<LI><A HREF="/Risks/5.40.html#subj4">  Add Vice to the Loveworn (Scot Wilcoxon)</A>
<LI><A HREF="/Risks/5.40.html#subj5">  Concorde tires burst: RISKS without the automatic system (Henry Spencer)</A>
<LI><A HREF="/Risks/5.40.html#subj6">  Risks of hot computers (Mark Brader)</A>
<LI><A HREF="/Risks/5.40.html#subj7">  Re: Risks in the Misuse of Databases? (Ross Patterson)</A>
<LI><A HREF="/Risks/5.40.html#subj8">  [SDI] Simulation (Jerry Freedman,Jr)</A>
<LI><A HREF="/Risks/5.40.html#subj9">  Re: An Aporkriffle Tail (William R. Somsky)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.41.html">Volume 5 Issue 41 (30 Sep 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.41.html#subj1">  CHANGE IN RISKS SITE Effective Immediately (PGN)</A>
<LI><A HREF="/Risks/5.41.html#subj2">  Life-critical use of a spelling corrector (Dave Horsfall)</A>
<LI><A HREF="/Risks/5.41.html#subj3">  AT&amp;T Computers Penetrated (Richard S D'Ippolito)</A>
<LI><A HREF="/Risks/5.41.html#subj4">  Satellites and Hackers (Paul Garnet)</A>
<LI><A HREF="/Risks/5.41.html#subj5">  Re: Risks in the Misuse of Databases?     (P. T. Withington, Scott E. Preece, J M Hicks)
</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.42.html">Volume 5 Issue 42 (5 Oct 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.42.html#subj1">  Credit Markets: computer interest is high! (Jerome H. Saltzer)</A>
<LI><A HREF="/Risks/5.42.html#subj2">  Telephone computers that work (Alan Wexelblat)</A>
<LI><A HREF="/Risks/5.42.html#subj3">  Computer Services as Property (Isaac K. Rabinovitch, Arthur Axelrod)</A>
<LI><A HREF="/Risks/5.42.html#subj4">  JOINing on public access data -- and insider trading (Brent Laminack)</A>
<LI><A HREF="/Risks/5.42.html#subj5">  TV Detectors (Lindsay F. Marshall, Ian G. Batten, David A Honig)</A>
<LI><A HREF="/Risks/5.42.html#subj6">  Confusing Input Request in Automatic Voting Systems (Eke van Batenburg)</A>
<LI><A HREF="/Risks/5.42.html#subj7">  Directions and Implications of Advanced Computing -- Call for Papers     (Douglas Schuler)
</A>
<LI><A HREF="/Risks/5.42.html#subj8">  Risks of receiving RISKS -- BITNET users BEWARE (jfp)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.43.html">Volume 5 Issue 43 (13 Oct 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.43.html#subj1">  IRS Accidentally Imposes $338.85 Lien On Reagans (Chris Koenigsberg)</A>
<LI><A HREF="/Risks/5.43.html#subj2">  Another ARPANET-collapse-like accidental virus effect (Jeffrey R Kell)</A>
<LI><A HREF="/Risks/5.43.html#subj3">  Computers and civil disobedience (Prentiss Riddle)</A>
<LI><A HREF="/Risks/5.43.html#subj4">  YAPB (yet another password bug) (Geof Cooper)</A>
<LI><A HREF="/Risks/5.43.html#subj5">  News Media about hackers and other comments (Jack Holleran)</A>
<LI><A HREF="/Risks/5.43.html#subj6">  Personalized Technology Side-effects (Scot Wilcoxon)</A>
<LI><A HREF="/Risks/5.43.html#subj7">  Anonymity and high-tech (Nic McPhee)</A>
<LI><A HREF="/Risks/5.43.html#subj8">  Naval Contemplation [Humor] (Don Chiasson)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.44.html">Volume 5 Issue 44 (15 Oct 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.44.html#subj1">  Costly computer risks (Gary A. Kremen)</A>
<LI><A HREF="/Risks/5.44.html#subj2">  Re: News Media about hackers and other comments (Amos Shapir)</A>
<LI><A HREF="/Risks/5.44.html#subj3">  Mailing Lists (Lindsay F. Marshall)</A>
<LI><A HREF="/Risks/5.44.html#subj4">  Discrimination considered pejorative (Geraint Jones) </A>
<LI><A HREF="/Risks/5.44.html#subj5">  Re: Anonymity and high-tech (Brint Cooper) </A>
<LI><A HREF="/Risks/5.44.html#subj6">  Pacemakers (Hal Schloss)</A>
<LI><A HREF="/Risks/5.44.html#subj7">  News Media about hackers and other comments (Bob English)</A>
<LI><A HREF="/Risks/5.44.html#subj8">  Password bug - It's everywhere. (Mike Russell)</A>
<LI><A HREF="/Risks/5.44.html#subj9">  Re: YAPB (yet another password bug) (Brint Cooper)</A>
<LI><A HREF="/Risks/5.44.html#subj10">  Civil Disobedience (Scott Dorsey, Bill Fisher, Eugene Miya)</A>
<LI><A HREF="/Risks/5.44.html#subj11">  Phalanx Revisited (Risks to Carrier Aircraft) (Marco Barbarisi)</A>
<LI><A HREF="/Risks/5.44.html#subj12">  SSNs (Bill Gunshannon)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.45.html">Volume 5 Issue 45 (19 Oct 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.45.html#subj1">  Stocks into Bondage?  Storm prediction?  Computer relevance? (PGN)</A>
<LI><A HREF="/Risks/5.45.html#subj2">  UNIX Passwords (Dave Curry)</A>
<LI><A HREF="/Risks/5.45.html#subj3">  Let the Punishment Fit the Crime... (Mike McLaughlin)</A>
<LI><A HREF="/Risks/5.45.html#subj4">  Re: Computers and civil disobedience     (James Peterson, Clif Flynt, Fulk, Brent Chapman)
</A>
<LI><A HREF="/Risks/5.45.html#subj5">  Unemployment Insurance Cheaters (William Smith)</A>
<LI><A HREF="/Risks/5.45.html#subj6">  Computer Services as Property (Doug Landauer)</A>
<LI><A HREF="/Risks/5.45.html#subj7">  Successor to Sun Spots (K. Richard Magill)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.46.html">Volume 5 Issue 46 (21 Oct 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.46.html#subj1">  Portfolio Insurance and Wall Street's meltdown (Rodney Hoffman)</A>
<LI><A HREF="/Risks/5.46.html#subj2">  Software firms put on guard by Act (Jonathan Bowen)</A>
<LI><A HREF="/Risks/5.46.html#subj3">  World Series Phone Snafu (Ted Lee)</A>
<LI><A HREF="/Risks/5.46.html#subj4">  Re: Civil Disobedience (Jim Jenal)</A>
<LI><A HREF="/Risks/5.46.html#subj5">  Destruction of confiscated computers (Lindsay F. Marshall)</A>
<LI><A HREF="/Risks/5.46.html#subj6">  Weather Forecasts (Lindsay F. Marshall)</A>
<LI><A HREF="/Risks/5.46.html#subj7">  Anonymity and high-tech: indirection (Robert Stanley)</A>
<LI><A HREF="/Risks/5.46.html#subj8">  Berkeley's computer security (Al Stangenberger, David Redell)</A>
<LI><A HREF="/Risks/5.46.html#subj9">  Computer Services as Property (Rick Busdiecker)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.47.html">Volume 5 Issue 47 (22 Oct 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.47.html#subj1">  Programmed Trading and the Stock Market Decline (Lt Scott A. Norton)</A>
<LI><A HREF="/Risks/5.47.html#subj2">  Overload closes Pacific Stock Exchange computers, and other sagas (PGN)</A>
<LI><A HREF="/Risks/5.47.html#subj3">  BankAmerica Aides Quit; Sources Cite Data System (Jerome H. Saltzer)</A>
<LI><A HREF="/Risks/5.47.html#subj4">  Air Force explores SDI-like technology (Walt Thode)</A>
<LI><A HREF="/Risks/5.47.html#subj5">  Who knows where the computer is? (Graeme Hirst)</A>
<LI><A HREF="/Risks/5.47.html#subj6">  Anonymity (Fred Baube)</A>
<LI><A HREF="/Risks/5.47.html#subj7">  Re: UNIX Passwords (Richard Outerbridge)</A>
<LI><A HREF="/Risks/5.47.html#subj8">  CD vs ADP security (Barry Nelson)</A>
<LI><A HREF="/Risks/5.47.html#subj9">  Civil Disobedience and Computers (Robert Stanley)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.48.html">Volume 5 Issue 48 (23 Oct 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.48.html#subj1">  Computer Weather Forecasting (Jonathan Bowen, Robert Stroud)</A>
<LI><A HREF="/Risks/5.48.html#subj2">  Phone Service Degradation -- and 911 (Scot Wilcoxon)</A>
<LI><A HREF="/Risks/5.48.html#subj3">  Terrorism (Charles Shub, William Swan, Elliott Frank)</A>
<LI><A HREF="/Risks/5.48.html#subj4">  More on password security -- clean up your act (Jeremy Cook via McCullough)</A>
<LI><A HREF="/Risks/5.48.html#subj5">  Consumer Protection Act (Richard S. D'Ippolito)</A>
<LI><A HREF="/Risks/5.48.html#subj6">  Re: UNIX Passwords (Russ Housley, Richard Outerbridge)</A>
<LI><A HREF="/Risks/5.48.html#subj7">  Use of Social Security Numbers (James Peterson)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.49.html">Volume 5 Issue 49 (26 Oct 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.49.html#subj1">  Freak winds in southern England (sufrin, Franklin Anthes)</A>
<LI><A HREF="/Risks/5.49.html#subj2">  On the Risks of Using Words That Sound Similar (Bruce N. Baker)</A>
<LI><A HREF="/Risks/5.49.html#subj3">  CD, Terrorism, Stocks (Jim Anderson)</A>
<LI><A HREF="/Risks/5.49.html#subj4">  The Stock Market Computers and SDI (Bob Berger)</A>
<LI><A HREF="/Risks/5.49.html#subj5">  (Almost too much of) Password Encryption (Matt Bishop, Mark Brader)</A>
<LI><A HREF="/Risks/5.49.html#subj6">  Re: Phone Service Degradation -- and 911 (R.M. Richardson)</A>
<LI><A HREF="/Risks/5.49.html#subj7">  INUSE.COM Program (Chris McDonald)</A>
<LI><A HREF="/Risks/5.49.html#subj8">  Free phone-calls (E. van Batenburg)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.50.html">Volume 5 Issue 50 (27 Oct 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.50.html#subj1">  Weather (Willis Ware, Geoff Lane, Eugene Miya)</A>
<LI><A HREF="/Risks/5.50.html#subj2">  Civil disobedience (David Redell)</A>
<LI><A HREF="/Risks/5.50.html#subj3">  Reported Japanese Autopilot Problems (Nancy Leveson)</A>
<LI><A HREF="/Risks/5.50.html#subj4">  Amusing bug: Business Week Computer (F)ails (GW Ryan)</A>
<LI><A HREF="/Risks/5.50.html#subj5">  Television series "Welcome to my world" (Clive Feather)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.51.html">Volume 5 Issue 51 (28 Oct 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.51.html#subj1">  Re: Reported Japanese Autopilot Problems (Will Martin)</A>
<LI><A HREF="/Risks/5.51.html#subj2">  (Non-)Japanese Autopilot Problems (Joe Morris)</A>
<LI><A HREF="/Risks/5.51.html#subj3">  Possible nuclear launch prevented by parked vehicle (Scot Wilcoxon)</A>
<LI><A HREF="/Risks/5.51.html#subj4">  SDI information system announced (Scot Wilcoxon)</A>
<LI><A HREF="/Risks/5.51.html#subj5">  'Computers In Battle' (Rodney Hoffman)</A>
<LI><A HREF="/Risks/5.51.html#subj6">  Re: Amusing bug: Business Week Computer (F)ails (John Pershing)</A>
<LI><A HREF="/Risks/5.51.html#subj7">  Civil Disobedience (Fred Baube)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.52.html">Volume 5 Issue 52 (31 Oct 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.52.html#subj1">  Risks in intelligent security algorithms (Peter J. Denning)</A>
<LI><A HREF="/Risks/5.52.html#subj2">  Computer's Normal Operation Delays Royal Visit (Mark Brader)</A>
<LI><A HREF="/Risks/5.52.html#subj3">  Public notice of a security leak (Rob van Hoboken based on Nils Plum)</A>
<LI><A HREF="/Risks/5.52.html#subj4">  sc.4.1 update dangerous (Fen Labalme)</A>
<LI><A HREF="/Risks/5.52.html#subj5">  Mitsubishi MU-2 problems (Peter Ladkin)</A>
<LI><A HREF="/Risks/5.52.html#subj6">  Autopilots and conflicting alarms (Matt Jaffe, Joe Morris)</A>
<LI><A HREF="/Risks/5.52.html#subj7">  New encryption method (Stevan Milunovic)</A>
<LI><A HREF="/Risks/5.52.html#subj8">  The Stock Market and Program Trading (Dan Blumenthal, Brent Laminack)</A>
<LI><A HREF="/Risks/5.52.html#subj9">  Minuteman Missiles... (John J. McMahon)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.53.html">Volume 5 Issue 53 (2 Nov 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.53.html#subj1">  Re: Risks in intelligent security algorithms (David Redell)</A>
<LI><A HREF="/Risks/5.53.html#subj2">  Danger of typing the wrong password (Scot Wilcoxon)</A>
<LI><A HREF="/Risks/5.53.html#subj3">  Inadvertent Launch (Kenneth R. Jongsma)</A>
<LI><A HREF="/Risks/5.53.html#subj4">  MX Missile guidance computer problems (John Haller)</A>
<LI><A HREF="/Risks/5.53.html#subj5">  Re: Autopilots (Jan Wolitzky)</A>
<LI><A HREF="/Risks/5.53.html#subj6">  Aircraft accident (Peter Ladkin)</A>
<LI><A HREF="/Risks/5.53.html#subj7">  Missiles; predicting disasters (David Chase)</A>
<LI><A HREF="/Risks/5.53.html#subj8">  DISCOVER Uncovered? (Bruce N. Baker)</A>
<LI><A HREF="/Risks/5.53.html#subj9">  TV Clipping Services (Tom Benson [and Charles Youman], Samuel B. Bassett)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.54.html">Volume 5 Issue 54 (4 Nov 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.54.html#subj1">  Erroneous $1M overdraft -- plus interest (Dave Horsfall)</A>
<LI><A HREF="/Risks/5.54.html#subj2">  Wrongful Traffic Tickets &amp; Changing Computers (David A. Honig)</A>
<LI><A HREF="/Risks/5.54.html#subj3">  Weather -- or not to blame the computer? (Stephen Colwill)</A>
<LI><A HREF="/Risks/5.54.html#subj4">  Re: Computer's Normal Operation Delays Royal Visit (Henry Spencer)</A>
<LI><A HREF="/Risks/5.54.html#subj5">  Auto-pilot Problems and Hardware Reliability (Craig Johnson)</A>
<LI><A HREF="/Risks/5.54.html#subj6">  Minuteman III (Bryce Nesbitt)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.55.html">Volume 5 Issue 55 (5 Nov 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.55.html#subj1">  Phone prefix change cuts BBN off from world (David Kovar)</A>
<LI><A HREF="/Risks/5.55.html#subj2">  A simple application of Murphy's Law (Geoff Lane)</A>
<LI><A HREF="/Risks/5.55.html#subj3">  Wrongful Accusations; Weather (Willis Ware)</A>
<LI><A HREF="/Risks/5.55.html#subj4">  Weather and expecting the unexpected (Edmondson)</A>
<LI><A HREF="/Risks/5.55.html#subj5">  UNIX setuid nasty -- watch your pathnames (Stephen Russell)</A>
<LI><A HREF="/Risks/5.55.html#subj6">  Penetrations of Commercial Systems (TMP Lee, PGN)</A>
<LI><A HREF="/Risks/5.55.html#subj7">  Re: Unix password encryption, again? (Dan Hoey)</A>
<LI><A HREF="/Risks/5.55.html#subj8">  Software Testing (Danny Padwa)</A>
<LI><A HREF="/Risks/5.55.html#subj9">  Risks of using mailing lists (Dave Horsfall)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.56.html">Volume 5 Issue 56 (9 Nov 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.56.html#subj1">  News article on EMI affecting Black Hawk helicopter (John Woods)</A>
<LI><A HREF="/Risks/5.56.html#subj2">  A New Twist with Cellular Phones (Leo Schwab)</A>
<LI><A HREF="/Risks/5.56.html#subj3">  Computers Amplify Black Monday (Bjorn Freeman-Benson)</A>
<LI><A HREF="/Risks/5.56.html#subj4">  Programmed stock trading (Michael R. Wade)</A>
<LI><A HREF="/Risks/5.56.html#subj5">  Tape label mismatch (Jeff Woolsey)</A>
<LI><A HREF="/Risks/5.56.html#subj6">  Phantom Traffic Tickets (Isaac K. Rabinovitch)</A>
<LI><A HREF="/Risks/5.56.html#subj7">  National ID Card (Australia)  (Tom Nemeth)</A>
<LI><A HREF="/Risks/5.56.html#subj8">  Unix 8-character password truncation and human interface (Geoffrey Cooper)</A>
<LI><A HREF="/Risks/5.56.html#subj9">  setuid (once more)  (George Kaplan)</A>
<LI><A HREF="/Risks/5.56.html#subj10">  Re: Minuteman Missiles (Mike Bell)</A>
<LI><A HREF="/Risks/5.56.html#subj11">  Mailing List Humor (Bjorn Freeman-Benson)</A>
<LI><A HREF="/Risks/5.56.html#subj12">  A new kind of computer crash (Steve Skabrat)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.57.html">Volume 5 Issue 57 (12 Nov 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.57.html#subj1">  Mobile Radio Interference With Vehicles (Steve Conklin, Bill Gunshannon)</A>
<LI><A HREF="/Risks/5.57.html#subj2">  Optimizing for cost savings, not safety (John McLeod)</A>
<LI><A HREF="/Risks/5.57.html#subj3">  "Welcome To My World", BBC1 Sundays 11PM -- A Review (Martin Smith)</A>
<LI><A HREF="/Risks/5.57.html#subj4">  Re: A simple application of Murphy's Law (Tape Labels) (Henry Spencer)</A>
<LI><A HREF="/Risks/5.57.html#subj5">  Overwrite of Tape Data (Ron Heiby)</A>
<LI><A HREF="/Risks/5.57.html#subj6">  Misplaced trust (B Snow)</A>
<LI><A HREF="/Risks/5.57.html#subj7">  Bar Codes (Elizabeth D. Zwicky)</A>
<LI><A HREF="/Risks/5.57.html#subj8">  Password truncation and human interfaces (Theodore Ts'o)</A>
<LI><A HREF="/Risks/5.57.html#subj9">  Re: UNIX setuid nasty (Geoff, David Phillip Oster)</A>
<LI><A HREF="/Risks/5.57.html#subj10">  How much physical security? (Martin Ewing, Alex Colvin, Mike Alexander)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.58.html">Volume 5 Issue 58 (15 Nov 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.58.html#subj1">  Son of Stark (Hugh Miller)</A>
<LI><A HREF="/Risks/5.58.html#subj2">  Follow-up to Black Hawk Failures article (Dave Newkirk)</A>
<LI><A HREF="/Risks/5.58.html#subj3">  Jamming the Chopper (Brint Cooper)</A>
<LI><A HREF="/Risks/5.58.html#subj4">  Computer systems hit by logic bombs (J.D. Bonser)</A>
<LI><A HREF="/Risks/5.58.html#subj5">  Risk of more computers (Arthur David Olson)</A>
<LI><A HREF="/Risks/5.58.html#subj6">  Reach out and (t)ouch! (Matthew Kruk)</A>
<LI><A HREF="/Risks/5.58.html#subj7">  Re: Password truncation and human interfaces (Mark W. Eichin)</A>
<LI><A HREF="/Risks/5.58.html#subj8">  Mobile Radio Interference With Vehicles (Ian Batten)</A>
<LI><A HREF="/Risks/5.58.html#subj9">  Computer terrorism (Brint Cooper)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.59.html">Volume 5 Issue 59 (16 Nov 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.59.html#subj1">  Risks in Voice Mail (PGN)</A>
<LI><A HREF="/Risks/5.59.html#subj2">  Stark Reality (LT Scott A. Norton)</A>
<LI><A HREF="/Risks/5.59.html#subj3">  Re: How much physical security? (R.M. Richardson)</A>
<LI><A HREF="/Risks/5.59.html#subj4">  Navy Seahawk helicopters (LT Scott A. Norton)</A>
<LI><A HREF="/Risks/5.59.html#subj5">  Army Black Hawk helicopters (Peter Ladkin)</A>
<LI><A HREF="/Risks/5.59.html#subj6">  External risks (John McLeod)</A>
<LI><A HREF="/Risks/5.59.html#subj7">  Re: A simple application of Murphy's Law (Tape Labels) (Barry Gold)</A>
<LI><A HREF="/Risks/5.59.html#subj8">  EAN and PIN codes (Otto J. Makela)</A>
<LI><A HREF="/Risks/5.59.html#subj9">  Computerized Fuel Injection (James M. Bodwin)</A>
<LI><A HREF="/Risks/5.59.html#subj10">  Re: Password truncation and human interfaces (Franklin Davis)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.60.html">Volume 5 Issue 60 (18 Nov 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.60.html#subj1">  Swedish trains collide (Rick Blake)</A>
<LI><A HREF="/Risks/5.60.html#subj2">  Hardware and configuration control problem in a DC-9 computer (Nancy Leveson)</A>
<LI><A HREF="/Risks/5.60.html#subj3">  Ethics, Liability, and Responsibility (Gene Spafford)</A>
<LI><A HREF="/Risks/5.60.html#subj4">  Blackhawks and Seahawks (Mike Brown)</A>
<LI><A HREF="/Risks/5.60.html#subj5">  Mobile Radio Interference With Vehicles (Peter Mabey)</A>
<LI><A HREF="/Risks/5.60.html#subj6">  VW Fastbacks/RFI/EFI (David Lesher)</A>
<LI><A HREF="/Risks/5.60.html#subj7">  CB frequencies and power (John McLeod)</A>
<LI><A HREF="/Risks/5.60.html#subj8">  Signs of the Times (Robert Morris)</A>
<LI><A HREF="/Risks/5.60.html#subj9">  The Mercaptan goes down with the strip (Burch Seymour)</A>
<LI><A HREF="/Risks/5.60.html#subj10">  Re: Reach out and (t)ouch (Michael Wagner)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.61.html">Volume 5 Issue 61 (18 Nov 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.61.html#subj1">  Risks of increased CATV technology (Allan Pratt)</A>
<LI><A HREF="/Risks/5.61.html#subj2">  Bank networks (David G. Grubbs)</A>
<LI><A HREF="/Risks/5.61.html#subj3">  Re: PIN Verification (John Pershing)</A>
<LI><A HREF="/Risks/5.61.html#subj4">  Re: More on computer security ()</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.62.html">Volume 5 Issue 62 (20 Nov 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.62.html#subj1">  A Two-Digit Stock Ticker in a Three-Digit World (Chuck Weinstock)</A>
<LI><A HREF="/Risks/5.62.html#subj2">  Stark - warning depends on operator action, intelligence data quality    (Jonathan Jacky)
</A>
<LI><A HREF="/Risks/5.62.html#subj3">  Task Force Slams DoD for Bungling Military Software (Jonathan Jacky)</A>
<LI><A HREF="/Risks/5.62.html#subj4">  Addressable CATV (Jerome H. Saltzer)</A>
<LI><A HREF="/Risks/5.62.html#subj5">  Human automata and inhuman automata (Chris Rusbridge)</A>
<LI><A HREF="/Risks/5.62.html#subj6">  Re: CB frequencies and power (Dan Franklin, John McLeod, Wm Brown III)</A>
<LI><A HREF="/Risks/5.62.html#subj7">  "UNIX setuid stupidity" (David Phillip Oster, Stephen Russell)</A>
<LI><A HREF="/Risks/5.62.html#subj8">  Software Safety Specification (Mike Brown)</A>
<LI><A HREF="/Risks/5.62.html#subj9">  Call for Papers, COMPASS '88 (Frank Houston)</A>
<LI><A HREF="/Risks/5.62.html#subj10">  "Normal Accidents" revisited (David Chase)</A>
<LI><A HREF="/Risks/5.62.html#subj11">  Space Shuttle Whistle-Blowers Sound Alarm Again (rdicamil)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.63.html">Volume 5 Issue 63 (23 Nov 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.63.html#subj1">  Logic bombs and other system attacks -- in Canada (PGN)</A>
<LI><A HREF="/Risks/5.63.html#subj2">  Video signal piracy hits WGN/WTTW (Rich Kulawiec)</A>
<LI><A HREF="/Risks/5.63.html#subj3">  Garage Door Openers (Brint Cooper)</A>
<LI><A HREF="/Risks/5.63.html#subj4">  Sudden acceleration revisited (Nancy Leveson)</A>
<LI><A HREF="/Risks/5.63.html#subj5">  Centralized Auto Locking (Lindsay F. Marshall)</A>
<LI><A HREF="/Risks/5.63.html#subj6">  Re: The Stark incident (Amos Shapir)</A>
<LI><A HREF="/Risks/5.63.html#subj7">  Bank Networks (George Bray)</A>
<LI><A HREF="/Risks/5.63.html#subj8">  Re: Optimizing for cost savings, not safety (Dave Horsfall)</A>
<LI><A HREF="/Risks/5.63.html#subj9">  L.A. Earthquake &amp; Telephone Service (LT Scott A. Norton, USN)</A>
<LI><A HREF="/Risks/5.63.html#subj10">  Gripen flight delayed (Henry Spencer)</A>
<LI><A HREF="/Risks/5.63.html#subj11">  Mariner 1 (Mark Brader)</A>
<LI><A HREF="/Risks/5.63.html#subj12">  Systemantics (John Gilmore, haynes)  [Old hat for old RISKers]</A>
<LI><A HREF="/Risks/5.63.html#subj13">  Re: "UNIX setuid stupidity" (Joseph G. Keane, Martin Minow)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.64.html">Volume 5 Issue 64 (24 Nov 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.64.html#subj1">  More on NASA Hackers (Dave Curry)</A>
<LI><A HREF="/Risks/5.64.html#subj2">  Re: Video signal piracy hits WGN/WTTW (Will Martin)</A>
<LI><A HREF="/Risks/5.64.html#subj3">  Logic Bombs; Centralized Auto Locking (P. T. Withington)</A>
<LI><A HREF="/Risks/5.64.html#subj4">  Re: Mariner 1 (Henry Spencer, Mary Shaw, Andrew Taylor, Martin Ewing)</A>
<LI><A HREF="/Risks/5.64.html#subj5">  Bank Transaction Control (Scott Dorsey)</A>
<LI><A HREF="/Risks/5.64.html#subj6">  Re: Sudden acceleration revisited (Donald A Gworek)</A>
<LI><A HREF="/Risks/5.64.html#subj7">  Re: CB radio and power (Jeffrey R Kell)</A>
<LI><A HREF="/Risks/5.64.html#subj8">  More on Garage Doors (Brint Cooper)</A>
<LI><A HREF="/Risks/5.64.html#subj9">  Train crash in Sweden (Matt Fichtenbaum)</A>
<LI><A HREF="/Risks/5.64.html#subj10">  Re: L.A. Earthquake &amp; Telephone Service (Darin McGrew)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.65.html">Volume 5 Issue 65 (25 Nov 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.65.html#subj1">  Mariner I and computer folklore (Jon Jacky, Jim Horning)</A>
<LI><A HREF="/Risks/5.65.html#subj2">  Computer-controlled train runs red light (Jon Jacky)</A>
<LI><A HREF="/Risks/5.65.html#subj3">  Addressable CATV information (Ted Kekatos)</A>
<LI><A HREF="/Risks/5.65.html#subj4">  A new legal first in Britain... (Gligor Tashkovich)</A>
<LI><A HREF="/Risks/5.65.html#subj5">  The rm * controversy in unix.wizards (Charles Shub)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.66.html">Volume 5 Issue 66 (27 Nov 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.66.html#subj1">  Mariner I (Eric Roberts)</A>
<LI><A HREF="/Risks/5.66.html#subj2">  FORTRAN pitfalls (Jim Duncan)</A>
<LI><A HREF="/Risks/5.66.html#subj3">  PIN verification (Otto J. Makela)</A>
<LI><A HREF="/Risks/5.66.html#subj4">  Sudden acceleration revisited (Leslie Burkholder)</A>
<LI><A HREF="/Risks/5.66.html#subj5">  Re: CB radio and power (Maj. Doug Hardie)</A>
<LI><A HREF="/Risks/5.66.html#subj6">  An earlier train crash -- Farnley Junction (Clive D.W. Feather)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.67.html">Volume 5 Issue 67 (30 Nov 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.67.html#subj1">  Aging air traffic computer fails again (Rodney Hoffman, Alan Wexelblat)</A>
<LI><A HREF="/Risks/5.67.html#subj2">  Computer Virus (Kenneth R. van Wyk via Jeffrey James Bryan Carpenter)</A>
<LI><A HREF="/Risks/5.67.html#subj3">  Fiber optic tap (Kenneth R. Jongsma)</A>
<LI><A HREF="/Risks/5.67.html#subj4">  A new and possibly risky use for computer chips (John Saponara)</A>
<LI><A HREF="/Risks/5.67.html#subj5">  Selling Science [a review] (Peter J. Denning)</A>
<LI><A HREF="/Risks/5.67.html#subj6">  Risks to computerised traffic control signs (Peter McMahon)</A>
<LI><A HREF="/Risks/5.67.html#subj7">  Risks in Energy Management Systems (Anon)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.68.html">Volume 5 Issue 68 (1 Dec 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.68.html#subj1">  Logic Bomb (Brian Randell, ZZASSGL)</A>
<LI><A HREF="/Risks/5.68.html#subj2">  Re: hyphens &amp; Mariner I (Jerome H. Saltzer)</A>
<LI><A HREF="/Risks/5.68.html#subj3">  Re: Mariner, and dropped code (Ronald J Wanttaja)</A>
<LI><A HREF="/Risks/5.68.html#subj4">  Minuteman and Falling Trucks (Joe Dellinger)</A>
<LI><A HREF="/Risks/5.68.html#subj5">  Re: Fiber optic tap (Mike Muuss)</A>
<LI><A HREF="/Risks/5.68.html#subj6">  Re: Garage door openers (Henry Spencer)</A>
<LI><A HREF="/Risks/5.68.html#subj7">  Dutch Database Privacy Laws (Robert Stanley)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.69.html">Volume 5 Issue 69 (4 Dec 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.69.html#subj1">  Can you sue an expert system? (Barry A. Stevens)</A>
<LI><A HREF="/Risks/5.69.html#subj2">  Risks of Portable Computers (PGN)</A>
<LI><A HREF="/Risks/5.69.html#subj3">  Beware the Temporary Employee (Howard Israel)</A>
<LI><A HREF="/Risks/5.69.html#subj4">  Truncated anything (Doug Mosher)</A>
<LI><A HREF="/Risks/5.69.html#subj5">  An ancient computer virus (Joe Dellinger)</A>
<LI><A HREF="/Risks/5.69.html#subj6">  Cable violations of privacy (Bob Rogers)</A>
<LI><A HREF="/Risks/5.69.html#subj7">  Re: Computer-controlled train runs red light (Steve Nuchia)</A>
<LI><A HREF="/Risks/5.69.html#subj8">  VM systems vulnerability (Doug Mosher)</A>
<LI><A HREF="/Risks/5.69.html#subj9">  Baby monitors end up 'bugging' the whole house (Shane Looker)</A>
<LI><A HREF="/Risks/5.69.html#subj10">  F4 in 'Nam (Re: Reversed signal polarity...) (Brent Chapman)</A>
<LI><A HREF="/Risks/5.69.html#subj11">  IRS computers (yet again!) (Joe Morris)</A>
<LI><A HREF="/Risks/5.69.html#subj12">  Journal of Computing and Society (Gary Chapman)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.70.html">Volume 5 Issue 70 (6 Dec 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.70.html#subj1">  Wall Street crash, computers, and SDI (Rodney Hoffman)</A>
<LI><A HREF="/Risks/5.70.html#subj2">  NW Flight 255 -- Simulator did, but wasn't (Scot E. Wilcoxon)</A>
<LI><A HREF="/Risks/5.70.html#subj3">  Whistle-blowers who aren't (Henry Spencer)</A>
<LI><A HREF="/Risks/5.70.html#subj4">  Re: Space Shuttle Whistle-Blowers Sound Alarm Again (Henry Spencer)</A>
<LI><A HREF="/Risks/5.70.html#subj5">  A new twist to password insecurity (Roy Smith)</A>
<LI><A HREF="/Risks/5.70.html#subj6">  More on PIN encoding (Chris Maltby)</A>
<LI><A HREF="/Risks/5.70.html#subj7">  Telephone overload (Stephen Grove)</A>
<LI><A HREF="/Risks/5.70.html#subj8">  Software licensing problems (Geof Cooper)</A>
<LI><A HREF="/Risks/5.70.html#subj9">  Re: Mariner 1 or Apollo 11? (Henry Spencer, Brent Chapman)</A>
<LI><A HREF="/Risks/5.70.html#subj10">  More on addressable converter box (Allan Pratt)</A>
<LI><A HREF="/Risks/5.70.html#subj11">  Centralized car locks (K. Richard Magill)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.71.html">Volume 5 Issue 71 (7 Dec 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.71.html#subj1">  The Amiga VIRUS (by Bill Koester)  (Bernie Cosell)</A>
<LI><A HREF="/Risks/5.71.html#subj2">  Radar's Growing Vulnerability (PGN)</A>
<LI><A HREF="/Risks/5.71.html#subj3">  Computerized vote counting (Lance J. Hoffman)</A>
<LI><A HREF="/Risks/5.71.html#subj4">  United Airlines O'Hare Sabotage? (Chuck Weinstock)</A>
<LI><A HREF="/Risks/5.71.html#subj5">  Re: Whistle-blowers who (allegedly) aren't (Jeffrey Mogul)</A>
<LI><A HREF="/Risks/5.71.html#subj6">  In Decent Alarm (Bruce N. Baker)</A>
<LI><A HREF="/Risks/5.71.html#subj7">  Need for first-person anonymous reporting systems (Eugene Miya)</A>
<LI><A HREF="/Risks/5.71.html#subj8">  Apollo 11 computer problems (Michael MacKenzie)</A>
<LI><A HREF="/Risks/5.71.html#subj9">  Interconnected ATM networks (Win Treese)</A>
<LI><A HREF="/Risks/5.71.html#subj10">  Can you sue an expert system? (Gary Chapman, Jerry Leichter, Bruce Hamilton)</A>
<LI><A HREF="/Risks/5.71.html#subj11">  What this country needs is a good nickel chroot (Bob English)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.72.html">Volume 5 Issue 72 (12 Dec 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.72.html#subj1">  Risks to the Rodent Public in the Use of Computers (Peter Ladkin)</A>
<LI><A HREF="/Risks/5.72.html#subj2">  Yet another virus program announcement fyi (Martin Minow)</A>
<LI><A HREF="/Risks/5.72.html#subj3">  IBM invaded by a Christmas virus (Dave Curry)</A>
<LI><A HREF="/Risks/5.72.html#subj4">  Virus Protection Strategies (Joe Dellinger)</A>
<LI><A HREF="/Risks/5.72.html#subj5">  New chain letter running around internet/usenet (Rich Kulawiec)</A>
<LI><A HREF="/Risks/5.72.html#subj6">  On-line bank credit cards (John R. Levine)</A>
<LI><A HREF="/Risks/5.72.html#subj7">  Central Locking (Martyn Thomas)</A>
<LI><A HREF="/Risks/5.72.html#subj8">  Product Liability (Martyn Thomas)</A>
<LI><A HREF="/Risks/5.72.html#subj9">  Wishing the deceased a merry christmas (automatically)  (Bill Lee)</A>
<LI><A HREF="/Risks/5.72.html#subj10">  Air Traffic Control Computer Replacement Schedule (Dan Ball)</A>
<LI><A HREF="/Risks/5.72.html#subj11">  Re: United Airlines O'Hare Sabotage? (Dave Mills)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.73.html">Volume 5 Issue 73 (13 Dec 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.73.html#subj1">  Australian datacom blackout (Barry Nelson)</A>
<LI><A HREF="/Risks/5.73.html#subj2">  Finally, a primary source on Mariner 1 (John Gilmore, Doug Mink, Marty Moore)</A>
<LI><A HREF="/Risks/5.73.html#subj3">  Re: Computer-controlled train runs red light (Nancy Leveson)</A>
<LI><A HREF="/Risks/5.73.html#subj4">  Re: interconnected ATM networks (John R. Levine, Darren New)</A>
<LI><A HREF="/Risks/5.73.html#subj5">  Control-tower fires (dvk)</A>
<LI><A HREF="/Risks/5.73.html#subj6">  Loss-of-orbiter (Dani Eder)</A>
<LI><A HREF="/Risks/5.73.html#subj7">  Re: EEC Product Liability (John Gilmore)</A>
<LI><A HREF="/Risks/5.73.html#subj8">  The Presidential "Football"... (Carl Schlachte)</A>
<LI><A HREF="/Risks/5.73.html#subj9">  Radar's Growing Vulnerability (Jon Eric Strayer)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.74.html">Volume 5 Issue 74 (14 Dec 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.74.html#subj1">  Rounding error costs DHSS 100 million pounds (Robert Stroud)</A>
<LI><A HREF="/Risks/5.74.html#subj2">  Computers' Role in Stock Market Crash (Rodney Hoffman)</A>
<LI><A HREF="/Risks/5.74.html#subj3">  The Infarmation Age (Ivan M. Milman)</A>
<LI><A HREF="/Risks/5.74.html#subj4">  Virus programs and Chain letters (David G. Grubbs)</A>
<LI><A HREF="/Risks/5.74.html#subj5">  Baby monitors can also be very efficient "jammers", too. (Rob Warnock)</A>
<LI><A HREF="/Risks/5.74.html#subj6">  The Saga of the Lost ATM Card (Alan Wexelblat)</A>
<LI><A HREF="/Risks/5.74.html#subj7">  Interchange of ATM Cards (Ted Lee)</A>
<LI><A HREF="/Risks/5.74.html#subj8">  PacBell Calling Card Security (or lack thereof) (Brent Chapman)</A>
<LI><A HREF="/Risks/5.74.html#subj9">  IBM invaded by a Christmas virus (Franklin Davis)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.75.html">Volume 5 Issue 75 (15 Dec 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.75.html#subj1">  Advice to the Risklorn (Steven McBride)</A>
<LI><A HREF="/Risks/5.75.html#subj2">  Expert systems liability (George S. Cole via Martin Minow, George Bray,      Dean Sutherland, Bjorn Freeman-Benson, William Swan, Wm Brown III)
</A>
<LI><A HREF="/Risks/5.75.html#subj3">  Microprocessors vs relay logic (Wm Brown III)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.76.html">Volume 5 Issue 76 (16 Dec 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.76.html#subj1">  Designing for Failure (Don Wegeng)</A>
<LI><A HREF="/Risks/5.76.html#subj2">  Computer MTBF and usage (Andy Freeman)</A>
<LI><A HREF="/Risks/5.76.html#subj3">  Liability and software bugs (Nancy Leveson)</A>
<LI><A HREF="/Risks/5.76.html#subj4">  Re: Need for Reporting Systems (Paul Garnet)</A>
<LI><A HREF="/Risks/5.76.html#subj5">  Tom Swift and his Electric Jockstrap (Arthur Axelrod)</A>
<LI><A HREF="/Risks/5.76.html#subj6">  Re: Expert Systems (Amos Shapir)</A>
<LI><A HREF="/Risks/5.76.html#subj7">  The Saga of the Lost ATM Card (Scott E. Preece)</A>
<LI><A HREF="/Risks/5.76.html#subj8">  Telephone Billing Risks (Fred Baube)</A>
<LI><A HREF="/Risks/5.76.html#subj9">  Re: F4 in 'Nam (Reversed signal polarity causing accidents) (Henry Spencer)</A>
<LI><A HREF="/Risks/5.76.html#subj10">  For Lack of a Nut (NASDAQ Power outage revisited)  (Bill McGarry)</A>
<LI><A HREF="/Risks/5.76.html#subj11">  Dutch Database Privacy Laws (Henk Cazemier)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.77.html">Volume 5 Issue 77 (17 Dec 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.77.html#subj1">  Lessons from a power failure (Jerome H. Saltzer)</A>
<LI><A HREF="/Risks/5.77.html#subj2">  Squirrels and other pesky animals (Frank Houston)</A>
<LI><A HREF="/Risks/5.77.html#subj3">  Security failures should have unlimited distributions (Andy Freeman)</A>
<LI><A HREF="/Risks/5.77.html#subj4">  2600 Magazine -- hackers, cracking systems, operating systems (Eric Corley)</A>
<LI><A HREF="/Risks/5.77.html#subj5">  Re: can you sue an expert system? (Roger Mann)</A>
<LI><A HREF="/Risks/5.77.html#subj6">  Re: Interchange of ATM cards (Douglas Jones)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.78.html">Volume 5 Issue 78 (18 Dec 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.78.html#subj1">  Roger Boisjoly and Ethical Behavior (Henry Spencer, Ronni Rosenberg)</A>
<LI><A HREF="/Risks/5.78.html#subj2">  Computer aids taxi dispatch (Jeff Lindorff)</A>
<LI><A HREF="/Risks/5.78.html#subj3">  Re: product liability (Martyn Thomas)</A>
<LI><A HREF="/Risks/5.78.html#subj4">  Re: Expert systems liability (Jonathan Krueger)</A>
<LI><A HREF="/Risks/5.78.html#subj5">  Re: Australian telecom blackouts and 'hidden' crimes (Jon A. Tankersley)</A>
<LI><A HREF="/Risks/5.78.html#subj6">  Wall Street Kills The Messenger (Scot E. Wilcoxon)</A>
<LI><A HREF="/Risks/5.78.html#subj7">  Expert systems; Ejection notice? (Steve Philipson)</A>
<LI><A HREF="/Risks/5.78.html#subj8">  Squirrels, mice, bugs, and Grace Hopper's moth (Mark Mandel)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.79.html">Volume 5 Issue 79 (20 Dec 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.79.html#subj1">  Re: Lehigh Virus (James Ford)</A>
<LI><A HREF="/Risks/5.79.html#subj2">  IBM Xmas Prank (Fred Baube)</A>
<LI><A HREF="/Risks/5.79.html#subj3">  National security clearinghouse (Alan Silverstein)</A>
<LI><A HREF="/Risks/5.79.html#subj4">  Financial brokers are buying Suns... (John Gilmore)</A>
<LI><A HREF="/Risks/5.79.html#subj5">  Toronto Stock Exchange Automation? (Hugh Miller)</A>
<LI><A HREF="/Risks/5.79.html#subj6">  Who Sues? (Marcus J. Ranum)</A>
<LI><A HREF="/Risks/5.79.html#subj7">  The Fable of the Computer that Made Something (Geraint Jones)</A>
<LI><A HREF="/Risks/5.79.html#subj8">  Re: Litigation over an expert system (Rich Richardson)</A>
<LI><A HREF="/Risks/5.79.html#subj9">  Tulsa; Bugs (Haynes)</A>
<LI><A HREF="/Risks/5.79.html#subj10">  More ATM information (George Bray)</A>
<LI><A HREF="/Risks/5.79.html#subj11">  Truncation (Alex Heatley)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.80.html">Volume 5 Issue 80 (21 Dec 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.80.html#subj1">  Re: IBM Christmas Virus (Ross Patterson)</A>
<LI><A HREF="/Risks/5.80.html#subj2">  Logic Bomb case thrown out of court (Geoff Lane)</A>
<LI><A HREF="/Risks/5.80.html#subj3">  Repository for Illicit Code (Steve Jong)</A>
<LI><A HREF="/Risks/5.80.html#subj4">  Roger Boisjoly and Ethical Behavior (Stuart Freedman)</A>
<LI><A HREF="/Risks/5.80.html#subj5">  Truncation and VM passwords (Joe Morris)</A>
<LI><A HREF="/Risks/5.80.html#subj6">  Competing ATM networks (Chris Koenigsberg)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.81.html">Volume 5 Issue 81 (22 Dec 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.81.html#subj1">  The Christmas Card Caper, (hopefully) concluded (Joe Morris)</A>
<LI><A HREF="/Risks/5.81.html#subj2">  The Virus of Christmas Past (Una Smith)</A>
<LI><A HREF="/Risks/5.81.html#subj3">  Viruses and "anti-bodies" (Brewster Kahle)</A>
<LI><A HREF="/Risks/5.81.html#subj4">  Cleaning Your PC Can Be Hazardous to Your Health (Brian M. Clapper)</A>
<LI><A HREF="/Risks/5.81.html#subj5">  Product liability (Mark A. Fulk)</A>
<LI><A HREF="/Risks/5.81.html#subj6">  Squirrels, mice, bugs, and Grace Hopper's moth (Peter Mabey)</A>
<LI><A HREF="/Risks/5.81.html#subj7">  Fire at O'Hare (Computerworld, Dec 14 issue) (Haynes)</A>
<LI><A HREF="/Risks/5.81.html#subj8">  American Express computer problem (Frank Wales)</A>
<LI><A HREF="/Risks/5.81.html#subj9">  NYT article on computers in stock crash (Hal Perkins)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.82.html">Volume 5 Issue 82 (23 Dec 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.82.html#subj1">  NYT article on computers in stock crash (P. T. Withington)</A>
<LI><A HREF="/Risks/5.82.html#subj2">  ...BAD PRACTICE to truncate anything without notice (Doug Rudoff)</A>
<LI><A HREF="/Risks/5.82.html#subj3">  The spread of viruses and news articles (Allan Pratt)</A>
<LI><A HREF="/Risks/5.82.html#subj4">  Common passwords list (Doug Mansur)</A>
<LI><A HREF="/Risks/5.82.html#subj5">  Re: IBM Christmas Virus (Skip Montanaro)</A>
<LI><A HREF="/Risks/5.82.html#subj6">  Cleaning PC's can be bad for your health...  (John McMahon)</A>
<LI><A HREF="/Risks/5.82.html#subj7">  PIN verification security (Otto Makela)</A>
<LI><A HREF="/Risks/5.82.html#subj8">  Social Insecurity (Roger Pick)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.83.html">Volume 5 Issue 83 (24 Dec 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.83.html#subj1">  Another article on the Christmas Virus (Mark Brader)</A>
<LI><A HREF="/Risks/5.83.html#subj2">  Social Insecurity (Willis H. Ware)</A>
<LI><A HREF="/Risks/5.83.html#subj3">  Expert systems (Peter da Silva)</A>
<LI><A HREF="/Risks/5.83.html#subj4">  Most-common passwords (Rodney Hoffman)</A>
<LI><A HREF="/Risks/5.83.html#subj5">  Permissions and setuid on UNIX (Philip Kos)</A>
<LI><A HREF="/Risks/5.83.html#subj6">  UNIX chroot and setuid (Michael S. Fischbein)</A>
</UL><DT><IMG SRC="/Images/redball.gif" ALT=p"o" WIDTH="14" HEIGHT="14">
<A HREF="/Risks/5.84.html">Volume 5 Issue 84 (31 Dec 87)</A>
<DD><UL>
<LI><A HREF="/Risks/5.84.html#subj1">  Risks of Robots (Eric Haines)</A>
<LI><A HREF="/Risks/5.84.html#subj2">  Christmas Exec AGAIN! (Eric Skinner)</A>
<LI><A HREF="/Risks/5.84.html#subj3">  Computer glitch stalls 3 million bank transactions for a day (Rodney Hoffman)</A>
<LI><A HREF="/Risks/5.84.html#subj4">  Switch malfunction disrupts phone service (Richard Nichols)</A>
<LI><A HREF="/Risks/5.84.html#subj5">  40,000 telephones on "hold" (Bob Cunningham)</A>
<LI><A HREF="/Risks/5.84.html#subj6">  Unions denied access to commercial database services    (Originally by Jeff Angus and Alice LaPlante via Michael Travers via
    Eric Haines via John Saponara)
</A>
<LI><A HREF="/Risks/5.84.html#subj7">  'Leg Irons' Keep Inmates Home (Randy Schulz)</A>
<LI><A HREF="/Risks/5.84.html#subj8">  Re: Logic Bomb case thrown out of court (Amos Shapir)</A>
<LI><A HREF="/Risks/5.84.html#subj9">  Missouri Court Decision on Computerized Voting (Charles Youman)</A>
<LI><A HREF="/Risks/5.84.html#subj10">  pc hard disk risks -- and a way out? (Martin Minow)</A>
<LI><A HREF="/Risks/5.84.html#subj11">  Viruses and Goedel bugs (Matthew P. Wiener)</A>
</UL></DL>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4/index.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Volume" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/6/index.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Volume" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-79</DOCNO>
<DOCOLDNO>IA012-000125-B044-75</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.2.html 128.240.150.127 19970217005753 text/html 17952
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:56:21 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 2</TITLE>
<LINK REL="Prev" HREF="/Risks/4.01.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.03.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.01.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.03.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 2</H1>
<H2> Sunday, 2 November 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Insurgent Squirrel Joins No-Ways Arc 
</A>
<DD>
<A HREF="#subj1.1">
Ross McKenrick
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Collision avoidance systems - FAA vs. Honeywell 
</A>
<DD>
<A HREF="#subj2.1">
Charlie Hurd
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  The Military and Automatic Humans 
</A>
<DD>
<A HREF="#subj3.1">
Ronald J Wanttaja
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Assessing system effectiveness 
</A>
<DD>
<A HREF="#subj4.1">
Scott E. Preece
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Computers in elections 
</A>
<DD>
<A HREF="#subj5.1">
Kurt Hyde
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  17th FAULT-TOLERANT COMPUTING SYMPOSIUM 
</A>
<DD>
<A HREF="#subj6.1">
Flaviu Cristian
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Insurgent Squirrel Joins No-Ways Arc       [Title adapted by PGN from 
</A>
</H3>
<address>
Ross McKenrick  
&lt;<A HREF="mailto:CRMCK%BROWNVM.BITNET@WISCVM.WISC.EDU">
CRMCK%BROWNVM.BITNET@WISCVM.WISC.EDU
</A>&gt;
</address>
<i>
Thu, 30 Oct 86 11:40:40 EST
</i><PRE>
                                                    1957 Bob Ashenhurst hoax
                                                    on Rick Gould's PhD Thesis]
"Lost Squirrel Causes Troublesome Power Surge"
Providence Journal, Thursday, October 30, 1986

   An electrical power surge caused computers to go on the blink in
Providence brokerage houses, banks, and office buildings yesterday.  A
Narragansett Electric Co. spokesman said a squirrel caused a short-circuit
in a transformer.  Charles Moran, the spokesman, said the squirrel got into
a transformer at the Narragansett Electric's Dyer Street substation at
11:10am.  Moran said a backup transformer took over automatically and
prevented a power failure in downtown Providence.  But "there was a slight
power surge," he said.

   Computers in the money-market divisions of the Fleet and Old Stone Banks
were down for half an hour after the power surge, but banking services were
not disrupted, spokesmen said.  Dean Witter Reynolds Inc., a brokerage firm,
had trouble getting quotes on stock prices, according to Sharon Tallman, who
said some of the firm's Quotron machines went down.  At Superior Court, the
computer was down for two hours, but it didn't affect court scheduling, a
spokeman said.  

"The mainframe on our IBM computer was down for over an hour," said Robert
Perreira of the Providence Journal Co.'s computer services unit.  Perreira
said 14 systems went down and "three of them did not come up immediately."
A Journal Co. electrician said the power surge caused "our lightning control
panel to behave like a runaway monster."  It caused a computer to activate a
program designed to save energy on weekends by shutting off the lights in part
of the building. "The computer thinks it's Sunday," the electrician said.

    [A similar squirrelcide happened at SRI a while back.  The side-effects
     were quite prolonged and unanticipated.  On occasional Saturdays for 
     several months all of SRI was powerless while repairs were repeatedly
     attempted but not quite completely accomplished.  PGN]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Collision avoidance systems - FAA vs. Honeywell
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: 31 Oct 86 11:01:30 EST (Fri)
From: churd@labs-b.bbn.com

A few months ago, Sixty Minutes ran an episode about the fact that the FAA had
rejected Honeywell's collision avoidance system in favor of its own (untested,
uncompleted) system.  I think the episode aired shortly after the Air Mexico
collision in California.  One of the people Sixty Minutes interviewed had been
an FAA official (executive?) until he became too vocal about the fact that the
FAA was ignoring a workable system.  It was his opinion that *many* collisions
and near-misses would never have happened if the Honeywell system had been
adopted when it was first introduced.

The Honeywell system resides in the aircraft and projects an envelope ahead of
the plane that can be detected by another Honeywell system.  The system 
communicates with the pilot by issuing a warning when an intersection with
another plane's envelope is detected and gives a direction in which to turn
to avoid collision.

The FAA system is tied into the ground-control system and seems to rely on 
tracking aircraft from radar on the ground.  I was not too clear on this.

The advantage of the Honeywell system is that it is small, cheap, and does
not require the pilot to rely on any outside assistance.  The drawback is
that *all* planes need to be equipped with the system.  But, since it is
small and cheap that would not be a great problem.

I can't remember all the pros and cons of the FAA system, but the cons had a
clear majority.  The system is much more complicated, involves ground-control
personnel notifying pilots about impending collisions, and is expensive.

	Charlie Hurd

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
 The Military and Automatic Humans
</A>
</H3>
<address>
Ronald J Wanttaja
&lt;<A HREF="mailto:nike!caip!uw-beaver!ssc-vax!wanttaja@cad.Berkeley.EDU ">
nike!caip!uw-beaver!ssc-vax!wanttaja@cad.Berkeley.EDU 
</A>&gt;
</address>
<i>
Wed, 29 Oct 86 09:49:53 pst
</i><PRE>

After graduating about ten years ago, I entered the Air Force as a
Satellite Systems Engineer.  I was assigned to a unit operating a
particular NORAD satellite system...no names, no mission statements,
please.  A buddy DID almost start World War III one night, though.

My job was real-time and non-real-time analysis of mission data
from the spacecraft; the end result of my analysis was to advice the NORAD
Senior Director of the validity of the data.  A lot of factors had to be
incorporated in my analysis...in "N" seconds, I had to take into account
which spacecraft had reported, its health and status, DEFCON level, and
"numerous other mission critical elements."  Nudge, nudge...

Anyway, the job was highly dependent upon the experience of the analyst,
as well as his intuition...we had to have a FEEL for what was right.

Three years after I joined the squadron, the unit was reassigned from the
Aerospace Defense Command (ADCOM) to the Strategic Air Command (SAC).  Now,
SAC is the largest producer of automatic humans in the free world.  In a
word, SAC is checklist crazy...every task is broken down to the largest
number of subtasks.

SAC treats its checklists as a way to eliminate the human element.  Training
two people to work as a team is unecessary...all they have to be able to do
is call off the proper steps from the checklist.  SAC uses simulators to
allow its people to practice every step, and to handle every contingency.
For instance, a missile launch officer has gone through the launch procedure in
the simulator dozens of times before he is placed in an actual control
room.  The opening sequence in WAR GAMES is an example of what SAC is trying
to avoid:  The crew must automatically perform its tasks, spending no time
thinking about what the consequences are.  The crew must not bring their
emotions into play, nor even any additional knowledge they must have.
Every action must be governed by a checklist step.

You can see what our problem was...how to you place "intuition" and "gut feel"
onto a checklist?  Our job could not be performed by an automaton; we had to
call on experience and a deep understanding of system operation in order to
provide our assessment.  We argued, to no avail.  We had to have a checklist.
So we thought and thought, and broke the analysis task into as many
subelements as we could.  The last subelement was OPERATOR INTUITION.

Did SAC complain?  Nahhhhh...they never read the thing.  Occasionally
they'd show up for Operational Readiness Inspections.  During the
simulation, their checklist called for them to verify that we had our EVENT
ASSESSMENT checklist open.  Their checklist didn't call for them to
actually read our checklists...

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Assessing system effectiveness
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%mycroft@GSWD-VMS.ARPA">
preece%mycroft@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Fri, 31 Oct 86 10:01:55 CST
</i><PRE>

  [Dave Benson said that we should assume that an overloaded system will
  fail to handle any load at all.  I said an overloaded system could
  fail by handling no load, by handling its ceiling load and no more, or
  by handling its ceiling load and some decreasing part of additional
  traffic, and that we had no grounds for making that decision until a
  design, designers, and implementors existed.  Dave Benson said history
  tells us no system works without extensive realistic testing.]

If that summary sounds as if I thought Dave's remarks didn't address
what I said, that's correct.  I know of systems (not military systems,
with which I have have no experience) which demonstrate each of
those overload behaviors; I'm sure he does, too.  Overload behavior
is something that certainly can be stated explicitly as part of the
design and it's generally a pretty easy thing to simulate, compared
with the problem of simulating all possible inputs.  Note that I
am talking ONLY about response to overload, which is where the
discussion started.

I have plenty of doubts about many parts of the SDI program and I don't for
a minute expect that they will come up with a design or an implementation
that I will be willing to trust.  But Dave's original statement that "We
should assume that a system capable of handling N targets/sec will, when
presented with 2N targets, fail to handle any at all." is without basis and
his further statements referring to 30 years of software development history
offer nothing to support it.  Systems fail in many ways and there is no
reason to assume a particular failure mode without looking at the design and
implementation.  Worst-case assumptions are often useful, but in this case
they are unenlightening; we all know that in the worst case nothing works,
all the missiles fall through, and c'est ca.  I'm a lot more interested in
the probability of that worst case than in the fact that that IS the worst
case.  Dave did not say anything to convince me that an arbitrary system's
most likely response to overload is total failure; in my own experience
(admittedly only 20 years) more systems respond to overload with degraded or
limited performance than with total failure.

scott preece  gould/csd - urbana
uucp:	ihnp4!uiucdcs!ccvaxa!preece

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Computers in elections
</A>
</H3>
<address>
Jekyll's Revenge 264-7759 MKO1-2/E02
&lt;<A HREF="mailto:hyde%abacus.DEC@decwrl.DEC.COM  ">
hyde%abacus.DEC@decwrl.DEC.COM  
</A>&gt;
</address>
<i>
Friday, 31 Oct 1986 11:32:53-PST
</i><PRE>

The latest issue of DATAMATION has an excellent article on computerized vote
counting.  I recommend it to all.  It addresses problems with punch card
voting, but doesn't address the problems with computerized voting booths.
The three biggest problems with computerized voting booths are secrecy of
internal operation, lack of recount capability, and inability for the voters
to ensure that the computer votes as instructed.  Some of the people whose
names are in the article were at BU in August for the Symposium on Security
and Reliability of Computers in the Electoral Process.  These people are
doing great work, especially considering the fact that they are generally
financing it on their own.

I am presently compiling some poll watching guidelines for computerized
elections.  I can send a copy to anyone who will be a poll watcher on Tuesday.

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
17th FAULT-TOLERANT COMPUTING SYMPOSIUM
</A>
</H3>
<address>
Flaviu Cristian 
&lt;<A HREF="mailto:FLAVIU@ibm.com">
FLAVIU@ibm.com
</A>&gt;
</address>
<i>
29 October 1986, 09:54:36 PST
</i><PRE>

   [Remembering that the RISKS Forum is aimed at fostering better systems
    in the future as well as exposing limitations with existing systems,
    it is appropriate to include the following item.  PGN]

                     CALL FOR PAPERS
                         FTCS17
          THE SEVENTEENTH INTERNATIONAL SYMPOSIUM
              ON FAULT-TOLERANT COMPUTING
       sponsored by IEEE Computer Society's Technical
          Committee on Fault-Tolerant Computing
             Pittsburgh, PA, July 6-8, 1987
               **** NOTE NEW DATES ****

The Fault-Tolerant Computing Symposium has, since 1971, become the most
important forum for discussion of the state-of-the-art in fault-tolerant
computing.  It addresses all aspects of specifying, designing, modeling,
implementing, testing, diagnosing and evaluating dependable and
fault-tolerant computing systems and their components.  A special theme of
the conference will be the practical application of fault-tolerance to the
design of safety critical systems, real-time systems, switching systems and
transaction systems.

Papers relating to the following areas are invited:

a) design methods, algorithms for distributed fault-tolerant software systems,

b) specification, design, testing, verification of reliable software,

c) specification, design, testing, verification, diagnosis of reliable hardware

d) fault-tolerant hardware system design and architecture,

e) reliability, availability, safety modeling and measurements,

f) fault-tolerant computing systems for safe process control, digital 
   switching, manufacturing automation, and on-line transaction processing.

Authors should submit 6 copies of papers before the submission deadline
December 5, 1986 to the program co-chairmen: Flaviu Cristian, IBM Research
K55/801, 650 Harry Rd., San Jose, Ca 95120-6099, USA, and Jack Goldberg, SRI
International, 333 Ravenswood Ave., Menlo Park, Ca 94025.  Papers in areas
a, b, and f should be sent to F. Cristian, and papers in areas c, d, and e
to J. Goldberg.

Papers should be no longer than 5000 words, should include a clear
description of the problem being discussed, comparisons with extant work,
and a section on major original contributions.  The front page should
include a contact author's complete mailing address, telephone number and
net address (if available), and should clearly indicate the paper's word
count and the area to which the paper is submitted.  Submissions arriving
late or departing from these guidelines risk rejection without consideration
of their merits.

The Symposium chair and vice-chair are John Shen and Dan Siewiorek, both
from Carnegie Mellon University, USA.  The program co-chairmen are: Flaviu
Cristian, IBM Research, USA, and Jack Goldberg, SRI International, USA.
Publicity chairman is Bella Bose, Oregon State Univ., USA. 
                                           [Program Committee omitted here.]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.01.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.03.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-80</DOCNO>
<DOCOLDNO>IA012-000125-B044-90</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.3.html 128.240.150.127 19970217005803 text/html 13576
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:56:35 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 3</TITLE>
<LINK REL="Prev" HREF="/Risks/4.02.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.04.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.02.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.04.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 3</H1>
<H2> Monday, 3 November 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
The Big Bang at the London Stock Exchange 
</A>
<DD>
<A HREF="#subj1.1">
Jonathan Bowen
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  UK computer security audit 
</A>
<DD>
<A HREF="#subj2.1">
Robert Stroud
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Austin's computer-controlled traffic lights 
</A>
<DD>
<A HREF="#subj3.1">
Alan Wexelblat
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Computers and Medical Charts 
</A>
<DD>
<A HREF="#subj4.1">
Elliott S. Frank
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
The Big Bang at the London Stock Exchange
</A>
</H3>
<address>
Jonathan Bowen 
&lt;<A HREF="mailto:bowen%sevax.prg.oxford.ac.uk@Cs.Ucl.AC.UK">
bowen%sevax.prg.oxford.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Tue, 28 Oct 86 17:24:41 GMT
</i><PRE>
Organization: PRG, Oxford University, UK

Headlines in `The Independent' (new British `serious' newspaper) on Tuesday
28 October 1986:

        Stock Exchange computers fail under strain
        Shambles as the Big Bang hits the floor

THE CITY'S "Big Bang" exploded after just 29 minutes' trading yesterday
morning when the computers buckled under the strain.  The Stock Exchange
system which speads information to dealers and investors went off the air at
8.29 am, to be followed 18 minutes later by the central dealing computer,
the Stock Exchange Automated Quotations system known as SEAQ.  By that time,
market makers were already experiencing problems in putting their prices
into the system, and some of them had ceased to trade at all. The failures
were blamed by the Stock Exchange on brokers overloading the system, both to
look at their competitors prices and out of pure curiosity.

Jonathan Bowen, Programming Research Group, Oxford University

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
UK computer security audit
</A>
</H3>
<address>
Robert Stroud 
&lt;<A HREF="mailto:robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Thu, 30 Oct 86 12:27:45 gmt
</i><PRE>

There was an item in today's Independent (a new UK paper) about the results
of a security audit of 50 UK companies. Sadly, the results will be all too
familiar to RISKS readers. When will practice catch up with theory?

Robert Stroud, Computing Laboratory, University of Newcastle upon Tyne.
UUCP ...!ukc!cheviot!robert

  [Sorry for the absence of a specific reference to the original report.  PGN]
  ["It is probably one of those expensive management consultancy things
   costing ten pounds a page!" - Robert]

      ============================================================

  Reproduced without permission from The Independent 30th October 1986 p.16
  
  "How Fred lets the fraudsters in" (c) Newspaper Publishing PLC
  by Michael Cross
  
  Frauds involving computers will cost British companies 40m pounds next year,
  the insurance broker Hogg Robinson said yesterday. The culprits are not
  usually teenage computer wizards but disgruntled employees and previous
  employees.
  
  Hogg Robinson's report, an audit of 50 firms, suggests that British
  companies are extraordinarily careless about looking after their computers.
  Apart from fraud, the dangers are sabotage, damage caused by carelessness,
  and run of the mill disasters such as fire or flood.

  The chink in most computers' armour is the password. All but three sites the
  auditors examined used passwords to control access to computers. Most were
  useless. When people choose their passwords, they often pick names of
  spouses or pets. These are easy for colleagues to guess. America's favourite
  password is "love", closely followed by "sex". Top of the list in Britain is
  "Fred".
  
  Other favourites, said David Davis, director of research at Hogg Robinson,
  are "pass", "God", "genius" and "hacker". "If a hacker tries these he will
  get through 20 per cent of the time", Mr Davis said.
  
  Passwords are particularly vulnerable when they remain unchanged for a long
  time.  The chairman of one major company the auditors investigated had kept
  the same password for five years. It was "chairman".
  
  Another danger point is in computers that allow unlimited guesses at
  passwords.  One in 10 of the sites surveyed allowed any number of attempts
  to "log in". The really secure passwords are the dual-key encrypted type.
  These are codes distributed in two parts, which link up inside a computer.
  But only two or three computers, all government installations, carry such
  protection in Britain.
  
  Despite the vulnerability of passwords, the report suggests that few
  computers fall victim to outside "hackers". Three of the sites inspected
  showed signs that hackers had gained access to the computers through
  external telephone lines.  Dr Frank Taylor, chairman of the British Computer
  Society's security committee, said there is no real evidence that hackers
  are causing large financial losses.
  
  Dr Taylor's horror stories have a more humdrum flavour. One concerns a
  building supplies company which had no security on its counter terminals.
  Crooked employees were able to give huge discounts to friends, and the
  company went broke. Another company lost its data - and nearly everything
  else - when lightning struck a power cable.
  
  Computers face a host of dangers from everyday activities, the report says.
  Mr Davis said that computers are designed to be operated by, "a race of
  supermen who do not eat, drink or smoke". He has a useful tip for computer
  people who cannot give up human habits; drink black coffee rather than
  white. It causes less damage if spilt.
  
</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Austin's computer-controlled traffic lights
</A>
</H3>
<address>
Alan Wexelblat 
&lt;<A HREF="mailto:wex@mcc.com">
wex@mcc.com
</A>&gt;
</address>
<i>
Mon, 3 Nov 86 13:07:27 CST
</i><PRE>

A while back I reported that a lighning strike had taken out the computer
that controlled the synchronization of Austin's downtown traffic lights.
(Local control units took over - only two lights went "on the blink".)

I recently learned that there was more to the story.  It seems that Austin
has a "traffic flow program" embedded in that system that changes the
durations of red/yellow/green lights for given intersections based on the
time of day.  The goal is to give more time for people to get intown in the
mornings and out of town in the evening.  The local control units fall back
to an "equal time for all" scheme, regardless of time of day.

Since the power loss occurred late in the afternoon, evening rush hour
traffic was snarled more than usual.  In addition, there were several near-
accidents caused by people who "knew" that the yellow light would be long
enough (based on months of commuting experience).

Alan Wexelblat
UUCP: {seismo, harvard, gatech, pyramid, &amp;c.}!ut-sally!im4u!milano!wex

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Computers and Medical Charts
</A>
</H3>
<address>
Elliott S. Frank
&lt;<A HREF="mailto:amdahl!esf00@decwrl.DEC.COM ">
amdahl!esf00@decwrl.DEC.COM 
</A>&gt;
</address>
<i>
Mon, 3 Nov 86 12:44:14 PST
</i><PRE>

The following items were posted to the delphi digest on mod.mac.  The issues
have been covered before in mod.risks, but the example is worth noting.

Elliott S Frank    ...!{ihnp4,hplabs,amd,nsc}!amdahl!esf00     (408) 746-6384

     ==============================

Delphi Mac Digest          Thursday, 30 October 1986      Volume 2 : Issue 55
From: PIZZAMAN (14213)
Subject: Computers and Medical Charts
Date: 26-OCT 16:26 Business Mac
 
The most amazing thing happened at the hospital yesterday. I was accused of
unethical behavior because I used my computer to prepare a conference for the
Department of Surgery!

Let me explain.... I am the Clinical Coordinator of the Department of
Surgery at a rural community hospital. This is a voluntary job, in addition
to my regular practice of surgery. My responsibilities include the preparing
of the mortality and morbidity conferences each month, as well as trying to
put together educational topics of interest for the other surgeons. Having
trained at a University Hospital in Philadelphia, I enjoy doing this teaching.

In order to prepare for one of these conferences, I took my Tandy 100 to the
record room, and took my notes on it. When I got to the office, I plugged
the Imagewriter cable into the RS-232 connector on the back of the Tandy,
and using Smartcom II, loaded the information into the Mac for work
processing, spread sheeting, and graph creation.

Now, I am being accused of taking confidential information out of the
hospital in the form of patient records and doctors names! All I had on the
computer were my notes. The paranoid medical staff is afraid that having
this information in my "COMPUTER" is dangerous, in some way. Since I
consider my two computers just extensions of other work tools that I use, I
can't understand this. Would they be just as paranoid if I used a legal pad
to make notes instead of the computer?

By the way, the bylaws of the hospital allow for the use of records for
research, and I had permission from the President of the Medical Staff to do
the study in question.
 
Pretty amazing paranoia, huh? Do people really still fear computers this way?
Any physicians out there have similar experiences? Any legal advice?
 
     ==============================           

From: PEABO (14226)
Subject: RE: Computers and Medical Charts (Re: Msg 14213)
Date: 26-OCT 19:45 Business Mac

It might have something to do with Legislators, who tend to know even less
about computers than hospital staff.  I've read some stories about how some
corporations are getting concerned about what J. Q.  Middlemanager is taking
home to work on using his own computer after downloading from the company
mainframe.
 
peter
 
     ==============================

From: LAMG (14239)
Subject: RE: Computers and Medical Charts (Re: Msg 14213)
Date: 27-OCT 01:20 Business Mac
 
Yes, it's paranoid behavior, but no, it's not amazing, I'm afraid.  In my
institution (UCLA Dept. of Radiological Sciences) most of the data used for
teaching and research is in "machine readable" form at one time or another.
Clearly there is a valid issue related to the removal of confidential patient
records from the hospital (I don't know what the regulations are there) but
these would apply equally to data whether in handwritten, printed or machine
readable form.
 
You didn't say exactly who is objecting to your work and on what
grounds, but it sounds like they don't have a very good idea of what
you're using the computers for.  I can't give you legal advice though.
 
Franklin Tessler, M.D.
 
</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.02.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.04.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-81</DOCNO>
<DOCOLDNO>IA012-000125-B044-110</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.4.html 128.240.150.127 19970217005815 text/html 12197
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:56:45 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 4</TITLE>
<LINK REL="Prev" HREF="/Risks/4.03.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.05.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.03.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.05.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 4</H1>
<H2> Tuesday, 4 November 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Flawed Radars in Air Traffic Control 
</A>
<DD>
<A HREF="#subj1.1">
PGN/UPI
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  The Future of English (risks of technocrats, risks of word processors)        
</A>
<DD>
<A HREF="#subj2.1">
Martin Minow
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Flawed Radars in Air Traffic Control
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Tue 4 Nov 86 09:55:22-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

            FAA Says It Has Fixed Flawed Radar Systems

Santa Ana (UPI, 4 Nov 86; from the San Francisco Chronicle of that date, p. 40)

Malfunctions in key radar systems that track airliners in Southern California 
reached a hazardous level in recent years, but officials said yesterday that
the most serious problems have been found and fixed.  According to Federal
Aviation Administration reports obtained by the Orange County Register,
there were frequent breakdowns in the past four years in the Laguna Radar,
which monitors the area in a 200-mile radius around its perch east of San
Diego, and the San Pedro Radar, which scans a 200-mile circle around the
Palos Verdes Peninsula.  The systems monitor air traffic for Los Angeles
International Airport, John Wayne Airport and Lindbergh Field in San Diego.

The radar malfunctions grew critical enough that the FAA sent tecnicians
from Washington, D.C., to the Air Route Traffic Control Center in Palmdale
two weeks ago to monitor both systems and make adjustments.  Among the 
malfunctions were frequent disappearances of airplanes from radar screens
for 15 to 30 minutes and radar displays that show planes in a turn pattern
when they are actually on a straight course.

In some instances, the Register reported, air controllers saw aircraft
"jump" on their radar scopes, which made planes appear to have changed
direction when they had not.  In others, radars tracking plane descents in
an especially busy corridor showed jets traveling faster than they actually
were.  In addition, important altitude data that helps controllers avoid
midair collisions frequently disappeared from radar screens.  

FAA official Russell Park confirmed the problem and acknowledged that the
situtation could have been hazardous.  He said the malfunctions played no
part in any collisions, including that of an Aeromexico DC-9 and a small
plane over Cerritos on August 31.  He said the troubleshooting team from
Washington was able to fix the most serious malfunctions quickly.

                         [Quickly?  But this went on for FOUR YEARS?  PGN]

   [By the way, the November 1986 issue of the IEEE SPECTRUM is devoted to
    "Our Burdened Skies", and is a goldmine for those of you interested in
    our air transportation system.]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
The Future of English (risks of technocrats, risks of word processors)
</A>
</H3>
<address>
Martin Minow, DECtalk Engineering, ML3-1/U47 223-9922
&lt;<A HREF="mailto:minow%regent.DEC@decwrl.DEC.COM  ">
minow%regent.DEC@decwrl.DEC.COM  
</A>&gt;
</address>
<i>
29-Oct-1986 1645
</i><PRE>

[Prediction]
THE FUTURE OF LANGUAGE

[By Anthony Burgess.  From "2020: A Vision of the Future," in the 17 June 1986
"London Telegraph Sunday Magazine," a special issue devoted to the future.
Burgess is the author of "A Clockwork Orange," "Earthly Powers," "Napoleon
Symphony," "Nineteen Eighty-Five," "Re Joyce," and many other books.]

  Prime ministers speaking to the nation still attempt, like Mrs. Thatcher, to
  use "Standard English" and a supraregional or classless accent.  By 2020
  they will not have to do that.  What they will have to do is speak a kind of
  English that denies the fact of education, avoids allusion to Shakespeare or
  the Bible, and, where it rises above the level of conversational usage,
  gains a pose of learning and authority from the use of technological terms.
  At the same time, with a kind of ultimate authority seeming to be vested in
  the hard but high-flown language of science, there will be more mendacity
  and evasion dressed up as technology.  The Pentagon has already shown the
  way with such expressions as "anticipatory retaliation," which does not
  sound like striking the enemy without due declaration of war.
  
  America's language is already far advanced in the direction of combining the
  loose colloquial with the cant terms of the technical specialists -- who
  include sociologists and psychologists, as well as cybernetics experts and
  aerospace men.  When not being expertly evasive ("at this time the nuclear
  capability of this nation is not anticipated to assume a role of preemptive
  preparatory action"), it is slangy, unlearned, unwitty, inelegant.  At its
  most disconcerting it combines two modes of discourse: "Now we zero in on
  the nitty-gritty of the suprasegmental prosodic feature and find that we're
  into a different ball game."  It is already, perhaps, the matrix of British
  English of 2020.
  
  As for the sound of the English of 2020, some of its characteristics are in
  active preparation.  Assimilation -- a natural enough process, which,
  however, must never be allowed to go too far -- is drawing a lot of vowels
  to the middle of the mouth, where the phoneme called schwa (the second
  syllable of "butter," "father;" the first a in "apart") waits like a spider
  for flies.  The "a" of "man" is already a muzzy, neuter sound with the
  young.  Assimilation of consonants is giving us "corm beef: and "tim
  peaches" and "vogka" (Kingsly Amis spotted these in the early seventies).
  Grammar has been simplified, so that most sentences are constructed to the
  "and...and...and..." Biblical formula (hypotactic, to be technical).  Losing
  Latin in our schools, we are finding it hard to understand Milton and to
  appreciate the beauties of the periodic sentence.
  
  This will get worse.  The English of 2020 will combine structural
  infantilism with hard-nosed technology.  It will be harsh, and it will lack
  both modesty and humor.
  
  The written word is only a ghost without the solidity of the spoken word to
  give it substance, but to many it seems to be the primary reality.  After
  all, the voices of dead poets and novelists survive only as black marks on
  white paper.  Still, writers write well only when they listen to what they
  are writing -- either on magnetic tape or in the auditorium set silently in
  their skulls.  But more and more writers -- not only of pseudoliterature but
  of political speeches -- ignore the claims of the voice and ear.
  
  I think that, with the increasing use of the word processor, the separation
  of the word as sound from the word as visual symbol is likely to grow.  The
  magical reality has become the set of signs glowing on a screen: this takes
  precedence over any possible auditory significance.  The speed with which
  words can be set down with such an apparatus (as also with the electric
  typewriter), the total lack of muscular effort involved -- these turn
  writing into a curiously nonphysical activity, in which there is no manual
  analogue to the process of breathing out, using the tongue, lips, and teeth,
  and accepting language as a bodily exercise that expends energy.
  
  What is wrong with most writing today is its flaccidity, its lack of
  pleasure in the manipulation of sounds and pauses.  The written word is
  becoming inert.  One dreads to think what is will be like in 2020.
  
  I have never yet ventured a prophecy that came true.  In my little novel
  "Nineteen Eighty-Five" I get nothing except the name of the son of the Prince
  of Wales.  It is altogether possible that, rejecting the easy way of pop
  music, drugs, and television, the youthy of the near future will stage a
  reactionary revolution and go back to Latin, Shakespeare, and the Bible and
  insist on school courses in rhetoric.  But I do not think it likely.
  
[It should be noted, perhaps, that the Boston Globe recently published an
article that stated the offering of Latin in public high schools has increased
markedly in the last five years.  MM]

Burgess notes that word processors make writing too easy.  You can see the
result in the bloated junk novels, all over 300 pages long, that seem to be
designed only to fill waiting time at airports.

One of my colleagues once edited a computer textbook written by one of the
more important educators in the field (and he is a well-known writer
himself).  He said that "he nearly wore out the delete-paragraph key on the
word-processor."  The bad news is that there seems to be no real interest in
good editing in the commercial marketplace.  I would claim that this is a
direct result of the ease of writing with word processors.
                                                                 Martin

  [In a recent memo, EWD976-0, 10 Sep 86, Edsger W. Dijkstra makes a plea
   against bad writing.  One of his suggestions for making it easier on your 
   readers was this: ``Avoid if possible using one-letter identifiers that are 
   all by themselves words in the language of the surrounding prose, such as 
   "U" in Dutch and "a" and "I" in English, as they may confront you with
   unpleasant surprises.  (There is a page by David Gries, in which "I"
   occurs in three different roles: as a personal pronoun, as identifier for
   an invariant and as a Roman numeral! Of course, the reader can sort this
   confusion out, but it is better avoided.)''  EWD via PGN]
  
</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.03.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.05.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-82</DOCNO>
<DOCOLDNO>IA012-000125-B044-127</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.5.html 128.240.150.127 19970217005826 text/html 14937
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:56:56 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 5</TITLE>
<LINK REL="Prev" HREF="/Risks/4.04.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.06.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.04.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.06.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 5</H1>
<H2> Wednesday, 5 November 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Computer causes chaos in Brazilian Election 
</A>
<DD>
<A HREF="#subj1.1">
Jonathan Bowen
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Risks of FAA Philosophy ? 
</A>
<DD>
<A HREF="#subj2.1">
Robert DiCamillo
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Computers and Medical Charts 
</A>
<DD>
<A HREF="#subj3.1">
Christopher C. Stacy
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: Insurgent Squirrel Joins No-Ways Arc 
</A>
<DD>
<A HREF="#subj4.1">
rsk
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Micros in Car engines 
</A>
<DD>
<A HREF="#subj5.1">
Peter Stokes
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Computer causes chaos in Brazilian Election
</A>
</H3>
<address>
Jonathan Bowen 
&lt;<A HREF="mailto:bowen%sevax.prg.oxford.ac.uk@Cs.Ucl.AC.UK">
bowen%sevax.prg.oxford.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Tue, 4 Nov 86 15:23:54 GMT
</i><PRE>

  From Daily Telegraph, Monday November 3rd:

  ``Hundreds of thousands of Brazilians may not be able to vote in the
  forthcoming general election because of bureacratic bungles. ... only
  70% of the electorate have been issued with the essential voting card.
  .... queues and frayed tempers are a result of a 30 million pound [c $42
  million] computerisation programme which was designed to streamline
  voting and eliminate fraud. ... Flaws in the system only became evident
  when distribution started three weeks ago. ...  [the computer] has been
  programmed to cancel all duplicate applications in order to weed out
  fraudulent "phantom" voters. ... while it showed that 1,400 dead people
  had voted for the mayor in the north-eastern town of Teresinha last
  year, and 100,000 falsified cards were in circulation in the southern
  state of Santa Catarina, it also cancelled legitimate names.
  Programmers overlooked that twins are born on the same day to the same
  parents. Consequently, the voting rights of an estimated 70,000 twins
  were cancelled. The Federal Electoral Tribunal in Brasilia is currently
  wading through 140,000 appeals, including the case of a certain Jose
  Francisco, who says all his 14 brothers were baptised with identical
  names. ... It is hoped that all those eligible will have their cards by
  the 15th. Those that do not will have to pay a 4 pound [c $5.50] fine
  or brave more queues and bureacracy to prove that they both exist and
  have the right to vote.''
   
Surely these sorts of problems have occurred before in other
countries.  What methods are available, if any, the avoid such risks
using computers without human intervention? Are such problems a
result of there not being *enough* computerised information on
the population to start with?
   
</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Risks of FAA Philosophy ?
</A>
</H3>
<address>
Robert DiCamillo 
&lt;<A HREF="mailto:rdicamil@cc2.bbn.com">
rdicamil@cc2.bbn.com
</A>&gt;
</address>
<i>
Wed, 5 Nov 86 16:18:19 EST
</i><PRE>
To: risks@csl.sri.com

The recent entries in the  Risks  Journal  about  collision  avoidance  systems
reminds me of a comment a professor once made to me about the philosophy of the
FAA. For many years this professor in  the  Engineering  Design  Department  at
Tufts  University  worked  on  a  better  engineered cockpit layout and display
system. This included improvements in human factoring,  multi-function  graphic
displays  to  eliminate the number of indicators needed, and more functionality
in the cockpit to allow the pilot to detect and avoid other aircraft. 

After several years of work, where along the way  many  graduate  students  had
also  contributed, the system was presented to the FAA and turned down for what
the inventors could not fathom as valid  technical  reasons.   The  system  was
better,  easier  to  use,  and  provided  the pilot with more functionality and
autonomy over his aircraft and flight path.

The professor noted that the catch was the  FAA's  "apparent"  philosophy  that
they  don't  want  the pilots to have more autonomy in determining their flight
path and collision avoidance, as this task  is  considered  the  realm  of  the
ground (air traffic) controllers. His opinion was that any system that included
decentralization from ground control would be rejected because the FAA does not
want to threaten the job security of air traffic controllers.

This  political  "unspoken"  philosophy  of  the  FAA would still seem to be in
effect, providing you are willing to believe that technical  reasons  (good  or
bad)  will be used to defend such political objective(s). Perhaps the Honeywell
System is just another casualty.

This of course leads to the question of policy making. Does anyone know if  the
FAA  charter  contains  any  such  implicit  endorsement pro or con relative to
evaluating technology ? Does the FAA even have an  agreed  upon  philosophy  in
this regard that is published and accessible to the public ?  Or does some high
ranking, politically inclined, individual have the absolute veto  power  within
the government (FAA or otherwise) ?

This  seems  like  one  of those issues that will be difficult to substantiate,
most suitable to think about while flying in planes.  Note  that  the  November
1986 issue of the IEEE Spectrum is devoted to "Our Burdened Skies".  Although I
haven't read it yet, I will be interested to see if  there  is  any  reflection
(real or ghost) of such an FAA philosophy.
        					- Robert DiCamillo

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Computers and Medical Charts
</A>
</H3>
<address>
Christopher C. Stacy 
&lt;<A HREF="mailto:CSTACY@JASPER.Palladian.COM">
CSTACY@JASPER.Palladian.COM
</A>&gt;
</address>
<i>
Wed, 5 Nov 86 21:33 EST
</i><PRE>
To: Elliott S. Frank &lt;amdahl!esf00@decwrl.DEC.COM&gt;
cc: risks@sri-csl.ARPA

I talked to an R.R.A. today to get an opinion on PIZZAMAN's story
about taking the medical records information home on his computer.

The hospital sets up regulations to control access to the medical records,
which are carefully guarded as sensitive confidential information. The
physical record is considered to be owned by the hospital, and the
information is considered to be owned by the patient.  Typically, physicians
are allowed to take copies of medical records to their offices or home in
order to perform work directly related to patient care.  Preparing research
reports is generally considered to be within that scope.

People are generally not allowed to remove the original physical record from
the hospital, but copies may be OK.  The administrator I talked to didn't
think that it was significant that the information was copied using a
computer.  Of course, the physician has a serious responsibility to protect
the information from perusal by random persons, including his family,
visitors to his office, people logging in to his computer over the phone, etc.

So, the opinion of one medical records administrator seems to concur with
that of Dr. Tessler; the people at that hospital probably were over-reacting
inappropriately.

I don't know how well most medical personnel understand what computers
are; the person I talked to currently works for a company that writes
software for hospital administration.

So, this situation presents the familiar risk of paranoid confusion.
However, I would identify the major risk here as related to computer and
telecommunications security.  This is the same concern as for the hospital
which keeps their actual medical records online.  The two risks can be
related, of course.

If people have other questions or thoughts about this, I would be glad
to forward them along to my friend; she was interested that people
were discussing this sort of thing.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: Insurgent Squirrel Joins No-Ways Arc
</A>
</H3>
<address>
Wombat 
&lt;<A HREF="mailto:rsk@j.cc.purdue.edu">
rsk@j.cc.purdue.edu
</A>&gt;
</address>
<i>
Wed, 5 Nov 86 21:31:22 EST
</i><PRE>

Ross's story reminds me of a similar incident which took place at Purdue
about five years ago; a misplaced rodent [in a power transformer] caused
most of the campus to lose power for about half a day.  The university
physical plant crews actually aggravated the situation while trying to fix
it by mis-diagnosing the trouble, in ways that have never been clear.  One
of the physical plant officials was quoted on the front page of the Exponent
(Purdue's daily) as saying "You've got to understand, with electricity you
never quite know what's going on".  I'm sure he was thrilled when a group of
EE students reprinted that quote on T-shirts and proceeded to sell them at a
brisk pace for the rest of the semester.  [I still wear mine!]

Rich Kulawiec, rsk@j.cc.purdue.edu

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Micros in Car engines
</A>
</H3>
<address>
Peter Stokes 
&lt;<A HREF="mailto:stokes%cmc.cdn%ubc.csnet@CSNET-RELAY.ARPA">
stokes%cmc.cdn%ubc.csnet@CSNET-RELAY.ARPA
</A>&gt;
</address>
<i>
Wed, 5 Nov 86 11:46:07 pst
</i><PRE>

My 1986 Ford Mustang has (according to the literature) a micro-processor
controlled engine.  When driving it, you can tell that the engine RPM's
are contolled by something "intelligent" :

 - the high idle when cold to normal idle when warm transition has a
   distinctive change sequence as the engine warms up and this response
   is IDENTICAL every morning as I drive to work.  

 - If you hit the accelerator pedal and let go quickly, the engine
   speed returns to normal in about 3 distinctive steps: 
     1: a sharp drop of several hundred RPM's, 
     2: a smoother drop to very near the idle speed, and finally, 
     3: a small adjustment to the true idle speed.

 - If you disengage the clutch while the car is moving (first step 
   in gearing down), the engine speed drops quickly to a low of 
   200 RPM's (I can sometimes feel it shudder) and then the processor 
   corrects this with a "shot of gas".  If you leave your foot on the 
   clutch and just coast, you can observe the tachometer settle on the 
   idle speed after a small amount of overshoot and undershoot.

 - and finally, if you try to stall the car (starting off in first 
   gear without pushing the gas for example), the processor responds by
   trying to keep the engine speed at idle speed.

My Question... What are the risks in buying and driving an automobile with
               a computer controlled engine?

       Safety:  What are the odds of a malfunction causing acceleration?
  Performance:  Is this a feature?  Will the benefits of the microprocessor
                control continue to serve as the engine grows old and changes?
      Service:  Can a "Saturday Morning Mechanic" still tune his/her car or 
                is specialized equipment now a pre-requisite for the job?
       Safety:  Can the control over the engine be affected by an external 
                source (e.g. radio transmitter)?  I have noticed erratic 
                engine idle while in an automatic car wash....

Peter Stokes                          
Envoy100: cmc.vlsiic                       (...usual disclaimer...)
CDNnet:   stokes@cmc.cdn
BITNET:   stokes@qucdncmc.bitnet

  [...probably not much risk in BUYING one, but DRIVING ONE is another matter.
  Since you probably do not read every line of RISKS, let me remind you of the
  following cases, summarized in RISKS-4.1.  (The Mercedes case was noted in
  <A HREF="/Risks/2.12.html">RISKS-2.12</A>.)  PGN]

  AUTOMOBILES:
  Mercedes 500SE with graceful-stop no-skid brake computer left 368-foot 
    skid marks; passenger killed (SEN 10 3)
  Sudden auto acceleration due to interference from CB transmitter (SEN 11 1);
  Microprocessors in 1.4M Fords, 100K Audis, 350K Nissans, 400K Alliances/
    Encores, 140K Cressidas under investigation (SEN 10 3)
  El Dorado brake computer bug caused recall of that model [1979] (SEN 4 4)
  Ford Mark VII wiring fires: flaw in computerized air suspension (SEN 10 3)

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.04.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.06.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-83</DOCNO>
<DOCOLDNO>IA012-000125-B044-145</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.6.html 128.240.150.127 19970217005844 text/html 12618
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:57:07 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 6</TITLE>
<LINK REL="Prev" HREF="/Risks/4.05.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.07.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.05.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.07.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 6</H1>
<H2> Thursday, 6 November 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Computerized Reagan swamps Hospital with calls    
</A>
<DD>
<A HREF="#subj1.1">
David Whiteman via Werner Uhrig
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Aftermath of the Big Bang 
</A>
<DD>
<A HREF="#subj2.1">
Robert Stroud
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Fault tolerant computer manufacturer RISKS 
</A>
<DD>
<A HREF="#subj3.1">
Robert Stroud
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: Micros in Car engines 
</A>
<DD>
<A HREF="#subj4.1">
Don Wegeng
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Re:airplanes and risks, Risks 3.89 
</A>
<DD>
<A HREF="#subj5.1">
Udo Voges
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Computerized Reagan swamps Hospital with calls
</A>
</H3>
<address>
Werner Uhrig
&lt;<A HREF="mailto:werner@ngp.utexas.edu ">
werner@ngp.utexas.edu 
</A>&gt;
</address>
<i>
Thu, 6 Nov 86 05:14:08 CST
</i><PRE>
Really-From: ix21@sdcc6.ucsd.EDU (David Whiteman @ UCSD School of Medicine)

[Wed 5 Nov 86 15:38]

In the San Diego Union was an article from the AP newswire.  A tape
recording of President Reagan urging voters to go out and vote
Republican went haywire and continuously called phone lines at a
hospital in Texas.  Over a six hour period several of the hospital
phone lines received a phone call every three minutes.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Aftermath of the Big Bang
</A>
</H3>
<address>
Robert Stroud 
&lt;<A HREF="mailto:robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Thu, 6 Nov 86 16:51:48 gmt
</i><PRE>

Today (November 6th) is the first day that there has NOT been an item in my
paper about some computer failure or other problem resulting from the Big
Bang!  Accordingly, it seems like a good time to take stock, and report what
has been going on. But first I would like to deal with a comment Jerry
Saltzer made about my original posting.

I quoted a newspaper article which referred to the TOPIC terminal network
used by the Stock Exchange as being
 
  &gt; . . . six years old and considered fairly antiquated by today's standards.

and Jerry Saltzer replied

  &gt; I wonder who it is that considers that system as antiquated?  Another
  &gt; perspective says that a complex system that has been running for six
  &gt; years is just beginning to be seasoned enough that its users can have
  &gt; some confidence in it...

Well, it was Sir Nicholas Goodison, the chairman of the Stock Exchange who
said that TOPIC was antiquated rather than a computer scientist, although
perhaps he was influenced in this view by his technical staff. He was also
quoted as "having breathed a sigh of relief" when he heard that the problems
were only with TOPIC and not the brand new and expensive (18 million pounds?)
SEAQ system. To its credit, as far as I know, SEAQ has not failed yet, although
it has been taken out of service on several occasions when TOPIC has broken
in the interests of fairness - some people can access SEAQ directly and this
would give them an unfair advantage.

Anyway, TOPIC probably was very stable ("tried and trusted" was another phrase
in the article I quoted) until the Stock Exchange started tinkering with it
just before the Big Bang. Indeed, according to an article in Computing 
(Oct 30th), the Stock Exchange "opened an electronic gateway" allowing
access to detailed SEAQ price information by an additional 7,500 screens
at the last minute, effectively quadrupling the load. The rest is history.

As far as the technology being antiquated goes, I believe that TOPIC provides
a video feed (Teletext) whereas SEAQ provides a digital feed, and perhaps
it is significant that it was the TOPIC/SEAQ link that failed. Apparently,
video is much less convenient for wiring up a dealing room so that you can
switch information between desks flexibly.

So perhaps, in that limited sense TOPIC is indeed antiquated, but the real
problem was caused by the tinkerers as Jerry said. However, I think that to
some extent, the issue here is akin to the recent discussions about whether
software rots. What changes are the assumptions a system makes about its
environment, and the Big Bang certainly produced a radically new environment.

Anyway, back to what's been happening since last Monday (Big Bang day).
TOPIC went down again on Tuesday at lunchtime, but since then has been
reasonably well behaved thanks to various emergency measures designed to
minimise the load.  In particular, there are restrictions on the time of day
that you can enter new pricing information, and the page refresh rate has
been decreased. The Stock Exchange anticipated a 50% increase in demand, but
the load actually doubled. The Sunday Times quoted the figure of 2.2 million
page requests/day (as opposed to 500,000 on NASDAQ, a comparable system on
Wall Street). Two new computers have been ordered to add to the eight which
already support the network, and should increase the capacity by 50%. On
Monday, a malfunction replaced the British Aerospace share prices with those
for Bass (a brewery).

But perhaps the most serious problem is the backlog of unmatched trade reports
which will have to be sorted out before accounts can be settled. At the
weekend, after one weeks trading, there were 55,000 such unmatched records,
and even worse, despite working at it all weekend, only 2,000 were resolved.
By Tuesday, there were at least another 4,000 bringing the total to 59,000
and 15 security firms are reported to be having difficulties with the new 
settlement system.

It is difficult to put these figures into perspective without knowing the total
number of trades in a week. 55,000 seems pretty big to me, and is apparently
five times the average, but then 11,000 also seems pretty big! A semi-informed
guess would be that 55,000 represents about 30% of the weeks trading.

The main reason for the backlog is a power failure at a computer bureau last
week, but human error caused by lack of familiarity with the new systems,
and "insufficient decimal precision" have also been blamed.

So with nothing in the paper today, everything appears calm, but as the
Independent put it yesterday, "behind the scenes, officials are faced with 
nightmarish problems". The next big test of the system will be in December
when trading starts in 6 billion pounds worth of British Gas shares, the
biggest share issue ever, aimed at getting as many share holders as possible,
(7 million people have expressed an interest!). I think the dealers might
just be going back to the deserted trading floor of the Stock Exchange...

[Sources: Computing, Sunday Times, Independent]

Robert Stroud, Computing Laboratory, University of Newcastle upon Tyne.
UUCP ...!ukc!cheviot!robert

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Fault-tolerant-computer manufacturer RISKS
</A>
</H3>
<address>
Robert Stroud 
&lt;<A HREF="mailto:robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Thu, 6 Nov 86 17:05:57 gmt
</i><PRE>

This is my favourite Big Bang story and comes from the not entirely serious
Backbytes column of Computing (Oct 30th), reproduced without permission.

Robert Stroud, Computing Laboratory, University of Newcastle upon Tyne.
UUCP ...!ukc!cheviot!robert

  "Dog days for dire Stratus" (c) Computing
  
  As the blue touch paper for the Big Bang was finally lit this week,
  one company that must have allowed itself a sigh of relief is 
  fault-tolerant computer manufacturer Stratus.
  
  The trouble is that, while stockbroker companies are usually delighted
  with their Stratus machines, they [the companies] have an unfortunate
  habit of demonstrating the non-stop capabilities to clients by wrenching
  out a circuit board while the computer is in operation.
  
  Over recent months this habit has caused havoc at the UK customer
  assistance centre of Stratus in downtown Hounslow, Middlesex.
  
  All Stratus computers sold in the UK are linked to the centre by autodial
  modem. In the case of any part apparently 'failing', red lights flash
  in the centre and the requisite replacement is hastily dispatched,
  complete with service engineer.
  
  With the boom in fault-tolerant sales as financial institutions geared
  up for Big Bang, the 'cry wolf' situation began to get out of hand.
  Desperate engineers have now solved the problem by placing a timing delay
  in the alarm system to allow sticky fingered stockbrokers time to put the
  board back.
  
  With computer-based dealing starting for real this week and keeping
  everyone in the financial institutions well occupied, Backbytes is sure
  that the problem will disappear anyway.
  
</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: Micros in Car engines
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
6 Nov 86 11:43:53 EST (Thursday)
</i><PRE>
To: risks@CSL.SRI.COM
From: dw &lt;Wegeng.Henr@Xerox.COM&gt;

My father once told me about a semi-truck that was being used to test an
experimental microprocessor-controlled engine.  Apparently the micro would
crash (the computer, not the truck) whenever the truck was driven near the
local airport.  It was finally determined that the cause was EMI from a radar
transmitter at the airport.  Fortunately, when the micro crashed the engine
simply died, although one can easily imagine worse consequences.

I'm told that they now test their experimental systems by simply driving
them past the Voice of America transmitter near Cincinnati.  If the
system can operate under the conditions there, then they believe that it
should operate almost anywhere!

/Don                           [A new definition of "exhaustive testing"?  PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.05.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.07.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-84</DOCNO>
<DOCOLDNO>IA012-000125-B044-172</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.7.html 128.240.150.127 19970217005859 text/html 20891
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:57:26 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 7</TITLE>
<LINK REL="Prev" HREF="/Risks/4.06.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.08.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.06.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.08.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 7</H1>
<H2> Friday, 7 November 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Risks of RISKS 
</A>
<DD>
<A HREF="#subj1.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Details on the British Air Traffic Control computer outage 
</A>
<DD>
<A HREF="#subj2.1">
from Herb Hecht
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: UK computer security audit 
</A>
<DD>
<A HREF="#subj3.1">
Robert Stroud
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  USS Liberty 
</A>
<DD>
<A HREF="#subj4.1">
Matthew P Wiener
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Grassroots sneak attack on NSA 
</A>
<DD>
<A HREF="#subj5.1">
Matthew P Wiener
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  A variation of the Stanford breakin method 
</A>
<DD>
<A HREF="#subj6.1">
Arno Diehl
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Re: Subject: Computers and Medical Charts 
</A>
<DD>
<A HREF="#subj7.1">
Roy Smith
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  DDN Net breakdown (?) on 6 Nov 86? 
</A>
<DD>
<A HREF="#subj8.1">
Will Martin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj9">
  Re: Linguistic decay 
</A>
<DD>
<A HREF="#subj9.1">
Matthew P Wiener
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj10">
  Mechanical Aids to Writing 
</A>
<DD>
<A HREF="#subj10.1">
Earl Boebert
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Risks of RISKS
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Fri 7 Nov 86 22:20:56-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

"Nothing in the foregoing to the contrary notwithstanding," foresight is a
great thing.  I discovered a forgotten squirrelled safety copy of an
intermediate draft of RISKS-4.7 in another directory, and so am very happy
to be able to provide a recreation of RISKS-4.7 after all, despite the
previous message announcing what I thought was my first real panic in
running RISKS.  

    [BBOARD MANTAINERS:  PLEASE REMOVE PREVIOUS JUNK MESSAGES.  PGN]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Details on the British Air Traffic Control computer outage [6 Oct 86]
</A>
</H3>
<address>
&lt;<A HREF="mailto:Peter G. Neumann <Neumann@CSL.SRI.COM>       [SnailMail from Herb Hecht]">
Peter G. Neumann &lt;Neumann@CSL.SRI.COM&gt;       [SnailMail from Herb Hecht]
</A>&gt;
</address>
<i>
Thu 6 Nov 86 21:23:32-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

On Monday, 6 October 1986, the British air traffic control system was put
into manual backup mode by the crash of an IBM 9020D system that was
responsible for flight-plan data.  The computer in the London ATC Centre at
West Drayton (near Heathrow) crashed and was down for two hours -- with
traffic at Heathrow, Gatwick, and Manchester (among others) encountering
delays of up to six hours.  Because this system is also used by the military
air-traffic control system in West Drayton, British defenses were also
affected.

Overnight, ATC computer staff ``had loaded a new version of the main program
containing routine updates.  Software for running air-traffic control has to
be changed regularly to take account of new routes, aircraft types and
operating procedures for controllers.'' (Major updates are done once a year
at the London center.)  ``The changeover to a "new load" is normally a
tricky business.''  (One million lines of code run on a six-processor
system, networked with at least 10 other systems.)  

Unknown to the system programming staff, the software contained an
"unexpected flaw".  ``The centre was planning to connect an additional
computer to the existing 9020D complex.  Provision for the machine had been
made in the new software.  But that morning the computer was not connected
to the 9020D system.  Unaware of this, the program began collecting data
which should have been sent to the non-existent computer.  Data backed up
until alarms were sounded and supervisors decided to stop the system.  Staff
raced to adjust the 9020D and reload the old software.  Two hours later, the
machine was back in action.''  Meanwhile, operation reverted to the manual
flight-strip operation.

[Drawn from New Scientist, 9 October 1986, p. 13.  Thanks to Herb Hecht of
SoHaR]

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: UK computer security audit
</A>
</H3>
<address>
Robert Stroud 
&lt;<A HREF="mailto:robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Fri, 7 Nov 86 13:05:21 gmt
</i><PRE>

There was an item in [the 6 Nov 1986] Guardian about the same report that my
[earlier] submission described, so I can give you a better reference. The
report is called "Computer Security in Practice" and is published by the
Risk Management Services division of Hogg Robinson Ltd. who are a firm of
insurance brokers and presumably have an office in London.

The Guardian article paints a bleak picture of just how ill-prepared for
disaster the 50 or so companies visited are. 80% are not adequately
protected against fire, 96% are not protected against flood, (the two
exceptions had only installed detectors after sustaining water damage
previously), 70% don't have a stand-by power supply, 97% don't have enough
stand-by power to keep the user areas going as well as the hardware, etc.
Only 4% had fully calculated the cost of a disaster while 6% thought they
had a plan but either couldn't find it or admitted that it was hopelessly
out of date.

The article concluded with the observation that if these findings were
typical, most companies were doing the equivalent of walking across the
North Circular* with their eyes shut. However, Hogg-Robinson thought that
these results were probably not typical because at least these firms had
asked for a security risk audit. What about all the others?

* For the benefit of American readers who have not driven in London,
I should explain that the North Circular is a notorious inner ring-road.

[Source: Guardian 6th November, p.13]

I would be interested to know of any similar studies of American companies.

Robert Stroud, Computing Laboratory, University of Newcastle upon Tyne.
UUCP ...!ukc!cheviot!robert
   
   [By the way, the Newcastle mailer apparently ran amok sending this
    message -- among others.  I received 20 COPIES.  I probably would have 
    received more had not John Rushby been having the same experience with
    a message from Tom Anderson at Newcastle.  He finally made a call to 
    Tom, who evidently initiated a rectification of the problem.  I wonder
    whether the presence of two simultaneous messages from Newcastle to
    CSL.SRI had anything to do with the infinite loop! PGN]

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
USS Liberty (RISKS-4.1)
</A>
</H3>
<address>
Matthew P Wiener
&lt;<A HREF="mailto:weemba@brahms.berkeley.edu ">
weemba@brahms.berkeley.edu 
</A>&gt;
</address>
<i>
Fri, 7 Nov 86 01:17:57 PST
</i><PRE>

  REVISED SUMMARY ITEM FOR RISKS-4.1:
  $!! USS Liberty:  34 dead; injured; 3 warnings to withdraw lost? (SEN 11 5) 

There was a story, I believe in the Atlantic two years ago, giving some sort
of "official" Israeli explanation (as told by two highly respected Israeli
reporters) of how the Israelis came to "accidently" attack the USS Liberty,
involving sad coincidence after coincidence on their side, with things like
the properly identified US ship on the war map had its flag put aside
temporarily by General X, and then General Y took his place at that point,
and other such things.  While their version is almost certainly a complete
crock, it is intriguing that breakdowns in protocol are so freely invoked as
cover stories.

(Is this a new brand of computer/systems meta-risk?  That is, have we become
so inured to "computer error" that we will take such as an excuse blindly?
Note that I am not referring to using the computer as a scapegoat to avoid
blaming humans, just because there happened to be a computer in the
pipeline.  I wonder whether making a computer the catchall wholecloth
scapegoat on the principle that no one would check for the real story has
become SOP?)

In the long long run, by the way, the USS Liberty and the USS Pueblo
incident led to the scrapping of NSA's spy ship program, with unknowable
consequences.  Presumably the development of spy satellites and the like
filled the gap, but again, who really knows.  Trying to measure the risks
associated with intelligence can be well nigh impossible.

Actually, breakdowns in protocol are common in diplomacy.  There was a flap
some years ago about an anti-Israel vote by the US in the UN that was blamed
on such.  Cryptographic failure could have been responsible, but that would
never be admitted.

Speaking of which, successful cryptanalysis can lead to striking diplomatic
victories in sensitive treaties.  Of course, the military impact of
cryptanalysis is potentially unlimited.

These particular incidents do not really involve computers per se, although
the mentality is identical.  

ucbvax!brahms!weemba	Matthew P Wiener/UCB Math Dept/Berkeley CA 94720

                   [The above contribution was excerpted from two informal 
                    private communications (with permission).  It was not
                    originally intended as a RISKS message.   PGN]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Grassroots sneak attack on NSA
</A>
</H3>
<address>
Matthew P Wiener
&lt;<A HREF="mailto:weemba@brahms.berkeley.edu ">
weemba@brahms.berkeley.edu 
</A>&gt;
</address>
<i>
Fri, 7 Nov 86 04:34:58 PST
</i><PRE>

This past week, a rather bizarre attempt to annoy NSA via computer has
begun on USENET.  Several people have started inserting cute words like
"crypt" or "terror" or "CIA" in their signatures in an attempt to over-
load NSA's automatic grep for cute words in overseas traffic.  Consider-
ing the minuteness of the added load, and the likelihood that NSA already
filters out obvious traffic like the net, the effort is nothing more than
a good old fashioned American form of protest.  Even though it is using
(a trivial amount of) OPM to pay for a symbolic sabotage, I love it.

But obviously uglier scenarios can be imagined.  Is a grep-bomb possible?

ucbvax!brahms!weemba	Matthew P Wiener/UCB Math Dept/Berkeley CA 94720

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
 A variation of the Stanford breakin method
</A>
</H3>
<address>
    Arno Diehl 
&lt;<A HREF="mailto:DIEHL%v750%germany.csnet@RELAY.CS.NET">
DIEHL%v750%germany.csnet@RELAY.CS.NET
</A>&gt;
</address>
<i>

</i><PRE>

We just installed some SUN workstations (UNIX 4.2bsd) connected to 
an ethernet using TCP/IP protocols. 

We learned from the stanford breakin to be extremely careful when using
".rhosts". So we only entered such workstations into ".rhosts" located
in the office of trusted users.

One night a student operating a SINIX workstation experimented with
TCP/IP. He configured his machine to use the IP address of a trusted
host and he entered the username of a trusted user into "/etc/passwd" 
of his maschine. Then he rlogin'ed into a SUN-workstation as a trusted user.

==&gt; Do not use ".rhosts" unless you have EVERY host and EVERY communication
    path totally under control!

Arno Diehl, University of Karlsruhe, West Germany

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Re: Subject: Computers and Medical Charts (RISKS 4.5)
</A>
</H3>
<address>
Roy Smith
&lt;<A HREF="mailto:allegra!phri!roy@seismo.CSS.GOV ">
allegra!phri!roy@seismo.CSS.GOV 
</A>&gt;
</address>
<i>
Thu, 6 Nov 86 11:54:44 est
</i><PRE>
Reply-To: phri!roy@seismo.CSS.GOV (Roy Smith)
Organization: Public Health Research Inst. (NY, NY)

&gt; From: Christopher C. Stacy &lt;CSTACY@JASPER.Palladian.COM&gt;
&gt; Subject: Computers and Medical Charts
&gt; 
&gt; So, the opinion of one medical records administrator seems to concur with
&gt; that of Dr. Tessler; the people at that hospital probably were over-reacting
&gt; inappropriately. [...] this situation presents the familiar risk of
&gt; paranoid confusion.

	In my (limited) experience, the other problem is more common;
people under-reacting inappropriately to the security risks of storing data
in computers.  We are a biological research lab and use our computer
systems to store everything from mundane experimental results to patent
applications.  Somehow, people have gotten the impression that once it's in
the computer, it's safe.  It's hard enough to convince everybody to keep
their password secret, let alone read-protect their files or (God forbid!)
think about encryption or off-line storage when appropriate.  Even when we
had a rather sophisticated breakin a couple of months ago, and I sent
around what I intended to be a scare-the-blank-out-of-them memo, people
still trust the machine to safeguard their data more than is probably
prudent.

	It gets worse. There was recently an (apparently unrelated)
incident involving researchers at two nearby research institutes where one
researcher (call him thief) stole some important data from a competitor
(victim).  I got the original story from a mutual competitor of those two
who works here (fool).  When I spoke with victim to get the whole story, he
admitted it was purely his fault.  Victim was 1) using the same system as
thief to store his data and 2) didn't read-protect his files because he
wanted certain other people to be able to read them (not thief or fool,
however).  I then went back to fool and told him what had happened and
urged him to take at least some simple precautions -- change his password
for example.  He refused, saying that 1) he thought his data was safe
enough and 2) he couldn't imagine that anybody would/could break in.  Even
when I reminded fool that he had just had a big fight with one of his
post-docs and ended up firing the post-doc, he wouldn't believe me that
there might be people out there with the motive and capability to steal or
destroy his data!

	So, what am I supposed to do?  Here we have a person who, in the
face of overwhelming evidence that his data might be in peril, insists on
clinging to his belief that if it's in the computer, it must be safe.  In
my opinion, this is a far more dangerous situation than what CSTACY@JASPER
reports.

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
 DDN Net breakdown (?) on 6 Nov 86?
</A>
</H3>
<address>
    Will Martin -- AMXAL-RI 
&lt;<A HREF="mailto:wmartin@ALMSA-1.ARPA">
wmartin@ALMSA-1.ARPA
</A>&gt;
</address>
<i>
Fri, 7 Nov 86 12:20:38 CST
</i><PRE>

Since the well-known ARPANET breakdown is one of the RISKS archive items,
I was wondering if anyone on the list could contribute information about
what seemed to have been a DDN (or maybe just MILNET?) breakdown that
happened yesterday, 6 Nov 86? All I know of it was that our data
communications people got a call from Army Communications to let them
know that the reason we were off the net was not just a local area or
in-house problem, but some sort of general malaise or trouble all over
the network. I know no more details as to the nature or true extent of
the problem(s) and would like to read details or at least a description
of the symptoms. It was cleared up within hours, so was not as severe as
the historic ARPANET collapse, but it would probably be worthy of
mention in RISKS.

Will Martin

    [I asked Ole Jorgen Jacobsen &lt;OLE@SRI-NIC.ARPA&gt; of the Network
     Information Center whether he had heard anything.  ``The only thing 
     that comes to mind is the TAC problems we had yesterday, where
     lots of TACs gave "bad login" and needed to be reloaded.''  PGN]

</PRE>
<A NAME="subj9"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj9.1">
Re: Linguistic decay (RISKS-4.4)
</A>
</H3>
<address>
Matthew P Wiener
&lt;<A HREF="mailto:weemba@brahms.berkeley.edu ">
weemba@brahms.berkeley.edu 
</A>&gt;
</address>
<i>
Fri, 7 Nov 86 01:26:38 PST
</i><PRE>

There was a discussion in mod.comp-soc when it was still a mailing list
last spring on word processors =&gt; linguistic decay.  As someone who loves
the language for the sake of language, it is depressing to contemplate.

ucbvax!brahms!weemba	Matthew P Wiener/UCB Math Dept/Berkeley CA 94720

</PRE>
<A NAME="subj10"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj10.1">
 Mechanical Aids to Writing
</A>
</H3>
<address>
&lt;<A HREF="mailto: Boebert@HI-MULTICS.ARPA">
 Boebert@HI-MULTICS.ARPA
</A>&gt;
</address>
<i>
Fri, 7 Nov 86 19:25 CST
</i><PRE>
To:  RISKS@CSL.SRI.COM

I couldn't resist, after reading M. Minow's quoting of the redoubtable Burgess.

  Headline:  "Reporters Should Cultivate the Use of the Fountain Pen"

  "In a recent address delivered at Columbia University, Mr. Edward W.
  Townsend, newspaper and magazine writer and Congressman, expressed the
  opinion that it was a misfortune that the typewriter had come to be so
  generally used in newspaper rooms, because it made the translation of
  thought into copy somewhat too easy.  The view point is that the somewhat
  slower and more careful handwriting of any article or news item is better,
  clearer thought and is always better constructed when written with a
  fountain pen than when rambled off on a typewriter..."

This from the "Pen Prophet", the house organ of the Waterman pen company,
Volume XII, No. 1, June 1914.  So there, Red Smith, Ernie Pyle, and E. B.
White.

            [A well-known exponent of pens is Edsger W. Dijkstra, much of 
             whose EWD series is still written very carefully in pen.  PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.06.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.08.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-85</DOCNO>
<DOCOLDNO>IA012-000125-B044-194</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.8.html 128.240.150.127 19970217005922 text/html 17866
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:57:47 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 8</TITLE>
<LINK REL="Prev" HREF="/Risks/4.07.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.09.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.07.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.09.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 8</H1>
<H2> Sunday, 9 November 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Brazilian laws require proof of voting. People NEED those cards.       
</A>
<DD>
<A HREF="#subj1.1">
Scot E. Wilcoxon
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Grassroots sneak attack on NSA 
</A>
<DD>
<A HREF="#subj2.1">
Herb Lin
</A><br>
<A HREF="#subj2.2">
 Matthew P Wiener
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Ethernet Security Risks 
</A>
<DD>
<A HREF="#subj3.1">
Phil Ngai
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Perfection 
</A>
<DD>
<A HREF="#subj4.1">
Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Information replacing knowledge 
</A>
<DD>
<A HREF="#subj5.1">
Daniel G. Rabe
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Word Processors / The Future of English 
</A>
<DD>
<A HREF="#subj6.1">
Stephen Page
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Copyrights; passwords; medical information 
</A>
<DD>
<A HREF="#subj7.1">
Matthew P Wiener
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Re: Computer causes chaos in Brazilian Election
</A>
</H3>
<address>
&lt;<A HREF="mailto:rutgers!meccts!mecc!sewilco@seismo.CSS.GOV">
rutgers!meccts!mecc!sewilco@seismo.CSS.GOV
</A>&gt;
</address>
<i>
Sun, 9 Nov 86 01:14:36 EST
</i><PRE>
Summary: Brazilian laws require proof of voting. People NEED those cards.
To: seismo!csl.sri.com!risks

This situation involving computers is severe due to Brazil's laws, with
which most of the RISKS readers are undoubtedly not familiar.

The "frayed tempers" due to not getting the "essential voting card" in
Brazil are not simply because everyone likes to vote.  Everyone MUST vote in
Brazil.  Proof of recent voting is one of the required legal documents for
several situations, including simply getting a job.  Those missing voting
registration cards are the prerequisite to being able to vote and be a
law-abiding citizen qualified to live a normal life.  (My wife is from
Brazil and had to carry those documents.)

&gt;  Programmers overlooked that twins are born on the same day to the same
&gt;  parents. Consequently, the voting rights of an estimated 70,000 twins
&gt;  were cancelled. The Federal Electoral Tribunal in Brasilia is currently
&gt;  wading through 140,000 appeals, including the case of a certain Jose
&gt;  Francisco, who says all his 14 brothers were baptised with identical
&gt;  names.

All this is familiar to analysts and programmers.  The voting documents
were formerly handled by humans who modified the processing procedure
as required by common sense and local situations ("Yeah, I know Jose
Francisco.  All 14 were here last year, I still have to see 6 of them this
year.")  The written procedures are undoubtedly what guided the programmers.
If the implementation schedule was the same for the whole country, it is
little wonder that many exceptions were found at the same time.

Scot E. Wilcoxon    Minn Ed Comp Corp  {quest,dayton,meccts}!mecc!sewilco

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Grassroots sneak attack on NSA
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Sat, 8 Nov 1986  09:42 EST
</i><PRE>

    From: weemba at brahms.berkeley.edu (Matthew P Wiener)

    Several people have started inserting cute words like
    "crypt" or "terror" or "CIA" in their signatures in an attempt to over-
    load NSA's automatic grep for cute words in overseas traffic.  Consider-
    ing the minuteness of the added load, and the likelihood that NSA already
    filters out obvious traffic like the net...

That would be inconsistent with the oft-repeated claims that NSA
monitors ALL overseas telephone calls.  I have been told (someone pls
confirm or deny?) that voice recognition technology is good enough
that given Crays on an NSA budget, such a feat is possible when you
are looking for certain key words, and that recognition can be done on
a very limited vocabulary independent of speaker.

Comments?

</PRE>
<HR><H3><A NAME="subj2.2">
Re:  Grassroots sneak attack on NSA
</A>
</H3>
<address>
Matthew P Wiener
&lt;<A HREF="mailto:weemba@brahms.berkeley.edu ">
weemba@brahms.berkeley.edu 
</A>&gt;
</address>
<i>
Sat, 8 Nov 86 14:33:51 PST
</i><PRE>
Cc: risks@csl.sri.com

   &gt;    Considering ... the likelihood that NSA already
   &gt;    filters out obvious traffic like the net...     [MPW]
   &gt;
   &gt;That would be inconsistent with the oft-repeated claims that NSA
   &gt;monitors ALL overseas telephone calls.              [HL]

Of course they intercept the net, but if you were snooping around through
all overseas telephone calls, you too would set some priorities.

   &gt;[voice recognition rumor]

Well if that's how they do it, I *hope* they know enough to filter the net!

ucbvax!brahms!weemba	Matthew P Wiener/UCB Math Dept/Berkeley CA 94720

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Ethernet Security Risks
</A>
</H3>
<address>
Phil Ngai
&lt;<A HREF="mailto:lll-crg!amdcad!phil@seismo.CSS.GOV ">
lll-crg!amdcad!phil@seismo.CSS.GOV 
</A>&gt;
</address>
<i>
Sat, 8 Nov 86 12:49:41 pst
</i><PRE>

Security on an Ethernet is a very tricky business. If you use the Berkeley
rhosts scheme, it is easy to spoof someone else's ip address, although there
is some code in Berkeley Unix that detects when someone is impersonating
you, the message only comes out on the system console. And if the bad guy
makes your machine crash while you are away, no one will be the wiser.

If you ban rhosts and only allow ftp and telnet, you are vulnerable
to people grabbing packets off the Ethernet and getting your password.

Which is worse? Would you rather freeze to death or burn to death?
I don't know if it matters. I think that if security matters, it
would be best not to let machines you don't trust on your Ethernet.

Sun proposed an interesting scheme at the last Usenix. Two machines that
wanted to communicate would use an encrypted timestamp on each packet as
authentication. This assumes, of course, that the two machines have
synchronized their clocks and that they have a common key no one else knows.
(their scheme included a key distribution method which I will not discuss
here) There is also a performance penalty. They did some back of the
envelope calculations showing it would be acceptable in many cases.

Is it unreasonable to put machines you don't trust on another Ethernet, 
with a router between your group and them?
                                        	Phil Ngai

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Perfection
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Tue, 28 Oct 1986  10:48 EST
</i><PRE>

   From: Douglas Humphrey &lt;deh at eneevax.umd.edu&gt;
   To LIN : In response to a message, you state that none of the anti-SDI
            folk ever stated that the software had to be perfect. I have 
            heard constantly in both the widely read (Washington Post) and
            limited (?) distribution industry media (Aviation Leak and 
            Space Mythology) SDI critics that conte[x]t that it must be perfect
            or it is useless. I don't bel[ie]ve this, and I would hope you
            don't either, but saying that the whole must be perfect 
            certainly implies that the parts must be perfect.

Please give a citation. The only place I have ever seen a statement about
required "perfection" came in an article written by James Fletcher, of the
Fletcher Commission, who clearly states that "an enormous and error-free
program" would be required.  Fletcher hardly counts as a critic.

                        [I held this one up hoping for a response...  PGN]

I don't deny that you have heard what you say you have heard, but the
only inference I can draw is that neither the Post nor AWAST have
correctly reported the critics' position.

What critics DO say is that you can never know if BMD software will work in
the absence of realistic testing.  KNOWING that a program will work properly
without error is a different, and more demanding, condition than whether or
not it *actually* will if put to the test.  Moreover, critics do not have
faith that it is possible to predict all of the ways that the Soviets might
attack; we do believe that the Soviets *might* be able to attack in a way
that would result in catastrophic failure.

On the general issue of "perfection", critics believe that the statement of
mission requirements comes from the President of the United States and the
Secretary of Defense, who assert that SDI is a way to protect everybody
against nuclear ballistic missiles.  They get funding from Congress on that
claim, and they present it to the American people that way.  If they want to
use it for something else, such as improving the ability of the U.S. to
retaliate, then let them say so.  Until the proponents admit POLITICALLY
that their goal is infeasible, critics have a responsibility to confront
them with their fallacies.  You may acknowledge that their goal is
infeasible, in which case we can argue about what goals are feasible, but
you are not the President.  If you want to criticize someone for asserting
perfection, dump on the highest levels of the Administration, because they
are the ones that set the terms of the debate.
 
   [As usual, we tread some fine lines, e.g., among technically motivated 
    nonpolitical arguments, nontechically motivated political arguments, etc.
    Just as we often note that RISKS issues are holistic system issues, it
    is risky to try to talk of just the technical arguments in isolation.
    Although RISKS seeks to avoid political issues, this one is metapolitical 
    and is closely intertwined with technological evaluations.   PGN]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
 Information replacing knowledge
</A>
</H3>
<address>
Daniel G. Rabe
&lt;<A HREF="mailto:       <DAN09697%NUACC.BITNET@WISCVM.WISC.EDU> ">
       &lt;DAN09697%NUACC.BITNET@WISCVM.WISC.EDU&gt; 
</A>&gt;
</address>
<i>
Sat, 8 Nov 86 14:20 CST
</i><PRE>
To:  risks@csl.sri.com

In RISKS 4.4, Martin Minow makes the point that computerization makes
it easier to substitute quantity for quality in our writing.  I would
go one step farther and say that the easy access to information made
possible by computer systems has also degraded our ability (or at least
our desire) to gain and retain knowledge.

The following is excerpted from an essay entitled "Look it up!
Check it out!" by Jacques Barzun in the Autumn 1986 *American Scholar.*

  ``... the age of ready reference is one in which knowledge inevitably
  declines into information.  The master of so much packaged stuff needs to
  grasp context or meaning much less than his forebears:  he can always look
  it up.  His live memory is otherwise engaged anyway, full of the arbitrary
  names, initials, and code numbers essential to carrying on daily life.  He
  can be vague about the rest: he can always check it out.

  ``... But what we are experiencing is not the knowledge explosion so often 
  boasted of; it is a torrent of information, made possible by first reducing
  the known to compact form and then bulking it up again -- adding water.
  That is why the product so often tastes like dried soup.''

As computer scientists, I think we find it all too easy to divide
and compartmentalize information as we see fit.  As I see it, one
of the greatest risks of widespread computing is that we'll all stop
learning.  We've got spelling checkers, so why bother learning to
spell?  We've got calculators and home computers, so why bother learning
any math?  We've got electronic mail and conferencing, so why bother
to learn or practice the art of public speaking?  Are we reaching the
point where being an expert simply means having a large computer
database, as opposed to years of learning and knowledge?  I don't
think we're there yet, but I fear that our society's heavy emphasis on
"information" and computing might be leading us there.

Daniel G. Rabe
Northwestern University

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Word Processors / The Future of English
</A>
</H3>
<address>
Stephen Page 
&lt;<A HREF="mailto:munnari!uqcspe.oz!sdpage@seismo.CSS.GOV">
munnari!uqcspe.oz!sdpage@seismo.CSS.GOV
</A>&gt;
</address>
<i>
Sunday, 9 Nov 1986 14:07-EST
</i><PRE>

The interesting article by Anthony Burgess reproduced in RISKS-4.4 reminded
me that when the first lap-top computers were introduced a few years ago,
some professional writers noticed that their sentences were becoming shorter
and their paragraphs chunkier, as they relied on a 40-column, 8-line display
(e.g.)  when composing texts.  Has this really been cured by newer
technology?  Or is our familiar 80x25 model just as likely to have an
adverse impact on writing style?

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Copyrights; passwords; medical information
</A>
</H3>
<address>
Matthew P Wiener
&lt;<A HREF="mailto:weemba@brahms.berkeley.edu ">
weemba@brahms.berkeley.edu 
</A>&gt;
</address>
<i>
Sat, 8 Nov 86 01:16:22 PST
</i><PRE>

&gt;  "How Fred lets the fraudsters in" (c) Newspaper Publishing PLC
                                     ^^^ 
Considering the frequency with which we see this half-circled c used as an
ASCII replacement for the genuine circled c, it is obvious that a lot of
people have let their primitive keyboards delude them into a non-copyright.
("Copyright", spelled out, takes longer than "(c)", but it has legal standing.)

&gt;  Passwords are particularly vulnerable when they remain unchanged for a long
&gt;  time.  The chairman of one major company the auditors investigated had kept
&gt;  the same password for five years. It was "chairman".

This reminds me of the WWII story in Feynman's book about the hot-shot
military big boss with his fancy-dancy super-safe: the combination was never
changed from the factory original.  "The more things change, the more they
stay the same."

&gt;Now, I am being accused of taking confidential information out of the
&gt;hospital in the form of patient records and doctors names! All I had on the
&gt;computer were my notes. The paranoid medical staff is afraid that having
&gt;this information in my "COMPUTER" is dangerous, [...]
&gt;Pretty amazing paranoia, huh? Do people really still fear computers this way?

In this situation, it strikes me as typical computer ignorance.  But in
general, the use of a computer as opposed to a legal pad leads to more
security problems.  Handwritten notes are both unmistakeable as such and are
naturally limited in content.  (I assume this is old hat to RISKers.)

ucbvax!brahms!weemba	Matthew P Wiener/UCB Math Dept/Berkeley CA 94720

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.07.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.09.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-86</DOCNO>
<DOCOLDNO>IA012-000125-B044-217</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.9.html 128.240.150.127 19970217005939 text/html 21491
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:58:04 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 9</TITLE>
<LINK REL="Prev" HREF="/Risks/4.08.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.10.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.08.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.10.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 9</H1>
<H2> Monday, 10 November 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Risk of aging 
</A>
<DD>
<A HREF="#subj1.1">
Lee F. Breisacher
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Re: UK computer security audit 
</A>
<DD>
<A HREF="#subj2.1">
Henry Spencer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Lost files 
</A>
<DD>
<A HREF="#subj3.1">
Norman Yusol
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Canard!! [Looping Mailers] 
</A>
<DD>
<A HREF="#subj4.1">
Lindsay F. Marshall
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Friend-foe identification 
</A>
<DD>
<A HREF="#subj5.1">
Henry Spencer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Micros in Car Engines 
</A>
<DD>
<A HREF="#subj6.1">
Jed Sutherland
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Information replacing knowledge 
</A>
<DD>
<A HREF="#subj7.1">
Bard Bloom
</A><br>
<A HREF="#subj7.2">
 Herb Lin
</A><br>
<A HREF="#subj7.3">
 Jerry Saltzer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Spelling becoming obsolete? 
</A>
<DD>
<A HREF="#subj8.1">
Ted Lee
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj9">
  They almost got me! [A motor-vehicle database saga] 
</A>
<DD>
<A HREF="#subj9.1">
Mark Hittinger
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Risk of aging
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
10 Nov 86 12:26:56 PST (Monday)
</i><PRE>
From: Breisacher.OsbuSouth@Xerox.COM (Lee F. Breisacher)
To: RISKS@CSL.SRI.COM

From LA Times, Saturday, November 8, 1986:

G.C. Blodgett, a living legend as an outdoorsman in New England, drives a
car to his favorite fishing spots from his home in West Babylon, Mass., but
he almost quit this year when his insurance bill arrived.  His son told the
Providence Journal: "He wanted to know why the premium was three times as
much as the previous year.  So we called the insurance company, and after a
while, the fellow there came back laughing and explained that their computer
calculated premiums for drivers up to 100 years old.  After that, it started
at the beginning again, so he was being charged the premium of a teen-ager."

Blodgett is 101.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Re: UK computer security audit
</A>
</H3>
<address>
&lt;<A HREF="mailto:hplabs!pyramid!utzoo!henry@ucbvax.Berkeley.EDU">
hplabs!pyramid!utzoo!henry@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Sun, 9 Nov 86 08:40:40 pst
</i><PRE>

&gt; The Guardian article paints a bleak picture of just how ill-prepared for
&gt; disaster the 50 or so companies visited are. 80% are not adequately
&gt; protected against fire, 96% are not protected against flood, (the two
&gt; exceptions had only installed detectors after sustaining water damage
&gt; previously), 70% don't have a stand-by power supply, ...

It is worth noting that even the companies which theoretically *are*
prepared may find their preparations wasted in practice.  The first NYC
blackout caught a number of hospitals with, so to speak, their pants down.
Things like emergency generators with electric starters!  Another example
that I remember was a place that had a fine emergency generator, started
up properly and actually ran for a while.  Trouble was, it was in the
basement, which was below the local water table and was kept dry by pumps
running continuously.  You guessed it, the pumps weren't on the emergency
power.  The only people who had reliable power throughout the blackout
were the professional paranoids:  the military and the phone company.

It might be worth finding out whether there was any attempt to compile a
list of such experiences from that blackout.  I heard about this by chance.

(The electrically-started-generator problem was larger than it looked.
Modern power plants need startup power for things like pumps and control
systems.  No need for emergency generators, you can always get startup
power from the network.  But what do you do when the *whole* network is
down?  A combination of luck and improvisation sufficed that time.)

				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
 Lost files
</A>
</H3>
<address>
       
&lt;<A HREF="mailto:CS117341%YUSOL.BITNET@WISCVM.WISC.EDU">
CS117341%YUSOL.BITNET@WISCVM.WISC.EDU
</A>&gt;
</address>
<i>
Sun, 9 Nov 86 18:57 EST
</i><PRE>
To:  risks-request@sri-csl.arpa
   
    [After a request to resend missing copies of <A HREF="/Risks/3.92.html">RISKS-3.92</A>, 4.1 and 4.2]

I believe these files were lost on the net on 3 Nov.  Apparently, one of
the computers on Bitnet had a severe hardware crash and lost about 1500
files...  Unfortunately, I don't have any more info on this.  Norman

    [This happens far too often.  I presume we need some research on 
     really reliable, "guaranteed-service" protocols.  On the other hand,
     the computational cost associated with such algorithms may be far too
     high for just sending net mail, and besides there is no such beast that
     will work correctly under all possible circumstances.  PGN]

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Canard!! [Looping Mailers]
</A>
</H3>
<address>
"Lindsay F. Marshall" 
&lt;<A HREF="mailto:lindsay%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK">
lindsay%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Mon, 10 Nov 86 09:40:12 gmt
</i><PRE>

Let me hasten to assure the RISKS list that the 20 messages reported by
PGN were not generated by our mailer at Newcastle as far as we can tell.
I think that the problem was much further down the line.   Lindsay

       [I thought about changing the SUBJECT line of this message to make it
        more explicit, but then I would be guilty of being a Canard Liner.
        However, since the implication of "canard" ("a fabricated story") is
        meaningful, I did not want to duck it.  (An aquacktive nuisance.)  
        Can anyone else provide a report of this happening elsewhere at
        the same time, on or around Friday, 7 Nov 86, 13:05:21 gmt?  PGN]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Friend-foe identification
</A>
</H3>
<address>
&lt;<A HREF="mailto:hplabs!pyramid!utzoo!henry@ucbvax.Berkeley.EDU">
hplabs!pyramid!utzoo!henry@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Sun, 9 Nov 86 08:41:08 pst
</i><PRE>

In the course of catching up on Flight International (the British analog to
Aviation Leak), I ran across an interesting item in the 7 June 1986 issue.
The UK Ministry of Defence officially admitted that a British helicopter,
shot down in the Falklands War with all four aboard killed, was downed by
a Sea Dart missile from a British destroyer.  On 6 June 1982, HMS Cardiff
reported shooting down an Argentine helicopter flying in darkness toward
Port Stanley.  It was actually a British Army Gazelle on a resupply flight
between Darwin and Mount Pleasant.  The lack of Argentine wreckage and
the coincidence of timing were noticed, but a forensic investigation was
unable to establish a firm connection.  Forensic tests in the last year or
so have pretty much settled the question.  MoD apparently won't discuss
how the misidentification occurred.

(This sort of thing is far more common in combat than most people think.  In
WW2 there was a standing joke about how antiaircraft gunners decided whether an
aircraft was friendly or hostile:  approaching = hostile, receding = friendly.)

				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Micros in Car Engines
</A>
</H3>
<address>
jed sutherland 
&lt;<A HREF="mailto:jed%noah.arc.cdn%ubc.csnet@RELAY.CS.NET">
jed%noah.arc.cdn%ubc.csnet@RELAY.CS.NET
</A>&gt;
</address>
<i>
Mon, 10 Nov 86 09:32:27 pst
</i><PRE>

Considering the amount of duties undertaken by micros in today's
automobiles, I can only conclude that it is a case of "Because we can do
it".  Sure, computer controlled fuel injection is very efficient and is a
good idea.  But my brother just bought a new BMW with all sorts of standard
stuff on it.  It will tell you the outside temperature, warn you when the
temp is low enough that the roads are likely to be icy, etc. The radio is
more complicated than the oil pressure, water temperature indications.

I am also amazed at the fact that one can buy a car with totally digital
instrumentation. What possible advantage can there be to all of this?

I noted a while back that when boosting the newer car, one runs the risk of
blowing any computer that may be on board due to power surges.  These things
cost about $1000 to replace.  Most mechanics nowadays are trained to identify
the faulty module and replace it without trying to find the bad component.

I think that the average driver loves all the pretty lights but doesn't
usually use all his instruments anyway.  For one thing, most drivers seem to
be able to handle very little at one time and it is all they can do to keep
the car between the lines.  They don't need more distractions provided by
today's auto-toys.

Jed Sutherland

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Information replacing knowledge
</A>
</H3>
<address>
Bard Bloom 
&lt;<A HREF="mailto:bard@THEORY.LCS.MIT.EDU">
bard@THEORY.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Sun, 9 Nov 86 17:41:55 est
</i><PRE>

  &gt; As I see it, one of the greatest risks of widespread computing is that
  &gt; we'll all stop learning...

Most of the time, people learn things because someone (often the person
herself) thinks the things are useful.  So, for instance, very few Americans
this decade know a whole lot about the care and tending of a horse or about the
growing seasons of various plants, despite the fact that these were vital facts
for much of the American population a century or two ago.  Mathematics (e.g.,
things like algebra and basic set theory) have become a lot more popular.  
As the environment changes, the set of things chosen as "essential knowledge"
changes.  We may expect to see this continue, and a good thing too.  I don't
*want* to know a lot about mucking out stables.

Some might argue that some things are good to learn in and of themselves.
I'd agree for some areas (e.g., the arts), and disagree for others (e.g.,
spelling).

  &gt; Are we reaching the point where being an expert simply means having a large
  &gt; computer database, as opposed to years of learning and knowledge?

I hope not.  We might be reaching the point where being an expert means having
a large computer database as well as knowing the subject well.  This is not
particularly different in character from having a large physical library in
one's area of expertise, which most experts do.  Part of the point of expertise
it that one can do things that aren't in one's library or database.

  &gt; I don't think we're there yet, but I fear that our society's heavy
  &gt; emphasis on "information" and computing might be leading us there.

Possibly so.  I've noticed a general feeling that computer answers are more to
be trusted than human ones.  

Bard Bloom, MIT

</PRE>
<HR><H3><A NAME="subj7.2">
Information replacing knowledge
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Sun, 9 Nov 1986  15:52 EST
</i><PRE>

    From: &lt;DAN09697%NUACC.BITNET at WISCVM.WISC.EDU&gt; (Daniel G. Rabe)

    As I see it, one of the greatest risks of widespread computing is that
    we'll all stop learning.  We've got spelling checkers, so why bother
    learning to spell?...

It's an old fear.  It was said about Xeroxing -- and who has not had the
experience of copying an article in the hopes that its information would
seep from the file cabinet to the brain?  It was said about books and
printing -- and who has not bought a book without the same experience.  It
was apparently even said about writing -- and who has not wished that (s)he
could speak as well as (s)he could write?

That's not to say that all these fears are unjustified.  But it is not
new with the advent of computers.

</PRE>
<HR><H3><A NAME="subj7.3">
Information replacing knowledge
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
Sun, 9 Nov 86 18:52:04 EST
</i><PRE>
From: Jerome H. Saltzer &lt;Saltzer@ATHENA.MIT.EDU&gt;

&gt;  [...] some professional writers noticed that their sentences were
&gt;  becoming shorter [...], as they relied on a 40-column, 8-line display...

From what I have seen of the output of some professional writers, that is a
RISK that I am willing to tolerate, perhaps even encourage.
                                                                Jerry

        [It even sounds like a fine idea for RISKS contributors.  PGN]

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
 Spelling becoming obsolete?
</A>
</H3>
<address>
&lt;<A HREF="mailto: TMPLee@DOCKMASTER.ARPA">
 TMPLee@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Mon, 10 Nov 86 00:11 EST
</i><PRE>
To:  Risks@CSL.SRI.COM

Yes, spelling checkers are allowing students to get by without learning to
spell -- *and the schools are endorsing that trend*! I have yet (slight
hyperbole here) to get over the words I heard three years ago from our
oldest son's seventh-grade English teacher (yes, "English").  It was during
the beginning-of-the-year parents' orientation meeting where we have the
opportunity to meet all the teachers and hear their plans for the year.  I
can't remember the precise context any more, but I think we had asked some
kind of question about whether she took spelling into consideration in
grading compositions.  The answer was roughly this:  "Not very much -- after
all, all these kids will be using word processors in the future and won't
have to know how to spell."  Fortunately this view was not shared by most of
the rest of the teachers.  (The school district, by the way, and the
particular junior high itself, is among the top few percent in the country,
as judged by scores on the SAT and the various awards it has received.)

</PRE>
<A NAME="subj9"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj9.1">
       They almost got me!        [A motor-vehicle database saga]
</A>
</H3>
<address>
             
&lt;<A HREF="mailto:SYSMSH%ULKYVX.BITNET@WISCVM.WISC.EDU">
SYSMSH%ULKYVX.BITNET@WISCVM.WISC.EDU
</A>&gt;
</address>
<i>
Sun, 9 Nov 86 15:57 EDT
</i><PRE>
To:  risks@csl.sri.com

I scored big on some DEC call options recently.  I used the proceeds to
purchase an expensive 87 turbo mazda RX-7.  After driving it for a month I
realized my driver's license had been expired for a year.  Kentucky sends you
a post card when it is time to renew.  I simply assumed that mine got lost
somehow and went downtown to renew.  They took my license and told me it was
suspended in February of 85! Arg! Since my license was suspended I did not
get a renewal notice.

The clerk was very helpful and gave me a phone number to call.  I called the
number and the gentleman on the other end told me that because my license
was issued under an older system, it would be awhile before he could
retrieve my record and tell me why the suspension occurred.  The State was
switching over to a social security number based system, and evidently the
old system existed only in hard copy form.  He then said, "By the way, may I
have your social security number?  Please call us back after lunch and we
should have some information for you."

I called back and found out that a speeding ticket obtained in February of
85 had pushed me to the limit of "points" and that the state had sent me a
notice to appear at court to plead my case.  If I had shown up, the judge
would have given me "traffic school" and I would have kept my license.  I
never received any notice.  I didn't show up in court so they suspended my
license for 6 months in retaliation.  I asked the clerk on the phone to tell
me where they sent the notice.  He said "6103 glimmer way apartment 4".
After finding out what the procedure for getting my license back was I
thanked the clerk for his assistance.

(Plot thickens here) In 1981 I lived at 8103 glimmer way apartment 4.  In
1982 I moved from there, and sent the state a letter informing them of my
change in address.  I did not include my social security number.  Since the
state was converting from an older system to the new SSN based system the
address change did not get made.  Evidently, they just re-entered all the
data from the old system to the new system and mis-keyed my address.  State
law states that my obligation was to inform them of a change in address.

So, bottom line, I was driving from March 85 to November 86 with a suspended
driver's license.  I continued to pay auto insurance.  I rented cars during
several business trips (I consult on the side).  I get another(!) speeding
ticket on the interstate.  The officer called in to "run" my license, but
since it was "old-system" they didn't give him the info that I was
suspended.  I drove off, paid the fine, never heard anything.  My car was
towed twice for being parked improperly, I paid the fines, showed my
license, got the car back twice.

Here is the real kicker.  My insurance company states clearly that they are
not liable if I have an accident without a valid driver's license.  The loan
on the unfunded portion of my sleek black RX-7 states that if I don't
maintain insurance I can be sued for the loan.  What if....I had gotten in
my RX-7 and wiped out some people and the car?  I'd have been found to be in
violation of the law, been denied insurance coverage, lost the funds I put
in to the car, and still been liable for the remaining portion of the loan I
took out!!!!

Well I have my license back now, smiling in my RX-7 (insured).  I feel VERY
lucky that nothing happened to me.  The total cost for me to get out of this
one was $38! It makes me wonder if there others are in the same boat (massive 
personal liability indirectly induced by a change from one computer record
system to another).  I just fell through the cracks and didn't even know it.

Mark Hittinger/systems programmer iv/ocis south center
University of Louisville, Louisville, Ky 40292
sysmsh%ulkyvx.bitnet@wiscvm.wisc.edu

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.08.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.10.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-87</DOCNO>
<DOCOLDNO>IA012-000125-B044-235</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.10.html 128.240.150.127 19970217005952 text/html 15253
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:58:21 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 10</TITLE>
<LINK REL="Prev" HREF="/Risks/4.09.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.11.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.09.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.11.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 10</H1>
<H2>Wednesday, 12 November 1986</H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Extreme computer risks in British business 
</A>
<DD>
<A HREF="#subj1.1">
Lindsay F. Marshall
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Alabama election snafu caused by programmer 
</A>
<DD>
<A HREF="#subj2.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Looping mailer strikes again 
</A>
<DD>
<A HREF="#subj3.1">
Brian Reid
</A><br>
<A HREF="#subj3.2">
 Nancy Leveson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Lost files on Bitnet 
</A>
<DD>
<A HREF="#subj4.1">
Niall Mansfield
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  VOA car testing 
</A>
<DD>
<A HREF="#subj5.1">
Bill Janssen
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Re: Aftermath of the Big Bang (apology) 
</A>
<DD>
<A HREF="#subj6.1">
Robert Stroud
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Re: The Future of English 
</A>
<DD>
<A HREF="#subj7.1">
T. H. Crowley [both of them]
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Word-processors Not a Risk 
</A>
<DD>
<A HREF="#subj8.1">
Ralph Johnson
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
 Extreme computer risks in British business
</A>
</H3>
<address>
"Lindsay F. Marshall" 
&lt;<A HREF="mailto:lindsay%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK">
lindsay%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Tue, 11 Nov 86 09:54:27 gmt
</i><PRE>

FIRMS 'SUICIDAL' ON COMPUTERS, by Peter Large (From The Guardian 10/11/86)
                                                               [10 Nov 86?]

British business suffers nearly 30 computer disasters a year, involving
firms in direct losses running into millions, according to a survey
published today. 

Datasolve, the computer software arm of Thorn EMI, questioned the UK's
biggest 500 accountancy firms and found that 28 per cent of them had
encountered computer disasters among their clients in the past five
years; and at least 67 per cent of those breakdowns were avoidable. 

These are not cases of computer fraud or interference by young computer
"hackers": they are cases of accidental loss of data, through system
breakdowns or operator errors, and through fire and flood.  In some
cases firms have lost all records of staff pay, orders, and contracts. 

Mr.  Chris Wood, chief executive of Datasolve, said: "The survey shows
that many firms are risking commercial suicide.  Figures from the US
indicate that 90 per cent of firms suffering a major computer disaster
subsequently went out of business within 18 months. 

"The only reason we are not seeing the same statistics here is because
UK firms are currently less computerised than their US counterparts."

The Datasolve report says that small and medium-sized firms, operating
micro- and mini-computers without full-time professional staff, are most at
risk.  The accountants questioned blamed ignorance, lack of resources, and
perceived cost for the unnecessary risks that firms are taking.

Most of the accountants said that firms needed to spend between 1 and 4 per
cent of their annual computer budgets on stand-by computers and other
protection methods.  A third of them suggested that auditors should warn
shareholders if a company's protection measures are inadequate.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Alabama election snafu caused by programmer
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Wed 12 Nov 86 12:55:00-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

Election results in Mobile, Alabama, were delayed for several hours due to
"computer problems".  According to a report on WKRG-TV in Mobile, the
problem was caused by a programmer improperly opening an output file,
causing the vote totals to be sent to the bit bucket.  The results were not
lost, they just could not be printed out until the bug was found and fixed.
The delay in reporting caused the outcome of the Senate race to be
undetermined for quite some time.  (Mobile is the hometown of Sen. Denton,
who was narrowly re-elected.)  [I hope this is a correct version.  I had
several earlier fragmentary versions...]

             [If you suspect any hanky-panky, be sure to (re)read the previous 
             messages on RISKS on this subject, including <A HREF="/Risks/2.42.html">RISKS-2.42</A>.  PGN]

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Looping mailer strikes again
</A>
</H3>
<address>
Brian Reid
&lt;<A HREF="mailto:reid@decwrl.DEC.COM ">
reid@decwrl.DEC.COM 
</A>&gt;
</address>
<i>
11 Nov 1986 2314-PST (Tuesday)
</i><PRE>

On November 7, Andrew Walker of Nottingham University sent me a mail
message. I received 72 copies of the message on November 7, the first
arriving at 09:53 PST and the last arriving at 17:22 PST. Two days
later on November 9 I got 21 more copies.  Note that all 93 copies of
this message (1890 characters) were sent across the Atlantic separately.

The guilty party is the PDP-11/44 mail relay computer at University
College, London. Most outgoing mail from the UK to the ARPAnet passes
through this machine. I have not contacted the management of the
machine to find out what the story was.

I think that this supports Lindsay's claim that he didn't do it....
			Brian

</PRE>
<HR><H3><A NAME="subj3.2">
Looping mailer strikes again
</A>
</H3>
<address>
Nancy Leveson 
&lt;<A HREF="mailto:nancy@ICSD.UCI.EDU">
nancy@ICSD.UCI.EDU
</A>&gt;
</address>
<i>
11 Nov 86 08:51:49 PST (Tue)
</i><PRE>
ReSent-To: RISKS@CSL.SRI.COM

You requested any information about another similar incident.  Well, on
7 Nov. 86 at 14:12:47 gmt I received 10 identical copies of a message
from Tom Anderson. Nancy

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
       Lost files on Bitnet (cf RISKS-4.9)
</A>
</H3>
<address>
Niall Mansfield  
&lt;<A HREF="mailto:MANSFIEL%EMBL.BITNET@WISCVM.WISC.EDU">
MANSFIEL%EMBL.BITNET@WISCVM.WISC.EDU
</A>&gt;
</address>
<i>
Tue 11 Nov 86 14:41:34 N
</i><PRE>

Losing files on Bitnet through IBM machines going down is very common. It
seems RSCS holds its store and forward files in a spooling area which is
often lost if the machine crashes. We get several such losses reported every
month, and it's not uncommon for thousands of files to be lost.

It's hard to see why this shouldn't be fairly easy to fix:
it would certainly improve net reliability, and  without any
research on guaranteed-service protocols.

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
VOA car testing
</A>
</H3>
<address>
Bill Janssen 
&lt;<A HREF="mailto:janssen@mcc.com">
janssen@mcc.com
</A>&gt;
</address>
<i>
Tue, 11 Nov 86 10:19:13 CST
</i><PRE>
ReSent-To: RISKS@CSL.SRI.COM

                        [This is the tail-end of a private exchange 
                        regarding testing, e.g., for interference...  PGN]

Unfortunately, that's not as singular an example as one might hope.
Characterization of electrical noise under most industrial circumstances
is very poor.  Many microprocessor-based systems are tested with a
"showering arc generator", which is a bunch of relays and coils and
loops of wire hooked up to motor driven interrupters.  The tester turns
on the showering arc generator, places the item to be tested near it,
and sees if it can perform its standard functions.  This is thought to
be a "worst case" test, though in fact it's not at all clear that it is.

Bill

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Re: Aftermath of the Big Bang (apology)
</A>
</H3>
<address>
Robert Stroud 
&lt;<A HREF="mailto:robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
robert%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Tue, 11 Nov 86 19:09:07 gmt
</i><PRE>

In my previous article about the Big Bang I said that one of the biggest
outstanding problems was a backlog of 55,000 unmatched trade reports at the
end of the first week, which had increased to 59,000 by the following
Tuesday. In an attempt to put this figure into perspective, I unwisely added
that "a semi-informed guess" would be that this represented about 30% of the
weeks trading.

"Semi-informed" was meant to indicate that it was not totally random, but
resulted from some data and some reasoning on my part. Unfortunately, both
turn out to be wrong - the correct figure is 15% (I think!). My hesitation
arises from having to perform two unit conversions - it said in yesterday's
Independent (10th November) that "10,250 represents about 2.5% of the average
number [of bargains] in a normal account". That figure is presumably correct,
but there are two transactions in a bargain, and two weeks in an account,
(at least, I *think* there are two weeks in an account...).

Anyway, please accept my humble apologies for dropping a factor of two due
to neglecting the transactions/bargain conversion. (It was a factor of four
until I remembered the weeks/account figure!)

The good news is that the number was down to 20,500 by Saturday morning
and should be cleared by Thursday morning - the deadline being Friday night.
I don't think it could have been 59,000 last Tuesday in that case, so maybe
the problem has been not just keeping records of transactions but keeping
records of the records! One of the difficulties in sorting things out has been
that some of the computer systems did not allow the records of transactions
to be altered (presumably to prevent fraud and preserve an audit trail).

Robert Stroud, Computing Laboratory, University of Newcastle upon Tyne.
UUCP ...!ukc!cheviot!robert

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Re: The Future of English (RISKS DIGEST 4.8)
</A>
</H3>
<address>
&lt;<A HREF="mailto:allegra!thc@ucbvax.Berkeley.EDU">
allegra!thc@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Mon, 10 Nov 86 22:36:33 PST
</i><PRE>

The word processor is leading to a decay of the English language, and now we
discover that the typewriter leads to a similar decay.  Who knows what evils
were caused by the fountain pen and the quill?  Well, you can forget all
that because the problem can be traced back much farther.

A quotation from Plato:

   ``Said Thoth to the King of Egypt, `This invention, O King, will make the
   Egyptians wiser and will improve their memories; for it is an elixir of
   memory and wisdom that I have discovered,' but the king was not convinced
   and feared that the invention of writing would impair the memory instead of
   improving it and that the people would read without understanding.''

So, papyrus started this long, slow tumble into chaos.  What say you we
start a lobby to bring back the clay tablet?

[Note:  I don't mean to belittle the arguments that warn of the dangers of
word processing.  Too little thought goes into much of what I read (and
write).  I just thought this echo from the past brought a new perspective to
the discussion.

The quotation comes from p. 134 of "Understanding Computers" by
Thomas Crowley (my father)]

        [... and coincidentally, my first boss at Bell Labs in 1960!  PGN]

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
Word-processors Not a Risk
</A>
</H3>
<address>
Ralph Johnson
&lt;<A HREF="mailto:johnson@p.cs.uiuc.edu ">
johnson@p.cs.uiuc.edu 
</A>&gt;
</address>
<i>
Tue, 11 Nov 86 10:16:00 CST
</i><PRE>

I do not believe that word-processors damage the quality of writing.
Good writing occurs only when the document is revised and reworked
extensively.  If we write a document first with pen and then type it,
we will get at least one chance to revise it.  The problem is with
those who create a document at the keyboard but never read or revise it.
However, even revising a document once is not enough to gain high quality.
It takes many, many revisions to create a high-quality document, for which
word-processors are invaluable.  This applies to software as well as to
English, though few programmers seem to realize it.

Ralph Johnson

"Master, how many times should I revise my documents?  Up to seven times?"
"I tell you, not seven times, but seventy times seven."

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.09.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.11.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-88</DOCNO>
<DOCOLDNO>IA012-000125-B044-257</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.11.html 128.240.150.127 19970217010007 text/html 18714
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:58:34 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 11</TITLE>
<LINK REL="Prev" HREF="/Risks/4.10.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.12.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.10.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.12.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 11</H1>
<H2> Friday, 14 November 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Computers don't kill people, people kill people 
</A>
<DD>
<A HREF="#subj1.1">
Howard Israel
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Open microphone in the sky 
</A>
<DD>
<A HREF="#subj2.1">
Bob Parnass
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Computerized Voting in Texas 
</A>
<DD>
<A HREF="#subj3.1">
Jerry Leichter
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Problems with HNN 
</A>
<DD>
<A HREF="#subj4.1">
Alan Wexelblat
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Post-hacker-era computer crime 
</A>
<DD>
<A HREF="#subj5.1">
Talk by Sandy Sherizen
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Re: They almost got me! [A motor-vehicle database saga] 
</A>
<DD>
<A HREF="#subj6.1">
Doug Hardie
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Re: information replacing knowledge 
</A>
<DD>
<A HREF="#subj7.1">
G.L. Sicherman
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
 Computers don't kill people, people kill people
</A>
</H3>
<address>
 Howard Israel 
&lt;<A HREF="mailto:HIsrael@DOCKMASTER.ARPA">
HIsrael@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Tue, 11 Nov 86 11:45 EST
</i><PRE>
To:  risks@CSL.SRI.COM

"Child Dies of Grill's Fumes In House Without Utilities"

 Employee Error Kept Power Turned Off
(Washington Post, Sunday, November 9, 1986, pg A46)

(AP) NEW BRITIAN, Conn., Nov.  8--A mistake by a utility employe deprived a
house of power and a 7-year-old girl suffocated from the fumes of a charcoal
grill being used to heat the residence, state investigators said.  The
Department of Public Utility Control said the family of Lucita Morales had
requested and been granted "hardship status", which is intended to guarantee
service to needy customers.  Gas and electric service should have been
turned on Nov.  1, the report said, but a Northeast Utilities computer
operator recorded the order incorrectly, punching a "no print" button
instead of a "print".  As a result, service was not restored until Nov 3.,
the day after the girl was found asphyxiated in an upstairs bedroom.  Police
said a habachi that the girl's mother, Paula Craig, was using to cook and
heat the room generated carbon monoxide.

  Electric service to the home in Bristol had been shut off Sept. 30, and
gas was discontinued Oct. 7. Utility Spokeswoman Jane Strachan said no
action would be taken against the employe, whom she declined to identify.  A
department spokeswoman, Toni Blood, said the incident would be reviewed to
determine whether the system for tracking the hardship cases needs
improving, but no action was pending against the utility.

  Avila Craig, Lucita's grandmother and the owner of the two-story house,
said she did not blame Northeast for the girl's death.  "It's sad so many
people get caught up in the bureaucracy," she said.  "It's about time people
in Bristol wake up and realize people are hungry."  "I don't feel
victimized," she added.  "My daughter was just caught up in what is
happening in America ....  She represents all the girls that have babies and
no income."

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Open microphone in the sky
</A>
</H3>
<address>
&lt;<A HREF="mailto:ihnp4!ihuxz!parnass@ucbvax.Berkeley.EDU">
ihnp4!ihuxz!parnass@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Thu, 13 Nov 86 09:29:38 PST
</i><PRE>

NBC News reported last night [Nov. 12], and CBS News reported today, that a
Braniff passenger jet nearly collided with a United passenger jet over
Tennessee.  An air traffic controller in Atlanta witnessed the situation on
his radar screen, attempted to warn the pilots, but was thwarted because the
frequency was blocked by an "open microphone".

Bob Parnass,  Bell Telephone Laboratories - ihnp4!ihuxz!parnass - (312)979-5414

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
 Computerized voting in Texas - from 4-Nov-86 New York Times
</A>
</H3>
<address>

&lt;<A HREF="mailto:LEICHTER-JERRY@YALE.ARPA">
LEICHTER-JERRY@YALE.ARPA
</A>&gt;
</address>
<i>
14 NOV 1986 12:44:15 EST
</i><PRE>
                     [Remailed after delay due to Yale network-table problems.]

			Computer Fraud Fought in Texas 
		Official Orders More Security for All Counties
		      That Tally Ballots Electronically

			     By Robert Reinhold

Houston, Nov. 3 -- The Secretary of State of Texas has ordered "additional
security" procedures in Tuesday's election to prevent fraud in the 40 or so
counties that use computerized vote counting and reporting.

Under the directive issued by the Secretary, Myra A. McDaniel, the computer-
generated printed log of the vote tabulation must record all operator commands
and the "inputs," and the log may not be turned off at any time.

The Attorney General of Texas, Jim Mattox, is investigating charges of vote
fraud arising from last year's mayoral election in Dallas.  No findings have
yet been issued in the inquiry, for which the state has hired Arthur Anderson
&amp; Company, the accounting and consulting concern.

According to Karen Gladney, Director of Elections in the Secretary of State's
office, no significant changes in local vote-counting procedures are expected
because of the directive.  "Basically what we've done is ask counties if
they do not already have them in place, to make sure these procedures are in
place," she said, adding that state inspectors will be dispatched, as usual,
to a number of counties throughout the state.  She said that while the
Secretary was aware of the Dallas inquiry, the order was not issued as a
direct result of it.

In Dallas, Bruce Sherbet, elections coordinator for Dallas County, said the
county already practiced "99 percent" of the precautions.  But he said there
would be a few changes at local precincts, where additional signatures from
election judges and clerks would be required to validate computer tapes
holding vote counts.  In Houston, where, unlike Dallas, ballots are tallied
at a central station, officials said there would be no difference.  "There
is nothing in the directive that we don't do all the time," said Anita
Rodeheaver, a voting official in Harris County.

In Texas counties using electronic tally systems, people vote either by
punching holes in a card that is read by a machine or by marking boxes that
are read by optical scanning.

Among the other security procedures ordered, computer terminals outside the
central counting station are to be permitted only to make inquiries, and the
county clerk or election administrator must produce at least three cumulative
reports in the course of tabulation and prepare a report on the number of
ballots cast in each precinct.  As a final measure, the Secretary of State
said she had the authority to order a manual count of the original paper
ballots to verify the accuracy of electronic counts.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Problems with HNN
</A>
</H3>
<address>
Alan Wexelblat 
&lt;<A HREF="mailto:wex@mcc.com">
wex@mcc.com
</A>&gt;
</address>
<i>
Thu, 13 Nov 86 09:34:23 CST
</i><PRE>

Last night, at around 6:40PM CST, the Headline News Network (HNN) signal
was disrupted for about 10 minutes.  The picture that replaced it was too
distorted to see but the audio was fairly clear.  It was an advertisement
for satellite-signal de-scramblers.

Does anyone have any info on why/how this happened?  Did someone deliberately
spoof the HNN signal?  Or was it just an accidental foulup?

Alan Wexelblat
UUCP: {seismo, harvard, gatech, pyramid, &amp;c.}!ut-sally!im4u!milano!wex

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
 Post-hacker-era computer crime
</A>
</H3>
<address>
&lt;<A HREF="mailto: Mandel@BCO-MULTICS.ARPA">
 Mandel@BCO-MULTICS.ARPA
</A>&gt;
</address>
<i>
Thu, 13 Nov 86 09:09 EST
</i><PRE>
To:  RISKS@BCO-MULTICS.ARPA

           Predicting Future Trends in Computer Crime:
                    The Post-Hacker Era
                      Dr. Sandy Sherizen
              President, Data Security Systems, Inc.
       Wednesday, November 19, 1986, 7:30 PM at MIT (see below)
 
 Abstract: This talk is based on a paper that examines computer
 crime patterns and suggests the factors which will lead to
 increasingly sophisticated computer crimes and criminals in the
 future.  There are several recent aspects of computer crime which
 indicate that computer crime has turned a corner, dramatically
 changing from earlier and possibly less serious versions.  As we
 enter what can be called the post-hacker era of computer crime,
 we need a social road map which will guide us in preparing
 information security measures and computer crime laws.  The
 information in the paper/talk is from a series that Sherizen is
 preparing on criminological models of computer crime.
 
 Dr. Sherizen, a criminologist, consults with corporations, banks,
 and governments on computer crime prevention.  He specializes in
 information security, providing executives with a translation of
 complex technical requirements into managerially relevant
 policies and controls.  Author of "How to Protect Your Computer"
 and numerous articles, he has written reports for the U.S.
 Congress' office of Technology Assessment and conducted seminars
 around the U.S. and Asia.
 
 (Sponsored by Computer Professionals for Social Responsibility)
 
 CPSR/Boston meets on the third Wednesday of each month, at 545 Technology
 Square, in the lounge on the 8th floor.  545 Tech Square is located at
 the corner of Main and Vassar Streets in Cambridge, near the Kendall
 Square stop on the red line.  Meetings are free and open to the public,
 and free parking is available.
 
 For more information, contact CPSR/Boston at P.O. Box 962, Cambridge, MA,
 02142, or call (617) 666-2777.
 
</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
They almost got me! [A motor-vehicle database saga] (Mark Hittinger)
</A>
</H3>
<address>
 "Maj. Doug Hardie" 
&lt;<A HREF="mailto:Hardie@DOCKMASTER.ARPA">
Hardie@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Wed, 12 Nov 86 09:50 EST
</i><PRE>

     I had a similar situation in college many years ago.  However, the
associated risks were much different.  The school had a honors program in
humanities that replaced al the undergraduate general requirements with one
two-year course.  Competition to get in the program was stiff.  As I remember
the requirements, you had to have all A's in English etc., plus outstanding
scores on the entrance exams.  Only 1 percent or so of each new class was
selected for this program.  It was a real honor and a big deal was made at our
hign school graduation for those who were accepted.  I graduated from
highschool with 2 D's in English and never expected to be considered for this
program.  However, the day after graduation, I received an invitation which I
accepted immediately.  It was a great program.  However, 4 or so years later,
I was running the school's computer center.  The admissions people asked me to
rewrite their program which selected new students for the humanities program.
Since they paid real money, I took the job.  The original program was written
in machine language, not assembler language.  It had one instruction per card
in numeric form.  That was a common approach in the school.  Since the program
was unintelligible, they provided the old algorithm and the new.  It took a
few hours to get the new program working.  Basically, each student had a card
which contained the necessary information.  All that had to be done was to
compare the various values on the card with the criteria and select only those
that met the criteria.  The admissions people provided a deck that had been
run earlier so it was simple to test the new program by running it and
comparing the outputs.  After doing that, we found the new program selected
one less person than the old.  After extensive analysis, we discovered that
the extra should never have been selected in the first place.  That caused
some consternation in the school as it meant that someone who was not
qualified had taken a valuable slot in the program.  So the immediate question
was how many times could this have occurred?  The analysis indicated that
there was only one possible way to be selected improperly and it required a
specific set of values for some 20 different items (including 2 D's in
English).  That set off a bell, and I went back to my hysterical records and
found my copy of my card from years earlier.  There were at least two who made
it through that filter.

-- Doug

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Re: information replacing knowledge
</A>
</H3>
<address>
"Col. G. L. Sicherman" 
&lt;<A HREF="mailto:colonel%buffalo.csnet@RELAY.CS.NET">
colonel%buffalo.csnet@RELAY.CS.NET
</A>&gt;
</address>
<i>
Wed, 12 Nov 86 14:16:08 EST
</i><PRE>
To: risks@csl.sri.com

I sympathize with Daniel G. Rabe's argument about communication:
 
&gt;                                                  As I see it, one
&gt; of the greatest risks of widespread computing is that we'll all stop
&gt; learning.  We've got spelling checkers, so why bother learning to
&gt; spell?  We've got calculators and home computers, so why bother learning
&gt; any math?  We've got electronic mail and conferencing, so why bother
&gt; to learn or practice the art of public speaking?

But I doubt that the millions of otherwise intelligent people who cannot
spell right will agree with this characterization of learning!  Indeed,
all his examples belong to specific media of communication.

"Standard" spelling did not exist in Shakespeare's day; words were spelled
out ad hoc.  The pressure to spell each word in just one way came from
printing, when people discovered that they could read faster than they
could listen.  Standard spelling is invaluable for the efficiency of
reading print.

The flip side is that standard spelling is _not_ invaluable for electronic
communication, because efficiency no longer matters--it's a measure left
over from the machine age.  Efficient absorption is important only in one-
way, bulk media like print.  Electronic communication is interactive.

Similar arguments about the nature of mathematics turn up now and then
in journals like _Mathematics Magazine._ Modern mathematics is designed
for the page; its methods don't allow for a Ramanujan.  As for public
speaking, print killed it long ago!  Listen to any political debate and
you'll know what I mean.  Oratory is just a toy these days.

All technological progress alters us. "Why learn to walk great
distances when we have trains?  Why learn beautiful handwriting when we
have typewriters?  Why learn to use tinder and flint when we have
matches?" And of course the ancient "Why learn to remember everything
we hear when we have paper, ink, and alphabet?" Just remember:

	1. You don't have to go along with it.  Dijkstra is said
	   to write his books with pen and ink.
                                                             [Knuth too!]

	2. If you don't like how progress alters people, you can
	   associate with resisters like yourself--if you can find
	   them.  For example, people who believe that the prevalence
	   of clothing weakens the body's natural defenses tend to
	   congregate.

	3. Let others choose for themselves; don't moralize about it.
	   I for one intend to go on using spelling checkers, e-mail,
	   and clothes.

                        [I rejected a bunch of other messages on this
                         topic, as we begin to get into second-order points
                         and some repetition.  Thanks, anyway.  PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.10.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.12.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-89</DOCNO>
<DOCOLDNO>IA012-000125-B044-272</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.12.html 128.240.150.127 19970217010017 text/html 17642
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:58:48 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 12</TITLE>
<LINK REL="Prev" HREF="/Risks/4.11.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.13.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.11.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.13.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 12</H1>
<H2> Sunday, 16 November 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Air Traffic Control radar problems
</A>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Stuck Microphone and Near-Collision of 727s 
</A>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Gwinnett County Voting 
</A>
<DD>
<A HREF="#subj3.1">
Scott Dorsey
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Micros in cars 
</A>
<DD>
<A HREF="#subj4.1">
Paul Kalapathy
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  DMV computer networks 
</A>
<DD>
<A HREF="#subj5.1">
Bob Campbell
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Serious security bug in 3.4 
</A>
<DD>
<A HREF="#subj6.1">
Dave Martindale
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  "Maj. Doug Hardie" and his story 
</A>
<DD>
<A HREF="#subj7.1">
Bruce Schuck
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Necessity of language skills 
</A>
<DD>
<A HREF="#subj8.1">
Daniel G. Rabe
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj9">
  Call for Papers -- Safety and Reliability Society Symposium 
</A>
<DD>
<A HREF="#subj9.1">
Nancy Leveson
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Air Traffic Control radar problems
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Sun 16 Nov 86 20:33:49-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

(Adapted from AP, 12 Nov 86) Radar controlling high-altitude air traffic
from the Texas Panhandle to southern California was knocked out for 40
minutes by a power failure at the Albuquerque NM ATC.  In addition a radar
station near Phoenix, Arizona, was down for more than 59 hours due to a
power failure.  Both power failures occurred on 6 Nov 1986.  The Albuquerque
failure was the first there in 18 years, according to the FAA sector
manager.  The backup procedures are very awkward, but they worked well to
avoid any accidents.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Stuck Microphone and Near-Collision of 727s
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Sun 16 Nov 86 20:40:24-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

A stuck cockpit microphone jammed a controller-pilot frequency last week and
prevented an air traffic controller from warning two Boeing 727 jetliners
that they were on a collision course.  The Braniff and United planes carried
175 people, and passed perpendicularly within something like 500 feet of one
another.  (The Sunday NY Times News of the Week in Review, 16 Nov 86, noted
that there were 777 near collisions in 1985, about 30 percent of which
involved scheduled airliners.)

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Gwinnett County Voting
</A>
</H3>
<address>
Scott Dorsey 
&lt;<A HREF="mailto:kludge%gitpyr%gatech.csnet@RELAY.CS.NET">
kludge%gitpyr%gatech.csnet@RELAY.CS.NET
</A>&gt;
</address>
<i>
Thu, 13 Nov 86 18:36:13 est
</i><PRE>

   A recount of votes in Gwinnett county, Ga. has led to a few interesting
problems that might be of interest to readers of Risks.  Ballots from one
area were accidentally not counted in the first tally, because they had been
mislaid in a stockroom (and possibly tampered with).  There was apparently
no safeguard to prevent anyone from recognizing that a large population was
not represented.  Later, it was discovered that the tabulating machines used
for the counting gave different results between runs.  Although there is
some question about the reliability of the count, it seems to be accepted as
accurate.
    
   Stan Kelly-Bootle speaks of "CREVM", the Conditioned Response Electric
Voting Machine, which trains voters to press the correct lever by a series
of electric shocks.

Scott Dorsey, ICS Programming Lab,  Rich 110,
    Georgia Institute of Technology, Box 36681, Atlanta, Georgia 30332
    ...!{akgua,allegra,amd,hplabs,ihnp4,seismo,ut-ngp}!gatech!gitpyr!kludge

       [In nearby Alabama, the Shelby-Denton election was still unresolved
        according to the last report I saw (in the 12 Nov 86 Washington
        Post).  It seemed that each recount reversed the previous one, with 
        more new votes being discovered each time for the previous apparent
        loser, who became the new apparent winner.  PGN]
  
</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Micros in cars
</A>
</H3>
<address>
Paul Kalapathy
&lt;<A HREF="mailto:convex!paulk@a.cs.uiuc.edu ">
convex!paulk@a.cs.uiuc.edu 
</A>&gt;
</address>
<i>
Fri, 14 Nov 86 19:41:12 cst
</i><PRE>

    Personal anecdote:  A close friend of mine bought a 1984 Firebird.  He
was somewhat dissatisfied with the performance given that it had a
moderately large engine.  He is a software weenie, and had a friend who went
to work for GM doing whatever it is they do with those micros they put in
cars to control the engine.  This friend of his provided him with the
commented source code for the ROMs that are in the micros for that
particular model.  The ROMs were of the variety that is compatible with
2716s or 2732s or one of the other common EPROMS.  So, my friend proceeded
to mangle the micro in his Firebird to include a socket for the EPROM.  He
programmed an EPROM to change the fuel mixture vs. engine speed, etc.  The
car had better performance, and a top speed about 30mph higher than before
his modification (the gas mileage was substantailly worse, as were the
noxious emissions, I suppose).  In a word, it became a race car.

    I don't know what risk this poses to society, but it is rather amusing.

        -Paul Kalapathy
    [There are some interesting warranty and liability questions as to
     what the manufacturer and dealer roles are once you have tinkered. 
     There are also questions about what happens if you market such an
     extension, if it fails and causes loss of life, etc.  PGN]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
DMV computer networks
</A>
</H3>
<address>
Bob Campbell
&lt;<A HREF="mailto:hpdsd!campbelr@hplabs.HP.COM ">
hpdsd!campbelr@hplabs.HP.COM 
</A>&gt;
</address>
<i>
Sat, 15 Nov 86 01:03:39 pst
</i><PRE>
To: risks@csl.sri.com

My license also fell prey to the magic of computers.  Two miles across
the Ohio border, I was stopped going down a rather large hill and ticketed
for speeding.  Being a college student that would shortly be hundreds of 
miles away, I spent the money for the ticket in my local pub.

I was ticketed on my Illinois license which had the address of my father's
old house.  After graduation, I packed my bags and set out for California.
After living here for six months, I received notice from my insurance agent
that my policy was about to be cancelled.  It seems that they had finally
checked and found that my Illinois license had been cancelled by the state
of Ohio.  

The California DMV didn't care about my past record or that the license was
expired.  They stapled my old license to a form and in a quick (for CA) two
months I had a valid license.  

After paying bozo rates for 6 months with an insurance agent who worked
out of his car, I decided to check around.  Worried about losing coverage
again, I told the whole sad story to the agent.  Bad record, dropped policy
and all.  She called the California DMV to run a check.  Three days later
I was not only insured, but I now get the good driver rate.

I ran through computers that talked too much, that ignored each other and
that had the right information but didn't bother to tell.  Also involved
were the "computers must be right" people who wouldn't let me pay the higher
rates. (Not that they had to work to talk me out of it :-)

If nothing else, I think I figured out why so many bad drivers seem to be
on California highways . . .

Bob Campbell   Hewlett Packard Information Technology Group
               hplabs!hpdsd!campbelr

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Serious security bug in 3.4 
</A>
</H3>
<address>
Dave Martindale 
&lt;<A HREF="mailto:dave%onfcanim.waterloo.edu@RELAY.CS.NET">
dave%onfcanim.waterloo.edu@RELAY.CS.NET
</A>&gt;
</address>
<i>
Mon, 10 Nov 86 15:10:17 est
</i><PRE>
Remailed-To-RISKS-From: Bill Park &lt;PARK@SRI-STRIPE.ARPA&gt;

In the 3.4 release, the cp/mv/ln command is setuid root in order to be
able to rename directories.  (Cp, mv, and ln are three links to the
same file).  Unfortunately, it isn't careful enough about where it
makes use of its root privileges.  Making use of this bug, anyone can
become the super-user by typing just a few commands.

I do not intend to describe this method of breaking security here.
However, to avoid becoming victim to it, you should remove setuid from
cp/mv/ln.  Although this means that only root will be able to rename
directories, I can see no other way of protecting yourself from the bug
until SGI fixes the program.

This bug existed in a previous release and I reported the problem to
SGI.  Whoever "fixed" the bug simply masked some of the symptoms
without fixing the problem.  I've reported it once again; let's hope
they fix it correctly this time.

	Dave Martindale, watmath!onfcanim!dave

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
"Maj. Doug Hardie" and his story
</A>
</H3>
<address>
&lt;<A HREF="mailto:Bruce_Schuck%SFU.Mailnet@MIT-MULTICS.ARPA">
Bruce_Schuck%SFU.Mailnet@MIT-MULTICS.ARPA
</A>&gt;
</address>
<i>
Sat, 15 Nov 86 08:58:40 PST
</i><PRE>

I certainly hope the Major left that filter in place.

Maybe programs like the one he describes should have the occasional
student who doesn't fit the profile just to see what the result is.

In this case it seems to have worked out.

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
 Necessity of language skills
</A>
</H3>
<address>
Daniel G. Rabe
&lt;<A HREF="mailto:       <DAN09697%NUACC.BITNET@WISCVM.WISC.EDU> ">
       &lt;DAN09697%NUACC.BITNET@WISCVM.WISC.EDU&gt; 
</A>&gt;
</address>
<i>
Sat, 15 Nov 86 14:58 CST
</i><PRE>
To:  risks@csl.sri.com

I do hope we're not beating this topic into the ground, but I am
compelled to respond to Col. G. L. Sicherman's response to my
orignal message on the dangers of letting computers do our thinking.

I agree with his point that

&gt; Standard spelling is invaluable for the efficiency of reading print.

Then he says:

&gt; The flip side is that standard spelling is _not_ invaluable for electronic
&gt; communication, because efficiency no longer matters--it's a measure left
&gt; over from the machine age.  Efficient absorption is important only in one-
&gt; way, bulk media like print.  Electronic communication is interactive.

I cannot agree that "efficiency no longer matters".  Electronic communication
puts unprecedented amounts of information at our fingertips.  Now that
we have so much more to read, efficient absorption is even more important.

Even if the interactive nature of electronic communication makes it easier
to ask for clarification or to ask questions, we must still communicate
with some people non-electronically.  Non-computer people often judge
communication skills by one's ability to follow the standard rules of
spelling, grammar, and punctuation.  If we ignore these rules, we will
probably just alienate ourselves from those who follow and respect them.

This introduces another potential risk: that the inability to communicate
effectively with non-computer professionals will adversely affect the
usability of the systems we develop for them.  An even more immediate
risk is a loss of confidence: "He can't even follow the rules of English;
how can I be sure he's a good programmer?"  From our perspective, this
is an obvious _non sequitur_; from another perspective, it might make
a lot of sense.

(To make myself clear, I don't consider "following the rules" to be any
indication of intelligence or ability.  The point is that a lot of people
consider language skills to be a prerequisite for effective communication.)

     [Since Daniel started this one, I thought I'd let him have another shot.
      But I am still rejecting most commentaries on this subject.  PGN]

</PRE>
<A NAME="subj9"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj9.1">
Call for Papers -- Safety and Reliability Society Symposium
</A>
</H3>
<address>
Nancy Leveson 
&lt;<A HREF="mailto:nancy@ICSD.UCI.EDU">
nancy@ICSD.UCI.EDU
</A>&gt;
</address>
<i>
14 Nov 86 20:52:37 PST (Fri)
</i><PRE>

        "Achieving Safety and Reliability with Computer Systems"
            Manchester, United Kingdom, 11-12 November, 1987

Papers relating to the following system aspects of real-time computers
are invited:
       
       Integrity throughout the lifecycle
       Safety Assessment
       Reliability Assessment
       Reliability Criteria
       Safety Criteria
       Specification for safety and reliability
       Design for safety and reliability
       Architecture for safety and reliability
       Development for safety and reliability
       Operation for safety and reliability

Papers are also invited that report on experience of the implementation
and use of computers in safety and reliability critical applications.

HOW TO SUBMIT A PAPER
   Synopses giving the title, authors, affiliations, and up to 500 words
should be returned to the organiser by 7 January 1987.  The initial
selection of papers by the International Programme Committee will be
based on the synopses.  Authors will be notified of acceptance at synopsis
stage by 28 February 1987.  Full text papers of not more than 4000 words
required before 15 May 1987.  Papers will then be reviewed, and formal
acceptance notified to authors in July 1987 following satisfactory revision
of the paper by the author.

ORGANISER:  SARSS '87, The Safety and Reliability Society Ltd., Clayton House,
            59 Piccadilly, Manchester M1 2AQ, United Kingdom

INTERNATIONAL PROGRAMME COMMITTEE:  B.K Daniels, Chairman; T. Anderson, UK;
N. Leveson, USA; E. de Agostino, Italy; R. Bell, UK; P. Bishop, UK; R.
Bloomfield, UK; S. Bologna, Italy; J. Cullyer, UK; G. Dahll, Norway; W.
Ehrenberger, Germany; R. Genser, Austria; J. Gorski, Poland; G.B. Guy, UK;
E. Johnson, UK; S. Lindskov Hansen, Denmark; S.R. Nunns, UK; I. Pyle, UK;
W.J. Quirk, UK; J.M.A. Rata, France; F. Redmill, UK; C. Roberts, Belgium; B.
Runge, Denmark; L. Sintonen, Finland; I.C. Smith, UK; U. Voges, Germany; T.
Williams, USA; R. Yunker, USA

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.11.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.13.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-90</DOCNO>
<DOCOLDNO>IA012-000125-B044-293</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.13.html 128.240.150.127 19970217010031 text/html 15858
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:58:58 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 13</TITLE>
<LINK REL="Prev" HREF="/Risks/4.12.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.14.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.12.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.14.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 13</H1>
<H2> Tuesday, 18 November 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Framing of life-and-death situations 
</A>
<DD>
<A HREF="#subj1.1">
Jim Horning
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  On placing the blame 
</A>
<DD>
<A HREF="#subj2.1">
Peter J. Denning
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Computer picks wife 
</A>
<DD>
<A HREF="#subj3.1">
Matthew Kruk
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: Micros in cars 
</A>
<DD>
<A HREF="#subj4.1">
Brint Cooper
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Re: They almost got me! 
</A>
<DD>
<A HREF="#subj5.1">
Will Martin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Re:  A variation of the Stanford breakin method 
</A>
<DD>
<A HREF="#subj6.1">
Joe Pistritto
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Microfiched income-tax records stolen 
</A>
<DD>
<A HREF="#subj7.1">
John Coughlin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Re: Copyrights 
</A>
<DD>
<A HREF="#subj8.1">
Andrew Klossner
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Framing of life-and-death situations
</A>
</H3>
<address>
Jim Horning
&lt;<A HREF="mailto:horning@src.DEC.COM ">
horning@src.DEC.COM 
</A>&gt;
</address>
<i>
Tue, 18 Nov 86 17:31:40 pst
</i><PRE>

In the "1986 Accent on Research Magazine" published by Carnegie Mellon
University there is an article on "The Science of Decision Making" by
Robyn Dawes. The whole article is interesting, but I was particularly
struck by a passage that succinctly states an issue we have often skated
around in Risks:

    ... Such a contradiction violates any model of human decision making
    based on a premise of rational choice. Such framing effects also
    lead decision makers faced with life and death situations to act
    conservatively when the alternatives are framed in terms of lives
    saved (because the first life saved is the most important), but take
    risks when the same alternatives are framed in terms of lives lost
    (because the first life lost is the most important--thereby leading
    to a desire to avoid losing any lives at all). The result can be a
    contradictory choice for identical life and death problems, depending
    upon how they are framed.

    ... have demonstrated not only that framing affects decision, but
    that people systematically violate the rules of probability theory
    by adopting--either explicitly or implicitly--certain heuristics to
    evaluate the likelihood of future outcomes. ...

Jim H.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
On placing the blame
</A>
</H3>
<address>
Peter J. Denning 
&lt;<A HREF="mailto:pjd@riacs.edu">
pjd@riacs.edu
</A>&gt;
</address>
<i>
Tue, 18 Nov 86 14:34:50 pst
</i><PRE>
ReSent-To: RISKS@CSL.SRI.COM

In recent issues of RISKS there were two items that on the surface
did not appear to be in the stated purview of RISKS:

   A.  Two jetliners in near-miss.  Controller unable to warn
       the pilots because there was an open microphone jamming
       the frequency.

   B.  Young girl suffocates from carbon monoxide fumes generated
       by home grille after power company turned off power for non-
       payment of bills but delayed resumption due to operator error.

I asked Peter Neumann about this.  With respect to (A), he said,
radar is a vital component of the system: it is called INPUT.
Vulnerabilities of radars affect the ability of the computer to
do its job.  With respect to (B), he said, a computer operator
put in incorrect data, which contributed to the problem.

In both cases, there is a total system containing an embedded computer
system.  In (A), for example, the total system includes the jetliners,
the pilots, the radars, the radios, the computers, and the controllers.
In (B), the total system includes the customers (especially the
unfortunate family), power distribution, review of requests for welfare
status, and the computer accounting system.

In both cases, there is a temptation to ascribe safety failures in the
total system to one of its components, the embedded computer, and by
implication to make the designers of that software responsible.  In (A),
the computer could not possibly have compensated for jammed radio
frequencies.  In (B), there is a possibility that, had the computer
operator entered correct data, power would have been restored a few
days sooner, in time to forestall the death of someone in that
household; however, the child's parent, not the computer designers or
operator, chose to heat the cold house with a lethal fuel and to defer
application for welfare status until after the power was turned off.

In both cases, a variety of factors combined to create the unfortunate
circumstance.  The embedded computer systems could not have been
programmed to prevent the mishap.  And yet the news reports contain
suggestions that computers, or their operators, are somehow at fault.
Have some journalists become unduly accustomed to fingering the
computer for every mishap?  Have some computer people become unduly
eager to accept the blame when there is a mishap in a system that
contains a computer?

Peter Denning

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Computer picks wife
</A>
</H3>
<address>
&lt;<A HREF="mailto:Matthew_Kruk%UBC.MAILNET@MIT-MULTICS.ARPA">
Matthew_Kruk%UBC.MAILNET@MIT-MULTICS.ARPA
</A>&gt;
</address>
<i>
Mon, 17 Nov 86 08:00:52 PST
</i><PRE>

(Associated Press) November 15th

      IZMIR, Turkey - A man who divorced his wife after a bitter
   six-year court battle and turned to a computer service to find
   himself the "ideal" mate was surprised when - from 2,000
   prospective brides - the machine selected his former wife.

      "I did not know that my ex-wife had been the ideal counterpart
   for a marriage," Suleyman Guresci was quoted as saying by the
   Anatolia News Agency before re-marrying Nesrin Caglasa.

      "I decided to try being more tolerant toward her," He said.

      The couple, whose first marriage lasted 21 years, were divorced
   nine months ago due to "severe disharmony" after living apart for
   six years, Anatolia reported.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
 Re: Micros in cars
</A>
</H3>
<address>
    Brint Cooper 
&lt;<A HREF="mailto:abc@BRL.ARPA">
abc@BRL.ARPA
</A>&gt;
</address>
<i>
Mon, 17 Nov 86 15:42:40 EST
</i><PRE>

There's another risk of re-programming your engine control ROMs.  
It's a federal offense to remove or alter the operation of emission 
control equipment.  Since fuel mixture and ignition affect emission 
levels, they are considered emission control.

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
 Re: They almost got me!
</A>
</H3>
<address>
    Will Martin -- AMXAL-RI 
&lt;<A HREF="mailto:wmartin@ALMSA-1.ARPA">
wmartin@ALMSA-1.ARPA
</A>&gt;
</address>
<i>
Tue, 18 Nov 86 9:50:24 CST
</i><PRE>

Your note on RISKS impressed me tremendously. What you described has so many
odds against it that the fact that it happened just HAS to be significant.
Just what that significance is, I am not sure, but it must be important!
The odds against the occurrence  of the unlikely combination of grades and
data that would get through the filtering code are themselves high, but,
as you said, at least two people's records produced this -- the number
of possible students and their grade combinations could easily explain
this, so that, in itself, isn't significant. But the fact that you,
yourself one of these very few that fit this unusual mix of historical
data and participated in this special course, were then asked to rewrite
the computer program that contained this flaw is an incredible coincidence
in itself. However, the fact that this was a special honors humanities
course, the graduates of which would NOT be likely to be computer or
programmer types, takes the odds out of the merely "incredible" category
and puts them into some utterly indescribable astronomical range.

Thanks for sharing this with us. 

Regards,
Will Martin

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
 Re:  A variation of the Stanford breakin method
</A>
</H3>
<address>
Joe Pistritto (JHU|mike) 
&lt;<A HREF="mailto:@RELAY.CS.NET,@CSNET-RELAY.CSNET:jcp@BRL.ARPA">
@RELAY.CS.NET,@CSNET-RELAY.CSNET:jcp@BRL.ARPA
</A>&gt;
</address>
<i>

</i><PRE>

What you have here is the standard 'spoofing' problem.  I think the only way
to control this problem (for a system attached to the Internet) is to route
all the traffic thru a gateway (over which you have physical access control)
that will DROP immediately any packets originating from the Internet world
with SOURCE addresses that are anywhere on your local nets.  (You could put
insecure nets on the other side of a similar gateway inhouse, to protect the
'trusted' networks.)  Prevents anyone from spoofing along as one of your
hosts.  (This might cause some loopback features of TCP to stop working in
some implementations, however)  And yes, it means that the 'trusted' hosts
have to be on 'trusted' networks that are physically distinct (and of course
physically secure).

Begins to sound like DoD already, doesn't it...
							-jcp-

PS: Security is a pain the ass...  [So may be the absence of security!  PGN]

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
 Microfiched income-tax records stolen
</A>
</H3>
<address>
      John Coughlin  
&lt;<A HREF="mailto:JC%CARLETON.BITNET@WISCVM.WISC.EDU">
JC%CARLETON.BITNET@WISCVM.WISC.EDU
</A>&gt;
</address>
<i>
17 Nov 86 23:41:00 EST
</i><PRE>

It was announced in the Canadian House of Commons today that microfiche
containing personal income tax records for 16 million Canadian taxpayers was
stolen from a Toronto office of Revenue Canada on November 4.  The
microfiche was returned November 17 after being retrieved by the RCMP.  It
is not known whether the material was duplicated by the thief, who has not
been identified.

CTV news said that several hundred people had access to the microfiche in
the Toronto office.  Duplicate copies are kept in several district offices
as well.  This incident adds a new dimension to the recently discussed RISKS
of easily portable information media, such as hospital medical records on
computer diskettes.
                                                                 /jc

     [This item is at first blush of marginal relevance to RISKS strictly
      from the computer point of view -- unless the microfiche was computer
      generated (it was probably just a record of actual returns).  
      Nevertheless, I include it as symptomatic of the deeper problems.  PGN]

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
Re: Copyrights (RISKS DIGEST 4.8)
</A>
</H3>
<address>
Andrew Klossner 
&lt;<A HREF="mailto:tektronix!hammer.TEK.COM!andrew@ucbvax.Berkeley.EDU">
tektronix!hammer.TEK.COM!andrew@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Mon, 17 Nov 86 10:23:58 PST
</i><PRE>

    [Andrew wished to clarify the issue of whether there is a risk in 
     using "(c)" or a half-circled "c".  Although his response does not
     seem strictly RISKS related, I think it may clarify a thorny issue 
     for some of you who are willing to contribute to RISKS but want to
     protect your rights.  I have abridged it somewhat.  PGN]

It is the considered opinion of the chief legal counsel at Tektronix that
the genuine circled-c can be replaced only by the string "Copyright (c)".
Both the word "Copyright" and the pseudo-glyph "(c)" are required...

The three basic elements needed to obtain copyright protection in the
United States and the member countries of the Universal Copyright
Convention (most countries of any significance) are the copyright
symbol (circle-c or string "Copyright (c)"), the name of the copyright
owner, and the year date of first public distribution.  The law
requires that the notice "be affixed to the copies in such manner and
location as to give reasonable notice of the claim of copyright."

The phrase "All rights reserved" extends protection to member countries
of the Buenos Aires Convention who are not also members of the
Universal Copyright Convention (a few Latin American countries).

Whenever the program or document is revised significantly, the year
date of the revision must be added to the notice, as in:

	Copyright (c) 19XX, 19YY.

When licensing software to the (US) federal government under the the
Defense Federal Acquisition Regulation Supplement (DFARS), a completely
different set of legends is required.

  -=- Andrew Klossner   (decvax!tektronix!tekecs!andrew)       [UUCP]
                        (tekecs!andrew.tektronix@csnet-relay)  [ARPA]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.12.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.14.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-91</DOCNO>
<DOCOLDNO>IA012-000125-B044-315</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.14.html 128.240.150.127 19970217010047 text/html 19170
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:59:12 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 14</TITLE>
<LINK REL="Prev" HREF="/Risks/4.13.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.15.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.13.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.15.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 14</H1>
<H2>Wednesday, 19 November 1986</H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Re: On placing the blame 
</A>
<DD>
<A HREF="#subj1.1">
Matt Bishop
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  At last, a way to reduce [net]news traffic                  
</A>
<DD>
<A HREF="#subj2.1">
Jerry Aguirre via Matthew P Wiener
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Safety-Critical Software in the UK 
</A>
<DD>
<A HREF="#subj3.1">
Appendix B of ACARD report
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Re: On placing the blame (RISKS 4.13)
</A>
</H3>
<address>
Matt Bishop 
&lt;<A HREF="mailto:mab@riacs.edu">
mab@riacs.edu
</A>&gt;
</address>
<i>

</i><PRE>
Date: Wed, 19 Nov 86 15:59:31

There's an old joke about computer scientists who build the most
advanced, intelligent computer ever.  As a test, they ask it "Is there
a God?"  It responds, "There is now!"

Sadly, a lot of people tend to think of computers as infallible.
(We've discussed this in Risks before, I think.)  Computer scientists
know better, and try to educate the public to this fact of life.
Peter Denning asks "Have some computer people become unduly eager to
accept the blame when there is a mishap in a system that contains a
computer?"   If the answer is yes, one cause may be an eagerness to
demonstrate to the public that the machines are not perfect.

Others once thought of the computer as infallible but have come to realize
it is only as good as the people who build it, program it, and feed it data.
When something (or someone, for that matter) once put on a pedestal falls
off, there is a very human tendency to be more harsh towards that thing than
something never put on a pedestal.  We may be seeing some of this in
"journalists [becoming] unduly accustomed to fingering the computer for
every mishap" (although I suspect it's not just journalists who do this!)

There's also a third tendency at work here -- it's a lot easier to blame
someone whom you don't have to look in the eye.  With a human, you would
have to (say, when you were firing him, when you were prosecuting him, or so
forth.)  Also, a human can strike back verbally or nonverbally.  A computer
can do none of these things, and best of all you don't have to think about
its feelings when you chastise it.  Maybe that's part of it too.

Matt Bishop

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
At last, a way to reduce [net]news traffic 
</A>
</H3>
<address>
Matthew P Wiener
&lt;<A HREF="mailto:weemba@brahms.Berkeley.EDU ">
weemba@brahms.Berkeley.EDU 
</A>&gt;
</address>
<i>
Wed, 12 Nov 86 14:41:03 PST
</i><PRE>

Newsgroups: net.news
From: jerry@oliveb.UUCP &lt;Jerry Aguirre, Olivetti ATC; Cupertino, Ca &gt;
Date: 11 Nov 86 17:39:44 GMT

Most of you are probably aware that there was a premature posting of
newsgroup messages for all the proposed newsgroup renamings.  This caused
many (if not most) sites to exceed the maximum allowed number of newsgroups
in their active files.  Some sites are still recovering from this problem.

It is interesting to note that the volume of news articles for last week
was less than half what it was for the previous week.

	Oct 27 11:48 to Nov 1 23:58	6,755 articles
	Nov 1 23:59 to Nov 10 15:15	3,102 articles

The reduction in volume gives you some idea of the number of sites that
were blown off the air.

I know it took me a couple of hours to clean up old newsgroups,
recompile news with larger tables, and reprocess the failing batches.
(My news daemon renames and saves batches when rnews exits with an error
status.)  Multiply that times the number of sites on the net and you
probably get many thousands of manhours spent cleaning up.

Amazing to think how vulnerable the net is to the actions of one individual.

					Jerry Aguirre, Olivetti ATC

   [And this was precisely the glitch that triggered the macro error that
    led to the saga prior to the real RISKS-4.7!  To add to the irony,
    MPW's message slipped through a crack last week while I was travelling.
    I just found it while cleaning up the RISKS mailbox!  PGN]

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Safety-Critical Software in the UK
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Wed 19 Nov 86 14:20:48-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

             [John Rushby called to my attention a remarkable report on the
              British view of software in the future.  The entire report is
              fascinating reading, but in particular the following appendix
              is of sufficient interest to the RISKS community that it is
              reproduced here in its entirety for the private use of RISKS
              readers.  It represents an important step toward the problems
              of developing safety-critical software.  PGN]


            ``Software: A Vital Key to UK Competitiveness''
  Cabinet Office: Advisory Council for Applied Research and Development (ACARD)
      London, Her Majesty's Stationery Office.  (C) Crown Copyright 1986

                 Appendix B: Safety-Critical Software

The problem: non-technical

B.1 No computer software failure has killed or injured a large number of
people.  It is just conceivable that such a tragedy could occur.  What steps
should be taken to:

* prevent such a disaster,

* cope with it when it does occur,

* ensure such a disaster, having happened once, cannot recur?


The problem: technical

B.2 Stored-program digital computers must be among the most reliable
mechanisms ever built by man.  Millions of computers throughout the
world are executing millions of instructions per second for millions
of seconds without a single error in any of the millions of bits from
which each computer is made.  In spite of this, nobody trusts a
computer; and this lack of faith is amply justified.

B.3 The fault lies not so much in the computer hardware as in the programs
which control them, programs full of the errors, oversights, inadequacies
and misunderstandings of the programmers who compose them.  There are some
large and widely used programs in which hundreds of new errors are
discovered each month; and even when these errors are corrected, the error
rate remains constant over several decades.  Indeed it is suspected that
each correction introduces on average more than one new error.  Other
estimates offer the dubious comfort that only a negligible proportion of all
the errors in these programs will ever be discovered.

B.4 New computers are beginning to be used in increasingly life-critical
applications, where the correction of errors on discovery is not an
acceptable option, for example industrial process control, nuclear reactors,
weapon systems, station-keeping of ships close to oil rigs, aero engines and
railway signalling .  The engineers in charge of these projects are
naturally worried about the correctness of the programs performing these
tasks, and they have suggested a number of expedients for tackling the
problem.  Many of these methods are of limited effectiveness because they
are based on false analogies rather than on a true appreciation of the
nature of computer programs and the activity of programming.

B.5 The steps which ACARD has been considering in answer to the
introductory question are discussed under the following headings:

* Disaster prevention

* Disaster management

* Disaster analysis


Disaster prevention

B.6 The initiative for disaster prevention must come from the UK government
and system customers.  Current software is built, operated and maintained
using methods and tools which are not keeping pace with the development of
the hardware, nor with the increased sophistication demanded by new
applications; nor does it take account of progress of research into the
reliability of programs.  The necessary improvements in software engineering
require investment in advanced development and production techniques,
education, training and legislation.  Legal obligations should be at least
as stringent as those imposed by the Data Protection Act, and the care and
time required for detailed drafting of legislation will be just as great.  A
start must be made immediately.

B.7 The remainder of this appendix outlines an imaginable solution that may emerge over the next fifteen years.  It is intended to promote rather than to pre-preempt a discussion of the details.


Registration

B.8 A register must be established of those (software) systems which,
if they fail, will endanger lives or public safety.


Operation (demand side)

B.9 Before any organization can operate a life-critical computer
system it must first obtain a License To Operate (LTO), which will
only be issued when the operator can demonstrate that certain
conditions (detailed below) have been met.

B.10 Each life-critical system must be operated by a Certified Software
Engineer who is named as being personally responsible for the system.  This
Certified Software Engineer must have received the appropriate mathematical
training in safety-critical software engineering.

B.11 A life-critical system must be adequately maintained; this must
be one of the conditions of the LTO.  Maintenance (that is,
rectification and development) must be the responsibility of a named
Certified Software Engineer.


Certification

B.13 An LTO must only be granted when a Safety Certificate has been issued.
Certificates must be issued for limited periods, for example, five years.
Operational systems will thus need to be recertificated (relicensed)
periodically (analogous to Certificate of Airworthiness).


Reliability data collection

B.14 To aid research into system reliability, and to assist Boards of
Enquiry, all registered life-critical software systems must supply
operating data on the Licensing Authority.


Disaster management

B.15 In the past, the danger arising from failure of computer hardware and
software has been limited by switching off the computer and reverting to
manual operation if necessary.  In future, there will be applications for
which this fall-back procedure is not available.  The computers will have to
continue to run, and any necessary software changes and corrections will
have to be inserted into the incorrectly running system.  For these
applications, specially stringent precautions are necessary.


Procedures

B.16 The Licensing Authority should require disaster management procedures
to be laid down in advance of operation and practiced regularly during
operation (that is 'fire drill practice').  The documentation of the system
must need a standard which would permit a team of experts/specialists to
master it during the progress of an emergency.


Data Logging

B.17 The disaster management procedures should include the logging of data
so that any subsequent Enquiry can ascertain the progress and cause of the
disaster (analogous to the 'black box recorder' in an aeroplane).


Emergency call-out

B.18 There must be more than one Certified Software Engineer available to
the operating company; and a duty rota should ensure that one of them is
always available at short notice.  Procedures must be set up for calling out
a team of expert specialists in a longer-lasting emergency.


Disaster analysis

B.19 During the normal (safe) operation of any life-critical system,
data on its performance and reliability must be made available to the
Licensing Authority.  This data will be made available to any Enquiry.
(This is additional to the data logging required in para B.14.)


Board of Enquiry

B.20 Any disaster should be the subject of an official Board of Enquiry
(similar to rail and air disaster enquiries).  A Board of Enquiry must have
the power to make changes to the system under investigation and/or the
methods, tools, products and staff associated with the certification
procedure.


Any error triggers Board of Enquiry

B.21 Any error, no matter how 'small', in a software system which has been
certified as being safe must be subject of an Enquiry.  This is the only way
of discovering weaknesses in the certification process itself, or misuse or
misunderstanding of its application.  Enquiries concerning non-fatal errors
should not have disciplinary implications, so that operators are encouraged
always to give notification of minor faults.


Near Miss

B.22 Any serious 'near miss' must be reported to the Licensing
Authority.  An Enquiry should be held if the Licensing Authority is
concerned at the incident's implications.


Safety certification

B.23 The UK must develop the ability to certify safety aspects of
software system construction and operation.  These include:

* certification of the mathematical soundness of the methods of construction;

* certification that certified methods are properly applied during
construction and subsequent maintenance (rectification and
development);

* certification of the tools used during construction and maintenance;

* certification of the software engineers who build and maintain the systems;

* certification of the end product, that is, the software itself.


B.24 Methods should not be certified which are merely 'good practice'.
Safety and reliability require more rigorous theoretical bases than
existing good practice, so that system behavior can be accurately and
consistently predicted; hence the need for mathematical soundness to
enable prediction to be based on mathematical proof.

B.25 Certification of a tool will only be given when it is shown that
the tool preserves the mathematical soundness of the method is supports.

B.26 Certification of software engineers will only be given when they have
completed an approved level of formal mathematical and methodological
training together with an approved track record of experience.
Certification should be of limited duration; recertification should require
additional formal training both of the refresher type and new developments.
Recertification should occur at regular intervals.

B.27 Certification of end products (and their components) implies proof
obligations in addition to thorough testing.  Proofs must be performed and
checked by competent mathematicians or by a machine running certified
software.

B.28 As in other branches of engineering, the rigour of the inspection
procedures should be adjusted to the degree of risk, the severity of
the danger and the cost.  For example, we can imagine the emergence of
several levels of certification:

	a.  Disaster Level.  Failure could involve more than ten deaths.
The whole of the software must be checked by formal mathematical proof,
which is itself checked by a competent mathematician.  Further precautions
required if damage limitation by switch-off is not feasible (para B.15).

	b.  Safety Level.  Where failure could cause one death, but further
danger can be averted by switch-off.  The whole of the software must be
constructed by proof-oriented methods, checked by a competent mathematician.
On occurrence of a fatality, the mandatory Enquiry must name the programmer
and mathematician responsible, who might be liable for criminal negligence.
Perhaps one error per 100,000 lines of code would be a realistic
expectation, so that most shorter programs will contain no errors.

	c.  High Quality Level.  Appropriate for software sold commercially,
where error could bring financial loss to the customer.  By law, such losses
should be reimbursed.  All programmers involved should be certified
competent in mathematical methods of software design and construction.
Their use of the methods is checked by sampling.  An acceptable error rate
would be one error per 10,000 lines of code delivered.  Each error corrected
requires recertification at Safety Level.  If the target error rate is
exceeded, certification is withdrawn.  Eventually, all software used to
construct other certified software should be certified to this level; and
the construction of 'disaster level' software should include independent
checks on the correct working of support software used (for example, check
of binary code against higher level source codes).

	d.  Normal Quality.  Corresponds roughly to the best of current
practice (say, one error per 1,000 lines of code).  The methods used to
construct software to higher levels of reliability may also be used to
achieve normal reliability; and this should bring a significant improvement
in programmer productivity and a reduction in the whole life cycle costs of
the programs they produce.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.13.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.15.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-92</DOCNO>
<DOCOLDNO>IA012-000125-B044-343</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.15.html 128.240.150.127 19970217010118 text/html 20263
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 00:59:39 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 15</TITLE>
<LINK REL="Prev" HREF="/Risks/4.14.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.16.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.14.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.16.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 15</H1>
<H2>Thursday, 20 November 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
IBM VM/SP SP Cracked 
</A>
<DD>
<A HREF="#subj1.1">
Jack Shaw
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  On placing the blame AND Safety-Critical UK Software 
</A>
<DD>
<A HREF="#subj2.1">
Bjorn Freeman-Benson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  On placing the blame 
</A>
<DD>
<A HREF="#subj3.1">
Scot Wilcoxon
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Safety-Critical Software in the UK 
</A>
<DD>
<A HREF="#subj4.1">
Scott E. Preece
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Computer-based stock trading 
</A>
<DD>
<A HREF="#subj5.1">
from Discover
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  FAA's Role in Developing a Mid-Air Collision-Avoidance System 
</A>
<DD>
<A HREF="#subj6.1">
Chuck Youman
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
     IBM VM/SP Cracked
</A>
</H3>
<address>
Jack Shaw  
&lt;<A HREF="mailto:JDS2F%UOTTAWA.BITNET@WISCVM.WISC.EDU">
JDS2F%UOTTAWA.BITNET@WISCVM.WISC.EDU
</A>&gt;
</address>
<i>
Tue, 4 Nov 1986 22:32:40 EST
</i><PRE>
To: Security List &lt;SECURITY@RUTGERS.ARPA&gt;
Remailed-To: RISKS@CSL.SRI.COM

It appears someone (student hacker) has cracked VM. Anyone interested
in this should contact their IBM SE about APAR VM26824. Looks like
a pretty serious breach too...Hacker was able to change anyone's CP
class from A-H or their own CP class.
                                         Jack Shaw, Univ. of Ottawa

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
On placing the blame and Safety-Critical UK Software (RISKS 4.14)
</A>
</H3>
<address>
Bjorn Freeman-Benson
&lt;<A HREF="mailto:bnfb@beaver.cs.washington.edu ">
bnfb@beaver.cs.washington.edu 
</A>&gt;
</address>
<i>
Thu, 20 Nov 86 12:44:36 PST
</i><PRE>
Organization: U of Washington, Computer Science, Seattle

I do not have a copy of the ACARD report, but judging from Appendix B,
this report attempts to put almost all the blame for computer failures
on the software, rather than the hardware, operation or the combined system.

  &gt;B.3 The fault lies not so much in the computer hardware as in the programs
  &gt;which control them, programs full of the errors, oversights, inadequacies
  &gt;and misunderstandings of the programmers who compose them...

Any system is only as strong as it's weakest link, and so any Certified
software will have to be written, installed, run on and operated by Certifed
people and machines.  But, worse than that, what about interactions between
the software and the hardware, or even other software (like the OS)?  For
example, Certified package A runs on Certified OS B on Certified hardware C.
Something in C fails and B takes care of it, but in doing so response time
falls until A fails (such as running a ship into an oil-rig).  

Certifying software would be a big step forward, but I think that
concentrating on just one part of the whole system will not safely Certify
that system.  For example, reread the past N RISKS where time after time an
operator error has caused problems.  Or look at the real experience that I
based the previous example on:

    We had a PDP-11 (not Certified) in which a board failed sending an
    inordinate number of spurious interrupts to the CPU.  The OS handled them
    all, but response time went down by 80%.

If the system failed that way, who would be held liable?
                                                               Bjorn

   [First, we have been around this question on numerous occasions.  There
    is often NO ONE PLACE TO PUT THE BLAME.  Second, the ACARD report sets
    out to make a strong case for what the UK should do WITH RESPECT TO
    SOFTWARE.  In that context, I don't think the report as a whole denies
    that other factors are not also critical; it just focuses on software.  
    (The rest of the report is certainly of interest to software engineers.
    By the way, don't ask me about how to get copies.  Ask HMSO.  Perhaps
    one of our British correspondents can provide ordering information.)  PGN]

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: On placing the blame (Peter J. Denning, <A HREF="/Risks/4.14.html">RISKS-4.14</A>)
</A>
</H3>
<address>
rutgers!meccts!mecc!sewilco@seismo.CSS.GOV 
&lt;<A HREF="mailto:Scot Wilcoxon">
Scot Wilcoxon
</A>&gt;
</address>
<i>
Thu, 20 Nov 86 11:35:24 EST
</i><PRE>

In the [first cited] example, the collision-avoidance method failed because
the air traffic controller could not communicate with the aircraft.  The
present method cannot compensate for jammed radio frequencies, unless the
aircraft are monitoring the international emergency channel and the
controller thinks of trying it.
                   [Observation: Even though the jammed frequency is not a 
                    computer problem per se, it greatly impacts the ability
                    of the computerized ATC system to do its job.  PGN]

Other recent postings have pointed out the centralized characteristic of the
existing collision-avoidance methods preferred by the FAA and compared them
to an aircraft-based Honeywell system.  The distributed Honeywell system has
the advantage of not depending upon the ground-based computer and
communication with it.

The present system includes distributed jammers, one on board every aircraft.
Scot E. Wilcoxon   Minn Ed Comp Corp  {quest,dayton,meccts}!mecc!sewilco
(612)481-3507           sewilco@MECC.COM       ihnp4!meccts!mecc!sewilco

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Safety-Critical Software in the UK
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%mycroft@GSWD-VMS.ARPA">
preece%mycroft@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Thu, 20 Nov 86 09:27:44 CST
</i><PRE>

The proposed regulation of safety-critical software in the U.K. is very
interesting.  What kind of status does the committee that wrote it have?
Are these proposals that are likely to turn into law or are they just
suggestions?
     [This report comes from a very highly respected committee.  After 
      some debate, the proposals may very well get turned into law!  PGN]

The notion of responsibility is a central element of the proposal.  That's a
very good thing.  Everyone building systems should be thinking at all times
that they are assuming responsibility for the use of their products.  That
responsibility should extend to anticipating the potential misuses of the
system as well as to failures to perform to spec.

The proposed definitions at the end make it clear that this proposal is
broader than it might first seem.  They apparently propose to classify and,
presumably, certify systems which endanger money as well as lives.

Defining the threat to life is, of course, non-trivial (shades of the 3+
laws of robotics).  Would the administrative system implicated in the
power-shutoff death reported here a few days ago have been considered
life-critical?  Would avionics systems for which non-automated, but less
capable, backups are available?  Is a program doing image enhancement on
satellite pictures used by weather forecasters life-critical?  How about the
operating system it runs on?

scott preece, gould/csd - urbana, uucp:	ihnp4!uiucdcs!ccvaxa!preece

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Computer-based stock trading        [Some repetition, some new things]
</A>
</H3>
<address>
&lt;<A HREF="mailto:rutgers!meccts!ems!adam@seismo.CSS.GOV">
rutgers!meccts!ems!adam@seismo.CSS.GOV
</A>&gt;
</address>
<i>
Thu, 20 Nov 86 15:58:04 EST
</i><PRE>
Organization: EMS/McGraw-Hill, Eden Pairie, MN

               December 1986 DISCOVER, v7 #12 p13:

                   "SCIENCE BEHIND THE NEWS"
            "DID COMPUTERS MAKE STOCK PRICES PLUMMET?"

News item:  On Thursday, Sept. 11, 1986, the Dow Jones industrial average
dropped 86.61 points, to 1792.89 -- a 4.61 per cent plunge.  A record 237.6
million shares changed hands.  The next day 240.5 million shares were traded,
and the Dow fell 34.17 more points.  Though the decline on Black Thursday
paled next to that of Black Friday, Oct. 28, 1929, when the Dow fell 38.33
points, or a whopping 12.82 pre cent, Wall Street was shaken, and it's still
looking for the cause.  The Securities Exchange Commission (SEC) is
now investigating the possibility that computerized program trading may
have been a contributing factor.
	The decline actually began on Wednesday, Sept. 10, the day before
the big drop.  The bond market in London looked weak, which suggested that
interest rates would remain high, and there were signs of impending
inflation. As always, these indications of a slumping economy drove the
price of stocks down.
	But many analysts believe that the drop was accelerated (though not
initiated) by computer-assisted arbitrage.  Arbitrageurs capitalize on
what's known as the spread: a short-term difference between the price of
stock futures, which are contracts to buy stocks at a set time and price,
and that of the underlying stocks.  The arbitrageurs' computers constantly
monitor the spread and let them know when it's large enough so that they can
transfer their holdings from stocks to stock futures or vice-versa, and make
a profit that more than covers the cost of the transaction.
	The computer programs used by arbitrageurs are based on simple
mathematical formulas that take into account the prices of stocks and
futures, dividends, and interest rates.  "It doesn't require you to have 20
megabytes," says John Barbanel, director of futures trading at Gruntal and
Co. in New York.  In fact, the math can be done on the back of an envelope.
But by the time a trader could do the calculations for his entire portfolio,
the market opportunity would've passed, the price of futures and stocks
changed.  With computers, arbitrageurs are constantly aware of where a
profit can be made.
	However, throngs of arbitrageurs working with the latest information
can set up perturbations in the market.  Because arbitrageurs are all
"massaging" the same basic information, a profitable spread is likely to
show up on many of their computers at once. And since arbitrageurs take
advantage of small spreads, they must deal in great volume to make it worth
their while.  All this adds up to a lot of trading in a little time, which
can markedly alter the price of a stock.  If, say, the arbitrageurs see that
the price of a future has dropped below the price of its underlying stock,
they may buy futures and sell the stock, en masse.  Although Barbanel
emphasizes that arbitrage stabilizes the market over a period of weeks and
months, it can cause a lot of volatility within a single day.
	"Some trader on the floor of the New York Stock Exchange sees all
the arbitrageurs selling at once and bringing down the value of stocks," so
he sells too, says Hayne Leland, the director of Leland O'Brien Rubinstein
Associates, a Los Angeles investment management firm.  Heavy selling leads
to more heavy selling -- and even lower stock prices.  And the fast
calculations of computers can only magnify these effects.  Barbanel says
that 20 per cent of the 86-point drop on Thursday may have come from
computer-assisted arbitrage.

    [A different item included in the same message noted that Standard&amp;Poor
     now reports the S&amp;P 500 index and S&amp;P 100 composite stock price index
     every fifteen seconds instead of once each minute.  (For those people who
     really like to think they are inside the action?  In case you want to
     make your computer-program-based trading "more precise"?)  PGN]

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
FAA's Role in Developing a Mid-Air Collision-Avoidance System
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Thu, 20 Nov 86 16:01:28 -0500
From: Chuck Youman &lt;m14817@mitre.ARPA&gt;

There have been a couple of items in RISKS lately about mid-air
collision-avoidance systems.  The FAA's role in developing a mid-air
collision-avoidance system was the subject of testimony presented at a
Congressional hearing in September by a GAO official, Herbert R. McLure.  A
copy of his statement can be ordered from the GAO.  The accession number is
131086.  See RISKS 3-67 for their address.  Some of the points Mr. McLure
made in his testimony:

  Controversy still surrounds FAA's 1976 decision to pursue its own system
  rather than fund one that was being developed commercially.  This
  controversy remains largely because the technical problems associated with
  developing FAA's system have proved to be much more complex and
  time-consuming than originally anticipated.  Our work has shown, however,
  that FAA's decision was supported by the aviation community and that, while
  a number of technical problems have delayed the commercial availability of
  FAA's system, these problems have apparently been solved.  Significant
  issues must still be addressed, however, during the testing and
  certification process before FAA's system is ready for commercial use.
 
  By the 1970's private industry was developing several different systems.
  After testing three, FAA decided that the Honeywell AVOIDS was the most
  promising, but even it had shortcomings.  While the technical problems
  found with AVOIDS were correctable, the most serious shortcoming in all
  three systems FAA tested was that converging aircraft would only be warned
  of each other's proximity if they were both equipped with the system.  
  Since no aircraft had AVOIDS, FAA surmised that a federal mandate would
  have been required to ensure that the system was installed in enough
  aircraft to provide an adequate level of protection.

  Conversely, commercial aircraft equipped with FAA's system, then called
  the Beacon Collision Avoidance System, or BCAS, would be warned of the
  proximity of all other aircraft having a transponder and would receive
  recommended collision-avoidance maneuvers if the other aircraft had an
  altitude encoder.  Since over 100,000 aircraft, or about 65 percent of the
  air fleet, already had transponders, [. . .] FAA believed that its system
  would offer more immediate protection at less cost to the avaition 
  community and that an adequate level of protection could be obtained
  without mandating the system's purchase by all aircraft owners.  Polls
  of aircraft owner and user groups in 1976 and 1979 showed that FAA's
  decision held substantial aviation community support.

  Honeywell stopped development of its AVOIDS system soon after FAA decided
  to proceed with BCAS.  In the intervening 10 years, FAA has encountered
  a number of technical problems that have slowed the development of its
  system, now called TCAS.  In June 1981, FAA's Administrator announced that
  TCAS would be the national standard for mid-air collision avoidance, and
  that the system would be operational nationwide by mid-1985 at the latest.
  While this announcement was overly optimistic, it now appears that the
  known technical problems with the system have been solved.  Testing the
  system in an operational environment and certification are all that remain
  before at least one model of TCAS can be commercially produced.

  FAA's involvement in TCAS research and development has been unusual in that
  it has been conducted in-house by FAA's TCAS program engineering group
  instead of by private industry.  Through its Office of Airworthiness,
  certification of TCAS' effectiveness is also FAA's responsibility.

  Some TCAS program officials felt that FAA's involvement in research and
  development has resulted in over-cautiousness by the Office of 
  Airworthiness in the certification process, and that TCAS is being 
  subjected to much more scrutiny than it otherwise would have been.

  Another kind of problem involves product liability.  FAA officials told
  us they are concerned that if a mid-air collision should occur because
  pilots follow a faulty TCAS resolution advisory, FAA may have to accept
  responsibility and liability for the collision.  They also think the
  issue of product liability would have been a major concern for private
  industry if it had developed the system.

A more complete report is also available from GAO:  "Air Safety:  Federal
Aviation Administration's Role in Developing Mid-Air Collision Avoidance
Back-Up Systems," GAO/RCED-86-105FS, Accession number 129832, April 22, 1986.

A number of the comments I have seen seem to imply that it would still be
possible to implement the Honeywell system.  Since its development was stopped
10 years ago, I doubt it.  Also, I don't think it is valid to criticize a 
decision because in retrospect in may not have been the "best" decision.
I think the criteria should be whether the decision was reasonable based on
the information that was available at the time the decision was made.
Both alternatives were viewed as being technically feasible (and this appears
to be correct even in retrospect).  

An issue that I think we should be discussing in RISKS is whether it is 
appropriate for the same organization to develop and approve critical
systems.  I think some degree of organizational independence is an absolute
requirement.

Charles Youman (youman@mitre.arpa)

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.14.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.16.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-93</DOCNO>
<DOCOLDNO>IA012-000125-B044-366</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.16.html 128.240.150.127 19970217010133 text/html 15356
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:00:02 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 16</TITLE>
<LINK REL="Prev" HREF="/Risks/4.15.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.17.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.15.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.17.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 16</H1>
<H2>Saturday, 22 November 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Banking machine almost ruins love life of Vancouver couple 
</A>
<DD>
<A HREF="#subj1.1">
Mark Brader
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  2+2= ? (Risks of self-testing, especially with nonexistent tests) 
</A>
<DD>
<A HREF="#subj2.1">
Lindsay
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: Computer-based stock trading 
</A>
<DD>
<A HREF="#subj3.1">
Roger Mann
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: appendix to ACARD report 
</A>
<DD>
<A HREF="#subj4.1">
Nancy Leveson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Some further thoughts on the UK software-certification proposals 
</A>
<DD>
<A HREF="#subj5.1">
Dave Platt
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Dependable Computing and the ACM Communications 
</A>
<DD>
<A HREF="#subj6.1">
PGN
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
"Banking machine almost ruins love life of Vancouver couple"
</A>
</H3>
<address>
mnetor!lsuc!dciem!msb@seismo.CSS.GOV 
&lt;<A HREF="mailto:Mark Brader">
Mark Brader
</A>&gt;
</address>
<i>
Fri, 21 Nov 86 14:28:20 est
</i><PRE>

VANCOUVER (CP) -- Automated banking machines could prove hazardous to
your love life, as an unidentified Vancouver woman can testify.

The woman tried to use her banking card to get money from an automatic
teller in Honolulu.

"But by the time the message went, via satellite, from Hawaii to the
central computer in New Jersey, then via land line to Seattle and
Vancouver, then back to Hawaii, the teller machines [sic] had gone past
its allowable waiting time," says a credit union spokesman.  The woman
did not get any money but the credit union in Vancouver took the money
out of her account.

When the woman learned her account had been debited $1,100, she accused
her fiance of taking it.

The fiance moved out and the woman reported the theft to the police,
who picked up the man for questioning.  It took almost a month for the
two banks involved to solve the problem.  The couple has since reunited.

[Reproduced from the Toronto Star, November 20, 1986.  Submitted
to RISKS by Mark Brader.  Glossary for foreign readers: a "credit
union" is similar to a bank; $1,100 Canadian is about $800 US.]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
2+2= ? (Risks of self-testing, especially with nonexistent tests)
</A>
</H3>
<address>
&lt;<A HREF="mailto:LINDSAY@TL-20B.ARPA">
LINDSAY@TL-20B.ARPA
</A>&gt;
</address>
<i>
Sat 22 Nov 86 19:18:45-EST
</i><PRE>
To: risks@CSL.SRI.COM

If another car cuts in front of mine, then I would usually be alert enough
to take evasive action. But, suppose ! The day will come when I happen
to be looking at the scenery, or when there is a patch of mud on the road:
and then the two problems compound into something serious.

It is in just this compounding manner that minor events turn into major events.

Once upon a time, a friend of mine was using a microprogrammed box to process
satellite images. One day, it seemed to be malfunctioning: and in fact, when
he looked inside, some of the error-indication LEDs were glowing.

Naturally, he ran the hardware test suite. However, the suite indicated that
all was well. And thus it came about that my friend investigated the suite -
and found that although they had written it, and although he ran it, it
wasn't there !

The tests had been written in the only language which the box had, namely, 
a pretty homebrew assembler for its (wide) microcode. The assembler gave
rather difficult listings, and did not finish by giving a count of errors.
As a result, 4 of the 8 tests had in fact never assembled, and the
programmer hadn't noticed.

Now, the host machine had a downloader, and it had an idiotic property.  When 
asked to download a file which did not exist, it would simply create a null
file, and then download that. Pardon ?  Did I hear the phrase "error message" ?

On top of all this, the box's loader did not set the memory to a known
state (like, all zero) before loading a file.

Worse yet, all of the 8 tests started at the same address, and printed the
same messages (e.g. "Test starting"). 

We therefore see how an operator could faithfully run tests 1 through 8
without ever knowing that in fact, tests 5, 6, 7 and 8 did not exist !

We can also blame the original programmer for never having simulated a
hardware error, to see if the tests caught it. And where was his manager ?
And where is he now - out building missile guidance systems, maybe ?

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
 Re: Computer-based stock trading
</A>
</H3>
<address>
&lt;<A HREF="mailto: RMann%pco@HI-MULTICS.ARPA">
 RMann%pco@HI-MULTICS.ARPA
</A>&gt;
</address>
<i>
Fri, 21 Nov 86 14:51 MST
</i><PRE>
To:  risks@SRI-CSL.ARPA

Computer arbitrage should be self-limiting, just as pre-computer arbitrage
is self-limiting.  The price differential between a future and the stock
index tends to permit arbitrage to occur.  The question is who profits and
who loses ?  Clearly, after one of the huge price moves in a stock, the last
arbitrager will experience a loss.  Too many losses and he exits the game.
Thus we have one less computer trader.  Eventually, the number of successful
computer traders should be the number who don't experience losses, and the
stock price moves we see should be limited to smaller percentage moves.

Why hasn't this occurred ?  A couple answers suggest themselves.  (1)
Computer arbitrage is not to blame any more than human-speed arbitrage is to
blame.  (2) Volatility as is perceived is not there (the same percentage
move now as in 1974 would be three times as much in a absolute stock move.)
(3) Other factors which are hidden and not well understood.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: appendix to ACARD report
</A>
</H3>
<address>
Nancy Leveson 
&lt;<A HREF="mailto:nancy@ICSD.UCI.EDU">
nancy@ICSD.UCI.EDU
</A>&gt;
</address>
<i>
21 Nov 86 16:26:19 PST (Fri)
</i><PRE>

I am somewhat concerned by the implication in the report that checking the
software by formal mathematical proof is the answer to the safety problem.

Although I believe that mathematical proof and certainly mathematical
analysis should play an important role in building safety-critical software,
it alone certainly will not guarantee an acceptable level of risk.  Putting
aside technical questions of whether it can be accomplished at all (e.g.
what if the software contains real numbers?), formal mathematical proof can
be used to show only the consistency between the specification and the
program (or between levels of specification).  BUT most accidents involving
software have not been caused by coding errors but rather by
misunderstandings about what the software should have been doing at all or
erroneous assumptions about the actions of the environment or the controlled
system, i.e. specification errors.  It is the things that are left out or
forgotten that cause the most problems.  Furthermore, mathematical proof of
the software will not handle the cases where the accident occurs because of
the interaction between the software and the controlled system -- the
software was "correct" in the usual formal mathematical sense.

Safety is a system problem and one cannot guarantee software safety by 
looking only at the software or by mathematically proving properties of 
the software in isolation from the operation of the rest of the system.

Nancy Leveson

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Some further thoughts on the UK software-certification proposals
</A>
</H3>
<address>
Dave Platt
&lt;<A HREF="mailto:dplatt@teknowledge-vaxc.ARPA ">
dplatt@teknowledge-vaxc.ARPA 
</A>&gt;
</address>
<i>
Fri, 21 Nov 86 10:28:33 PST
</i><PRE>

The proposals in the ACARD report seem to place a great deal of emphasis on
mathematical proof-of-correctness of computer programs (and the tools used
to build them).  I wonder just how practical this is, given the current
state-of-the-art in software construction and theory, and I have a few
questions to toss out.

Disclaimer: I'm a [reasonably good] programmer, not a high-power
computer-science theorist;  my knowledge of the state-of-the-art in
correctness proofs is fragmentary and badly out of date.  If I speak
from ignorance, please feel free to correct and enlighten me!

1) Are existing programming languages constructed in a way that makes
   valid proofs-of-correctness practical (or even possible)?  I can
   imagine that a thoroughly-specified language such as Ada [trademark
   (tm) Department of Defense] might be better suited for proofs than
   machine language; there's probably a whole spectrum in between.

2) Is the state of the art well enough advanced to permit proofs of
   correctness of programs running in a highly asynchronous, real-time
   environment?

3) Will the compilers have to be proved mathematically correct also?  or
   might something like the Ada compiler/toolkit validation be adequate?

4) The report seems to imply that once a system is proven correct/safe,
   it can be assumed to remain so (for the [limited] lifetime of its
   License to Operate) so long as maintenance is performed by a
   certified software engineer.  Is this reasonable?  My own experience
   is that _any_ patch or modification to a program, no matter how minor
   it may seem, has a pretty substantial chance of causing unwanted
   side effects and thus voiding the program's correctness.  Seems to me
   that a life-critical system should be completely revalidated (if not
   necessarily recertified) after any change, and that changes should
   probably be made in the original programming language rather than by
   low-level patches.

5) Many of the program "failures" I've encountered in "stable" software
   have been due to unexpected inputs or unplanned conditions, rather than
   to any identifiable error in the program itself.  Can any proof-of-
   correctness guard against this sort of situation?

6) What are the legal aspects of this sort of proposal, from the programmer's
   point of view?  Anybody got a good source of Programmers' Malpractice
   insurance?

7) Are the error-rate goals suggested in the report (1 error per 100,000
   lines of code, or even less?) reachable?

8) Military systems such as the SDI control software would appear to belong
   to the "disaster-level" classification... will they be subject to this
   level of verification and legal responsibility, or will they be exempted
   under national-security laws?  [Of course, if an SDI system fails,
   I don't suppose that filing lawsuits against the programmer(s) is going
   to be at the top of anybody's priority list...]

9) If the certified software engineer responsible for a particular
   piece of life-critical code resigns or is reassigned, is it reasonable
   to assume that another (equally-qualified) CSE could in fact take over
   the job immediately (on an urgent-call-out basis, for example)?

I respect the committee's concern for this problem, but I wonder whether
they haven't focused too much on one aspect (software correctness) at
the expense of considering other aspects (hardware reliability, adequate
specification of operating conditions, interfaces to humans and external
physical control systems, etc.).

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Dependable Computing and the ACM Communications
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Wed 19 Nov 86 19:40:40-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

There is an announcement by John Rushby in the November 1986 issue of the
Communications of the ACM (pp. 1031-2) regarding the establishment of
Dependable Computing as a CACM department -- regarding systems that must
dependably satisfy certain critical requirements such as safety, fault
avoidance, and fault tolerance.  This announcement is also noteworthy in
that it provides a concise, easily accessible summary of some generally
accepted terminology that contributors to RISKS would do well to observe and
practice, including the Melliar-Smith / Randell distinctions among faults,
failures, and errors.  It is hoped that RISKS readers with serious technical
contributions may find this CACM department an appropriate printed medium.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.15.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.17.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-94</DOCNO>
<DOCOLDNO>IA012-000125-B045-9</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.17.html 128.240.150.127 19970217010150 text/html 21685
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:00:16 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 17</TITLE>
<LINK REL="Prev" HREF="/Risks/4.16.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.18.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.16.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.18.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 17</H1>
<H2> Monday, 24 November 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Computer Risks and the Audi 5000    
</A>
<DD>
<A HREF="#subj1.1">
Howard Israel with excerpts from Brint Cooper
</A><br>
<A HREF="#subj1.2">
 Charlie Hurd
</A><br>
<A HREF="#subj1.3">
 Clive Dawson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Risks of changing Air Traffic Control software? 
</A>
<DD>
<A HREF="#subj2.1">
Greg Earle
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: the UK Software-Verification Proposal 
</A>
<DD>
<A HREF="#subj3.1">
Bard Bloom
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Program Trading 
</A>
<DD>
<A HREF="#subj4.1">
Howard Israel
</A><br>
<A HREF="#subj4.2">
 Eric Nickell
</A><br>
<A HREF="#subj4.3">
 dmc
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Decision Making 
</A>
<DD>
<A HREF="#subj5.1">
Clive Dawson
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
 Computer Risks and the Audi 5000
</A>
</H3>
<address>
 Howard Israel 
&lt;<A HREF="mailto:HIsrael@DOCKMASTER.ARPA">
HIsrael@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Sun, 23 Nov 86 23:49 EST
</i><PRE>
To:  neumann@CSL.SRI.COM
ReSent-To: RISKS@CSL.SRI.COM

The 23 November 60-Minutes tore apart Audi, Inc.  It seems that the Audi
5000 model (automatic transmission) has a terrible habit of accelerating
when moved from PARK to either FORWARD or REVERSE.  The problem has been
denied by Audi.  They blame driver error ("They step on the gas instead of
the brake").  Of course this appears to only be a problem with drivers of
the 5000 model, none of the other models has such poor drivers.

The "alleged" defect is blamed for about 250 known accidents, and at
least one death.                                       [See more below.]

The alleged causes of the alleged problem (I should have been an alleged
lawyer) include 1) excessive pressure build-up in the transmission, 2) a
faulty "vacuum" (not sure of the exact words ??) unit (which Audi has
voluntarily notified its customers needs replacement, or will result in
"performance" problems), and 3) a faulty on-board computer.

Although Audi insists that they cannot find a problem with the car, an
"independent" expert hired by a group of people that have all experienced
problems with the car (there are enough victims out there that a self-help
group was formed) actually demonstrated the gas pedal *visibly* moving
downward when the car was put into gear causing the alleged surging.

Even some valet parking garages have posted signs that they will not
accept the Audi 5000 with automatic transmissions.

Audi is so convinced that the problem is driver error that they have
issued a recall notice to install a safety switch so that the driver
could not change the gear unless pressure was on the brake.

Footnote:  Three accidents have occurred similiar to the stated alleged
problem that had the brake safety switch installed.  Audi said that 2 of the
cars had the switch improperly installed, and the third was unexplained.
      [Clive Dawson recalled "driver error" being cited for the third case.]

This whole incident is reminiscent of the Ford Pinto fiasco.

The Federal Transportation Safety Board (??)  is investigating.

Audi (The Art of Engineering) came out looking very bad.  (But what else
would you expect from 60 Minutes?)

Corporate responsibility appears very low.  I would not be surprised if
they came out with a corporate apology within a week (in time for the
next broadcast) to try to save face.
                                               Howard Israel

   [It is unusual for RISKS to get four different reviews of the same TV
    program! Excerpts from the others follow, with moderator's effort to
    minimize duplication and achieve accuracy.  PGN]

  Excerpts-From: Brint Cooper &lt;abc@BRL.ARPA&gt;

    Even while the driver (quite literally) stands on the brake pedal, the
    car roars ahead.  One young woman ran over and killed her own
    three-year-old son. 

    The "idle stabilizer" was said to be responsible for keeping a minimum
    flow of fuel to the engine during idle when the brakes are applied.  The
    idle stabilizer is either a part of a computer-controlled system or is
    controlled by an on-board computer; it wasn't clear which.  

    Audi denies that anything is wrong.  Two Audi representatives appeared
    on camera to assert that they could find nothing wrong with the car.  
    They even claimed that the motorists are stepping on the wrong pedal.

         Brint

  Excerpts-From: churd@labs-b.bbn.com &lt;Charlie Hurd&gt;

    The cars have accelerated with enough force to punch through walls.
    Many of the cars have been totalled.

    Audi has checked the cars in question and failed to find any defects.  They
    claim that the drivers became confused and pressed on the gas pedal instead
    of the brake.  The drivers (one of them a police officer trained to drive
    under extreme conditions) maintain that they were trying to put the brake
    pedal through the floor, without effect. 

    It seems to me that this is good response to Peter Stokes's question
    (RISKS-4.5) about the risks of buying/driving a car with a computer-
    controlled engine.  The only question I have is why the brakes did not
    stop the car.  Some of the victims said that they had to turn off the
    car to stop it.  Do Audi 5000s have anti-skid braking?  Could this have
    allowed the cars to keep moving?  Is this an example of many small
    malfunctions resulting in a *major* problem?

         Charlie

  Excerpts-From: Clive Dawson &lt;AI.CLIVE@MCC.COM&gt;

    This has resulted in at least one death. A young (6-year-old?) boy
    was let out of the car to open the garage door, after which the mother
    stepped on the brake and shifted to forward.  The car hit the boy, pushed
    him completely through the garage door and pinned his already-crushed body
    against the rear wall of the garage.  A heavy black skid mark was left
    which showed how even then the wheels continued to spin at a high rate of
    speed.  The Audi people claim that all of these accidents are the result of
    driver error, in which the accelerator is mistaken for the brake.  One of
    the more memorable quotes from Audi: "We're not saying we can't FIND
    anything wrong with the car; we're saying there ISN'T anything wrong with
    the car."

    Attention is focusing on a microprocessor-controlled mechanism which
    regulates the idle speed.  Apparently Audi has sent letters to all owners
    of the vehicles involved stating that this part will be replaced by Audi
    for "performance reasons".  The report didn't make it clear whether the
    microprocessor was an integral part of this part or not, so I don't know
    if this replacement will involve a change in the processor or its software.

    I don't know what the final verdict on this will be.  But listening
    to that devastated mother tell how she witnessed the death of her
    son, and knowing the cause might eventually be tracked down to
    some software bug sent chills down my spine.

         Clive

</PRE>
<HR><H3><A NAME="subj1.2">
Risks of changing Air Traffic Control software?
</A>
</H3>
<address>
Greg Earle
&lt;<A HREF="mailto:elroy!smeagol!earle@csvax.caltech.edu ">
elroy!smeagol!earle@csvax.caltech.edu 
</A>&gt;
</address>
<i>
Fri, 21 Nov 86 22:47:04 pst
</i><PRE>

I haven't seen it mentioned yet, but I believe that last week I saw
a news story that purported to blame a crash of a small light plane
in the Southern California area on a changeover of software in
either a radar system or a general flight controller computer system,
causing either the plane to be lost from the screens or directed into
a hillside.  Since my memory is vague, perhaps someone else can
provide a better recollection of this RISK of computer software.

Greg Earle, JPL

            [The delay in running this item was due to an unsuccessful
             attempt to get further information...  PGN]

</PRE>
<HR><H3><A NAME="subj1.3">
Re: the UK Software-Verification Proposal 
</A>
</H3>
<address>
Bard Bloom 
&lt;<A HREF="mailto:bard@THEORY.LCS.MIT.EDU">
bard@THEORY.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Sun, 23 Nov 86 12:41:15 est
</i><PRE>

Disclaimer: I'm a grad student working in semantics of programming languages,
and therefore qualified to pretend to know the answers to these questions.  I
haven't been studying semantics all that long, though.  These are solely my
opinions and bear no necessary resemblance to those of my advisor, my
department, or my ceramic dragon.

&gt; From: dplatt@teknowledge-vaxc.ARPA (Dave Platt) (RISKS 4.16)

&gt; 1) Are existing programming languages constructed in a way that makes
&gt;    valid proofs-of-correctness practical (or even possible)?  I can
&gt;    imagine that a thoroughly-specified language such as Ada [trademark
&gt;    (tm) Department of Defense] might be better suited for proofs than
&gt;    machine language; there's probably a whole spectrum in between.

No, they are not.  Actually, there are a few existing programming languages
(Euclid, for one) which are, but most popular ones are not.  A
precisely-specified language is easier to prove things about than an
imprecisely-specified one, of course.  I haven't seen anything approaching a
precise mathematical semantics for Ada; if the research in semantics of
distributed semantics goes very well we might be able to give you one in ten or
fifteen years if we're lucky.  The best languages for proving things about are
functional languages (FP, Hope, Lucid, ISWIM).  I have yet to hear of a "real
program" written in any of these.  

&gt; 2) Is the state of the art well enough advanced to permit proofs of
&gt;    correctness of programs running in a highly asynchronous, real-time
&gt;    environment?

No.  Not even remotely.  We can't cope with slightly-asynchronous,
non-real-time environments in any general way.

&gt; 3) Will the compilers have to be proved mathematically correct also?  or
&gt;    might something like the Ada compiler/toolkit validation be adequate?

The compiler will have to be proved too, if the idea of proving programs
correct is to make any sense.

&gt; 4) The report seems to imply that once a system is proven correct/safe,
&gt;    it can be assumed to remain so (for the [limited] lifetime of its
&gt;    License to Operate) so long as maintenance is performed by a
&gt;    certified software engineer.  Is this reasonable?  [...]

It is reasonable if you re-prove the patched system.  I can't imagine it being
reasonable otherwise.  Note: you can probably patch the proof also, if it is
arranged in a nicely modular form.  

&gt; 5) Many of the program "failures" I've encountered in "stable" software
&gt;    have been due to unexpected inputs or unplanned conditions, rather than
&gt;    to any identifiable error in the program itself.  Can any proof-of-
&gt;    correctness guard against this sort of situation?

Not really.  All the proof guarantees is that the software does what the
specification does.  That's a big help, since you don't usually have even that.
But you have to get the specification right.

(I can't even pretend to answer questions about legal aspects.)

&gt; 8) Military systems such as the SDI control software would appear to belong
&gt;    to the "disaster-level" classification... will they be subject to this
&gt;    level of verification and legal responsibility, or will they be exempted
&gt;    under national-security laws?  [Of course, if an SDI system fails,
&gt;    I don't suppose that filing lawsuits against the programmer(s) is going
&gt;    to be at the top of anybody's priority list...]

That's a terrifying thought: don't verify Star Wars, it's too secret to have
the code so exposed!  

-- Bard Bloom

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
 Program Trading 
</A>
</H3>
<address>
 Howard Israel 
&lt;<A HREF="mailto:HIsrael@DOCKMASTER.ARPA">
HIsrael@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Sun, 23 Nov 86 23:49 EST            [Other half of Howard's message]
</i><PRE>
To:  neumann@CSL.SRI.COM
ReSent-To: RISKS@CSL.SRI.COM        

Today's Washington Post, Sunday, November 23, 1986, pg K1 [Business
Section] contains an interesting article on program trading and the
Finance theory behind it all.  (The following is partly based on the
article and partly from my own knowledge.)

The basic idea is to view all of the different financial instruments as
interrelated, even though the instruments may be traded on different
markets across the country (or around the globe).  The people that make
it all work are called "Quants" (standing for Quantitative analyst).
The "Quants" create models based on the markets and their
interelationships and known financial theory.  When an "inefficiency"
occurs (i.e., the price differential of an underlying security in two or
more markets occurs that is big enough to cover the cost of the
transactions involved), the computers that monitor the information issue
simultanous buy and sell orders in the appropriate places.  The net
effect is the *total* elimination of risks once the initial set of
transactions are complete (the winding up).  (This is a simplification
of it all.  But the previous assertion in today's RISKS entry concerning
the "last trader" losing money is not accurate.)

The profit is "locked in" when the first set of trades are completed,
but will not be actually known until the positions are closed out
(winding down) at the end of the finanical instruments life.  The
"published" profit margin is said to be in the 7% to 9% (annualized)
range.  (Anything above the current T-bill rate is considered good.)
However, only each trader really knows what he is making.  (A personal
friend on "the street" claims that the profits are really much, much
higher because the "invested money" stays in the market a very short
time.  I am not convinced of this based upon my knowledge of the trading
--and "margin"-- necessary.)

The "Quants" differ from "Qualitative" traders, in that, Qualitative
traders base their trades on the perceived quality of the companies
(traditional recommendations of buy company ABC and sell company XYZ).

A nice analogy is made in the article to the gambling world.  The
"Quants" are the bookies, while the "Qualitative" traders are the River
Boat Gamblers that bet on instinct.

Has anybody thought of the implications (since the computers, based on
its programmed models and incoming data) of an error ?  Not only is big
money involved (it is estimated that one needs a *minimum* of $50
million to play the game), but so are bigger reputations (not just the
brokerage houses, but insurance companies, too).

Is it "bad" for the market ?  I think not.  When the computer generated
trades are executed they force market correction.  The article makes a
point that new financial instruments are emerging that will "play the
game", much like a mutual fund does now for the small investor.  These
instruments will limit the "downside" loss, while maintaining unlimited
"upside" gain.  Then these new instruments can be used in conjuction
with the already existing ones to create even more instruments, the end
result, potentially being, that in time anyone can bet the market in any
way.
                                              ---H

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: RISKS DIGEST 4.16, Computer-based stock trading
</A>
</H3>
<address>
&lt;<A HREF="mailto:Nickell.pasa@Xerox.COM">
Nickell.pasa@Xerox.COM
</A>&gt;
</address>
<i>
Sun, 23 Nov 86 19:33:07 PST
</i><PRE>
To: RISKS@CSL.SRI.COM

In response to Roger Mann:

(I mentioned this about a year ago in our last discussion of computerized
stock markets.)  Instantaneous and non-instantaneous negative feedback to
not produce the same results.  In this case, the fact that thousands of
computers can respond to the possibility of profit before the effects of the
responses get back (through whatever feedback loop) to any of them, opens
the door for disaster.

Eric Nickell

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Computer-based stock trading
</A>
</H3>
<address>
&lt;<A HREF="mailto:dmc%videovax.tek.csnet@RELAY.CS.NET">
dmc%videovax.tek.csnet@RELAY.CS.NET
</A>&gt;
</address>
<i>
Mon, 24 Nov 86 10:37:48 PST
</i><PRE>

The problem of decreasing system stability as time constants change is not a
new one.  Take the steam engine: "Watt's use of the flyball governor can be
taken as the starting point for the development of automatic control as a
science.  The early Watt governors worked satisfactorily, no doubt largely
due to the considerable amounts of friction present in their mechanism, and
the device was therefore widely adopted. ... However, during the middle of
the 19th century, as engine designs changed and manufacturing techniques
improved, an increasing tendency for such systems to hunt became apparent;
that is, for the engine speed to vary cyclically with time. ... This problem
of the hunting of governed engines became a very serious one (75,000
engines, large numbers of them hunting!) and so attracted the attention of a
number of outstandingly able engineers and physicists.  It was solved by
classic investigations made by Maxwell, who founded the theory of automatic
control systems with his paper "On Governors," and by the Russian engineer
Vyschnegradsky, who published his results in terms of a design rule,
relating the engineering parameters of the system to its stability.
Vyschnegradsky's analysis showed that the engine design changes which had
been taking place since Watt's time - a decrease in friction due o improved
manufacturing techniques, a decreased moment of inertia arising from the use
of smaller flywheels, and an increased mass of flyball weights to cope with
larger steam valves - were all destabilizing..."
	"The Development of Frequency-Response Methods in
	  Automatic Control",  Alistair G. J. MacFarlane,
	  IEEE Trans. Automat. Contr., pp. 250 - 265, Apr. 1979

</PRE>
<HR><H3><A NAME="subj4.2">
Decision Making 
</A>
</H3>
<address>
Clive Dawson 
&lt;<A HREF="mailto:AI.CLIVE@MCC.COM">
AI.CLIVE@MCC.COM
</A>&gt;
</address>
<i>
Mon 24 Nov 86 13:34:19-CST
</i><PRE>
To: risks@CSL.SRI.COM

Those interested in the recent item on the science of decision-making [see
Jim Horning, "Framing of Life-and-Death Situations", Risks 4.13] might find
this reference a bit more accessible:

   "Decisions, Decisions", by Kevin McKean.  DISCOVER Magazine,	June, 1985.

This article is a well written account of the work done by a number
of researchers, notably Daniel Kahneman and Amos Tversky, and has several
very nice examples of how the framing of a question affects the
decision making process.  

Anybody who had trouble locating CMU's "1986 Accent on Research Magazine"
would have better luck with Discover Magazine.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.16.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.18.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-95</DOCNO>
<DOCOLDNO>IA012-000125-B045-37</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.18.html 128.240.150.127 19970217010205 text/html 16129
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:00:33 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 18</TITLE>
<LINK REL="Prev" HREF="/Risks/4.17.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.19.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.17.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.19.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 18</H1>
<H2>Wednesday, 25 November 1986</H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
RISKS, computer-relevance, where-to-place-the-blame, etc. 
</A>
<DD>
<A HREF="#subj1.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Verification and the UK proposal 
</A>
<DD>
<A HREF="#subj2.1">
Jim Horning
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  When the going gets tough, the tough use the phone... 
</A>
<DD>
<A HREF="#subj3.1">
Jerry Leichter
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: 60 minutes reporting on the Audi 5000 
</A>
<DD>
<A HREF="#subj4.1">
Eugene Miya
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Minireviews of Challenger article and computerized-roulette book     
</A>
<DD>
<A HREF="#subj5.1">
Martin Minow
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  More on the UK Software-Verification Proposal 
</A>
<DD>
<A HREF="#subj6.1">
Bill Janssen
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
RISKS, computer-relevance, where-to-place-the-blame, etc.
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Tue 25 Nov 86 18:58:38-PST
</i><PRE>

This is another note on the risks of running RISKS.  We get a variety of
contributions that are not included in RISKS, on the grounds of relevance,
taste, lack of objectivity, politicization, etc.  (Once in a while I get a
flame about censorship from someone whose message is not included, but I
tend to stand by the masthead guidelines.)  I also get an occasional
complaint about my judgement regarding RISKS messages that have been
included.  So, it is time for some further comments from your moderator.

One of the most important things to me in running RISKS is that there is a
social process going on, at many levels.  First, there is an educational
function, in raising the level of awareness in many computer professionals
and students, whether naive or young, whether sophisticated or old.  Second,
there is a communications function of letting people try out their ideas in
an open public forum.  They also have an opportunity to become more
responsible communicators -- combining both of those functions.  Also, there
is the very valuable asset of the remarkably widespread RISKS community
itself -- there is always someone who has the appropriate experience on the
topic at hand.  By the way, I try not to squelch far-out thinking unless it
is clearly out of the guidelines.  This sometimes leads to unnecessary
thrashing -- although I try to minimize that with some of my [parenthetical]
interstices.

The Audi case is one in which computer relevance is not at all clear.
However, the presence of microprocessors indicates that it is worth our
while discussing the issues here.  The Audi problem is of course a total
system problem (like so many other problems).  I tend to include those cases
for which there is a reasonable connection with computer technology, but not
necessarily only those.  There are various important issues that seem worth
including anyway -- even if the computer connection is marginal.  First,
there are total systems wherein there is an important lesson for us, both
technological and human.  Second, there are total systems that are NOT AT
PRESENT COMPUTER BASED or ONLY MARGINALLY COMPUTER BASED where greater use
of the computer might have been warranted.  (Nuclear power is a borderline
case that is exacerbated by the power people saying that the technology is
too critical [or sensitive?] for computers to be used.  THEY REALLY NEED
DEPENDABLE COMPUTING TECHNOLOGY.  Besides, then THEY could blame the
computer if something went wrong! -- see second paragraph down.)

There is an issue in computer-controlled automobiles (even if the computer
is clearly "not to blame" in a given case) whether the increased complexity
introduced by the mere existence of the computer has escalated the risks.
But that is somewhat more subtle -- even though I think it is RISKS related...

The issue of simplistically placing blame on the computer, or on people (or
on some mechanical or electrical part), or whatever, has been raised here
many times.  I would like all RISKS contributors to be more careful in not
trying to seek out a single source of "guilt".

There are undoubtably a few people in our field who are bothered by
technological guilt.  There are others who are totally oblivious to remorse
if their system were to be implicated in an otherwise avoidable death.
However, the debates over blame, guilt, and reparation are also a part of
the "total systems" view that RISKS tries to take.

I try not to interject too many comments and not to alter the intended
meaning.  However, what YOU say reflects on YOU -- although it also reflects
on me if I let something really stupid out into the great Internet.  Also,
some discussions are just not worth starting (or restarting) unless
something really new comes along -- although newer readers have not been
through the earlier process, and that is worth something.

I have an awkard choice when a constructive contribution contains a value
judgement that is somewhat off the wall.  I sometimes edit the flagrant
comments out, despite my policy of trying to maintain the author's editorial
integrity.  I thought for a while about Clive Dawson's "knowing the cause
might eventually be tracked down to some software bug sent chills down my
spine."  The same could be said for the products of other technical
professionals such as engineers and auto mechanics.  (But that statement is
a sincere statement of Clive's feelings, and this one was left in.)

[Apologies for some long-windedness.  I probably have to do this every now
and then for newer readers.]  PGN

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Verification and the UK proposal (RISKS 4.17)
</A>
</H3>
<address>
Jim Horning
&lt;<A HREF="mailto:horning@src.DEC.COM ">
horning@src.DEC.COM 
</A>&gt;
</address>
<i>
Tue, 25 Nov 86 11:37:49 pst
</i><PRE>

I find myself largely in agreement with Bard Bloom's comments in
RISKS 4.17. However, it seems to me that recent discussion has
overlooked one of the most important points I saw in the UK proposal:
verification is a way of FINDING errors in programs, not a way of
absolutely ensuring that there are none. (The same is true of testing.)

Thus the kinds of question we should be asking are

  - How many errors (in programs AND in specifications) can be found by
  presently available proof techniques? How many errors would be avoided
  altogether by "constructing the program along with its proof"?

  - What is the cost per error detected of verification compared with
  testing? Does this ratio change as software gets larger? as the
  reliability requirements become more stringent?

  - Do verification and testing tend to discover different kinds of errors?
  (If so, that strengthens the case for using both when high reliability
  is required, and may also indicate applications for which one or the other
  is more appropriate.)

  - Can (partial) verification be applied earlier in the process of
  software development, or to different parts of the software than testing?

  - Is there a point of diminishing returns in making specifications
  more complete? more precise? more formal? of having more independent
  specifications for a program?

I would dearly love to have convincing evidence that verification wins
all round, since it would indicate that my work on formal specification
is more valuable. But, to date, I haven't seen any convincing studies,
and the arguments I can offer have been around for 10 or 15 years.
(They look plausible. Why can't we prove them?)
                                                            Jim H.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
 When the going gets tough, the tough use the phone...
</A>
</H3>
<address>

&lt;<A HREF="mailto:LEICHTER-JERRY@YALE.ARPA">
LEICHTER-JERRY@YALE.ARPA
</A>&gt;
</address>
<i>
25 NOV 1986 14:54:31 EST
</i><PRE>
  or, Would you trust your teen-aged computer with a phone of its own?

From Monday's (24-Nov-86) New York Times:

Lyons, Ore., a town of about 875 people about 25 miles east of Salem, the
state capital, has a small budget and a big problem.  The monthly city
budget is about $3,500.  Back in October, the public library, with an annual
budget of $1,000, installed a computer that made it possible to find a
requested book at any library in the county through one telephone call to
Salem.

After the trial run, no one knew that it was necessary to unplug
the computer.	It maintained the connection and ran up a bill of $1,328
with the Peoples' Telephone Company, the cooperative that runs the Lyons
phone system.

"It leaves a problem I've got to figure out," said Mayor Jerry Welter.  "I'm
going before the phone company board to ask them to forgive the bill, and I
don't know just how we'll manage if they won't do it."

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: 60 minutes reporting on the Audi 5000
</A>
</H3>
<address>
Eugene Miya
&lt;<A HREF="mailto:eugene@AMES-NAS.ARPA ">
eugene@AMES-NAS.ARPA 
</A>&gt;
</address>
<i>
Mon, 24 Nov 86 22:43:49 pst
</i><PRE>

It's interesting -- the four perspectives collected on this telecast.

  1) This was a subject broadcast several months ago on ABC 20/20.  No mention
  on the microprocessor problem was made at that time, but the idle problem
  was demonstrated.

  2) The microprocessor problem took very little time in the show, yet
  generated so much on RISKs (as it probably should).

  3) I recall TWO deaths in the program, not just one, and probably more.  Two
  correspondents pointed out the dead child, but the others did not mention
  the gas station attendent who was dragged underneath the car when it lurched
  backward over 200 feet.  Five different views of the same show.  (Rashomon)
  Could we expect a computer to do better?  I hope so.

--eugene miya

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Minireviews of Challenger article and a computerized-roulette book
</A>
</H3>
<address>
25-Nov-1986 1808
&lt;<A HREF="mailto:minow%bolt.DEC@src.DEC.COM ">
minow%bolt.DEC@src.DEC.COM 
</A>&gt;
</address>
<i>
Tue, 25 Nov 86 15:22:31 pst
</i><PRE>

"Letter from the Space Center" by Henry S. F. Cooper in the New Yorker,
November 10, 1986, pp. 83-114.  Discusses the Challenger accident and
the way it was investigated.  New (to me) information includes some
things that were known to the engineers before the accident, but not
taken into account when the decision to fly was made.  There is also
mention of a few things "hidden" in the appendices to the Presidential
Commission's report.

Nothing specific on computers, but a lot on the *management* of
technological risks, and -- as such -- would be interesting reading to the
Risks community.

Book: The Eudaemonic Pie, by Thomas A. Bass.  Vintage Books (paper),
Houghton-Mifflin (hardbound).  ISBN 0-394-74310-5 (paper).  Relates the
engrossing tale of a bunch of California grad students who decided that
roulette is "just" an experiment in ballistics (with a bit of mathematical
chaos theory thrown in).  Unfortunately, the adventurers were better
physicists than engineers and their computer system, built into a pair of
shoes, never worked well enough to break the bank.  They had some good
moments, though.  The physicists went on to more and better things, and have
just published an article on chaos in the current Scientific American.

Martin

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
More on the UK Software-Verification Proposal
</A>
</H3>
<address>
Bill Janssen 
&lt;<A HREF="mailto:janssen@mcc.com">
janssen@mcc.com
</A>&gt;
</address>
<i>
Tue, 25 Nov 86 18:09:22 CST
</i><PRE>

  &gt; Bard Bloom in RISKS 4.17:

  &gt; 1) Are existing programming languages constructed in a way that makes
  &gt;    valid proofs-of-correctness practical (or even possible)?  I can
  &gt;    imagine that a thoroughly-specified language such as Ada [trademark
  &gt;    (tm) Department of Defense] might be better suited for proofs than
  &gt;    machine language; there's probably a whole spectrum in between.
  &gt; 2) Is the state of the art well enough advanced to permit proofs of
  &gt;    correctness of programs running in a highly asynchronous, real-time
  &gt;    environment?

Drs. K. Mani Chandy and Jayadev Misra of the University of Texas at Austin
have developed a language called UNITY, which allows one to write programs
for distributed asynchronous systems, and reason about the relationship
between the program and its specification, which may allow one to prove that
the program correctly implements the spec.  (More often, one proves it does
not...)  At least one compiler for UNITY exists.

     [Further discussion on this probably belongs in Soft-Eng@XX.MIT.EDU.
      (See also various papers by Leslie Lamport.)  But I let this one
      through because proving properties of asynchronous programs is
      generally a very high-risk area.  Many asynchronous algorithms widely
      thought to be "correct" or "safe" or whatever are not...  PGN]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.17.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.19.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-96</DOCNO>
<DOCOLDNO>IA012-000125-B045-53</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.19.html 128.240.150.127 19970217010218 text/html 16799
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:00:48 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 19</TITLE>
<LINK REL="Prev" HREF="/Risks/4.18.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.20.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.18.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.20.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 19</H1>
<H2>Wednesday, 26 November 1986</H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Very Brief Comments on the Current Issues 
</A>
<DD>
<A HREF="#subj1.1">
Kim Collins
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  The Audi discussion is relevant 
</A>
<DD>
<A HREF="#subj2.1">
Hal Murray
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Audi 5000 
</A>
<DD>
<A HREF="#subj3.1">
Roy Smith
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Laser-printer health risks; also, how to get ACARD report 
</A>
<DD>
<A HREF="#subj4.1">
Jonathan Bowen
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Data point on error rate in large systems 
</A>
<DD>
<A HREF="#subj5.1">
Hal Murray
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Re: Program Trading 
</A>
<DD>
<A HREF="#subj6.1">
Roger Mann
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Technical merits of SDI 
</A>
<DD>
<A HREF="#subj7.1">
from Richard Scribner
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Very Brief Comments on the Current Issues
</A>
</H3>
<address>
"Kim P. Collins" 
&lt;<A HREF="mailto:kpc%duke.csnet@RELAY.CS.NET">
kpc%duke.csnet@RELAY.CS.NET
</A>&gt;
</address>
<i>
Wed, 26 Nov 86 13:32:05 EST
</i><PRE>

Verification
  It seems to me that there are a priori limits to the usefulness of
  verification for software engineering.  To prove that a program will
  work, even with the best system, is no doubt a non-trivial process, and
  hence subject to some of the same problems that the software design
  process has.

Relevance of contributions
  I think that we need not have computers involved for contributions to
  be relevant.  I see the limits being only those things that definitely
  belong elsewhere.  Computer science is a cybernetic science and a 
  science of cybernetics.  Cybernetics covers a lot.

Audi 5000
  The car must be incredibly powerful, or its control system INCREDIBLY
  unstable, to have caused so much damage.  From an engineering perspective, 
  assuming that it is not human error that is causing these occurrences, it
  seems that the unwise thing was to use an active control system (hence a
  risky one) with such a powerful machine.

New subject
  Any comments on active vs. passive control structures?  For instance,
  having a skyscraper that has flexible material so that in high winds
  it bends and does not fail, VS having a skyscraper that has guy wires
  connected to winches that are controlled by a computer that tests wind
  velocities, etc.

  My opinion is that ceteris paribus (and even ceteris non paribus in many
  cases) passive control structures are to be trusted and used far more
  than active control structures.  With active control structures, there
  are far more layers of abstraction and far more theories, designs, and
  sometimes materials that can fail.  (Other reasons exist.)

Computerization of nuclear power plants
  Computers can reduce the risks of cognitive overload and other human 
  problems, but they also have some of the problems raised above.  One
  advertisement by Carolina Power and Light during the most heated part
  of the Shearon Harris plant controversy here in NC said that the plant
  here would fail by dint of gravity in a relatively safe manner.  The
  plant in Chernobyl, it said or implied (I don't remember which), would
  not.  This is the active/passive control structure dichotomy applied to 
  one particular part of the computerization of a nuclear plant.  I think
  that we need to look at the different parts during the design stage and
  make certain that we minimize the active.  (No opinion on nuclear power is
  intended here.)

CSNET: kpc@duke, UUCP:  {ihnp4!decvax}!duke!kpc

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
The Audi discussion is relevant
</A>
</H3>
<address>
&lt;<A HREF="mailto:Murray.pa@Xerox.COM">
Murray.pa@Xerox.COM
</A>&gt;
</address>
<i>
Wed, 26 Nov 86 17:09:20 PST
</i><PRE>
To: NEUMANN@CSL.SRI.COM
ReSent-To: RISKS@CSL.SRI.COM

"The Audi case is one in which computer relevance is not at all clear."

It seemed quite relevant to me on two grounds.

First, adding a computer to an automobile is an important social
experiment, even if Audi didn't know they were taking part in one. I
can't think of any other application where people who probably don't
know much about computers are now depending upon computers as part of a
large complicated system where errors can easily kill people. I don't
watch TV, so I'm pleased to see that sort of information in RISKS.

The second aspect is the normal computer engineering problem (in a high
risk situation). I would like to know what went wrong. Hardware?
Software? System integration? Specification oversight? .... It's
probably a small computer and thus not very exciting relative to big
systems with megabytes of memory and millions of lines of code. Since
the results of a problem have been demonstrated to be very important (to
at least a few people), I think we should investigate this case in hopes
of learning something. Maybe it will even be easier to analyze because
the computer part of the system is so small.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Audi 5000
</A>
</H3>
<address>
Roy Smith
&lt;<A HREF="mailto:allegra!phri!roy@ucbvax.Berkeley.EDU ">
allegra!phri!roy@ucbvax.Berkeley.EDU 
</A>&gt;
</address>
<i>
Wed, 26 Nov 86 00:23:49 est
</i><PRE>

I also saw the 60 Minutes episode.  From the tone of the various messages in
RISKS 4.17, it sounds like everybody believes Audi is at fault.  All I saw
was a lot of anecdotal evidence and a lot of people who seem to think that
if they say something often enough and with enough emotion, it will become
true.  Lacking any real facts, I can't begin to make up my mind what the
answer is.  I'm certainly not going to decide based on the 60 Minutes
testimony of a woman who ran over her own son.  This is admittedly a
terrible thing to happen, but why should we give her claim that she had her
foot on the brake pedal any more or less credence than the claims of the
Audi engineers?  A comment:

  &gt; Clive Dawson &lt;AI.CLIVE@MCC.COM&gt;
  &gt; One of the more memorable quotes from Audi: "We're not saying we can't FIND
  &gt; anything wrong with the car; we're saying there ISN'T anything wrong with
  &gt; the car."

	Indeed, this is such a patently stupid thing to say that I'm now
almost *convinced* that there must be something wrong with the car.  Any
company that could hire somebody that would say something so absurd must
have problems.  Imagine somebody telling you "I'm not saying we can't FIND
any bugs in the SDI system, I'm telling you there AREN'T any." :-)

Roy Smith, {allegra,cmcl2,philabs}!phri!roy
System Administrator, Public Health Research Institute
455 First Avenue, New York, NY 10016

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Laser-printer health risks; also, how to get ACARD report
</A>
</H3>
<address>
Jonathan Bowen 
&lt;<A HREF="mailto:bowen%sevax.prg.oxford.ac.uk@Cs.Ucl.AC.UK">
bowen%sevax.prg.oxford.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Wed, 26 Nov 86 15:22:08 GMT
</i><PRE>

  Front page headlines from Computer News, 20 November 1986:

  `Health risk fears spur CCTA to probe laser standards'

  `Fears over laser printers have spurred the government's computer purchasing
  agency into questioning health and safety standards.  The Treasury's Central
  Computer and Telecommunications Agency (CCTA) has said it will investigate
  claims that the printers can cause chest infections, blindness and other
  serious health problems.  Already one major UK user, British Rail (BR), has
  delayed a decision on buying printers because of a lack of published safety
  standards.  
  ....leading laser printer-makers Apple, Hewlett-Packard and Xerox
  denied their products could be harmful.
  ....A senior CCTA official said: "We have looked at lasers...they
  can cause temporary blindness to some people."
  ....white collar union Apex, said: "...Many of our members within
  the industry have reservations about the safety of laser printers."
  Already the use of laser printers has caused a three-day strike
  by Danish postal workers until they were given safety assurances
  by the government.
  A report from a leading Danish laboratory has said damage to the
  retina and lungs can be caused by laser printers.
  ...In 1981, IBM voluntarily withdrew one substance, trinitroflurenone (TNF), 
  which was a photoconductor constituent in its Model 1 3800 laser printer.
  An IBM spokesman said: "We established it had a potential to be harmful,
  although not in the way we were using it."'

Is this going to be the same sort of scare as that associated
with VDUs? Has anyone else heard of these problems? Are there
appropriate safety standards in the US or elsewhere?

By the way, for anyone interested in the ACARD report, here is
an HMSO address:

  Her Majesty's Stationery Office, PO Box 276, London SW8 5DT, England
  Tel +44-1-622-3316

The cost of the report is 6 pounds. The HMSO will invoice you if
you apply to the above address. (Be prepared to pay in pounds.)

Jonathan Bowen

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Data point on error rate in large systems [Grapevine rot?]
</A>
</H3>
<address>
&lt;<A HREF="mailto:Murray.pa@Xerox.COM">
Murray.pa@Xerox.COM
</A>&gt;
</address>
<i>
Wed, 26 Nov 86 18:55:35 PST
</i><PRE>
To: NEUMANN@CSL.SRI.COM
ReSent-To: RISKS@CSL.SRI.COM

Grapevine is the mail system used by the Xerox R+D community. It has been
operational since 1981. Currently, there are 21 servers and roughly 4000
users. The servers have accumulated roughly 75 server-years of up time.

This spring, we discovered a fatal bug in the server code. It's been there
from the start. It was a simple recursive error in a very unlikely case.
Because the case was also uninteresting, nobody had bothered to "try it".

Fine print, if anybody cares:

The Grapevine database has two types of entries: groups and individuals.  An
individual is normally a person who reads/sends mail. A group is normally a
distribution list or an access control list. The members of a group can be
either individuals or other groups. Aside from the membership list, a group
also has a list of owners. The owners of a list are allowed to update it.
There are also pseudo groups. If you send a message to "Owners-xxx", the
system distributes the message to the owners (rather than members) of xxx.

Since a group can have members that are groups, there is the obvious
recursive problem. To check for this, the code that expands the
membership of a group runs up the call stack to see if another instance
of itself is already expanding this group. Unfortunately, the code that
processed Owners-xxx asked if anybody was already expanding xxx, while
they all thought they were working on Owners-xxx. Thus if Owners-xxx was
an owner of xxx, and anybody asked if Joe was an owner of xxx, poof.

PS: Mike Schroeder told me that they used to discover a new horrible
bug/oversight roughly every time the size of the system doubled.

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
 Re: Program Trading
</A>
</H3>
<address>
&lt;<A HREF="mailto: RMann%pco@HI-MULTICS.ARPA">
 RMann%pco@HI-MULTICS.ARPA
</A>&gt;
</address>
<i>
Wed, 26 Nov 86 13:48 MST
</i><PRE>
To:  risks@SRI-CSL.ARPA

I apologize to anyone for carrying this on further, but I am still not
convinced that computers are creating the wide stock price swings that
we see today in the market.  Assuming a model of some sort that detects
"inefficiencies", there must be a range of stock or option or futures
for which the inefficiency holds.  Beyond those thresholds, the no-lose
situation does not exist and should be avoided.

Now I am a dabbler in stocks and I know about limit orders.  Limit orders
are filled if the price of the stock is below a certain price on a purchase
or if the price is above a certain price on a sale.  This is extremely
useful when trying to establish a hedged position.  Now, I can't imagine
these super-sophisticated arbitrageurs issuing MARKET orders -- it is too
absurd to imagine.  If the hedger issues limit orders, the trades do not
occur and the stock price stays relatively stable.

Now, is there anyone out there who has direct knowledge of these things
and is willing to spill the beans and give us the straight scoop ?  Are
computers the risk here or not ?

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Technical merits of SDI
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Wed 26 Nov 86 13:22:31-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

The following note from Richard A. Scribner, Committee on Science, Arms 
Control and National Security at the AAAS may be of interest to RISKS readers.

  A detailed discussion of the technical merits of SDI, particularly software,
  will be held as part of the First Annual AAAS Colloquium on Science, Arms
  Control, and National Security, 4-5 December 1986 in Washington DC. Among
  the distinguished speakers will be Lt.Gen. James Abrahamson, director of
  SDIO; James R. Schlesinger, Center for Strategic and International Studies;
  William Graham, science advisor to the President; Adm. Noel Gayler; Albert
  Carnesale, Dean of the Kennedy School of Government at Harvard; Dante 
  Fascell, chairman of the House Committee on Foreign Affairs and chairman of
  the House subcommittee on arms control; Kosta Tsipis, director of the MIT
  Program on Science and Technology for International Security.  For 
  information and registration details, please call the American Association
  for the Advancement of Science at 202-326-6490.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.18.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.20.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-97</DOCNO>
<DOCOLDNO>IA012-000125-B045-73</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.20.html 128.240.150.127 19970217010230 text/html 16279
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:00:59 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 20</TITLE>
<LINK REL="Prev" HREF="/Risks/4.19.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.21.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.19.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.21.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 20</H1>
<H2> Sunday, 30 November 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Smart metals 
</A>
<DD>
<A HREF="#subj1.1">
Steven H. Gutfreund
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Risks of having -- or not having -- records of telephone calls
</A>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Audi and 60 Minutes 
</A>
<DD>
<A HREF="#subj3.1">
Mark S. Brader
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Audi 5000/Micros in cars and the Mazda RX7 
</A>
<DD>
<A HREF="#subj4.1">
Peter Stokes
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Automated trading 
</A>
<DD>
<A HREF="#subj5.1">
Scott Dorsey
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  "Borrowed" Canadian tax records; Security of medical records 
</A>
<DD>
<A HREF="#subj6.1">
Mark S. Brader
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
 Smart metals
</A>
</H3>
<address>
    "Steven H. Gutfreund" 
&lt;<A HREF="mailto:GUTFREUND%cs.umass.edu@RELAY.CS.NET">
GUTFREUND%cs.umass.edu@RELAY.CS.NET
</A>&gt;
</address>
<i>
Fri, 28 Nov 86 15:04 EDT
</i><PRE>

In Risks V4.19 Kim Collins calls for a discussions of passive versus dynamic
control mechanism, and illustrates his definition with a skyscraper analogy: 

	Passive Control: a building that flexes in the wind
	Dynamic Control: computer-controlled guy wires

With the advent of cheap 'smart' metals, (metals that contract or perform
other mechanical functions in response to temperature and other environmental 
stimuli), is the distinction very important anymore? I can use a metal with
complex operational characteristics to control the windows and blowers in my
greenhouse and provide environmental control. The proper application and
installation of these metal control structures seems directly analogous the
the proper declaration of the constraints that a software control system
should carry out. Indeed I can conceive of a modeling system for a completely 
software based control system that uses a graphics environment that expresses 
these contraints visually in terms of their mechanical counterparts:  (e.g.
ThingLab or Maureen Stone's "Snap Dragging" in the SIGGRAPH '86 proceedings).

Let me phrase this in terms of a RISKS administration dilemna:

  If an engineer designs a control system in such a graphic modeling
  environment and has no knowledge whether the final implementation will be in
  terms of hardware (relay-ladder control, smart metals, etc) or in software.
  If his system fails and is submitted to RISKS, would the editor of RISKS
  consider this material valid RISKS DIGEST material if the final
  implementation was completely free of software and computers?      

				- Steven Gutfreund
				  University of Massachusetts, Amherst

            [You bet.  An algorithm is an algorithm is an algorithm.  Although
             it is not stated explicitly in the masthead, I consider this
             forum to be devoted to something like RISKS TO THE PUBLIC IN
             COMPUTER-RELATED TECHNOLOGIES, although don't ask for a
             specific definition of scope.  Nice example.  Thanks.  PGN]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Risks of billing information on all telephone calls
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Sun 30 Nov 86 14:47:25-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

  Sunnyvale CA (AP, 29 Nov 86)  A telephone bill has vindicated a physically
  handicapped teenager jailed more than a month ago on charges he beat his
  mother to death.  Charges were dismissed against Patrick Sparks, 17, when the
  bill found by his brother, Brad, 30, indicated their mother was still alive 
  when the youth left home on the morning of the slaying, police said...

Of course, it can work either way.  The record of all of your telephone
calls provides a remarkable chronicle of your activities...  

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Audi and 60 Minutes
</A>
</H3>
<address>
&lt;<A HREF="mailto:mnetor!lsuc!dciem!msb@seismo.CSS.GOV">
mnetor!lsuc!dciem!msb@seismo.CSS.GOV
</A>&gt;
</address>
<i>
Thu, 27 Nov 86 17:20:43 est
</i><PRE>
Cc: NEUMANN@csl.sri.com  [ReSent-To: RISKS@CSL.SRI.COM]

&gt; I also saw the 60 Minutes episode.  From the tone of the various messages in
&gt; RISKS 4.17, it sounds like everybody believes Audi is at fault.  All I saw
&gt; was a lot of anecdotal evidence ...

That's all you *saw* because anecdotes make good pictures.  If you listened
to the "text" of the article, you heard statistics on the number of runaway
Audis -- if I remember rightly, something like 1 in 300 owners of the model
in question had experienced this problem.  While they didn't give the
"control statistic", the same ratio for other cars, I can't believe it's
anywhere near that high -- can you?

Mark Brader, utzoo!dciem!msb

	... being sysadmin of such a central node involves a lot less
	hassle and frustration when I can confidently say, "I don't know
	whose software is broken, but it definitely is not ours."
	Speaking of which... "I don't know whose software is broken, but
	it definitely is not ours!"			-- Henry Spencer

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Audi 5000/Micros in cars and the Mazda RX7.
</A>
</H3>
<address>
Peter Stokes 
&lt;<A HREF="mailto:stokes%cmc.cdn%ubc.csnet@RELAY.CS.NET">
stokes%cmc.cdn%ubc.csnet@RELAY.CS.NET
</A>&gt;
</address>
<i>
Thu, 27 Nov 86 08:58:31 pst
</i><PRE>

[...]
I have heard that the new Mazda RX7's have microprocessor controlled steering 
or something of the like.  I guess this is the beginning of "drive by wire".
Peter Stokes, CMC

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Automated trading
</A>
</H3>
<address>
Scott Dorsey 
&lt;<A HREF="mailto:kludge%gitpyr%gatech.csnet@RELAY.CS.NET">
kludge%gitpyr%gatech.csnet@RELAY.CS.NET
</A>&gt;
</address>
<i>
Fri, 28 Nov 86 22:09:50 est
</i><PRE>

In the last Risks Digest, RMann%pco@HI-MULTICS.ARPA says:
  "Now, I can't imagine these super-sophisticated arbitrageurs issuing MARKET
  orders -- it is too absurd to imagine.  If the hedger issues limit orders,
  the trades do not occur and the stock price stays relatively stable."

Presumably the problem is not that of sophisticated arbitrageurs making
orders on enormous numbers of stock, but many thousands of not-so-sophisticated
people using computers for small market orders.  With the advent of modern
services, practically anyone with a Commodore-64 can make predictions and
issue remote buy and sell orders.  It's a strange world.

                           [And if they are all using the same program,
                            the effects can be even stranger.  PGN]

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
"Borrowed" Canadian tax records; Security of medical records
</A>
</H3>
<address>
&lt;<A HREF="mailto:mnetor!lsuc!dciem!msb@seismo.CSS.GOV">
mnetor!lsuc!dciem!msb@seismo.CSS.GOV
</A>&gt;
</address>
<i>
Thu, 27 Nov 86 17:19:18 est
</i><PRE>

Discussion has been going on in can.general about the "Borrowed" Canadian
income tax records, and the topic of security of medical records has arisen
as a sideline.  I thought these two articles contained material good for RISKS.

Glossary for foreign readers:  OHIP is the Ontario Health Insurance Plan.
Essentially all Ontario residents have coverage, but unless our income
is small, we (or our employers) have to pay a premium for it.

Mark Brader

================== Begin 1st forwarded article ==================
Path: dciem!utzoo!mnetor!spectrix!clewis
From: clewis@spectrix.UUCP (Chris Lewis)
Newsgroups: can.general
Subject: Re: Borrowed records from Revenue Canada
Date: 26 Nov 86 21:05:24 GMT

In article &lt;274@cognos.UUCP&gt; glee@cognos.UUCP (Godfrey Lee) writes:
&gt;Did anyone see the news report that the suspect "has opened"/"wants to open"
&gt;an agency to track down people for a fee?

[Interpolation by Mark Brader:  Another report was that he wanted to
 use the records to reunite people with their forgotten bank accounts,
 for a fee.  Of course, he could have been planning both things.]

Oops, forgot about that one.  Yes, indeedy, it would be good for "skip 
tracing".  Interestingly enough, in Ontario, the OHIP enrollment file
is even better - the dates are frequently far more up to date, because
even tax avoiders (and others attempting to avoid payments) want to keep
their OHIP coverage up-to-date.  Until 1978/9 police were able to obtain
such information - the general manager of OHIP didn't realize that the
legislation enabling the existence of OHIP didn't allow it.  Not any more.  
However, there were far more private investigators using pretext calls 
to OHIP for the same end.  

As an example of where things are compared to what they were like in 1978
(when the Health Records Commission started), OHIP didn't know how many copies
of the OHIP enrollment fiche were made, where they went and never noticed
any going missing (quite a few copies did - though, most likely they were
simply misplaced or destroyed without being reported to the COM group).

One of the more interesting (and sneaky) techniques we ran into for collection
agencies acquiring info was:
	1) Send letter saying "You have won....(something or other)" along
	   with a cheque for $5 "Deposit Only" to debtor.
	2) Find out the name of the debtor's bank from the cancelled cheque.

I was asked to report a few other incidents that the Commission found:

1) Catastrophic OHIP data processing oversight:

	It is the practise of OHIP to collect several days worth of data
	entry at one of their district offices (there were 7 in 1978-79)
	and do an audit on them.  Once every couple of months.  This is 
	done by taking the several days worth of claims (in the order of
	100,000-400,000 claims) and running them through a program that would
	generate a letter of the form:

	    Dear &lt;account holder&gt;

	    Our records indicate that you, or members of your family
	    [remember OHIP numbers are for whole families, not individuals]
	    saw the following doctors on the following dates:

	    Dr A, &lt;date&gt;
	    Dr B, &lt;date&gt;
	    ...

	    Could you please inform us if any of this information is
	    incorrect?

	Note that there is no diagnostic code, service code or family member
	name.
	
	In one particular case, the account holder knew that one of
	the doctors was a OB/GYN, and reasoned out that it was his daughter
	(mid teens) who made the visit.  To make it brief, his reaction
	had as end result his daughter committing suicide.

	When this occured, OHIP made some changes to their auditing program,
	such that when the diagnostic code or service code was a socially
	embarrassing thing (e.g.: abortions, D&amp;C's, VD treatments - 20
	codes in all, probably more now) the letters do *NOT* contain 
	any reference to the associated visit.  I was asked to personally 
	inspect the OHIP code to ensure that this was being done properly.  
	It was - sorta.  When the senior analyst gave me the code, he said 
	"it makes me want to cry" - if written in C (it was in a 
	particularly grotty COBOL style), the code would have looked 
	something like:

     int skipdiag[] = {100, 200, 202 ... }; /* 10 entries, sorted */

     skipclaim(code) {
       if (binarysearch(skipdiag, code))
          return(TRUE);
       else if (code == 400 || code == 501 || code == 722...) /* 10 clauses */
	  return(TRUE)
       else
	  return(FALSE);
       }

	I gather that the analyst responsible for this piece of junk
	got thoroughly yelled at.

Chris Lewis, Spectrix Microsystems Inc, Toronto, Ontario, Canada
UUCP: {utzoo|utcs|yetti|genat|seismo}!mnetor!spectrix!clewis
ARPA: mnetor!spectrix!clewis@seismo.css.gov
Phone: (416)-474-1955

================== Begin 2nd forwarded article ==================
From: mberkley@watdcsu.UUCP
Newsgroups: can.general
Subject: Re: Borrowed records from Revenue Canada
Date: 27 Nov 86 04:10:55 GMT

In article &lt;201@spectrix.UUCP&gt; clewis@spectrix.UUCP (Chris Lewis) writes:
&gt;1) Catastrophic OHIP data processing oversight:
&gt;	    Dear &lt;account holder&gt;
&gt;	    Our records indicate that you, or members of your family
&gt;	    [remember OHIP numbers are for whole families, not individuals]
&gt;	    saw the following doctors on the following dates:
&gt;	    Dr A, &lt;date&gt;
&gt;	    Dr B, &lt;date&gt;

I used to work for the auditor of one of the provincial medical plans, and
they had a similar program.  Every month they would send out these auditing
letters.  They decided to expand the audit one year, but slightly change the
letters.  The new program printed out the subscriber's name, address, and
medical claims on a standard form that would be heat sealed and mailed.

The auditor was very picky (a good trait for auditors), so he had us check
out the first batch, one more time, before it was mailed.  We discovered
that the programmer had somehow managed to print out the name and claims of
one subscriber, and then the address for the next subscriber.

Don't ask me how he managed to do it, but I'm sure glad that we checked!

Mike Berkley, Department of Computing Services, University of Waterloo

EAN:		mberkley@dcsu.waterloo.cdn
UUCP:		{allegra,ihnp4,utcsri,utzoo}!watmath!watdcsu!mberkley

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.19.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.21.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-98</DOCNO>
<DOCOLDNO>IA012-000125-B045-95</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.21.html 128.240.150.127 19970217010242 text/html 18076
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:01:11 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 21</TITLE>
<LINK REL="Prev" HREF="/Risks/4.20.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.22.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.20.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.22.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 21</H1>
<H2> Sunday, 30 November 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Risks of Computer Modeling and Related Subjects 
</A>
<DD>
<A HREF="#subj1.1">
Mike Williams--LONG MESSAGE
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Risks of Computer Modeling and Related Subjects (LONG MESSAGE)
</A>
</H3>
<address>
"John Michael (Mike) Williams" 
&lt;<A HREF="mailto:JWilliams@DOCKMASTER.ARPA">
JWilliams@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Fri, 28 Nov 86 13:02 EST
</i><PRE>

  Taking the meretricious "con" out of econometrics and computer modeling:
                  "Con"juring the Witch of Endor
                John Michael Williams, Bethesda MD

Quite a few years ago, the Club of Rome perpetrated its "Limits to Growth"
public relations exercise.  Although not my field, I instinctively found it
bordering on Aquarian numerology to assign a quantity, scalar or otherwise,
to "Quality of Life," and a gross abuse of both scientific method and
scientific responsibility to the culture at large.  Well after the initial
report's firestorm, I heard that a researcher at McGill proved the model was
not even internally consistent, had serious typographical/syntactical errors
that produced at least an order of magnitude error, and that when the errors
were corrected, the model actually predicted an improving, not declining
"Quality of Life."  I called the publisher of "Limits to Growth," into its
umpteenth edition, and asked if they intended to publish a correction or
retraction.  They were not enthusiastic, what with Jerry Brown, as Governor
and candidate for Presidential nomination, providing so much lucrative
publicity.  Jimmy Carter's "malaise" and other speeches suggest that these
dangerously flawed theses also affected, and not for the better, both his
campaign and administration.

This shaman-esque misuse of computers embarrassed the computing
community, but with no observable effect.

On 31 October 1986, Science ran a depressing article entitled:  "Asking
Impossible Questions About the Economy and Getting Impossible Answers"
(Gina Kolata, Research News, Vol.  234, Issue 4776, pp.  545-546).  The
subtitle and the sidebar insert are informative:

  Some economists say that large-scale computer models of the economy are no
  better at forecasting than economists who simply use their best judgment...
  "People are overly impressed by answers that come out of a computer"...

Additional pertinent citations (cited with permission):

   "There are two things you would be better not seeing in the making--
   sausages and econometric estimates," says Edward Learner, an economist at
   [UCLA].  These estimates are used by policymakers to decide, for example,
   how the new tax law will affect the economy or what would happen if a new
   oil import tax were imposed.  They are also used by businesses to decide
   whether there is a demand for a new product.  Yet the computer models that
   generate these estimates, say knowledgeable critics, have so many flaws
   that, in Learner's words, it is time to take the "con out of econometrics."

   ...[E]ven the defenders of the models... [such as e]conomists Kenneth
   Arrow of Stanford and Stephen McNees of the Federal Reserve Board in Boston
   say they believe the models can be useful but also say that one reason the
   models are made and their predictions so avidly purchased is that people
   want answers to impossible questions and are overly impressed by answers
   that come out of a computer...

   The problem, says statistician David Freedman of the University of
   California at Berkeley, is that "there is no economic theory that tells you
   exactly what the equations should look like."  Some model builders do not
   even try to use economic theory...: most end up curve-fitting--a risky
   business since there are an infinite number of equations that will fit any
   particular data set...

   "What you really have," says William Ascher of Duke University, "is a man-
   model system."  And this system, say the critics, is hardly scientific.
   Wassily Leontief of New York University remarks, "I'm very much in favor of
   mathematics, but you can do silly things with mathematics as well as with
   anything else."

   Defenders of the models point out that economists are just making the best
   of an impossible situation.  Their theory is inadequate and it is
   impossible to write down a set of equations to describe the economy in any
   event... But the critics of the models say that none of these defenses
   makes up for the fact that the models are, as Leontief says, "hot air."
   Very few of the models predict accurately, the economic theory behind the
   models is extremely weak if it exists at all, in many cases the data used to
   build the models are of such poor quality as to be essentially useless, and
   the model builders, with their subjective adjustments, produce what is,
   according to Learner, "an uncertain mixture of data and judgment."

When David Stockman made "subjective adjustments," he was reviled for
cooking the numbers.  It seems they may have been hash to begin with.

   [Douglas Hale, director of quality assurance at the (Federal) Energy
   Information Administration] whose agency is one of the few that regularly
   assess models to see how they are doing, reports that, "in many cases, the
   models are oversold.  The scholarship is very poor, the degree of testing
   and peer review is far from adequate by any scientific measure, and there
   is very little you can point to where one piece of work is a building block
   for the next."

   For example, the Energy Information Administration looked at the accuracy
   of short-term forecasts for the cost of crude oil...  At first glance, it
   looks as if they did not do too badly...  But, says Hale, "what we are
   really interested in is how much does the price change over time.  The
   error in predicting change is 91%"

This is about the same error, to the hour, of a stopped clock.

In the Washington Post for 23 November 1986, pg K1 et seq., in an
interview entitled "In Defense of Public Choice," Assar Lindbeck,
chairman of the Swedish Royal Academy's committee for selecting the
Nobel Prize in economics, explains the committee's choice of Professor
James M. Buchanan, and is asked by reporter Jane Seaberry:

   It seems the economics profession has come into some disrepute.  Economists
   forecast economic growth and forecasts are wrong.  The Reagan administration
   has really downplayed advice from economists.  What do you think about the 
   economics profession today?

Chairman Lindbeck replies:

   Well, there's something in what you say in the following sense, I think,
   that in the 1960s, it was a kind of hubris development in the economic
   profession ... in the sense that it was an overestimation of what research
   and scientific knowledge can provide about the possibilities of
   understanding the complex economic system.  And also an overestimation
   about the abilities of economists to give good advice and an overestimation
   of the abilities of politicians and public administrators to pursue public
   policy according to that advice.

   The idea about fine tuning the economy was based on an oversimplified
   vision of the economy.  So from that point of view, for instance,
   economists engaged in forecasting--they are, in my opinion, very much
   overestimating the possibilities of making forecasts because the economic
   system is too complex to forecast.  Buchanan has never been engaged in
   forecasting.  He does not even give policy advice because he thinks it's
   quite meaningless...

What econometric computer model is not "an oversimplified vision of the
economy?" When is forecasting an "economic system ...  too complex to
forecast" not fortune-telling?

To return to Kolata's article:

   [Victor Zarnowitz of the University of Chicago] finds that "when you
   combine the forecasts from the large models, and take an average, they are
   no better than the average of forecasts from people who just use their best
   judgment and do not use a model."

I cannot resist noting that when a President used his own judgment, and
pursued an economic policy that created the greatest Federal deficit in
history but the lowest interest rates in more than a decade, the high
priests of the dismal science called it "voodoo economics." It takes one
to know one, I guess.

   Ascher finds that "econometric models do a little bit worse than judgment.
   And for all the elaboration over the years they haven't gotten any better.
   Refining the models hasn't helped."  Ascher says he finds it "somewhat
   surprising that the models perform worse than judgment since judgment is
   actually part of the models; it is incorporated in when modelers readjust
   their data to conform to their judgment."

Fascinating! Assuming the same persons are rendering "judgments," at
different times perhaps, it implies that the elaboration and mathematical
sophistry of the models actually cloud their judgment when expressed through
the models:  they appear to have lost sight of the real forest for the
papier-mache trees.

   Another way of assessing models is to ask whether you would be better off
   using them, or just predicting that next year will be like this year.  This
   is the approach taken by McNees...  "I would argue that, if you average
   over all the periods [1974-1982] you would make smaller errors with the
   models [on GNP and inflation rates] than you would by simply assuming that
   next year will be just like this year," he says.  "But the errors would not
   be tremendously smaller.  We're talking about relatively small orders of
   improvement."

I seem to recall that this is the secret of the Farmer's Almanac success
in predicting weather, and that one will only be wrong 15% of the time
if one predicts tomorrow's weather will be exactly like today's.

   Other investigators are asking whether the models' results are
   reproducible...  Suprisingly the answer seems to be no.  "There is a real
   problem with scholarship in the profession," says Hale of the Energy
   Information Administration.  "Models are rarely documented well enough so
   that someone else can get the same result..."

   [In one study, about two-thirds of the] 62 authors whose papers were
   published in the [J]ournal [of Money, Credit and Banking]... were unwilling
   to supply their data in enough detail for replication.  In those cases
   where the data and equations were available, [the researchers] succeeded in
   replicating the original results only about half the time...

What a sorry testament!  What has become of scientific method, peer review?

   "Even if you think the models are complete garbage, until there is an
   obviously superior alternative, people will continue to use them," [McNees]
   says.

Saul, failing to receive a sign from Jehovah, consulted a fortune-teller on the
eve of a major battle.  The Witch of Endor's "model" was the wraith of Samuel, 
and it wasn't terribly good for the body politic either.  I keep a sprig of
laurel on my CRT, a "model" I gathered from the tree at Delphi, used to send
the Oracle into trance, to speak Apollo's "truth." I do it as amusement and
memento, not as talisman for public policy.  History and literature are filled 
with the mischief that superstition and fortune-telling have wrought, yet
some economic and computer scientists, the latter apparently as inept as the
Sorcerer's Apprentice, are perpetuating these ancient evils.  Are Dynamo and
decendents serving as late-twentieth-century substitutes for I Ching sticks?

Is the problem restricted to econometrics, or is the abuse of computer
modeling widespread?  Who reproduces the results of weather models, for
instance?  Who regularly assesses and reports on, and culls the unworthy
models?  Weather models are interesting because they may be among the
most easily "validated," yet there remains the institutional question:
when the Washington Redskins buy a weather service, for example, to
predict the next game's weather, how can they objectively predetermine
that they are buying acceptable, "validated" modeling rather than snake
oil?  After all, even snake oil can be objectively graded SAE 10W-40, or
not.  A posteriori "invalidation" by losing while playing in the "wrong"
weather is no answer, any more than invalidation by catastrophic engine
failure would be in motor oils.  The Society of Automotive Engineers at
least has promulgated a viscosity standard:  what have we done?

Where is scientific method at work in computer modeling?  When peer review
is necessarily limited by classification, in such applications as missile
engagement modeling and war gaming, what body of standards may the closed
community use to detect and eliminate profitable, or deadly, hokum?  Is this
just one more instance of falsified data and experiments in science
generally, of the sort reported on the front page of the Washington Post as
or before it hits the journals?  (See:  "Harvard Researchers Retract
Published Medical 'Discovery;'" Boyce Rensberger, Washington Post, 22
November 1986 pg 1 et seq.; and Science, Letters, 28 November 1986.)

Several reforms (based on the "publish or perish" practice that is
itself in need of reform) immediately suggest themselves.  I offer them
both as a basis for discussion, and as a call to action, or we shall
experience another aspect of Limits to Growth-- widespread rejection of
the contributions of computer science, as a suspect specialty:

   o Refusal to supply data to a peer for purposes of replication might
result in the journal immediately disclaiming the article, and temporary
or permanent prohibition from publication in the journal in question.

   o Discovery of falsified data in one publication resulting in
restriction from publication (except replies, clarification or
retraction) in all publications of the affiliated societies.  In
computer science, this might be all IEEE publications at the first
level, AFIPS, IFIPS and so on.

   o Widespread and continuing publication of the identities of the authors,
and in cases of multiple infractions, their sponsoring institutions, in
those same journals, as a databank of refuseniks and frauds.

   o Prohibition of the use of computer models in public policymaking (as in
sworn testimony before Congress) that have not been certified, or audited,
much as financial statements of publicly traded companies must now be audited.

   o Licensing by the state of sale and conveyance of computer models of
general economic or social significance, perhaps as defined and
maintained by the National Academy of Sciences.

The last is extreme, of course, implying enormous bureaucracy and
infrastructure to accomplish, and probably itself inevitably subject to
abuse.  The reforms are all distasteful in a free society.  But if we do
nothing to put our house in order, much worse is likely to come from the
pen or word-processor of a technically naive legislator.

In exchange for a profession's privileged status, society demands it be
self-policing.  Doctors, lawyers, CPAs and the like are expected to
discipline their membership and reform their methods when (preferably
before) there are gross abuses.  Although some of them have failed to do
so in recent years, is that an excuse for us not to?

Finally, how can we ensure that McNees' prediction, that people will
continue to re-engineer our society on models no better than garbage,
will prove as false as the models he has described?

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.20.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.22.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-99</DOCNO>
<DOCOLDNO>IA012-000125-B045-110</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.22.html 128.240.150.127 19970217010256 text/html 21321
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:01:23 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 22</TITLE>
<LINK REL="Prev" HREF="/Risks/4.21.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.23.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.21.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.23.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 22</H1>
<H2> Tuesday, 2 December 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
More Air Traffic Control Near-Collisions 
</A>
<DD>
<A HREF="#subj1.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Re: satellite interference 
</A>
<DD>
<A HREF="#subj2.1">
Jerome H. Saltzer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  "Welcome to the .......... system": An invitation? 
</A>
<DD>
<A HREF="#subj3.1">
Bruce N. Baker
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Replicability; econometrics 
</A>
<DD>
<A HREF="#subj4.1">
Charles Hedrick
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Re: Risks of computer modeling 
</A>
<DD>
<A HREF="#subj5.1">
John Gilmore
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Computerized weather models 
</A>
<DD>
<A HREF="#subj6.1">
Amos Shapir
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Active control of skyscrapers 
</A>
<DD>
<A HREF="#subj7.1">
Warwick Bolam
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Privacy in the office 
</A>
<DD>
<A HREF="#subj8.1">
Paul Czarnecki
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj9">
  Kremlin is purging dimwitted scientists 
</A>
<DD>
<A HREF="#subj9.1">
Matthew P Wiener; also in ARMS-D
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
More Air Traffic Control Near-Collisions
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Tue 2 Dec 86 10:08:05-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

  Chicago (UPI) 2 Dec 1986
  
  Two passenger jetliners on their landing approaches nearly collided with
  small planes in separate incidents here yesterday, the Federal Aviation
  Administration said.  The near-collisions, which occurred within 29 minutes
  of each other, involved a Midway Airlines DC-9 with 90 people aboard,
  arriving from Philadelphia, and a United Airlines Boeing 727 with 128 people,
  en route from Baltimore.  FAA spokesman Mort Edelstein said that the United
  pilot reported passing within 500 feet laterally of a twin-engine Beechcraft
  90 about 28 miles southeast of O'Hare International Airport at 7:41 a.m.
  The pilot made a sharp left turn to avoid the smaller aircraft, he said.
  According to Edelstein, a preliminary inquiry found that the smaller plane
  had a defective transponder that was transmitting inaccurate data on the
  plane's altitude to air traffic controllers in Aurora, west of Chicago.
           [The second incident was less close, and no explanation was given.] 

The problem of an accidentally malfunctioning transponder is in many ways
equivalent to that of an intentionally malfunctioning transponder -- either
one that has been purposely sabotaged in an attempt to jeopardize the plane
or one that has been altered "constructively" in an attempt to hide the true
altitude of the plane.  In all of these cases, the ATC system implicitly
trusts the authenticity and accuracy of the transponders with which it
communicates -- if such a transponder exists at all in a private plane.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Re: satellite interference
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
Mon, 1 Dec 86 23:37:50 EST
</i><PRE>
From: Jerome H. Saltzer &lt;Saltzer@ATHENA.MIT.EDU&gt;

About a month ago, Richard Wexelblat reported to RISKS that satellite
delivery of the Headline News Network to his location was disrupted for a
short time by an interfering signal whose picture wasn't intelligible but
whose sound seemed to be advertising satellite decoders.  In contrast with
the Captain Midnight attack on HBO, no big follow-up story appeared in
Newsweek and Time magazines.

My contact at the F.C.C. tells me that there are typically a couple of
incidents like this every day.  The primary problem is not malicious attacks
by a Captain Midnight.  It is simple screwups by uplink operators who forget
to throw switches, who set wrong channel numbers in their transmitters, who
aim their dishes at the wrong satellite, or who run automatically programmed
switching sequences that were intended for yesterday or tomorrow rather than
today.

The reported audio content sounds suspicious, but it turns out that a
scrambled video service usually has an unscrambled audio channel
accompanying it that explains how to obtain a decoder and subscribe to the
service.  The audio that goes with the picture is buried somewhere else in
the channel.

F.C.C. technicians have proposed to tackle the operator screwup problem by
requiring that uplink transmitters place encoded call letters in the
vertical retrace interval of their transmitted waveforms.  Then at least
someone who is being interfered with can quickly figure out which of the
1200 licensed uplink transmitters is muddled up and get the operator there
on the phone quickly.  That solution doesn't eliminate intentional attacks
by someone who knows how to forge the unique id, but from the F.C.C.'s point
of view it will solve 99.9% of the problem they face.  As for malicious
cases, detective work and $10,000 fines may help keep things under control.

Although the technology is different, don't the problem and the proposed
solution both sound quite familiar to the regular reader of RISKS?

						Jerry

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
"Welcome to the .......... system": An invitation? 
</A>
</H3>
<address>
Bruce N. Baker 
&lt;<A HREF="mailto:BNBaker@SRI-STRIPE.ARPA">
BNBaker@SRI-STRIPE.ARPA
</A>&gt;
</address>
<i>
Tue 2 Dec 86 10:35:05-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

At a local chapter meeting of the Information Systems Security Association,
a representative of VM Software Inc. told a story about a Massachusetts 
financial institution that had attempted to prosecute a hacker who had
penetrated their system.  The defense lawyer argued that the system had a 
greeting that welcomed people to the system and that was tantamount to
welcoming someone into your home (Goodbye, Welcome mats?).

The judge threw out the case accepting the arguments of the defense.

I have attempted to track down the authenticity of the story through the
VM Software rep but he will not divulge the name of the company.

Attempts to track it down through the law firm of Gaston Snow &amp; Ely Bartlett 
in Boston revealed no records of such a case.

Obviously, if there was such a case it has implications to the wording of the
Welcome banner on any system.

Can anyone provide a better lead or lend credence to the story?

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Replicability; econometrics (Re: <A HREF="/Risks/4.21.html">RISKS-4.21</A>)
</A>
</H3>
<address>
Charles Hedrick
&lt;<A HREF="mailto:hedrick@topaz.rutgers.edu ">
hedrick@topaz.rutgers.edu 
</A>&gt;
</address>
<i>
Mon, 1 Dec 86 12:13:24 est
</i><PRE>
To: risks@csl.sri.com

I have had a long-term concern with replicability of scientific experiments.
It does not appear that this concern is shared outside of certain physical
sciences.  When I was a grad student, I published an article in the American
Economic Review (the economic equivalent of CACM).  In order to allow
replicability, I included the actual data, together with the details on how
I had adjusted the raw data series (something which has to be done because
the agencies change definitions every few years).  The data was small
compared to the size of the article.  It was cut for space reasons.  My
article itself was a replication of an empirical study done some years ago.
It covered a period when the economy had behaved very differently.  It came
to the same conclusions.  The original study had been very careful about
econometric validity.  It is possible to do valid work in econometrics.  It
is also possible to duplicate carefully done work.  [I think these comments
are important because they show that econometrics is not necessarily voodoo.
What we need are professional standards to help us separate the good work
from the bad.]

Similar problems occur in computer science.  Several years ago, one of our
grad students attempted to duplicate the results of researchers in one area
of AI.  He was trying to do research into what actually causes success in
rule-based systems.  He ran into serious problems with another research
group, which went beyond simply refusing to give data.  I think our department 
would prefer for me not to give any more details.  But he concluded that
such work was impossible in the particular area with which he was involved.

In my opinion, anyone who publishes empirical claims in a scientific
journal should be required to give people access to the data needed to
replicate it or do further analysis of its model.  I have been unable
figure out what to do to try to make that happen.

By the way, I have a related Risk to describe.  As I mentioned above, in
econometrics one normally has to twiddle with the basic data series in order
to get useful numbers.  I am now a computing manager.  I find that our users
expect to have access to various commercially-prepared econometric data series.
As far as I can see, all that is there is a bunch of numbers and a one-line
description of what it is.  When I was doing work in the area, I would have
wanted to know a lot more about how the numbers had been prepared.  I'm
hoping there is some sort of hardcopy document available to users of the
databases, but I'd bet even if there is, a lot of our users never see it.

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Re: Risks of computer modeling
</A>
</H3>
<address>
John Gilmore
&lt;<A HREF="mailto:hoptoad!gnu@lll-crg.ARPA ">
hoptoad!gnu@lll-crg.ARPA 
</A>&gt;
</address>
<i>
Tue, 2 Dec 86 05:39:29 PST
</i><PRE>
To: risks@sri-csl.arpa

&gt; Is the problem restricted to econometrics, or is the abuse of computer
&gt; modeling widespread?

It is widespread.

One friend of mine has done extensive work in "decision analysis" systems,
with clients in the military, Bell System, etc.  I did some programming on
such a system for him while he was at SRI.  When looked at from the inside,
it is obvious that such a system will give you back exactly the answers that
you fed it as input, since most of the input data is "How important is this?
How important is that?".  But people will believe it because a computer
model said so, while if *you* told them that the widget acceptance ratio was
33% if priced at this level, they would ask you why.  They didn't get to ask
when you typed it in.

Another friend works for the World Bank in Washington, DC.  She has done a
lot of proposals and evaluations around funding of transportation projects
in third world countries.  I remember helping her get some of her modeling
programs right.  Her approach was always to figure out what the data
"means", in other words, what result she wanted, and then juggle the numbers
and equations until she could "prove" it.

I don't think that abuse of modeling is restricted to computer models, or
even that it is more prevalent with computer models.  In all disciplines,
experienced people with a feel for things figure out what is going on and
proceed from there.  If somebody wants better justification, they have to
cook one up, but don't mistake the source of the estimate:  the human mind,
not the later model.

Cf. ``How to Lie with Figures''.  Don't have the citation but it's a standard
work.  Maybe it needs an update to deal with new techniques.

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Computerized weather models
</A>
</H3>
<address>
Amos Shapir 
&lt;<A HREF="mailto:nsc!nsta!instable.ether!amos@decwrl.DEC.COM">
nsc!nsta!instable.ether!amos@decwrl.DEC.COM
</A>&gt;
</address>
<i>
1 Dec 86 10:02:31 GMT
</i><PRE>

About weather models: they are one of the few accurate forecasting models
possible; the only trouble is that the required answers, e.g. 'will it rain
on the game tomorrow?' are much more detailed than the base data (typically
a 3-6 hourly surface report from stations 50 miles apart). Besides, until
Crays came along, it was almost impossible to do it in real time.

Amos Shapir, National Semiconductor (Israel)
6 Maskit st. P.O.B. 3007, Herzlia 46104, Israel 
(011-972) 52-522261  amos%nsta@nsc 34.48'E 32.10'N

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Active control of skyscrapers
</A>
</H3>
<address>
Warwick Bolam
&lt;<A HREF="mailto:munnari!goanna.oz!wjb@seismo.CSS.GOV ">
munnari!goanna.oz!wjb@seismo.CSS.GOV 
</A>&gt;
</address>
<i>
Mon, 1 Dec 86 14:05:28 EST
</i><PRE>

&gt; Date: Wed, 26 Nov 86 13:32:05 EST
&gt; From: "Kim P. Collins" &lt;kpc%duke.csnet@RELAY.CS.NET&gt;
&gt; To: RISKS@CSL.SRI.COM
&gt; Subject: Very Brief Comments on the Current Issues
&gt; 
&gt; New subject
&gt;   Any comments on active vs. passive control structures?  For instance,
&gt;   having a skyscraper that has flexible material so that in high winds
&gt;   it bends and does not fail, VS having a skyscraper that has guy wires
&gt;   connected to winches that are controlled by a computer that tests wind
&gt;   velocities, etc.

There already exist active control systems for skyscrapers that
use a huge mass, that is "pushed around" by computer controlled
equipment to stabilise the building.  I'm afraid I have no
reference to this.  I saw it on TV.

Warwick Bolam

UUCP:  seismo!munnari!goanna.oz!wjb     ARPA: munnari!goanna.oz!wjb@SEISMO.ARPA

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
Privacy in the office
</A>
</H3>
<address>
Paul Czarnecki
&lt;<A HREF="mailto:harvard!munsell!pac@seismo.CSS.GOV ">
harvard!munsell!pac@seismo.CSS.GOV 
</A>&gt;
</address>
<i>
2 Dec 86 16:19:49 GMT
</i><PRE>
Organization: Eikonix Corp., Bedford, MA

There is an interesting article in the November/December 1986 issue of
Technology Review that I though may be of interest to RISKS readers.
The title is "Monitoring on the Job: How to Protect Privacy as Well as
Property." The authors are Gary T. Marx and Sanford Sherizen.

The article discusses how surveillance technology is used in the modern
office environment.  Everything from video cameras in the parking lot to
private data on corporate machines is discussed.  Although much of the
technology is not computer related, some of it is.

I thought the article was interesting overview of some of the issues
involved with technology and privacy.  It was not as in-depth as I
would have liked, but good anyhow.
                                                pZ

			    Paul Czarnecki -- Eikonix, Corp. -- Bedford, MA
	{{harvard,ll-xn}!adelie,{decvax,allegra,talcott}!encore}!munsell!pz

</PRE>
<A NAME="subj9"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj9.1">
!!! Kremlin is purging dimwitted scientists !!!
</A>
</H3>
<address>
M P Wiener
&lt;<A HREF="mailto:weemba@brahms.berkeley.edu ">
weemba@brahms.berkeley.edu 
</A>&gt;
</address>
<i>
Mon, 1 Dec 86 01:31:38 PST
</i><PRE>
Cc: risks@csl.sri.com

The following is shamelessly stolen from the 2 Dec 1986 edition of the
WEEKLY WORLD NEWS.  (You couldn't have missed that issue while shopping:
it had the banner headlines about the five-week long pregnancy [Bulgarian
natch] and a recipe for cooking Thanksgiving turkeys in the dishwasher.)
 -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -
    &lt;&lt; Lame-brained Russians try to fix computer -- with a hammer &gt;&gt;

	!!!	 Kremlin is purging dimwitted scientists      !!!

    Soviet official launched a massive investigation into the training
  of technical personnel after a repairman tried to fix a sophisticated
  missile guidance system with a hammer, a screwdriver and an oil can.
    A recent East German defector, Dr. Hermann Franz, blew the lid off
  the shameful state of Soviet technical know-how in a scathing letter
  to top science journals upon his arrival in the West.
    The computer scientist, who is now living in France, claims there
  is a very real danger that a poorly-trained Russian technician might
  accidently start World War 3.
    ``The repairman with the oil can is a glaring example of their in-
  eptitude,'' said the expert.  ``He was assigned to one of the most
  sensitive missile bases in the U.S.S.R.
    ``And yet, when he was called on to repair a circuit problem in a
  computer console, he showed up with carpenter's tools.
    ``First he walked over and kicked it.  Then he said, `Something is
  stuck.'  I thought he was joking until he started squirting oil and
  blew every circuit in the control center.
    ``It took six weeks to repair the damage -- six weeks to do a job
  that qualified technicians could have done in a matter of days.''
    Horrifyingly, the missile base near the foot of Ural Mountains is
  armed with some of the Soviet Union's most powerful intercontinental
  missiles and nuclear warheads, Dr. Franz said.
    A Soviet Air Force spokesman angrily denied the allegations, call-
  ing Soviet technicians ``the finest in the world.''
    One highly-placed military source conceded that Soviet training
  programs are being investigated.  But he insisted that the investi-
  gation was routine.
    Meanwhile, Dr. Franz has called on Western politicians and scien-
  tists to pressure the Soviets into monitoring the work of their tech-
  nicians more closely.
    ``The specter of nuclear holocaust is frightening enough,'' he
  said, ``without having to worry about some dimwit starting the war
  that would kill us all.''
						          -- Derek Clontz
 -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -
I don't quite follow the logic of that last quotation.  Personally I'm
more worried about some of the "dimwits" at the other end of the nuclear
chain of command.

I have two questions:

Q1) Can anyone identify a quote top science journal unquote that is pub-
lishing Dr Franz' letter?  (Heck, while we're on a roll, can anyone con-
firm the "Clark Gable's our god, says lost island tribe" story?)

Q2) What is known/believed about Soviet failsafe mechanisms?

ucbvax!brahms!weemba	Matthew	P Wiener/UCB Math Dept/Berkeley CA 94720

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.21.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.23.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-100</DOCNO>
<DOCOLDNO>IA012-000125-B045-132</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.23.html 128.240.150.127 19970217010315 text/html 16185
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:01:41 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 23</TITLE>
<LINK REL="Prev" HREF="/Risks/4.22.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.24.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.22.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.24.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 23</H1>
<H2>Wednesday, 3 December 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
The persistence of memory  [and customs officials] 
</A>
<DD>
<A HREF="#subj1.1">
Richard V. Clayton
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  America's Cup - floppies held to ransom 
</A>
<DD>
<A HREF="#subj2.1">
Computing Australia via Derek
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Some thoughts regarding recent postings: blame and causality 
</A>
<DD>
<A HREF="#subj3.1">
Eugene Miya
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Microcomputer controlled cars (not Audi) 
</A>
<DD>
<A HREF="#subj4.1">
Miriam Nadel
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Re: Welcome to the system 
</A>
<DD>
<A HREF="#subj5.1">
Ronda Henning
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Re: Automated trading 
</A>
<DD>
<A HREF="#subj6.1">
Scott Dorsey
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Active control of skyscrapers 
</A>
<DD>
<A HREF="#subj7.1">
Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Sanity in Automating Keyword Abstracting 
</A>
<DD>
<A HREF="#subj8.1">
Paul Ryan
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
The persistence of memory  [and customs officials]
</A>
</H3>
<address>
Richard V. Clayton
&lt;<A HREF="mailto:clayton@bambi.bellcore.com ">
clayton@bambi.bellcore.com 
</A>&gt;
</address>
<i>
Wed, 3 Dec 86 07:27:13 est
</i><PRE>

The 27 November issue of New Scientist has an article (page 20) about
a heroin smuggling ring convicted with the help of evidence obtained
from a pocket computer.  The smugglers used the computer, a Psion
Organizer, to store information about deals; after the deal was done,
the information was erased.  However, the Organizer uses EPROM
storage, so information wasn't erased, but flagged as being
unavailable.  After seizing the computer, customs officials took it to
Psion where in-house software recovered the information.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
America's Cup - floppies held to ransom
</A>
</H3>
<address>
&lt;<A HREF="mailto:derek%gucis.oz@RELAY.CS.NET">
derek%gucis.oz@RELAY.CS.NET
</A>&gt;
</address>
<i>

</i><PRE>
Date: 03 Dec 86 18:30:18 +1000 (Wed)

I thought RISK readers might be interested in some of the lighter risks
associated with the use of high technology in twelve metre yachting.
Not only must keels be covered!

CUP INFO RANSOMED  (From Computing Australia, 1 December 1981)

A stolen package of floppy disks holding sensitive telemetry data from one
of the America's Cup syndicates has been recovered after being held to
ransom through a hacker's bulletin board.  

The theft of the 17 disks came to light on a bulletin board called Inter-State
Connect, where a note was posted originally asking $10,000 for them.

It is not known which syndicate had the disks stolen as no name appeared
on them and none of the yachting teams have admitted ownership.

The disks were stolen in Fremantle and turned up in Melbourne where computer
security analyst, Stuart Gill, negotiated the retrieval of the disks through
a shadowy organisation of hackers known as TechHack.

TechHack became involved in the negotiations after being accused of mounting
the ransom operation.  In order to clear its name, TechHack acted as the
intermediary between Gill and the hacker responsible for the ransom notice.

Computing Australia has obtained a printout of the negotiations which took 
place on the bulletin board.

It reads:

	As at 21/10 we require the sum of $2,500 for the exchange of the disks
	Confirm there are 17 and you are aware from our Perth contact that they
	are Kosher.  We cannot continue talking for much longer as we don't 
	think you are serious.

In the end, the stolen property was retrieved with no exchange of money.

It is believed a number of syndicates approached Gill for copies on the disks
on the pretext of establishing where they came from.

&lt;end-of-article&gt;

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Some thoughts regarding recent postings: blame and causality
</A>
</H3>
<address>
Eugene Miya
&lt;<A HREF="mailto:eugene@AMES-NAS.ARPA ">
eugene@AMES-NAS.ARPA 
</A>&gt;
</address>
<i>
Wed, 3 Dec 86 10:13:07 pst
</i><PRE>

Peter, your recent note on the frequent but rarely discussed topic of "where
to place the blame" concerns me.  It seems that we in computers and computer
science have some what ill-defined concepts of CAUSALITY (where DO we put
the blame?), (non)determinism, and our poor use of reductionism.  Other
similar postings on modeling and empirical data as final proof also concern me.

Consider, the mistake we make when we confuse WORK and EFFORT:  we get
Brooks' mythical man-month.  (Brooks' classic example was 1 woman =&gt; 1 baby
in 9 months, therefore 9 women =&gt; 1 baby in 1 month.)  And there are many
people who don't see that this generalizes in high-performance computing in
terms of mythical MFLOPS:  some programs are not decomposible into parallel
parts.  And I suspect this is also manifest in the way we use redundancy in
fault-tolerant computing (multiple CPUs in hot-start configuration which
could be used for parallel computation but are used for reliability
instead).

I think we misunderstand causality for two reasons:

  1) Our empirical foundations tend to be a bit weak.  (We put theory quite
  high in esteem.)  In part, mathematical theory is our solution, but it also
  a source of bias.  I know many will disagree with this latter conclusion
  including PJD.  We try to envision problems outside of the complexities of
  the `real' world (modeling and simulation).  Where as theoretical physics
  had experimental physics to fall back on, computer science does not have a
  good equivalent.

  2) Some of our ideas do not tend to generalize across computers as
  mathematical concepts generalize across the mathematical sciences.
  We are not really JUST a mathematical science.  The recent econometric
  postings enforce some of this.

I heard an interesting thing about the way computing is done in third-world
countries (I heard the USSR was/is in this category) where computing is
expensive and thinking is cheap:

  Theoretical CS is held is high esteem because when a mistake is made in
  hardware or in a project it becomes glaringly visible to all.  When mistakes
  are made in theoretical CS (and probably math to a lesser degree), there are
  so few people who understand these ideas, and some ideas are so specific,
  that only a few people can criticise them (fewer/less negative reinforcement
  and punishment).

Consider the discussion on testing: computer people talk about testing with
respect to the correctness of a specification, but we don't talk about
testing with respect to the `real' world.  Testing of accounting programs is
one thing, but testing of models of the physical world like fluid dynamics
or population quality of life are different things.  Perhaps I should use
the word measurement here.  There are numerous cute computer models with
graphics like the LLNL crushed cone shown at the 1984 SIGGRAPH or similar
fluid dynamics works here.  (References provided on request.)  I fear that
we computer types have a greater chance of losing touch with reality.  This
would also make us among the poorest judges in our own discipline for things
such as the Turing Test because the Test is ultimately an empirical endeavour.

Anyway, these are some initial thoughts I have composed over the past few
days about computing's poor basis in empiricism.

--eugene miya

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Microcomputer controlled cars (not Audi)
</A>
</H3>
<address>
Controls Wizard
&lt;<A HREF="mailto:dma%euler.Berkeley.EDU@berkeley.edu ">
dma%euler.Berkeley.EDU@berkeley.edu 
</A>&gt;
</address>
<i>
Wed, 3 Dec 86 08:23:12 PST
</i><PRE>
[REPLY-TO: dma%... BUT SPECIFY "TO MNadel" -- ACCOUNT USED BY DIFFERENT PEOPLE]

There's been a lot of discussion about the Audi problems and I remembered a
similar incident.  The Sept. 1984 issue of Consumer Reports included a
review of the Mitsubishi Starion.  Under the heading "Defect of the Month"
they described uncontrollable acceleration that was only stoppable by
turning off the ignition.  The problem was eventually solved by replacing
the engine microprocessor and the problem was reported to the National
Highway Traffic Safety Administration.  It seems to me that NHTSA should be
getting the Audi complaints and telling Audi that they should look at the
source of the problem.  Precedents are a good way of convincing people that
there's a problem.  Miriam Nadel

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
 Re: Welcome to the system
</A>
</H3>
<address>
&lt;<A HREF="mailto: Henning@DOCKMASTER.ARPA">
 Henning@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Wed, 3 Dec 86 10:59 EST
</i><PRE>
To:  risks@CSL.SRI.COM

I know of a similar case that never made it to court.  The computer security
administrator at Roche, the drug company had been plagued by a hacker who
auto-dialed the entire Roche phone system in sequence.  It took a lot of
phone calls from company management to convince the phone company that this
was not just someone with fast fingers and a touch-tone phone.  They laid a
hacker trap on one of the PC`s and traced the call.  Once the suspect was
found, it was even harder to get him arrested since he was in New York and
Roche is in New Jersey, which somehow got the FBI involved.

The perpetrator was brought into the police station and had the riot act and
the fear of God scared into him.  He was not charged -- because there wasn't a
no trespassing sign on the hacker trap identifying the system as private
property of Roche.

                  [A tough Roche to phone?  (All Roche leads to phone?)
                  Yes, this has been a common problem in the past...  PGN]

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Re: Automated trading
</A>
</H3>
<address>
Scott Dorsey 
&lt;<A HREF="mailto:kludge%gitpyr%gatech.csnet@RELAY.CS.NET">
kludge%gitpyr%gatech.csnet@RELAY.CS.NET
</A>&gt;
</address>
<i>
Sun, 30 Nov 86 14:04:18 est
</i><PRE>

  I'm afraid to say that most of the programs all use very similar
algorithms with almost identical buy/sell setpoints.  That's where
the problem probably lies.  
  The solution is to predict the changes in the market that would
result from these (very predictable) programs operating at the same
time, and I am sure that some smart fellow will be making a lot of
money that way...

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Active control of skyscrapers
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Wed, 3 Dec 1986  09:20 EST
</i><PRE>

   [...]
I'm told that the John Hancock Building in Boston is built like this.

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
Sanity in Automating Keyword Abstracting
</A>
</H3>
<address>
Paul Ryan
&lt;<A HREF="mailto:dgis!ryan@lll-tis-b.ARPA ">
dgis!ryan@lll-tis-b.ARPA 
</A>&gt;
</address>
<i>
Mon Dec 1 16:30:30 1986
</i><PRE>
To: risks@csl.sri.com

The Defense Technical Information Center (DTIC) acts as the central
repository of scientific and technical information for the Department of
Defense (DoD).  One of the four online databases which DTIC maintains is the
Technical Reports Database.  It has recently come to our attention that the
29 September 1986 issue of the RISKS Digest [<A HREF="/Risks/3.70.html">RISKS-3.70</A>] was informed of a
"new policy" by DTIC that stated that technical report titles be designed
with keywords positioned in the first five words of the title.  THIS IS NOT
AND NEVER HAS BEEN A POLICY OF THE DEFENSE TECHNICAL INFORMATION CENTER.
Apparently, this erroneous information was forwarded to this forum as an
example of the risk to accurate dissemination of information caused by
faulty programming (or programmers?).

The policy of this organization regarding technical report titles is that they 
should reflect the author's effort to describe the content of the report.  

In trying to determine from where such an inaccurate statement might have
developed, our conclusion is that the individual (outside this organization)
who proposed the "policy statement" misapplied a long standing DTIC search
retrieval capability.  Our automated retrieval system has a search algorithm
which is constructed from the first five words of the title.  It allows a
searcher to identify a report title and bibliographic citation from our
online collection of 1.5 million titles.  The search retrieval algorithm
works for any word of the first five words of the title whether they be
prepositions, articles, or keywords in identifying the bibliographic
citation associated with a title.

For further information please contact:  R Paul Ryan, Director, Office of
User Services, Defense Technical Information Center, Cameron Station,
Alexandria, VA 22304   Phone (202) 274-6434   AV 284-6434

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.22.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.24.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-101</DOCNO>
<DOCOLDNO>IA012-000125-B045-155</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.24.html 128.240.150.127 19970217010333 text/html 16072
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:02:00 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 24</TITLE>
<LINK REL="Prev" HREF="/Risks/4.23.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.25.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.23.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.25.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 24</H1>
<H2>Friday, 5 December 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Criminal Encryption &amp; Long Term effects 
</A>
<DD>
<A HREF="#subj1.1">
Baxter
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Criminals and encryption 
</A>
<DD>
<A HREF="#subj2.1">
Phil Karn
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: ATC Near-Collisions 
</A>
<DD>
<A HREF="#subj3.1">
Rony Shapiro
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  High Availability Systems 
</A>
<DD>
<A HREF="#subj4.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Plug-compatible modules 
</A>
<DD>
<A HREF="#subj5.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  "Satellite interference" 
</A>
<DD>
<A HREF="#subj6.1">
Lauren Weinstein
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Re: Privacy in the office 
</A>
<DD>
<A HREF="#subj7.1">
Brint Cooper
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  ACARD Report 
</A>
<DD>
<A HREF="#subj8.1">
Samuel B. Bassett
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Criminal Encryption &amp; Long Term effects
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Thu, 04 Dec 86 17:54:14 -0800
From: baxter@ICSD.UCI.EDU

In a previous RISKS article, mention was made of a prostitution ring that
used computers to keep track of the customer base.  It was only moderately
surprising that "criminal elements" would finally arrive at a use for data
processing technology (as opposed to victimizing it...).  Having built file
systems which automatically decrypt records when accessed (for password
storage, etc.), I have long been surprised that the use of encryption as a
technique for storing such transactions has not received widespread use, or
that at least a spectacular instance has not been uncovered.  For a bookie,
surely the convenience of pulling the plug on a computer during a police
raid and taking the 5th when asked for an encryption key must outweigh the
difficulty of handling and destroying flimsy tissues (no, I'm not sure what
technology bookies actually use).  The first conclusion is that computing
(and technology supporting privacy) may make a life of crime more convenient
and safer, raising the spectre of a permanently entrenched criminal
computing element.

A more chilling thought is perhaps the long term consequence of police
reaction to this: acquisition of privacy-breaking technology, and use of
such technology to detect criminal transactions before arrests occur.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Criminals and encryption
</A>
</H3>
<address>
Phil Karn
&lt;<A HREF="mailto:karn@ka9q.ampr.net ">
karn@ka9q.ampr.net 
</A>&gt;
</address>
<i>
Fri, 5 Dec 86 03:17:51 EST
</i><PRE>

I am interested in documented criminal cases where defendants have encrypted
their communications or incriminating computer files and refused to divulge
the keys under their Fifth Amendment rights.  I am particularly interested
in the response of the legal system in such cases and the effect, if any,
encryption technology might have had on the outcome.  I can think of many
possible scenarios, such as:

1. The police either trick the defendent into revealing the key, or by
exploiting his carelessness (by finding it written down or easily guessed,
etc) recover the information which is then used in the prosecution.

2. When more than one person knows the key, one is given immunity and
compelled to divulge it to produce evidence against the other(s).

3. The police perform a successful cryptanalysis.

4. The police and prosecution are unable to recover the encrypted
information, but obtain a conviction anyway in the traditional way (through
witnesses, physical evidence, etc).

5. Without the encrypted information, the case is dropped due to lack of
evidence.

Much of the evidence in certain types of criminal cases consists of paper
records and intercepted telephone conversations obtained through warrants.
(Political corruption, drug rings and organized crime come to mind). I am
interested in the issue of how the widespread availability of computers and
encryption devices will affect the criminal justice system.  Clearly, it
will be impossible to keep this technology out of the hands of criminals (at
least in the US).  Will prosecutors find other, equally successful ways to
get convictions? Or will there be mounting pressure to erode Constitutional
due-process guarantees and the right against self-incrimination?  Even
worse, will there be misguided and futile attempts (along the lines of the
Electronic Communications Privacy Act) to control the availability of
computers within the United States in the name of "law and order" or
"national security"?

Phil

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: ATC Near-Collisions
</A>
</H3>
<address>
Rony Shapiro 
&lt;<A HREF="mailto:ronys%wisdom.bitnet@jade.berkeley.edu">
ronys%wisdom.bitnet@jade.berkeley.edu
</A>&gt;
</address>
<i>
Thu, 4 Dec 86 11:13:19 -0200
</i><PRE>

 I would like to comment on the article from Chicago (UPI) Dec 2 1986.

 The fact that the transponder on the light aircraft was defective may be
misleading. Air-traffic controllers are trained (at least here) NEVER to
rely on tranponder altitude reports when assigning altitudes to other
aircraft. In other words, the controller appeared to have erred in trusting
the transponder when giving the jet clearance to land.

 Transponders are not perfect, &amp; their transmissions may get garbled,
especially in a crowded airspace, such as Chicago. However, as long as they
are regarded as such, they are a useful aid in air traffic controlling.

 Trusting transponders too much is a great temptation under heavy workloads
(easier than asking the pilot of the aircraft in question his altitude - the
only sure method), but the blame is with the ATC, &amp; not with the transponder.

                Rony Shapiro. &lt;ronys@wisdom&gt;

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
High Availability Systems
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Thu 4 Dec 86 09:30:22-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

There is an ad in the 4 Dec 86's SF Chron for "California's most convenient
ATMs".  The banner across a depicted terminal screen says "NOW 24 HOURS A DAY"
(implying that until recently it wasn't?).  The text of the ad says
"VERSATELLER ATMs are there ... when you need them ... With 24-hour service
all day and all night.*" (as opposed to 24-hour service just during the
day?)  The footnote in VERY fine print says "* Available at most locations
and subject to routine system maintenance 2 a.m. to 6 a.m. Sumday." 

        [There was a time when ATMs would run stand-alone when the central
         computer was down, but there were some cases of people grossly 
         exceeding limits intentionally during such times.  Is this no
         longer the case?  The Airline reservation systems also have the
         maintenance problem of having to shut down, but that is presumably
         because of large numbers of schedule changes that for some peculiar
         reason cannot be queued up dynamically and cut over at a particular
         time.  There are lots of interesting risks associated with upgrading
         and/or maintaining more time-critical systems that cannot afford to
         be down at all...  PGN]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Plug-compatible modules
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Thu 4 Dec 86 09:36:38-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

The 4 Dec 86 SF Chron has an AP story on a 4-year old girl who was
electrocuted when a nurse accidentally plugged her heart-monitoring line
into an electrical circuit.  (Children's Hospital, Seattle WA)  Using a
standard male electrical plug on such a line seems incredible.  I mention
it here as a generalized example of the lack of strong typing (type safety).
Compatibility among different types is a common and serious problem in computer
programming languages and system calls, but this case is somehow amazing...

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
"Satellite interference" (CNN Headline News)
</A>
</H3>
<address>
Lauren Weinstein
&lt;<A HREF="mailto:vortex!lauren@rand-unix.ARPA ">
vortex!lauren@rand-unix.ARPA 
</A>&gt;
</address>
<i>
Wed, 3-Dec-86 12:33:42 PST
</i><PRE>
To: RISKS@CSL.SRI.COM

While misaimings and such are fairly common, they rarely result in total
capture of a satellite transponder, since it takes considerable power
to completely override the main signal.  In the case of the described
problem with CNN Headline News, the explanation is almost certainly
very simple and has nothing whatever to do with interfering signals.

Both CNN services (as are a variety of other satellite services) are scrambled 
with the VideoCipher II system (designed by MA/COM, now owned by General
Instruments).  The system uses DES technology and has the capability of an
in-the-clear "barker" audio channel that promotes the service and (in the case
of CNN) the sale of decoders as well.  The VC II technology is very sensitive 
to signal levels and quality--if the level drops off or glitches momentarily
the unit will fall back into its "deaddressed" mode and send the encrypted
video and the audible barker to the output (in most cases a cable system).
It can take anywhere from a second or two to many minutes (sometimes hours
under poor conditions) for the VC II to resync and restore normal output.

The case described almost certainly was a VC II dropout at the local
cable company that resulted in the encrypted picture and clear barker
being sent to the cable system subscribers.

By the way, the proposal the FCC has made about ID's in the vertical
interval will not sit well with many programmers--the vertical interval
is sometimes used for other purposes (teletext, audio services, etc.)
and those programmers can be expected to vigorously object to "wasting"
their interval on a visible I.D.  Of course, if only "occasional"
uplinkers (such as remote news crews) were required to do this, it would
not be such a problem since such crews virtually never are sending
any special vertical interval information.

--Lauren--

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
 Re: Privacy in the office
</A>
</H3>
<address>
    Brint Cooper 
&lt;<A HREF="mailto:abc@BRL.ARPA">
abc@BRL.ARPA
</A>&gt;
</address>
<i>
Wed, 3 Dec 86 23:13:16 EST
</i><PRE>

	All offices are not equivalent.  In components of the DoD, as we
are made painfully aware, "Use of official telephones implies consent to
monitoring."  How "they" monitor, whether computers are used, and how the
monitored content is validated, are anyone's guess.

Brint

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
ACARD Report
</A>
</H3>
<address>
Samuel B. Bassett
&lt;<A HREF="mailto:well!samlb@lll-crg.ARPA ">
well!samlb@lll-crg.ARPA 
</A>&gt;
</address>
<i>
Wed, 3 Dec 86 23:35:11 pst
</i><PRE>

     In regard to the ACARD report, it strikes me that what the British 
commission is trying to do is to force businesses and organizations to 
accept the idea of product liability in an increasingly critical area.  The 
British system allows the sort of "persuasion from on high" that we in the 
U.S. would never put up with.  (Can you imagine how much money would be 
available to a PAC to _defeat_ the first Congresscritter to introduce such 
a bill here?)
 
     It may be that this is a political "stalking horse" -- an early 
attempt to put the idea in the public domain, let it get argued over for a 
few years, and avoid a direct political battle in the near future.  The 
wording has that peculiar British Civil Servant flavor to it, which 
indicates to me that it is mostly a thoeretical exercise at the moment.
 
     In any event, serious programmers and software engineers should 
welcome the news of the report -- it will strengthen their hands when 
talking to management about realistic time scales for software projects.  
The literature has been full of breast-beating about how good software 
would be if management didn't persist in rushing it out the door without 
proper testing?  Now they have a good arguement to hit 'em with.
 
     Then too, in the last analysis, even if the report were enacted into 
law, it is doubtful if many (or any) programmers would go to jail -- but it 
would be almost certain that more than a few companies would lose a _lot_ 
of money.  Managers pay attention to such things . . .

Sam'l Bassett, Self-Employed Writer
34 Oakland Ave., San Anselmo  CA  94960;
DDD:         (415) 454-7282;     /  dual\
UUCP:        {...known world...}! lll-crg!well!samlb;
Compuserve:  71735,1776;         \hplabs/

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.23.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.25.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-102</DOCNO>
<DOCOLDNO>IA012-000125-B045-181</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.25.html 128.240.150.127 19970217010348 text/html 17044
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:02:15 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 25</TITLE>
<LINK REL="Prev" HREF="/Risks/4.24.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.26.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.24.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.26.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 25</H1>
<H2>Sunday, 7 December 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Child electrocuted 
</A>
<DD>
<A HREF="#subj1.1">
Anonymous
</A><br>
<A HREF="#subj1.2">
 Brad Davis
</A><br>
<A HREF="#subj1.3">
 Paul Nelson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  On models, publications, and credibility 
</A>
<DD>
<A HREF="#subj2.1">
Bob Estell
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Encryption and criminals 
</A>
<DD>
<A HREF="#subj3.1">
Perry Metzger
</A><br>
<A HREF="#subj3.2">
 Fred Hapgood
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Mode-C altitude transponders 
</A>
<DD>
<A HREF="#subj4.1">
Dan Nelson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  ATM Limits 
</A>
<DD>
<A HREF="#subj5.1">
Richard Outerbridge
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Taking the 5th 
</A>
<DD>
<A HREF="#subj6.1">
Jerry Leichter
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Child electrocuted (<A HREF="/Risks/4.24.html">RISKS-4.24</A>) (anonymous contribution)
</A>
</H3>
<address>
&lt;<A HREF="mailto:NEUMANN@CSL.SRI.COM ">
NEUMANN@CSL.SRI.COM 
</A>&gt;
</address>
<i>
Fri, 5 Dec 86 17:19 PST
</i><PRE>
To: RISKS@CSL.SRI.COM

This contribution was sent to me privately, but is being distributed
anonymously -- at my request -- with the permission of the author.
  
  I used to volunteer in the Emergency Room at a SF hospital, and the heart
  monitoring lines there had about six pins arranged in a circle, similar to
  the bottom of vacuum tubes.  The exposed pins were shielded by a heavy
  (1/8-in thick) metal ring with a key which permitted it to be plugged into
  the proper receptacle in only one orientation.  Every EKG line I've ever
  seen (including some at other locations) is compatible with this
  configuration.

  Unless someone had built a non-standard connector for this particular
  monitor line, such an electrocution would not have been possible with a
  standard electrical receptacle.  [...]

</PRE>
<HR><H3><A NAME="subj1.2">
 Child electrocuted
</A>
</H3>
<address>
Brad Davis
&lt;<A HREF="mailto:b-davis%utah-cai@utah-cs.arpa ">
b-davis%utah-cai@utah-cs.arpa 
</A>&gt;
</address>
<i>
Fri, 5 Dec 86 18:42:50 mst
</i><PRE>

If this is true then I don't think that the equipment met Underwriter Lab's
(UL) safety specs.  They have some strict requirements on what certain plug
designs can be used for and how current carrying plugs can be configured.

Brad Davis	{ihnp4, decvax, seismo}!utah-cs!b-davis   b-davis@utah-cs.ARPA

</PRE>
<HR><H3><A NAME="subj1.3">
Child electrocuted
</A>
</H3>
<address>
&lt;<A HREF="mailto:ssc-vax!ssc-bee!nelson@beaver.cs.washington.edu">
ssc-vax!ssc-bee!nelson@beaver.cs.washington.edu
</A>&gt;
</address>
<i>
Fri, 5 Dec 86 15:30:32 pst
</i><PRE>

[...] The leads were incorrectly inserted into the end of a power cord for
an IV pump, causing the electrocution.  This particular IV pump had a
detachable power cord for portable battery-powered operation.  Most all news
reports that I have heard put the blame on human error (Nurse electrocutes
child patient...).

How this could happen was beyond my comprehension until I watched the news
and had a look at the ends of the power cord for the IV pump and the cord
for the heart-monitor equipment.  The ends were very similar in shape and
the heart-monitor leads actually fit into either [termination] without much
difficulty.

Besides the obvious finger-pointing consequences of this incident, I was
immediately hit with the grave psychological damage that must have been
caused to both the nurse and child's family.  The ultimate risk of living in
our "high tech" society had certainly been realized by them.

				Paul Nelson, Boeing Aerospace Co.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
On models, publications, and credibility
</A>
</H3>
<address>
"ESTELL ROBERT G" 
&lt;<A HREF="mailto:estell@nwc-143b.ARPA">
estell@nwc-143b.ARPA
</A>&gt;
</address>
<i>
5 Dec 86 11:21:00 PST
</i><PRE>
To: "risks" &lt;risks@csl.sri.com&gt;

Perhaps one way to encourage researchers and authors to take more care
with their data and their models would be for some leading journals
[e.g., ACM and IEEE pubs, among others] to encourage authors to submit
complete listings of programs, and data, in appendices to papers.
For lack of page space, many such appendices would NOT be printed
with the articles.  But the information could be made available from 
the publisher, via network, or floppy disks, for a reasonable fee.

Some work will involve sensitive data, or proprietary models.  In such
cases, sometimes the data can be "sanitized" and sometimes the model can
be described generically.  That won't be a lot different from today's 
situation, where models and data rarely appear in detail.

On the subject of "getting the right(?) answer" we need to remember
[and tell our non-computing colleagues] that even the "facts" that we
seek as data influence the decision; ditto the model design, and the
parameterization of the model.  One of my grad school profs told a story
of working for Getty Oil: J. Paul's two sons [half brothers] were rivals;
one had North American operations; the other, European.  The European
leader proposed some corporate scheme; my prof's assignment was to
"prove him wrong."  So they went to work on a model; fed it some good
estimates; and it agreed with the European recommendation; modified the
parameters, and re-computed. ... On the 253rd such iteration, the model 
finally said that the brother was wrong.  It was that last case that
the USA manager took to his dad, who believed it.

That doesn't necessarily mean they cheated.  How many models of the DNA
structure were wrong, before the right one was found?  How many bad airplane
designs crashed, before Kittyhawk?  How many flawed page replacement
algorithms, or sort algorithms, et al, have we tried?  For the candy maker,
good "fudging" is obviously progress; the rest of us have to wonder.

The power of computer models is that they allow us to try out so many
ideas, or variations of them, so rapidly, so inexpensively.  The risk
of computer models is that we accept their results, without critique.
I contend that tinkering with a model and its data are proper; and that
the results of the Nth iteration may well be better than the results of
the "first best guess."  But the reasons for believing any model output
must rest on a *causal link to reality.*  

Bob

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Encryption and criminals
</A>
</H3>
<address>
Perry Metzger 
&lt;<A HREF="mailto:metzger@heathcliff.columbia.edu">
metzger@heathcliff.columbia.edu
</A>&gt;
</address>
<i>
Fri, 5 Dec 86 19:02:23 EST
</i><PRE>

One of the classic books on this subject, "The Code Breakers" by Kahn,
discusses the incidents during prohibition with rumrunners and encryption.
It seems that earlier in the century commercial codes were widely used.

One of the more humourous incidents listed (reminiscent of trials
involving technology today) was during the trial of one set of
smugglers in which a star witness was a cryptanalyst who was quite
incompetently questioned by the defense. The lawyer's ignorance of the
techniques used was hysterical, and reminiscent of what happens today.

But back to the subject, during prohibition the law enforcement
agencies would quite often call in outside help and try to break the
codes involved, often with success. So far as I could tell from the
book, this did not lead to wide-scale abuses of any sort involving the
police trying to crack commercially used codes and the like.

After all, breaking a code is a long and labour intensive task. You
don't do it unless you have to. Routine breaking of encryption by the
police will not be a reality any time soon.

Perry Metzger

     [Although in a real crunch, there are skilled cryptoanalysts 
      around who could probably be brought into the fray.]

</PRE>
<HR><H3><A NAME="subj3.2">
Encryption and criminals
</A>
</H3>
<address>
"Fred Hapgood" 
&lt;<A HREF="mailto:SIDNEY.G.HAPGOOD%OZ.AI.MIT.EDU@XX.LCS.MIT.EDU">
SIDNEY.G.HAPGOOD%OZ.AI.MIT.EDU@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Sun 7 Dec 86 07:40:35-EST
</i><PRE>
To: risks%OZ.AI.MIT.EDU@XX.LCS.MIT.EDU

	Re encryption by criminals. Some years ago I fell into conversation
with a gentleman who worked as an IRS prosecutor.  Occasionally he brought a
house of prostitution before the bar, and they routinely encrypt their
client lists and financial records, and probably have for millenia.
	He had no interest in spending the time trying to break their codes.
What he did was subpoena the records from their towel company, multiply the
number of towels they used by the average charge, and bill them for the tax
due on that amount. He said the Courts proved happy to accept that document.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Mode-C altitude transponders
</A>
</H3>
<address>
Dan Melson
&lt;<A HREF="mailto:crash!dm@pnet01 ">
crash!dm@pnet01 
</A>&gt;
</address>
<i>
Sat, 6 Dec 86 17:14:35 PST
</i><PRE>

ronys@wisdom writes  "ATC is trained to never trust a transponder"

I'm sorry, but this is incorrect information.  ATC is trained to verify, at
the time the pilot checks on frequency, that the mode-C is accurate.  If,
of course, the pilot is not talking to the controller, there is no way for
that controller to know that mode-C is verified.

Phraseology for issuing traffic on unverified mode-C readouts includes
telling the pilot that we have no confirming report that mode C is correct.

However, a verified mode-C readout *is* used as basis for separation.

                                                DM

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
ATM Limits
</A>
</H3>
<address>
Richard Outerbridge 
&lt;<A HREF="mailto:outer%csri.toronto.edu@RELAY.CS.NET">
outer%csri.toronto.edu@RELAY.CS.NET
</A>&gt;
</address>
<i>
Sat, 6 Dec 86 07:44:10 est
</i><PRE>

Typically ATMs are hung off a controller, which acts as a front-end for the
bank's mainframe host.  The controller often performs a lot of the normal
processing anyway - for instance, pin verification and sanity checking - and
can usually "stand-in" for the host while the latter is down.  One mechanism
used to prevent fraud is a "cycle file".  This keeps a record of all the
cards used within a 24-hour period along with the amount of cash dispensed
to each.  The "cycle limit" is either pre-defined (according to the "type"
of card) or read from the card itself.  So, if the host is down, you may be
able to withdraw up to your daily "cycle" limit at 23:55 and again at 00:05,
but only every two days.  If the cycle limit is recorded on the card, by
re-writing that field you may also be able to withdraw virtually unlimited
amounts of cash (again, if the host is offline).

If the controller is down, the ATMs will be closed, but usually the
controller is more stable than the host.  In the event of hardware
failure the only solution is a "hot" backup controller which can be
switched over to resume processing, albeit after a brief interruption
of service.  If more than one controller is attached to the host, then
each will maintain its own cycle file; if you knew the network you
could withdraw your cycle limit from each.

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
 Taking the 5th
</A>
</H3>
<address>

&lt;<A HREF="mailto:LEICHTER-JERRY@YALE.ARPA">
LEICHTER-JERRY@YALE.ARPA
</A>&gt;
</address>
<i>
6 DEC 1986 10:35:22 EST
</i><PRE>

I asked a lawyer friend about this issue - a criminal with encrypted records
refusing to divulge the key, citing the fifth amendment - a couple of years
back.  His strong feeling - and, of course, until someone actually pushes such
a case, probably to the Supreme Court, all you can GET are feelings - was that
there was no way a court would uphold such a claim.  The Fifth Amendment lets
you refuse to provide information ABOUT possibly-criminal activities.  It does
NOT allow you to avoid turning over evidence.  In general, the courts guard
their rights to obtain evidence very jealously, and interpret limitations on
those rights as narrowly as they possibly can.  (Consider the various "shield
laws" that states have passed to allow journalists to protect their sources.
Even with fairly explicit laws on the books, courts, when they've found a need
for journalists' testimony, have found ways to force it.)

In practice, I doubt it makes much difference.  The worst that is likely to
happen to you for refusing to testify is a couple of months in jail.  (There
are typically two stages:  The court first jails you "until you reconsider
your refusal".  In principle, this can be forever.  In practice, when it
becomes clear that you will not change your mind, we move to a second stage,
where you may or may not be held in contempt of court.  I don't know what the
maximum sentence for contempt is, but typical contempt sentences seem to be a
couple of months.)  So a real criminal is likely to see this as an excellent
trade-off.

This whole issue, BTW, illustrates an interesting point.  Those of us who are
heavily involved with computers, networks, and so on, as technologists, tend
to see what we do as entirely new and unprecedented.  Lawyers tend to view
EVERYTHING as a variation of some precedent.  It's been my experience that the
lawyers are usually closer to the truth.  You really don't need computers to
encrypt bookie's records - bookies have been doing that by hand for years.
(Perhaps you can figure out the quantities of money, but no bookie worth his
salt leaves customer's names in his records in any recognizable form.)

In fact, you don't need to consider encryption AT ALL in deciding whether
the Fifth Amendment applies in cases like this.  Consider, for example, an
arrested man found in possession of an unmarked key to a safety deposit box.
It's very, very likely that the box contains valuable evidence.  Can he be
compelled to reveal where the box is?  I don't know, but I'm sure similar
cases have arisen over the years.
							-- Jerry

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.24.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.26.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-103</DOCNO>
<DOCOLDNO>IA012-000125-B045-204</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.26.html 128.240.150.127 19970217010403 text/html 20243
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:02:30 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 26</TITLE>
<LINK REL="Prev" HREF="/Risks/4.25.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.27.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.25.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.27.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 26</H1>
<H2>Wednesday, 10 December 1986</H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Computer Error Endangers Hardware 
</A>
<DD>
<A HREF="#subj1.1">
Nancy I. Garman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  "One of the Worst Days Ever for Muni Metro, BART" 
</A>
<DD>
<A HREF="#subj2.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Korean Air Lines Flight 007 
</A>
<DD>
<A HREF="#subj3.1">
Steve Jong
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Plug Compatible Modules; Criminal Encryption 
</A>
<DD>
<A HREF="#subj4.1">
David Fetrow
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  More on skyscraper control 
</A>
<DD>
<A HREF="#subj5.1">
Mike Ekberg
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Satellite interference 
</A>
<DD>
<A HREF="#subj6.1">
James D. Carlson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  (Il)legal Encryption 
</A>
<DD>
<A HREF="#subj7.1">
Richard Outerbridge
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Software article in _Computer Design_ 
</A>
<DD>
<A HREF="#subj8.1">
Walt Thode
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj9">
  Heavy metal and light algorithms 
</A>
<DD>
<A HREF="#subj9.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj10">
  Suit against Lotus dropped 
</A>
<DD>
<A HREF="#subj10.1">
Bill Sommerfeld
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Computer Error Endangers Hardware  
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Wed, 10 Dec 86 08:44:43 PST
From: Nancy I. Garman &lt;ngarman@venera.isi.edu&gt;

I work for a group that also manages an offsite computer center.  There has
been so much difficulty with the contractor who is supposed to keep the
floor clean that our hardware folks were worried about disk drive
contamination from the dirty floor.

I spoke with the Director of Sales for the cleaning company.  He blamed their
computer for the dirty computer room floor that was risking damage to our disk
drives.  Apparently, their computer had us erroneously scheduled for fewer
cleanings than our contract called for.  

Of course, it is likely to be a data entry error.  Still, it makes me wonder -
what does their computer have against our computers?!

- Nancy Garman
  NGarman@VENERA.ISI.EDU

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
"One of the Worst Days Ever for Muni Metro, BART"
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Wed 10 Dec 86 17:38:57-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

(From an article by Harry W. Demoro and Carl Nolte in the San Francisco
Chronicle, 10 December 1986, p. 2.)

"... doors, signals, switches, brakes, and even a speedometer broke."

The worst mess began at 6 a.m. when an electrical short circuit caused
a "ghost train" to appear on the signaling equipment that guides Muni
Metro streetcars in and out of the Embarcadero station.  [This prevented
the switches from working automatically.]  Muni troubleshooters did not
eliminate the "ghost train" until 8:14 a.m...

By that time BART was a mess...  [for the second morning in a row]  At 5:10
a.m., a train broke down at the Richmond Yard.  Then at 7:10 a.m., the
switches that route trains through the MacArthur station in Oakland stuck,
creating a bottleneck because two lines converge there.  [Workers used hand
cranks.] The problem was fixed at 9.07 a.m.  Also at 9:07, switches stuck at
the Daly City station (18 miles away).

At 7:14 a.m. a door stuck open at MacArthur.  At 7:25 a.m. a train was taken
out of service because brakes locked for no apparent reason.  At 7:33 a.m. a
train stalled when the door stuck.  At 8:04 a.m. another train broke down in
the repair yard.  At 8:38 a.m., a train refused to budge because of a stuck
door.  At 9 a.m the speedometer stuck on a train, which had to be sidetracked.
At 10:28 a.m. another train was stalled by a stuck door.  The problem
"finally cleared itself up at noon," said a spokesman.  [Bad Car-ma resolved?]

Things have been fairly smooth for BART and Metro Muni for some time.  I
don't recall BART having such a disastrous day since 6 years ago.  (See
Software Engineering Notes 6 1, January 1981)  The "ghost train" problem 
had plagued Muni Metro in its early days, but I had not heard about it
recently.  (See Software Engineering Notes 8 3, July 1983)

Although the computer systems were not implicated, this "bad day" serves
to remind us that when we plan for things not going well, we need to plan
for things going REALLY BADLY.  

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Korean Air Lines Flight 007
</A>
</H3>
<address>
Steve Jong/NaC Pubs
&lt;<A HREF="mailto:jong%derep.DEC@decwrl.DEC.COM  ">
jong%derep.DEC@decwrl.DEC.COM  
</A>&gt;
</address>
<i>
Tuesday,  9 Dec 1986 11:07:53-PST
</i><PRE>

In his book "The Target is Destroyed" (1986), Pulitzer Prize-winning writer
Seymour M. Hirsh strives to explain why Korean Air Lines Flight 007 flew
serenely over the Soviet Union to its doom on September 1, 1983.  Since none
of the crew survived and the flight recorders were never recovered, any
explanation is highly conjectural, but he presents the arguments of a
veteran pilot, one who has flown that route many times.  After exhaustive
studies, including his knowledge of how flight crews work with their
equipment, the pilot concluded that a combination of human errors caused the
navigational snafu.  One of the errors was postulated to be a well-known
blind faith in the plane's inertial navigation system (INS).

This triply-redundant, highly accurate system flies the plane automatically
once coordinates are entered.  The crew enters starting, ending, and
"waystation" coordinates into each of the three components.  If there is an
entry error, or if the plane seems off course, an alarm sounds.

The full scenario is too complex to cover here, but the gist of it
is that a crew member fat-fingered the "you are here" coordinates.

How is it a RISK?  Consider the anecdotal evidence of other flights:

o	Crews place complete faith in INS.  They don't have to fly the
	plane, and sometimes have been known to nap in the cockpit.

o	Crews trust INS more than their radar.  The pilot who
	developed this scenario said if the KAL crew looked at
	their radar and saw the Kamchatka Penninsula where there
	should have been open ocean, they probably shut off the
	radar, because the INS was functioning normally.

o	The INS is so sensitive that if the plane strays down the
	wrong taxiway, it sounds off.  Crews will shut off the alarm.

o	Entry errors are common on long flights, because crews must
	enter three sets of ten coordinates (over a hundred numbers).

o	Though it is strictly against airline policy to do so, at
	the touch of a button the crew can "autoload" coordinates
	from one INS to another.

If you accept the scenario, 269 people died at least partly because of blind
faith in computers and a tedious interface that was too simply circumvented.

    [Reminder:  There are quite a few books on this subject.  Each tries to
     justify its own theory, but all seem to come to somewhat different
     conclusions.  PGN]

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Plug Compatible Modules; Criminal Encryption
</A>
</H3>
<address>
David Fetrow
&lt;<A HREF="mailto:fetrow@entropy.ms.washington.edu ">
fetrow@entropy.ms.washington.edu 
</A>&gt;
</address>
<i>
Mon, 8 Dec 86 01:45:09 PST
</i><PRE>

 Item 1: Plug Compatible Modules

 Concerning the Nurse who accidentally electrocuted a little girl by plugging
in AC power to heart monitor electrodes at Seattles' Childrens' Orthopedic.

 AP gave the impression that the heart-monitor plug was like a wall-plug. This
was not the case: The heart monitor plug consisted of three simple metal probes
(like those on an ohm meter). They were accidentally plugged into the slots
of the female end of an AC extension cord; which resembled the unit the probes
should have been attached to. The solution doesn't change: make unique plugs
for everthing around an ICU patient. (Source: KING TV on camera interview with
hospital administrator).

 Item 2: Criminal Encryption

 I remember reading in the Seattle Times a couple years ago about a computer
expert who encrypted his kid-porn information on a disk. The police had a
warrant for his files but couldn't crack the encryption. They turned to
hacker who tried the "decrypt" command without a key. It worked; the
evidence was admissible.  [No documentation for this one, though.]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
More on skyscraper control
</A>
</H3>
<address>
Mike Ekberg
&lt;<A HREF="mailto:weitek!mae@decwrl.DEC.COM ">
weitek!mae@decwrl.DEC.COM 
</A>&gt;
</address>
<i>
Fri, 5 Dec 86 17:11:15 pst
</i><PRE>

One of the buildings in Boston is indeed balanced by computer. I think it
is the John Hancock Building.

At any rate, the building was designed by I.M. Pei, a rather famous architect.
It was one of the first buildings ever built that is a parallelpiped with
non-90 degree angles. The skin of the building is almost solid glass. Soon
after the building was finished, glass sheets began falling off the building
onto the plaza below. (I don't know if anybody was squashed) This only occured
when the wind blew.

An aeronautical engineer at nearby MIT found out why the glass fell. He 
modelled the building as an vertical aircraft wing fixed on the bottom end.
When the wind blows, the wing(building) generates lift on one side. The
upper part of the building twists and window dimensions are altered causing
the glass to fall.

The solution was to install in the upper floor a large weight controlled by
computer. When the computer detects the building being twisted, it counters
the torque by moving this weight. 

In addition, all the glass in the building was replaced with a type more 
resistant to the effects of the building being twisted. The new glass has the
property that its optical characteristics are significantly modified 
when the panes are twisted. In periods of high wind, spotters near the 
building can monitor its status by using binolculars looking at sections
of glass.

mike   {turtlevax,cae780,pyramid}!weitek!mae

PS Most of this was related to me by a structural engineer living in Boston.

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Satellite interference
</A>
</H3>
<address>
James D. Carlson
&lt;<A HREF="mailto:jc37@andrew.cmu.edu ">
jc37@andrew.cmu.edu 
</A>&gt;
</address>
<i>
Mon,  8 Dec 86 20:06:02 est
</i><PRE>

  From Lauren Weinstein:

  &gt; While misaimings and such are fairly common, they rarely result in total
  &gt; capture of a satellite transponder, since it takes considerable power to
  &gt; completely override the main signal.  In the case of the described 
  &gt; problem with CNN Headline News, the explanation is almost certainly very 
  &gt; simple and has nothing whatever to do with interfering signals.

Unfortunately, uplink signals are usually fairly weak, about one watt,
since they are very narrow beam.  The uplink is also frequency
modulated, which means that another signal only 1dB stronger aimed in
the same direction will take over the satellite's receiver.

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
(Il)legal Encryption
</A>
</H3>
<address>
Richard Outerbridge 
&lt;<A HREF="mailto:outer%csri.toronto.edu@RELAY.CS.NET">
outer%csri.toronto.edu@RELAY.CS.NET
</A>&gt;
</address>
<i>
Mon, 8 Dec 86 20:41:18 est
</i><PRE>

In &gt;The Codebreakers&lt; David Kahn tells of several cases involving crooks,
codes and evidence, but none with 5th amendment implications.  A related
issue is high-order homophonic and "subliminal channel" coding, which are
capable of conveying two (or more) legitimate messages depending on the
key employed: using Key A out pops Grandma's secret recipe for marzipan;
use Key B and out pops the chemistry of the latest designer drug.  Even
were I legally compelled to divulge my keys, if the analyst can't find
'Key B' how can he prove that I haven't complied by revealing 'Key A'?

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
Software article in _Computer Design_
</A>
</H3>
<address>
&lt;<A HREF="mailto:thode@nprdc.arpa">
thode@nprdc.arpa
</A>&gt;
</address>
<i>
10 December 1986 0824-PST (Wednesday)
</i><PRE>

There is an article in a recent issue of _Computer Design_ magazine (the
November 15 issue) titled "Approaches to Software Testing Embroiled in
Debate."  Its author is William E. Suydam.  It covers a lot of the same
ground as some of the contributions to this list.  Quotations from David
Parnas, Nancy Leveson, and others are included.  It seems, from my inexpert
perspective, to be a decent summary of the problems in software reliability.

--Walt Thode (thode@NPRDC)

</PRE>
<A NAME="subj9"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj9.1">
Heavy metal and light algorithms
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Wed 10 Dec 86 17:31:32-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

Dave Parnas called my attention to an oversimplification in which I indulged
when I noted in <A HREF="/Risks/4.20.html">RISKS-4.20</A> that "an algorithm is an algorithm is an algorithm"
(This was in connection with Steve Gutfreund's note on encoding algorithms in
"smart metals".)

Indeed, Dave is right in suggesting that "the metal algorithm would be, to a
very useful approximation, a continuous function or at least piecewise
continuous with very few points of discontinuity.  As such it could be much
more easily analyzed and studied than its counterpart as a digital computer
program."

This raises interesting questions about the relative precision, accuracy,
and soundness of "metal algorithms" and comparable analog devices in
general.  The situation is somewhat akin to higher-level programming
languages.  Perhaps one is less likely to make low-level design and program
errors in the directly-implemented analog case, but it is of course still
possible to choose the wrong model.

</PRE>
<A NAME="subj10"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj10.1">
Suit against Lotus dropped
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Wed, 10 Dec 86 13:15:08 EST
From: Bill Sommerfeld &lt;wesommer@ATHENA.MIT.EDU&gt;

The following article may be of interest to Risks readers; it is from page
81 (the first page of the business section) of the Boston Globe of 10 Dec 86.

	Lawsuit charging errors in Lotus software dropped
               By Ronald Rosenberg, Globe Staff

  A case seen as a test for settling responsibility when computer software
  fails was dropped yesterday, a victory for an industry that had stood by
  nervously as the issue made its way to court.

  James A. Cummings, Inc., a Florida construction firm, yesterday ended
  its suit against Lotus Development Corp. of Cambridge [Massachusetts],
  a lawsuit that the industry feared could open the door to a host of
  liability claims against software developers.

  In its suit, Cummings had charged that errors in the Lotus software caused
  to underbid on, and lost a contract.  The company had sought $254,000 in
  damages from Lotus, a leading maker of personal computer software.

  If the case had gone to trial, it would have been the first to question
  whether a supplier of software tools, such as Lotus, is liable for wrong
  information produced by users of its programs.

  Neither Cummings nor its attorney, John R. Squitero of Miami, will receive
  anything from Lotus.  Squitero, who talked openly about the case last
  summer, refused to comment yesterday.

  Lotus said that under the termination agreement, Squitero and James A.
  Cummings, president of the Fort Lauderdale contracting company, agreed not
  to discuss [the] case.

  ``Lotus is pleased that this attack upon the integrity of Symphony, one
  [of] our leading products, has ended with the complete vindication of both
  Symphony and Lotus,'' said Jim P. Manzi, Lotus chairman and president.

  Squitero had expected to fly to Boston yesterday to take depositions from
  Lotus employees. Late Monday evening, Squitero decided to throw in the towel.

  ``I think they (Cummings and Squitero) hoped that there would be a financial
  settlement by now, and we persuaded them that we would never settle -- not a
  penny,'' said Hank Gutman, an attorney with O'Sullivan, Graev and Karabell
  of New York, which represented Lotus.

  Peter Marx, general counsel to the Information Industry Assn., a trade
  organization for 500 software and computer companies including Lotus,
  applauded the dismissal:

  ``Our fear was that as long as the case was hanging out, it might have
  encouraged creative lawyers to file suits that have no merit.'' 

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.25.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.27.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-104</DOCNO>
<DOCOLDNO>IA012-000125-B045-224</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.27.html 128.240.150.127 19970217010416 text/html 15638
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:02:45 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 27</TITLE>
<LINK REL="Prev" HREF="/Risks/4.26.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.28.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.26.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.28.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 27</H1>
<H2>Thursday, 11 December 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Computerised Discrimination 
</A>
<DD>
<A HREF="#subj1.1">
Brian Randell
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Belgian Paper transcends computer breakdown 
</A>
<DD>
<A HREF="#subj2.1">
Martin Minow
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: Plug-compatible modules 
</A>
<DD>
<A HREF="#subj3.1">
Keith F. Lynch
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: Criminal Encryption 
</A>
<DD>
<A HREF="#subj4.1">
Keith F. Lynch
</A><br>
<A HREF="#subj4.2">
 Ira D. Baxter
</A><br>
<A HREF="#subj4.3">
 Dave Platt
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Re: More on skyscraper control 
</A>
<DD>
<A HREF="#subj5.1">
Brint Cooper
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  The Second Labor of Hercules 
</A>
<DD>
<A HREF="#subj6.1">
Dave Benson
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Computerised Discrimination
</A>
</H3>
<address>
Brian Randell 
&lt;<A HREF="mailto:brian%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
brian%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Thu, 11 Dec 86 17:45:10 gmt
</i><PRE>

Perhaps the most worrying feature of the situation described in the
following extracts from an article in the Guardian, dated 8 Dec. 1986, is
that the computer "was only following orders"!


               Claims of Prejudice Against Women and Blacks

              MEDICAL SCHOOLS TO FACE DISCRIMINATION ENQUIRY

By Andrew Veitch
Medical Correspondent

  Leading medical schools face an investigation into allegations that they are
discriminating against women and black students.
  This follows the discovery by two consultants that their own school, St.
George's in south London, has been using a computer selection programme which
deliberately down grades applicants if they are female and non-white.
  It is thought that hundreds of well-qualified students may have been turned
away on those grounds. The hospital's ruling academic board has scrapped the 
programme and is likely to launch an internal inquiry when it meets tonight.
  Details of alleged discrimination at St. George's and nine other London 
schools were sent last week to the Council for Racial Equality, the Equal
Opportunities Board, and the Inner London Education Authority.
  "The matter is viewed very seriously," said the CRE's legal director,
Mr. John Whitmore. "The commission will be considering the St. George's
case on Wednesday and the position of other medical colleges in January."
  An EOC spokesman said there could be a case to answer. Under the Sex 
Discrimination Act, it is unlawful for a school to discriminate against a 
woman in the terms on which it offers to admit her, or by refusing or
deliberately omitting to accept her application for admission.
  The chairman of Ilea's higher education committee, Mr. Neil Fletcher, 
considered the allegations at the weekend. Ilea has warned schools that it 
will withhold grants if they do not comply with its non-discrimination
policy.
  The St. George's claim is particularly worrying because the school has a 
better record on discrimination than most other colleges.
  The computer selection programme was designed to mimic the decisions of
the school's panel which screened applicants to see who merited an interview.
  It matched the panel's results so closely that the panel was scrapped and 
for several years all St. george's applicants have been screened by computer...

Brian Randell - Computing Laboratory, University of Newcastle upon Tyne

  UUCP  : &lt;UK&gt;!ukc!cheviot!brian
  JANET : brian@uk.ac.newcastle.cheviot

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Belgian Paper transcends computer breakdown
</A>
</H3>
<address>
&lt;<A HREF="mailto:minow%bolt.DEC@decwrl.DEC.COM">
minow%bolt.DEC@decwrl.DEC.COM
</A>&gt;
</address>
<i>
11-Dec-1986 0844
</i><PRE>

This appeared on a local [computer-transmitted] newspaper on Thus 11 Dec
1986, as a note from Peter Van Avermaet.

  Today [Wednesday], the Belgian newspaper "De Morgen" has appeared
  as a hand-written newspaper.

  Yesterday morning [Tuesday], the type-setting computer broke down.
  After several hours, it became clear that it would not be available
  in time for today's edition. But "De Morgen" ["The Morning"] apparently
  survives anything - it went bankrupt some weeks ago.  Today's edition has
  been hand-written, and printed using the "normal" printing process.

  Some topics:
                graphology,

                plans to use more computers in the Ministry of Finance, 
                for the computation of the taxes we should pay.
Martin
                                                  [Goeden "Morgen"!  P.]

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: Plug-compatible modules
</A>
</H3>
<address>
"Keith F. Lynch" 
&lt;<A HREF="mailto:KFL%MX.LCS.MIT.EDU@MC.LCS.MIT.EDU">
KFL%MX.LCS.MIT.EDU@MC.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Wed, 10 Dec 86 23:54:57 EST
</i><PRE>
To: Risks@CSL.SRI.COM

Many terminals keyboards have plugs which are the same as modular telephone
connectors.  I have seen one with a prominent warning that plugging it into
a telephone outlet will destroy the keyboard and damage the phone line.
    								      ...Keith

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: Criminal Encryption 
</A>
</H3>
<address>
"Keith F. Lynch" 
&lt;<A HREF="mailto:KFL%MX.LCS.MIT.EDU@MC.LCS.MIT.EDU">
KFL%MX.LCS.MIT.EDU@MC.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Wed, 10 Dec 86 23:52:53 EST
</i><PRE>
To: baxter@ICSD.UCI.EDU, Risks@CSL.SRI.COM

  I can't see criminal encryption as much of a problem.  All REAL
crimes involve a victim, who is willing to testify.  Perhaps large
scale use of encryption will result in government abandoning its
wasteful and pointless attempt to prosecute victimless crimes.
								...Keith

</PRE>
<HR><H3><A NAME="subj4.2">
Re: Criminal Encryption 
</A>
</H3>
<address>
&lt;<A HREF="mailto:baxter@ICSD.UCI.EDU              [Ira D. Baxter, a.k.a. N.F.N. Baxter]">
baxter@ICSD.UCI.EDU              [Ira D. Baxter, a.k.a. N.F.N. Baxter]
</A>&gt;
</address>
<i>

</i><PRE>
Date: Thu, 11 Dec 86 09:46:23 -0800

Some crimes involve victims that aren't willing to testify.  Blackmail is
the classic example; an encrypted blackmail database ensures the victim that
his blackmail payments aren't wasted, and ensure the criminal that the
incriminating evidence is not easily found (using a needle-in-a-haystack
approach).

Dope pushers selling drugs to dope users appears to be a victimless crime
also... after all, both parties are (presumably) satisfied with the results
of individual transactions.  The problem is the activities on the part of
both parties to make the transactions possible (theft for the user, bribery
and coercion for the pusher) have victims.  Law enforcement is always
interested in the transactions between pushers (at least) because it usually
leads to other agents of victim-ful crime.  Thus the interest in data about
transactions.  Requirements for a secure business relationship between
dealers would lead to more attempts to store transaction data securely.

</PRE>
<HR><H3><A NAME="subj4.3">
Re: Criminal encryption
</A>
</H3>
<address>
Dave Platt
&lt;<A HREF="mailto:dplatt@teknowledge-vaxc.ARPA ">
dplatt@teknowledge-vaxc.ARPA 
</A>&gt;
</address>
<i>
Thu, 11 Dec 86 12:08:34 PST
</i><PRE>

Although I'm not a lawyer, I do have an opinion about the question asked
recently to the effect of "Could an alleged criminal be compelled to reveal
the encryption key for a database containing records related to an alleged
criminal enterprise?".  My opinion, for what it's worth, is that the courts
would probably not uphold any such compulsion, and would likely throw out
any evidence obtained by use of a coerced or compelled revelation of an
encryption key.

Jerry Leichter suggests (based on a conversation with a lawyer friend) that
this situation is analogous to a journalist being compelled to reveal
his/her sources.  I believe that this analogy is suspect... a journalist is
(generally) _not_ under criminal indictment, is _not_ being asked to provide
evidence that would incriminate him/herself, and thus the Fifth Amendment
does not apply at all.  The Fifth Amendment states only that a person cannot
be compelled to incriminate him/herself; it says nothing about compulsion to
incriminate another person.  "Contempt of court" rulings are sometimes used
to [attempt to] compel a person to provide testimony or evidence that can
incriminate _someone_else_, but they aren't (and can't be) used to coerce a
person to provide evidence or testimony that might result in that person's
conviction on criminal charges.  "Shield laws" are a different matter
entirely... they provide journalists with a limited ability to refuse to
turn over material in their possession that might possibly reveal the
identities of their "sources".

If the prosecution in a particular case chooses to grant legal immunity to a
suspect, then the person no longer has the ability to refuse to testify (or
provide evidence) concerning matters covered by the immunity, because s/he
can no longer "incriminate" him/herself regarding those matters.
Prosecutors sometimes grant immunity to a hostile witness (typically a
"minor player" in a larger case), so that they can use the threat of
"contempt of court" rulings to compel the witness to testify against his/her
associates.

Jerry Leichter asks, "Can an arrested man be compelled to reveal where
[a locked safe-deposit] box is?".  I believe that the answer is "No."
The police and prosecution can attempt to locate it themselves;  they
can obtain a search warrant that will permit them to open and examine
the box (or force it open without the key, for that matter);  and they
can use any evidence found by use of a legal search warrant in court.

By analogy, I believe that in the case involving an encrypted database full
of [allegedly] incriminating evidence, the following situation would
probably develop:  the police and prosecutor could seize the database using
a valid search warrant.  The same search warrant would permit them to
attempt to decrypt the data by brute-force or intelligent-search methods.
They could not coerce any of the defendants to reveal the encryption key
unless they were first willing to grant legal immunity to that person
(either via a voluntary agreement, or via an involuntary grant followed by a
contempt-of-court coercion).

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
 Re: More on skyscraper control
</A>
</H3>
<address>
    Brint Cooper 
&lt;<A HREF="mailto:abc@BRL.ARPA">
abc@BRL.ARPA
</A>&gt;
</address>
<i>
Thu, 11 Dec 86 15:01:20 EST
</i><PRE>

...(a discussion about the skyscraper in Boston which would "twist in the
wind" and drop pieces of its glass face to the ground)

&gt; The solution was to install in the upper floor a large weight controlled by
&gt; computer. When the computer detects the building being twisted, it counters
&gt; the torque by moving this weight. 

But if the wind is related to a storm which causes a wide-area power outage,
perhaps the computer won't be available when it is needed most?
Uninterruptible power and backup power are still rather expensive and, I
believe, not widely used.
                                             Brint

          [It is used where needed -- and can be quite cost-effective, given
           the alternatives.  Hospitals, some banks, and various other
           applications have realized how important continuous power is.  
           The Network Information Center (SRI-NIC) keeps running despite
           local power blips that down the rest of SRI's systems!  PGN]

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
 The Second Labor of Hercules
</A>
</H3>
<address>
Dave Benson 
&lt;<A HREF="mailto:benson%wsu.csnet@RELAY.CS.NET">
benson%wsu.csnet@RELAY.CS.NET
</A>&gt;
</address>
<i>
Sun, 7 Dec 86 18:43:37 pst
</i><PRE>

Free copies of the report

     David B. Benson, "The Second Labor of Hercules:  An essay on software
     engineering and the Strategic Defense Initiative -- Preliminary Draft",
     CS-86-148

are available from the Technical Reports Secretary, Computer Science
Department, Washington State University, Pullman WA 99164-1210, by written
request, while the supply lasts.

The essay was finished in May, 1986, and has been only slightly dated by
events.  I intend to begin revising this essay upon the turn of the new year, 
and would appreciate criticisms from all who would care to send such to me.

Thank you in advance for your cooperation.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.26.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.28.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-105</DOCNO>
<DOCOLDNO>IA012-000125-B045-247</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.28.html 128.240.150.127 19970217010442 text/html 27281
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:02:58 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 28</TITLE>
<LINK REL="Prev" HREF="/Risks/4.27.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.29.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.27.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.29.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 28</H1>
<H2> Friday, 12 December 1986 </H2>
<H3>Forum on Risks to the Public in Computers and Related Systems</H3>
<I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Mount a scratch giraffe, too?  Make that several.</A>
<DD><A HREF="#subj1.1">Jim Horning</A><BR></DD>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">Elf debuts as parking attendant</A>
<DD><A HREF="#subj2.1">Kevin B. Kenny</A><BR>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">Plug-compatible plugs</A>
<DD><A HREF="#subj3.1">Chris Koenigsberg</A><BR>
<A HREF="#subj3.2">Henry Schaffer</A><BR>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">An Amusing Article on the Taxonomy of "Bugs"</A>
<DD><A HREF="#subj4.1">Lindsay F. Marshall</A><BR>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">Satellite interference</A>
<DD><A HREF="#subj5.1">Lauren Weinstein</A><BR>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">Fast-food
computers</A>
<DD><A HREF="#subj6.1">Scott Guthery</A><BR>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">Re:  More
on skyscraper control</A>
<DD><A HREF="#subj7.1">Chuck Kennedy</A><BR>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">Re: Risks
of Computer Modeling</A>
<DD><A HREF="#subj8.1">Craig Paxton</A><BR>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj9">Re:
Computerized Discrimination</A>
<DD><A HREF="#subj9.1">Randall Davis</A><BR>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A
HREF="#subj10">Computers and Educational Decrepitude</A>
<DD><A HREF="#subj10.1">Geof Cooper</A><BR>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A
HREF="#subj11">Symposium -- Directions and Implications of Advanced -->
-- Computing</A>
<DD><A HREF="#subj11.1">Jon Jacky</A><BR>
</DL>
<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">Mount a scratch giraffe, too? Make that several.</A></H3>
<ADDRESS>Jim Horning &lt;<A HREF="mailto:horning@src.DEC.COM">horning@src.DEC.COM</A>&gt;</ADDRESS>
<I>Fri, 12 Dec 86 14:17:05 PST</I><PRE>

From DATAMATION, Dec. 15, 1986, p. 67

... The Amsterdam air cargo terminal, an enormous, fully automated
warehouse, is a major hub where cargo is stored before being routed
to destinations all over the world. In the cargo on any given day are
numerous crates of live animals, from dogs and cats to livestock and
zoo animals, many of which must be fed during their stopovers. A DBMS
is used to keep a mirror image of the warehouse and to track the physical
location of all freight traffic.

This system had first been installed by Computer Sciences Corp., El
Segundo, Calif., in the 1960s and had worked fine for several years until
the DBMS failed. All the data were lost. It took several days and several
dead giraffes before the problem was solved, according to Ken Bosomworth,
president of Information Resources Development, Norwalk, Conn., who
learned of this classic horror story through some former CSC employees.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Elf debuts as parking attendant</A></H3>
<ADDRESS>Kevin B. Kenny &lt;<A HREF="mailto:kenny@B.cs.uiuc.edu">kenny@B.cs.uiuc.edu</A>&gt;</ADDRESS>
<I>Fri, 12 Dec 86 15:16:46 CST</I><PRE>

From the (Champaign-Urbana, Ill.) Daily Illini, 12 December 1986:

		   Elf debuts as parking attendant

CONCORD, N.H. (AP)-- Concord's parking elf, captured after a nationwide
search, made his debut at the downtown garage Thursday, frustrated by the
computerized meter system he was hired to make ``user-friendly'' for the
holidays.

``It's flawed,'' said Charlie Bonjorso, a 76-year-old retired barber
who answered Concord's call for someone to wear the elf suit.

``You only get 20 seconds' time when you're supposed to remember where you
parked your car, have your change ready, and push the numbers,'' Bonjorso
said.  ``If you're slow . . . that's it, you've lost your money.''

Parking in the garage dropped from 100 percent to almost nothing when
a computerized meter requiring a good memory and quick fingers was
installed this year, said Ken Lurvey, the city's director of economic
development.  ``People got confused, they got ticketed and they got
frustrated,'' he said.  ``It's far from user-friendly.''

Kevin Kenny, Computer Science	UUCP: {ihnp4,pur-ee,convex}!uiucdcs!kenny
University of Illinois		CSNET: kenny@UIUC.CSNET
Urbana, Illinois, 61801		

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Plug-compatible plugs</A></H3>
<ADDRESS>Chris Koenigsberg
&lt;<A HREF="mailto:ckk#@andrew.cmu.edu">ckk#@andrew.cmu.edu</A>&gt;</ADDRESS>
<I>
Fri, 12 Dec 86 10:26:30 est
</I><PRE>

Someone discovered by accident that the IBM monochrome display adapter will
accept a Token Ring connector cable. (Both the Token Ring and the monochrome
display use standard D connectors.)  Then, when you power on the machine, the
display output brings down the entire local Token Ring that the machine is
on.  Anyone with a workstation that has a monochrome card can disable their
local token ring by plugging the wrong cable into the display adapter
(either accidentally or on purpose), and this is good until someone figures
out which workstation is causing the outage and removes it from the ring at
the wiring closet.

Carnegie Mellon University is wiring all campus buildings, including all
dormitories, with the IBM Cabling System. Every room will have at least one
outlet. The primary use is to attach personal workstations to the IBM Token
Ring. Typically, one or more floors of a building will be running one single
token ring. Fun with your dormitory workstation!

Notes:
- Why couldn't they have made the token ring connector a different kind than
the monochrome display connector? Did (or should) the hardware design process
include any analysis of its consequences in such conjunctions, given known
human tendencies?

- With the token ring, it is much easier to isolate the offending workstation
and remove it from the network than it would be on an Ethernet. Societal
pressures and conventions may evolve to control antisocial network behavior
(we hope!).

- Remember when you were an undergraduate, what would you do with a token
ring and a workstation in your dorm room?

</PRE>
<HR><H3><A NAME="subj3.2">
Plug-compatible plugs</A></H3>
<ADDRESS>
Henry Schaffer
&lt;<A HREF="mailto:ecsvax!hes%mcnc.csnet@RELAY.CS.NET">ecsvax!hes%mcnc.csnet@RELAY.CS.NET</A>&gt;</ADDRESS>
<I>
Thu, 11 Dec 86 16:48:02 est
</I><PRE>

The serial/parallel card on an IBM PC/AT has two (unlabeled) connectors.
These are a 9 pin male and a 25 pin (DB 25) female.  The owner's manual
didn't say which was which - and I wanted to hook up a modem, and I did
have a 25 pin male-male cable.

I shouldn't have figured it could be that easy.  The nice DB25 is the
parallel port and connecting it to a modem damaged it.  (The DB9 is the
serial port.)  I admit I was not as careful as I could have been, but
I also feel as if I'd been set up for this.

--henry schaffer  n c state univ

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
An Amusing Article on the Taxonomy of "Bugs"</A></H3>
<ADDRESS>
"Lindsay F. Marshall"
&lt;<A HREF="mailto:lindsay%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK">lindsay%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK</A>&gt;</ADDRESS>
<I>Fri, 12 Dec 86 08:24:19 GMT</I><PRE>

From "The Computer Bulletin" December 1986 by John Lansdown

One of the things Brian [Reffin Smith] touches on in his contribution [to
the book "Science*Art"] is the creative potential of software and hardware
bugs.  He suggests that these might be distinguished from the more
bothersome variety by being called, 'pugs'.  I have often thought that bugs
are as important to us in computing as snow is to Eskimos so, like them, we
should distinguish the many different sorts with different names.

To give a few instances.  There are some bugs which waylay unsuspecting
computer users and beat them into the ground - often fatally: following Brian's
example, these should be called 'thugs', particularly as they often arise
through the programmer's or manufacturer's misunderstanding of theory. Some
bugs are tiresome but the intrepid user can dismiss them as of no consequence:
'shrugs'.  All of us have written code that has a special class of bugs which,
whilst not being thuggish in themselves, obscure others that sometimes are and
hence make debugging particularly difficult: these obscuring bugs should be
called, 'fugs'.

Some people claim to write totally bug-free programs - if their programs don't
work it is not them that are to blame.  The manual, the system or, more likely,
the unintelligent user is at fault.  Bugs in these programmers' code should be
called 'smugs' or, perhaps, 'humbugs'.  Bugs which put the system to sleep
whilst it still appears to be working or, conversely, make it hyperactive -
resulting in reams of unrequired printing or an endless sequence of error
messages - should be called 'drugs'.  Finally, those which give rise to that
undesirable condition known as deadly embrace (brought about by such things as
incorrectly designed database lockout mechanisms) should be called 'hugs'.

Only by properly naming these types of errors can we hope to study their true
effects and ramifactions.  But what should such a study be called?  I'd be
happy to hear your (printable) suggestions.

  [Such a challenge will not go unheeded.
    'slugs' might be low-level bugs (like viruses?) that move slowly from
            one place to another, especially in systems having no shell.
    'dougs' might be named in honor of Bell Labs' legendary Doug McIlroy (who
            with Bob Morris was responsible for EPL, the Multics development-
            language supersubset of PL/1).  Doug used to make multiple patches
            to the live image of the compiler (which predated the official
            PL/1 compilers, by the way) ON-THE-FLY, oblivious to compilations
            in progress.  I remember some horrendous (and of course completely
            nonreproducible) compilations resulting therefrom.
  PGN]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Satellite interference</A></H3>
<ADDRESS>
Lauren Weinstein
&lt;<A HREF="mailto:vortex!lauren@rand-unix.ARPA ">vortex!lauren@rand-unix.ARPA</A>&gt;</ADDRESS>
<I>Thu, 11-Dec-86 13:19:16 PST</I><PRE>
To: RISKS@CSL.SRI.COM

... "uplinks are only about 1 watt" ...

This is incorrect.  Most commercial C-band uplinks (where 99% of the
cable services operate) run in the vicinity of 300-500 watts at 6 Ghz,
usually via a 10 meter diameter antenna.  Ku-band uplinks can run
with considerably less power (as low as 20-50 watts under some conditions,
sometimes lower for short-term telemetry-only uplinks) but even these uplinks
will tend to run much more power when they are running a "continuous"
(rather than occasional [e.g. remote news uplink]) service.

Experience has shown that for C-band services (where the studies have been
done to date) it requires on the order of a 10db differential to "capture"
a transponder--lower amounts may cause interference but not capture.
Most uplinks have considerable power in reserve to deal with accidental
(or intentional) interference.  In fact, some new techniques have been
developed of late specifically to deal with intentional interference,
some of which are quite clever.
                                           --Lauren--

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
 Fast-food computers</A></H3>
<ADDRESS>
&lt;<A HREF="mailto:guthery%ascvx5.asc@slb-test.CSNET">guthery%ascvx5.asc@slb-test.CSNET</A>&gt;
</ADDRESS>
<I>
Fri, 12 Dec 86 09:56 EDT
</I><PRE>

An observation I have made after being subjected to a fair number
of McDonalds and Taco Bell junk-food delivery systems is the following:

	In an evolving man-machine system, the man will get
	dumber faster than the machine gets smarter.

What seems to happen is that people always assume a computer-based system
is smarter than it really is and, as a result, assume they can be dumber
than they really need to be.  The result is continuing improvements in the
computer component of the system actually result in a net decline in overall
capability of the system.

When you couple this phenomena with the fact that our schools are turning
out system operators who not only are less well-educated but for the
most part devoid of initiative and common sense (having been pumped up on
gratuitous self-esteem and the notion of a risk-free life), I foresee many,
many more system catastrophes, life threatening and otherwise.

When it comes to improving these systems, I wonder what the impact of
focusing almost exclusively on the computer component of the system is.
Won't the tendency be for the computer component to take on more and
more responsibility?  If I, as the designer of the computer part of the
system, am going to be held primarily responsible for its malfunction,
isn't the wise course for me to design for an arbitrarily stupid operator?

The point is that by not regarding system performance as the joint
responsibility of the people and the machines which comprise the system
--- and at least trying to define precisely who is responsible for what ---
those who are to be held responsible will understandably assert the right
to build the system as they see fit.  You can't put people in the loop
without making them liable for the performance of the loop.  And yet this
seems to be exactly what humanist designers seem to be wishing for.

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
 Re:  More on skyscraper control</A></H3>
<ADDRESS>
    Chuck Kennedy
&lt;<A HREF="mailto:kermit@BRL.ARPA">
kermit@BRL.ARPA
</A>&gt;
</ADDRESS>
<I>
Fri, 12 Dec 86 4:12:55 EST
</I><PRE>

Yes, interestingly enough, even such mundane businesses as Sears are now
using UPSs [Uninterruptible Power Supplies].  I was recently in the local
mall (Whitemarsh) during a heavy thunderstorm and the lights went out.
Except in the Sears store where things continued normally.  Too bad the rest
of the mall didn't have UPSs.  (I believe the Penney's at the other end
remained lighted as well.)

The connection to computers of this story is, of course, the point of
sale terminals that need the juice so that sales can be made.  Also,
having the lights available makes for less panic.  The other merchants
in the mall started to close their doors and quickly stationed sales
people near them presumably to make sure that nothing "walked off".

I'm not sure what the cost of UPSs is, but if the power shortage were
moderately long and happened often enough (we get lots of thunderstorms
here at times) I think the UPSs would be worth it.  The benefit of being
able to continue to conduct business, and not worry about looting, etc.
seems well worth it.
                        -Chuck Kennedy, Ballistic Research Laboratory

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
 Re: Risks of Computer Modeling</A></H3>
<ADDRESS>

&lt;<A HREF="mailto:PAX00325%NUACC.BITNET@WISCVM.WISC.EDU">
PAX00325%NUACC.BITNET@WISCVM.WISC.EDU
</A>&gt;
</ADDRESS>
<I>Fri, 12 Dec 86 01:06 CST</I><PRE>
To:  RISKS@CSL.SRI.COM

Yes, there are problems in doing empirical work in economics that economists,
such as myself, are quick to point out.  Verification is done by most, to
some degree, but the costs to outside verification are much greater than
generally believed.  Subtle errors can slip by not only economists, but others
as well.  For example, in the article to which I am refering to, the name
of the professor at UCLA is wrong.  E. Leamer is the author of "...Con out of
Econometrics."

Craig Paxton, Northwestern University.

</PRE>
<A NAME="subj9"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj9.1">Re: Computerized Discrimination</A></H3>
<ADDRESS>Randall Davis&lt;<A
HREF="mailto:DAVIS%OZ.AI.MIT.EDU@XX.LCS.MIT.EDU">DAVIS%OZ.AI.MIT.EDU@XX.LCS.MIT.EDU</A>&gt;</ADDRESS>
<I>Fri 12 Dec 86 19:28-EST</I><PRE>
To: Risks@CSL.SRI.COM

&gt;  Perhaps the most worrying feature of the situation described in the
&gt;  following extracts from an article in the Guardian, dated 8 Dec. 1986, is
&gt;  that the computer "was only following orders"!
&gt;     [extract entitled "Claims of Prejudice Against Women and Blacks"]

Perhaps the most wonderful feature of this situation is that it happened and
demonstrates one of the powerful beneficial consequences of computers as one
vehicle for making knowledge explicit.  Discrimination cases are often
prosecuted on statistical arguments, which are at best circumstantial and
depending on the sample size can be weak.  It is very difficult to prove
intent and quite rare that anyone admits to it directly.  Yet the existence of
this program is explicit and direct evidence that the school has in fact been
discriminating for however long the program has been in use ("several years")
and is interesting circumstantial evidence that the school's panel was in the
past doing the same (they agreed that it matched them).

One can only imagine the reaction of the program authors when they discovered
what one last small change to the program's scoring function was necessary to
make it match the panel's results.  It raises interesting questions of
whistle-blowing.

The panel is now in an interesting position: they can no longer claim that the
admission judgment is "intuitive" or ephemeral: they have themselves agreed
that a program captured their behavior.  Now that the genie is out of the
bottle, it is public and examinable, and that is enormously important.  The
computer has in this case become an instrument to empower people to enforce
equal treatment.

It's quite unlikely that any of this would have come to light in the absence
computers and their application to this task; admissions would still be a
back-room task carried out with unspoken intuitions and feelings.

</PRE>
<A NAME="subj10"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj10.1">Computers and Educational Decrepitude</A></H3>
<ADDRESS>
Geof Cooper
&lt;<A HREF="mailto:imagen!geof@decwrl.DEC.COM ">
imagen!geof@decwrl.DEC.COM
</A>&gt;
</ADDRESS>
<I>
Fri, 12 Dec 86 10:17:36 pst
</I><PRE>

The other day I heard a report on NPR's Morning Edition that the
Educational Testing Service had expressed concern about the diminishing
literate capabilities of American high school (and thus, eventually,
college) students.  This concern struck me as ironic, since I consider
the ETS the prime backer of a great impediment to literacy, the multiple
choice question.  Because of the importance (or perceived importance)
of the SAT examinations, I believe that modern high school programs
have virtually standardized on the use of multiple choice questions to
test their students.  Tests that in earlier days demanded essays or
short, written answers -- tests that challenged not only the student's
knowledge of the subject matter, but also his or her literacy -- now
demand only smudges on a computer form.  Questions that earlier solicited
a clear exposition of the student's knowledge of the subject now instead
demand that the student distinguish between fine shades of meaning and
phraseology.  It is my experience that a student who has shown initiative
and learned extra subject material will often find this added information
enough to muddle the distinctions between possible answers to the question.
The more you know, the worse you do.

The popularity of multiple choice questions stems not from some theory
of their importance in education, but from the desire to automatically
grade examinations by computer.  This brings up a RISK that I haven't
yet seen mentioned on the Digest (I haven't been watching it long, so
apologies if it has been beaten to death earlier) -- the risk that
computers allow for poor solutions to problems by their ability to
allow impersonal, centralized institutions to scale up to larger
populations.

In the example, above, a desire to produce a standardized test that is
given to all students has led to a requirement that the test be multiple
choice, so that the exams can be graded by machine.  The importance of
these tests to the futures of young people has caused high schools to
shift their program away from essay questions, so that students' writing
skills are not emphasized in every subject, as they once were.  The net
effect is a societal problem, the decline of literacy in America.

If computers had not been available to correct multiple choice tests,
perhaps the ETS would have set up a more distributed testing system,
based on certified test graders.  Perhaps a wider range of question
types would be used, or perhaps oral examinations would have become
part of the test.  This question is moot, and the answer would not be
of pertinence to this digest.  The pertinent question does remain:

    * Does the ability of computers to process masses of social
      data encourage poor, centralized, solutions to social programs
      when distributed (non-computer) solutions would help society more?

- Geof Cooper, IMAGEN
</PRE>
<A NAME="subj11"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj11.1">
Call for papers - Directions and Implications of Advanced Computing</A></H3>
<ADDRESS>Jon Jacky&lt;<A HREF="mailto:jon@june.cs.washington.edu">jon@june.cs.washington.edu</A>&gt;</ADDRESS>
<I>Fri, 12 Dec 86 08:40:51 PST</I><PRE>
         (CPSR-sponsored symposium in Seattle, July 12 1987)

                               Call for Papers

              DIRECTIONS AND IMPLICATIONS OF ADVANCED COMPUTING

		    Seattle, Washington   July 12, 1987

The adoption of current computing technology, and of technologies that
seem likely to emerge in the near future, will have a significant impact
on the military, on financial affairs, on privacy and civil liberty, on
the medical and educational professions, and on commerce and business.

The aim of the symposium is to consider these influences in a social and
political context as well as a technical one.  The social implications of
current computing technology, particularly in artificial intelligence, are
such that attempts to separate science and policy are unrealistic.  We
therefore solicit papers that directly address the wide range of ethical
and moral questions that lie at the junction of science and policy.

Within this broad context, we request papers that address the following
particular topics.  The scope of the topics includes, but is not limited
to, the sub-topics listed.

RESEARCH FUNDING		   DEFENSE APPLICATIONS

 - Sources of Research Funding	    - Machine Autonomy and the Conduct of War
 - Effects of Research Funding      - Practical Limits to the Automation of War
 - Funding Alternatives             - Can An Automated Defense System Make War
				      Obsolete?

COMPUTING IN A DEMOCRATIC SOCIETY  COMPUTERS IN THE PUBLIC INTEREST

    - Community Access              - Computing Access for Handicapped People
    - Computerized Voting           - Resource Modeling
    - Civil Liberties               - Arbitration and Conflict Resolution
    - Computing and the Future      - Educational, Medical and Legal Software
      of Work
    - Risks of the New Technology


Submissions will be read by members of the program committee, with the
assistance of outside referees.  Tentative program committee includes
Andrew Black (U. WA), Alan Borning (U. WA), Jonathan Jacky (U. WA),
Nancy Leveson (UCI), Abbe Mowshowitz (CCNY), Herb Simon (CMU) and
Terry Winograd (Stanford).

Complete papers, not exceeding 6000 words, should include an abstract,
and a heading indicating to which topic it relates.  Papers related to
AI and/or in-progress work will be favored.  Submissions will be judged
on clarity, insight, significance, and originality.  Papers (3 copies)
are due by April 1, 1987.  Notices of acceptance or rejection will be
mailed by May 1, 1987.  Camera ready copy will be due by June 1, 1987.

Proceedings will be distributed at the Symposium, and will be on sale
during the 1987 AAAI conference.

For further information contact Jonathan Jacky (206-548-4117) or Doug
Schuler (206-783-0145).  Sponsored by Computer Professionals for Social
Responsibility, P.O. Box 85481, Seattle, WA  98105
</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.27.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.29.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-106</DOCNO>
<DOCOLDNO>IA012-000125-B045-282</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.29.html 128.240.150.127 19970217010503 text/html 14344
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:03:29 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 29</TITLE>
<LINK REL="Prev" HREF="/Risks/4.28.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.30.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.28.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.30.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 29</H1>
<H2> Sunday, 14 December 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
America's Cup: Left-over Digital Filter 
</A>
<DD>
<A HREF="#subj1.1">
Bruce Wampler
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Some additions to the "bug" taxonomy 
</A>
<DD>
<A HREF="#subj2.1">
Dick King
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: uninterruptible power 
</A>
<DD>
<A HREF="#subj3.1">
Ted Lee
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Trade-offs between BMD architecture and software tractability 
</A>
<DD>
<A HREF="#subj4.1">
Herb Lin
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Re: Criminal encryption 
</A>
<DD>
<A HREF="#subj5.1">
Garry Wiegand
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Computerised Discrimination 
</A>
<DD>
<A HREF="#subj6.1">
Scott Preece
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  More on Incompatible Plug-Compatible Monitors 
</A>
<DD>
<A HREF="#subj7.1">
Al Stangenberger
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
America's Cup: Left-over Digital Filter
</A>
</H3>
<address>
&lt;<A HREF="mailto:ames!rutgers!seismo!unmvax.unm.edu!wampler@cad.Berkeley.EDU">
ames!rutgers!seismo!unmvax.unm.edu!wampler@cad.Berkeley.EDU
</A>&gt;
</address>
<i>
Fri, 12 Dec 86 09:07:06 MST
</i><PRE>

This story is from the NOVA "Sail Wars" of 9 Dec 1986:

This NOVA was about the design of Stars &amp; Stripes, one of our entries in the
current America's Cup event in Australia.  There were two interesting
stories, both having to do with modelling and tank testing of scale models.

Apparently in the early 70's, Ted Turner had a boat built directly from a
tank model.  The boat worked wonderfully in the tank, but was a total dog in
full size.  This design disaster soured American designers on tank
modelling, ultimately resulting in the loss of the America's Cup 3 years ago
to the Australian boat, which had been designed using modelling.  In the
70's, the models were apparently on a 1:13 scale.

The current entry was designed using tank modelling (1:3 Scale).  Stars &amp;
Stripes went through 3 versions.  Much of the design was aided by computer
modelling, followed by building of scale models for tank testing.  The tank
testing was closely measured, and the results again fed through
computer-analysis programs.  The design was getting down to the wire for the
3rd version of the boat.  Measurements fed through the analysis programs
indicated a serious problem with the stern of the boat.  The designers were
visibly depressed.  After some modifications, new measurements indicated the
problem got worse.  At this point, they really were out of time - either
give up the 3rd version, or find the problem.

In a sort of "sanity test", the designers refused to believe the computer
output.  This was apparently standard naval architecture software and well
trusted, given the reluctance shown to disbelieve the results.  At any rate,
after a long all-night session, they discovered that "a digital filter used
previously for an oil platform test had inadvertly been left in the computer,"
thus causing the wrong results.  With the filter removed, the measurements
showed better than expected performance. (Not good enough, apparently.  The
yacht New Zealand seems to be cleaning up in the challenger races.)

                      [Moral: Don't forget to change the oil filter.  PGN]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Some additions to the "bug" taxonomy
</A>
</H3>
<address>
Dick King
&lt;<A HREF="mailto:king@kestrel.ARPA ">
king@kestrel.ARPA 
</A>&gt;
</address>
<i>
Sat, 13 Dec 86 11:35:12 pst
</i><PRE>

"mugs"  -- Trojan horses and other intentionally introduced anomalies

"plugs" -- interface errors

"ugs"   -- a bug isolated to a small piece of code, the sort of thing you can
           stare at for hours, and all of a sudden someone walks up to ask you
           if you want to go to lunch, glances at your work, points to the
           offending line of your CRT or listing, and says "you know, ..."
                                                                       [ughs?]

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
 Re: uninterruptible power
</A>
</H3>
<address>
&lt;<A HREF="mailto: TMPLee@DOCKMASTER.ARPA">
 TMPLee@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Sat, 13 Dec 86 00:41 EST
</i><PRE>
To:  risks@CSL.SRI.COM

And in the case of a large installation the back-up power is most impressive. 
I had a chance to visit Air France's computer center (somewhere near the
Riveria) several years ago (pure boondoggle, I admit.)  As I recall there were
about three floors (basketball court size, maybe) of Univac 11xx's and disk
farms (two approximately duplicate systems, each at least two processors) and
comm gear etc.  On the ground floor were at least two, maybe three diesel
generators that would do a small city proud.  Short of a nuclear attack that
system was not going to be shut down by anything! (and yes, they made sure the
fuel tanks were full and periodically tested the generators -- I don't remember
the mechanism used to keep power up while the generators were starting.)

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Trade-offs between BMD architecture and software tractability
</A>
</H3>
<address>
&lt;<A HREF="mailto:LIN@XX.LCS.MIT.EDU">
LIN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Sun, 14 Dec 1986  11:48 EST
</i><PRE>

It has been generally accepted that software for BMD must perform a variety
of functions, including tracking targets, discriminating between decoys and
RVs, and so on.  As importantly, the software must be constructed in such a
way that all the parties are confident that it will perform these functions
when called upon to do so.

This list of functions raises an interesting point.  I agree with the list, but
am troubled by its dependence on system architecture.  Specifically, we could 
imagine a "BMD" system that consisted of thick orbiting shells of gravel at 500
km altitude.  No ballistic missile now known could penetrate that, and we could
have confidence that it would work.  The software would not need to perform
any of the functions that both critics and supporters of SDI agree must be
performed.  The sole issue is the cost of putting all that junk in space.

The existence of this "alternative" BMD suggests that the "software" needed
to control it need not be complex, extensive or unreliable; the system just
proposed doesn't need it at all.  However, no one thinks that an actual BMD
will not require software.  Thus, we conclude that for deviations that are
"large enough" from "prototypical" architectures, the software problem can
be made tractable.  An interesting question arises:  How can we develop more
precise measures for the phrase "large enough deviations" and the word
"prototypical"?

The Eastport Study used such an approach; they said that an unconventional
architecture would make the software problem tractable.  The argument above
suggests that for a sufficiently unconventional architecture, they are
right.  My problem with the Eastport study is that they have not made an
argument that their preferred architecture is even in the right direction of
"unconventionality", let alone "far enough"; indeed, I think they have gone
in the wrong direction.  But my problem with my own position on BMD software
(i.e., very critical) is that I have constructed an existence proof that
says that in some circumstances, I am wrong.

What are those circumstances?  I can't speak in general, but obviously
one issue is cost.  If you are willing to spend enough money (in the
case above, on lift costs), the software problem is tractable.  My
intellectual question is "Where do I draw the line?" 

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Re: Criminal encryption
</A>
</H3>
<address>
Garry Wiegand
&lt;<A HREF="mailto:garry@tcgould.tn.cornell.edu ">
garry@tcgould.tn.cornell.edu 
</A>&gt;
</address>
<i>
Fri, 12 Dec 86 23:43:18 EST
</i><PRE>

I noticed in the paper recently that the former mayor of Syracuse (Lee
Alexander??) was fighting a federal court order. The court, on prosecution 
request, had ordered him to instruct a foreign bank to tell the prosecution 
all about his bank transactions. The paper said that the ability of the feds 
to require this was a matter of "settled law"; Mr. Alexander was merely 
fighting for the privilege of adding the words "under protest" before signing. 

Seems like the same rules might apply to other forms of records, such as 
computer disks. The penalty would be contempt-of-court.

garry wiegand   (garry%cadif-oak@cu-arpa.cs.cornell.edu)
Cornell Engineering &amp; Flying Moose Graphics

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Computerised Discrimination
</A>
</H3>
<address>
"Scott E. Preece" 
&lt;<A HREF="mailto:preece%mycroft@GSWD-VMS.ARPA">
preece%mycroft@GSWD-VMS.ARPA
</A>&gt;
</address>
<i>
Fri, 12 Dec 86 09:38:09 CST
</i><PRE>

Brian Randell writes:
&gt;   The St. George's claim is particularly worrying because the school has a 
&gt; better record on discrimination than most other colleges.
&gt;   The computer selection programme was designed to mimic the decisions of
&gt; the school's panel which screened applicants to see who merited an interview.
&gt;   It matched the panel's results so closely that the panel was scrapped and 
&gt; for several years all St. George's applicants have been screened by computer.

One is tempted to say that the two statements, (1) they were better than
average on discrimination and (2) they were following a process that was well
modelled by a discriminatory program, are contradictory.  Of course, they
aren't.  Assuming the program was just based on assigning weights to a lot of
factors typically used in admissions decisions, it's not hard to imagine that 
they hit on a set of weights which happened to work well on the training set 
but were not really reflective of the pre-existing judgment process.

This is dangerous, though, in that it may appear to courts and other bodies
that the inference can be drawn; that the existence of a biased model which
would explain a behavior is proof that the behavior was biased.  This would
make the concept of de facto discrimination much more broadly applicable
(though it is, in fact, the general basis of that concept).

It does remind one that testing the results of an "expert" system should be
coupled with review of its rules.

scott preece, gould/csd - urbana          uucp: ihnp4!uiucdcs!ccvaxa!preece

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
More on Incompatible Plug-Compatible Monitors
</A>
</H3>
<address>
&lt;<A HREF="mailto:forags%violet.Berkeley.EDU@berkeley.edu">
forags%violet.Berkeley.EDU@berkeley.edu
</A>&gt;
</address>
<i>
Sun, 14 Dec 86 15:21:47 PST
</i><PRE>

It's quite easy to damage an IBM Monochrome monitor by plugging it into an
adapter (like an Enhanced Graphics Adapter) which is configured for a color
monitor.  Both types of monitors use the same D-connector.

Admittedly, there is a warning in the manual about this, but, after setting
up about fifteen other PC's, I had pretty much given up reading the manual
in detail .....

Al Stangenberger, Forestry, Univ. of Calif., Berkeley

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.28.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.30.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-107</DOCNO>
<DOCOLDNO>IA012-000125-B045-301</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.30.html 128.240.150.127 19970217010525 text/html 18186
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:03:54 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 30</TITLE>
<LINK REL="Prev" HREF="/Risks/4.29.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.31.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.29.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.31.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 30</H1>
<H2>Tuesday, 16 December 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Arpanet outage 
</A>
<DD>
<A HREF="#subj1.1">
Andrew Malis
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Dynamic Signature Verification 
</A>
<DD>
<A HREF="#subj2.1">
Robert Stroud [and Brian Randell]
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Wobbly skyscrapers and passive vs. active controls 
</A>
<DD>
<A HREF="#subj3.1">
Niall Mansfield
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: The Audi 5000 problems 
</A>
<DD>
<A HREF="#subj4.1">
Matt Smiley
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Modifying bank cards 
</A>
<DD>
<A HREF="#subj5.1">
Rodney Hoffman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Credit card mag strips 
</A>
<DD>
<A HREF="#subj6.1">
Ted Marshall
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Fast-Food Computing 
</A>
<DD>
<A HREF="#subj7.1">
Edward Vielmetti
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  "bugs" 
</A>
<DD>
<A HREF="#subj8.1">
Doug McIlroy
</A><br>
<A HREF="#subj8.2">
 Jonathan Clark
</A><br>
<A HREF="#subj8.3">
 Bob Estell
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Arpanet outage
</A>
</H3>
<address>
Andrew Malis 
&lt;<A HREF="mailto:malis@ccs.bbn.com">
malis@ccs.bbn.com
</A>&gt;
</address>
<i>
Mon, 15 Dec 86 10:46:48 EST
</i><PRE>
To: hedrick@topaz.rutgers.edu
Cc: tcp-ip@sri-nic.arpa, malis@ccs.bbn.com

[An earlier message asked, "Why did the Northeast corridor disappear from
the Arpanet last weekend? The Network Operations Center said one trunk had
been broken, and they were cut off from most everyone, too. I thought there
was enough redundancy in the Arpanet to prevent a single trunk from causing
such extensive outage...":]

At 1:11 AM EST on Friday, AT&amp;T suffered a fiber optics cable break between
Newark NJ and White Plains NY.  They happen to have routed seven [different]
ARPANET trunks [all] through that one fiber optics cable.  When the cable
was cut, all seven trunks were lost, and the PSNs in the northeast were cut
off from the rest of the network.  Service was restored by AT&amp;T at 12:12.

The MILNET also suffered some trunk outages, but has more redundancy, so it
was not partitioned.
                                           Andy Malis

   [Robert W. Baldwin &lt;BALDWIN@XX.LCS.MIT.EDU&gt; noted:  This is a classic
   example of redundancy at one level of abstraction that turns out to be
   non-redundant at a lower level of abstraction.]
      [Redundancy works sometimes: I received several copies of Andy's
      note.  Yes, this is a lovely example.  By the way, AT&amp;T is laying a
      fiber-optic cable under the Atlantic.  That will provide LOTS of
      opportunities for virtually distinct paths to co-occupy the same
      physical channels.  PGN]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Dynamic Signature Verification
</A>
</H3>
<address>
Robert Stroud 
&lt;<A HREF="mailto:robert%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK">
robert%cheviot.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Tue, 16 Dec 86 12:15:20 GMT
</i><PRE>

There was an article in The Independent recently (2nd Dec 1986) about
dynamic signature verification and "the arrival of biometrics as a practical
security technology". A company called AI Transaction Security from
Cambridge have produced a gadget called Securisign, two of which are being
used to control access to "a very secure area" at the EEC's headquarters.
[EEC is European parliament]

The article concluded as follows:

  "Dynamic signature verification has turned up one disappointment.
  Researchers originally hoped that signature pads could test the sobriety of
  people such as nuclear plant operators when they signed on for a shift.
  However, research shows that most people can sign their names convincingly
  even when hopelessly drunk".  [Copyright (c) 1986 Newspaper Publishing PLC]

I found this last comment interesting because the last time this topic came
up on RISKS, I recall that the consensus was that the technology did not
work because you had to sign your name very carefully, i.e. not when you
were "tired and emotional". However, when I showed the article to Brian
Randell, he told me the following anecdote:

     Some years ago, I was involved (in an official capacity) in reviewing a
  research project, at a Laboratory which I would prefer not to identify, on
  dynamic signature verification. I was given a demonstration of the system,
  which involved my being asked to sign my name five times, and then being
  asked to sign again to confirm that the system had now "learnt" and could
  recognise my signature. Much to the consternation of the demonstrator, my
  entirely unpremeditated reaction was to turn to a colleague, and ask him to
  sign my name. Without any prior warning or practice, he roughly imitated
  what he could recall of my hand movements, without attempting to reproduce
  the written appearance of my signature. The machine accepted his efforts as
  my signature.  I was then informed, in tones of considerable embarassment,
  that in an effort to speed up the demonstration, the thresholds had been set
  low, and that all would be well if they were reset and I gave an adequate
  number of signatures.  So, they were reset, and I gave (more than) the
  requested numbers of signatures.  To my surprise, the demonstrator expressed
  surprise when I indicated that I felt it appropriate to repeat my
  experiment, and again challenged my colleague to repeat his "feat" -
  something he did immediately and effortlessly!

     The point of this story is that this struck me as an elementary check to
  make on dynamic signature verification systems - yet I do not recall ever
  seeing claims, in any of the (admittedly popular) articles I have read on the
  topic, regarding the ability of the system to defeat attacks based on seeing
  how a person signed his/her name.   [End of Brian's story]

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Wobbly skyscrapers and passive vs. active controls
</A>
</H3>
<address>
Niall Mansfield   
&lt;<A HREF="mailto:MANSFIEL%EMBL.BITNET@WISCVM.WISC.EDU">
MANSFIEL%EMBL.BITNET@WISCVM.WISC.EDU
</A>&gt;
</address>
<i>
Mon 15 Dec 86 17:28:20 N
</i><PRE>

PGN in <A HREF="/Risks/4.26.html">RISKS-4.26</A>
&gt; This raises interesting questions about the relative precision, accuracy, 
&gt; and soundness of "metal algorithms" and comparable analog devices in general.

If you change the scene a bit and take a mildly absurd example, you could
have the same sort of considerations in a desk lamp - either use a normal
passive Anglepoise type, or a hi-tech computer-controlled active
servo-postioned type lamp.  I'd reckon that the old fashioned lamp would
behave itself it power cuts (although not very brightly), electrical storms,
glitchy mains periods, the last day of february of the year 2000, etc.,
whereas I wouldn't be at all surprised if the robot lamp went berserk
sometime and brained me or smashed my teeth in because the chap next door
started radio broadcasting.

For whatever reason - perhaps that we have had such or similar artifacts for
centuries - we are confident and "know" that passive devices made of metal
tubes and weights and springs are not sensitive to various outside effects
which DO affect computers and consequently computer controlled devices, and
if only because they behave resonably, (i.e. as we expect them to) such
passive devices have a great safety advantage.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: The Audi 5000 problems
</A>
</H3>
<address>
Matt Smiley
&lt;<A HREF="mailto:crash!pnet01!msmiley@nosc.ARPA ">
crash!pnet01!msmiley@nosc.ARPA 
</A>&gt;
</address>
<i>
Tue, 16 Dec 86 00:52:37 PST
</i><PRE>

Audi did more damage with the '...there isn't anything wrong.' statement than
could be done by simply saying they don't know what it is. Statistically, the
rate of such accidents with the Audi should be proportional to the rate of
such accidents with other vehicles. It obviously is not, leading me to think
there's some defect in the engineering of the vehicle. I had a similar problem
with an old Ford truck of mine, and it took months for me to figure out that
it was due to a defective motor mount. The torque of the engine would lift it
off the mount and subsequently pull the accelerator linkage to the floor. A
similar oddity could be plaguing the Audis.

...nosc!crash!pnet01!msmiley@NOSC &lt;Matt Smiley&gt;

    [The summary list of RISKS-4.1 notes that an Audi investigation was 
     reported earlier in Software Engineering Notes, but I just noticed that
     the reference was wrong: it should have been SEN 11 2 (April 1986). PGN]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Modifying bank cards
</A>
</H3>
<address>
Hoffman.es@Xerox.COM  
&lt;<A HREF="mailto:Rodney Hoffman">
Rodney Hoffman
</A>&gt;
</address>
<i>
16 Dec 86 08:11:51 PST (Tuesday)
</i><PRE>
To: RISKS@CSL.SRI.COM

From the Los Angeles Times, Dec. 15, 1986 (Reuters):

	COMPUTER 'HACKERS' HELD IN W. GERMANY 

WIESBADEN, West Germany -- Police have arrested four computer "hackers" said
to have robbed banks in the Frankfurt area of more than $50,000 by
manipulating cash dispenser cards with a home computer.  Hesse State police
said the four, one woman and three men, had been roaming Frankfurt and
surrounding towns since May with a computer plugged into the battery of
their Mercedes limousine.  They were arrested at the end of November.

The four hackers bought bank cash cards for $1,500 apiece from their family
and friends, who then notified their bank that the cards had been stolen.
The four then used their computer to change the codes on the cards' magnetic
strip so that they could withdraw more money than the limit set by the cards
from automatic tellers, or to tap other accounts.  Under a law on computer
crime passed last August, the four face jail terms of up to five years if
charged and found guilty.

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Credit card mag strips
</A>
</H3>
<address>
Ted Marshall
&lt;<A HREF="mailto:blia.UUCP!ted@cgl.ucsf.edu ">
blia.UUCP!ted@cgl.ucsf.edu 
</A>&gt;
</address>
<i>
Mon, 15 Dec 86 11:45:59 PST
</i><PRE>
To: risks@sri-csl.ARPA

I have noticed a new trend in the way stores imprint credit card slips.
In the olden days, the embossed numbers and letters on the card were
mechanically transfered to the slip. The only use of the magnetic strip on
the back was for verification of the credit limit.

I have now seen two stores (including the local Radio Shack) where the
mag strip reader feeds data to an electronic cash register which not only
dials-up the bank to verify credit but also prints out the slip for the
customer to sign. Unless the clerk checks the printed information on the
slip against the embossed card, there is no verification of the information.

Credit card companies are making it harder to counterfit the embossed
information on the cards. But a hardware hack can still build a gizmo for
$20 that will copy the magnetic information from a "borrowed" card to his.
He then makes sure the other card gets returned so that the bank isn't
notified. The hack walks into the Radio Shack, buys $1000 worth of stuff
with "his" card, and it gets charged to his friend's account. The only
thing to trace him with is the signature on the slip, and it's easy to
sign your name so that it's close enough for the clerk but no one will
ever trace it to you.

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Fast-Food Computing
</A>
</H3>
<address>
&lt;<A HREF="mailto:Edward_Vielmetti@um.cc.umich.edu">
Edward_Vielmetti@um.cc.umich.edu
</A>&gt;
</address>
<i>
Tue, 16 Dec 86 16:15:04 EST
</i><PRE>
ReSent-To: RISK@CSL.SRI.COM

I must have been in the cycle early for McDonald's fast-food intelligent
man-machine systems, according to Guthery's law:
   &gt;     In an evolving man-machine system, the man will get
   &gt;     dumber faster than the machine gets smarter.

McDonald's fast food computers (i.e., cash registers) collect all sorts of
data on the individual employee at the counter and on all counter sales as a
whole.  They also do not have a &lt;no sale&gt; key that opens up the cash
register, probably to prevent theft.  That made it real hard to fix a
mistake without calling a manager to get a key to open the drawer.

Solution?  Well, the people I worked with at McD's had been around the
system long enough to figure out how to get around it.  Without getting into
too many details of why things were as they were, the easiest way to open
the drawer without a manager was to ring up a sale that gave away a tub of
barbecue sauce for McNuggets and nothing else.  
   (Hit &lt;promo&gt; &lt;barbecue&gt; &lt;promo&gt; &lt;total&gt; .)
Of course, that messed up the daily statistics some.
 
Edward Vielmetti, Ex-McDonalds employee, Computing Center Microgroup, U. Mich.

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
"bugs" 
</A>
</H3>
<address>
&lt;<A HREF="mailto:doug%btl.csnet@relay.cs.NET">
doug%btl.csnet@relay.cs.NET
</A>&gt;
</address>
<i>

</i><PRE>
Date: Sun 14 Dec EST 1986 21:39

plugs   Unwanted trash that contaminates output.  The classic example is a 
        cheery advertising blurb like "Welcome to MUCUP Version 2.7," 
        which cripples the next program	down the pipe.

drugs	Unwanted features that contaminate specs; something the cat drug in.

</PRE>
<HR><H3><A NAME="subj8.2">
"bugs" 
</A>
</H3>
<address>
Jonathan Clark
&lt;<A HREF="mailto:jhc%mtune.UUCP@harvard.HARVARD.EDU ">
jhc%mtune.UUCP@harvard.HARVARD.EDU 
</A>&gt;
</address>
<i>
Mon, 15 Dec 86 11:45:51 EST
</i><PRE>

At a recent course I heard Jim Gray of Tandem (seriously) describe two
more bug types:

Heisenbugs: generally transient failure conditions that exist inside
systems.  ('I can't let you have this resource now because it has been
locked'.)  Typically, when the operation is retried on another processor, it
succeeds because the backup processor is in a different internal state.

Bohrbugs: repeatable failures even when retried on another processor.
Typically these are 'hard errors'.

</PRE>
<HR><H3><A NAME="subj8.3">
"bugs"
</A>
</H3>
<address>
"ESTELL ROBERT G" 
&lt;<A HREF="mailto:estell@nwc-143b.ARPA">
estell@nwc-143b.ARPA
</A>&gt;
</address>
<i>
16 Dec 86 10:05:00 PST
</i><PRE>
To: "risks" &lt;risks@csl.sri.com&gt;

"augs" - induced while augmenting a system.
"dugs" - added while fixing other bugs, digging the hole deeper.
"jugs" - portable bugs, bottled and bonded.
"lugs" - which slow down the system [e.g., security features].
"nugs" - little "nuggets" of gold, which didn't pan out.
"qugs" - errors in queues that make batch jobs miss deadlines,
         and print files twice, or not at all.
"rugs" - evenly distributed throughout the code, and pervasive.
"tugs" - little interfaces which keep big systems in tow.
"xugs" - alien bugs [like E-Mail penetrations of UNIX systems]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.29.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.31.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-108</DOCNO>
<DOCOLDNO>IA012-000125-B045-325</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.31.html 128.240.150.127 19970217010546 text/html 20355
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:04:10 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 31</TITLE>
<LINK REL="Prev" HREF="/Risks/4.30.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.32.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.30.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.32.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 31</H1>
<H2>Wednesday, 17 December 1986</H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Don't sit too close!  ("And Now, Exploding Computers") 
</A>
<DD>
<A HREF="#subj1.1">
Jerry Leichter
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Car-stress syndrome 
</A>
<DD>
<A HREF="#subj2.1">
Robert D. Houk
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Korean Air Lines Flight 007 
</A>
<DD>
<A HREF="#subj3.1">
Niall Mansfield
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Heisenbugs 
</A>
<DD>
<A HREF="#subj4.1">
Rob Austein [an example]
</A><br>
<A HREF="#subj4.2">
 Doug Landauer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Criminal Encryption 
</A>
<DD>
<A HREF="#subj5.1">
Bill Gunshannon [counterexample?]
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Taking the "con" out of econometrics... correction and a plea 
</A>
<DD>
<A HREF="#subj6.1">
Mike Williams
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
 Don't sit too close!  ("And Now, Exploding Computers")
</A>
</H3>
<address>

&lt;<A HREF="mailto:LEICHTER-JERRY@YALE.ARPA">
LEICHTER-JERRY@YALE.ARPA
</A>&gt;
</address>
<i>
17 DEC 1986 16:48:51 EST
</i><PRE>

From the New York Times (17-Dec-86):

	And Now, Exploding Computers

...[T]wo owners of Compaq Portable II computers were rudely surprised recently
when their machines simply blew up.  The problem, said Jeff Stives, a spokes-
man for the Houston-based company, arose when service technicians improperly
rewired the battery circuits on the computers' main circuit boards.

Compaq engineers managed to blow up another computer in the tests, thus con-
firming the problem.

In each case the explosion, caused when the machine's lithium battery is acci-
dentally drained of energy and then re-energized by the computer's 5-volt
power supply, was strong enough to break the case of the computer but not
potent enough to shatter the glass of the built-in video display screen.  No
injuries were reported.

Owners of Compaq Portable II computers that have had repair work done on the
system board are advised to call Compaq at (800) 847-5785, or call their local
dealers for free inspections.
							-- Jerry
    
</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Car-stress syndrome
</A>
</H3>
<address>
Robert D. Houk 
&lt;<A HREF="mailto:Houk@RIVERSIDE.SCRC.Symbolics.COM">
Houk@RIVERSIDE.SCRC.Symbolics.COM
</A>&gt;
</address>
<i>
Wed, 17 Dec 86 18:42 EST
</i><PRE>
To: RISKS@CSL.SRI.COM

From "car" magazine (FF Publishing, 97 Earls Court Road, London W8 6QH),
December 1986 issue, "ORACLE" column, page 72

  Electronics are quickly becoming the star of the high-tech society. But they
  are not without problems. The electromagnetic waves generated by electronic
  equipment are causing concern among health professionals. Cars are no
  exception and Professor Kazuo Suenaga of Kurume University has for the first
  time found scientific proof that electromagnetic waves generated by a car's
  engine are the cause of car-stress syndrome. According to his findings, such
  waves from the spark plugs cause nervous stress in the driver and reduces
  alertness. A device called the Neutral Auto which oscillates
  micro-electronic waves prevents hazardous electromagnetic waves from
  entering the passenger compartment, and is said to be effective in
  preventing motion sickness.

Passed along, with my personal comments pre-censored out.  (Actually, the
"preventing motion sickness" is conceivable, but the rest sounds rather
flaky to me, even allowing for the difference in language 'tween England and
the USA.)  It would be interesting to see the original paper/report
(unfortunately not cited in the column) - maybe someone else out there is
familiar with the un-aforementioned paper or Professor Suenaga/Kurume
University???

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Korean Air Lines Flight 007
</A>
</H3>
<address>
Niall Mansfield 
&lt;<A HREF="mailto:MANSFIEL%EMBL.BITNET@WISCVM.WISC.EDU">
MANSFIEL%EMBL.BITNET@WISCVM.WISC.EDU
</A>&gt;
</address>
<i>
Wed 17 Dec 86 10:44:43 N
</i><PRE>

In <A HREF="/Risks/4.26.html">RISKS-4.26</A> Steve Jong, basing his discussion on Seymour
Hersh's  "The Target is Destroyed" (1986), said:-

&gt; [it was] concluded that a combination of human errors caused the 
&gt; navigational snafu.  One of the errors was postulated to be a well-known 
&gt; blind faith in the plane's inertial navigation system (INS).  
&gt; 
&gt; ... the gist of it that a crew member fat-fingered the "you are here" 
&gt; coordinates.
&gt;    ... if the KAL crew looked at their radar and saw the Kamchatka 
&gt;    Peninsula where there should have been open ocean, they probably 
&gt;    shut off the radar, because the INS was functioning normally.

Even though Peter Neumann did note that other books have different views on
what happened, I think one of the other possible explanations, which
exonerates the computers, should still be mentioned.

Very much in contradiction of the quoted arguments above, R.W.Johnson in
"Shootdown - the verdict on KAL 007" contends that the plane did not have
any INS trouble. Rather, the crew filed flight plans at Anchorage which
showed pencilled-in modifications to the computerised flight plan, and that
007's actual course agreed with this modified plan.  (Johnson reproduces
copies of the plans, which apparently were included in the International
Civil Aviation Organisation's report).

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Heisenbugs 
</A>
</H3>
<address>
Rob Austein 
&lt;<A HREF="mailto:SRA@XX.LCS.MIT.EDU">
SRA@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Wed, 17 Dec 1986  02:03 EST
</i><PRE>

The recent discussion on Bug Taxonomy reminded me of this one.  I may
have mangled some of the incidental details, but the gist is gospel.

We have this ITS machine called MC.  Its purpose in life is to provide
a place to put the big mailing lists for the MIT CS labs so that the
other lab machines aren't driven into the ground by the load the
mailer puts on the processor.  So we tend not to use MC for much else,
and COMSAT (the mailer) usually has the machine to itself except when
the maintainers are changing something.

Enter a curious hacker who wants to know why MC has not processed any
mail for the last 36 hours (this was a holiday weekend, or somebody
would have noticed it much sooner!).  He pokes around the mail queue
directory, checks to see if the filesytem is full, the net is hung,
any of the normal things.  Finds nothing odd.  Finally he examines the
COMSAT job with the PEEK program (like TOPS-20 SYSDPY unix ps).  Lo
and behold, the COMSAT job is now running, the mail queue is being
processed, and except for the gap between timestamps in the telemetry
file there is no evidence that this ever happened.

After much head scratching amongst the COMSAT and ITS maintainers, we
figured out what had (probably) happened.  It seems that COMSAT was
stuck in a system call, probably doing some network I/O; there was a
bug in the code for that system call which caused it to hang forever
instead of returning some kind of failure condition.  Certain
operations involved in examining another job (with PEEK or any other
program) cause the examinee to experience a context switch if it is in
the middle of a system call: the program counter gets set back to user
context, the user context page map and registers are restored, and so
forth.  This kind of involuntary context switch is a normal event on
ITS, and great pains are taken to make it invisible to the user code.
Among other things the program counter and any memory locations that
are modified by the system call are updated so that the interrupt is
transparent and the job can proceed as if nothing had happened.

So the act of looking at COMSAT broke COMSAT out of the losing system
call, and when it restarted the system call it exited properly with an
error condition (not surprising, since the machine on the other end of
the network connnection presumably had hung up the phone 35 hours and
55 minutes ago).

Did I hear somebody mention the Uncertainty Principle?

--Rob

</PRE>
<HR><H3><A NAME="subj4.2">
Heisenbugs
</A>
</H3>
<address>
Doug Landauer
&lt;<A HREF="mailto:landauer@Sun.COM ">
landauer@Sun.COM 
</A>&gt;
</address>
<i>
Wed, 17 Dec 86 12:19:20 PST
</i><PRE>

In the rest of the world (most of us don't get to retry our operations on
backup processors), Heisenbugs is already a fairly common term -- it refers
to bugs which go away as soon as you try to run them under a debugger (or
with the debugging compile- or run-time flags set).

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Criminal Encryption
</A>
</H3>
<address>
Bill Gunshannon
&lt;<A HREF="mailto:bill@westpt.UUCP ">
bill@westpt.UUCP 
</A>&gt;
</address>
<i>

</i><PRE>
Date: 17 Dec 86 13:44:24 GMT

In his Item 2 (in RISKS 4.26) David Fetrow mentions an incident from a few
years ago about a man arrested for kid-porn and the hacker who "broke" his
encrypted file for the courts. I think it is time that we finally laid to
rest the notion of all these 12 year old hackers out there who are more
powerful than a Cray XMP. The article also was printed in TIME magazine and
even rated nearly a full page with a photograph of the hacker as well.  The
fact of the matter is nothing on the disk was encrypted and what the hacker
did was public information and being done by micro-computer users all over
the country. An explanation follows.

The file the court was interested in was not encrypted, it was password
protected and as you might expect the defendant was not likely to freely
give them the password. At this point for reasons I can't even imagine
they brought the hacker in to the case. 

For more background information the computer was a Radio Shack Model III.

Here is an example of a dump of a directory of a disk I created for this
demonstration:

            file      file name &amp; extension
location  attributes        in ascii
on disk      \/                \/
  \/   |            |                         |
110B00: 0000 0000 0000 0000 0000 0000 0000 0000 ................
110B10: 0000 0000 0000 0000 0000 0000 0000 0000 ................
       |    |    |                             |
          ^    ^                ^
          |    |                |
          |    |       Hash Index Table(HIT)
          |    |
          |    User Password
          |
          Owner Password

110B00: 1000 0000 0046 494C 4532 2020 2044 4154 .....FILE2   DAT
110B10: E042 E042 0000 FFFF FFFF FFFF FFFF FFFF .B.B............

110240: 1000 0000 0046 494C 4531 2020 2044 4154 .....FILE1   DAT
110250: 9642 9642 0000 FFFF FFFF FFFF FFFF FFFF .B.B............

There are two passwords for each file, a "owner" password and a "user"
password.

The file named "FILE1   DAT" is not password protected.
The file named "FILE2   DAT" is password protected.

A quick look at the directory entry for each file shows you the location 
of the passwords in the entry. The passwords are not really encrypted.
They are merely hashed. This allows an 8 character password to be stored
in 2 bytes(1 word on this machine). It also means that any given 8 letter
combination will always hash to the same value. The entry for no password
is 8 spaces(ASCII 32). All that means is by changing the entry for the
"owner" and "user" passwords on "FILE2   DAT" to the same thing as you see
for "FILE1   DAT" you have effectively removed the passwords.
This information was provided in numerous magazines like "80 Micro" and
"Kilobaud" which had wide readership in the early days of microcomputers.
The reason the information was provided was because companies like Tandy 
and Microsoft distributed their software on single sided disks which was
what a store bought Radio Shack computer had in it. But most people(read
hackers) who used their machines seriously had modified them to use 80
track and double sided disks. Because of passwords that were not published
it was impossible to just copy such as Microsofts Fortran Compiler onto
another disk. With the release of this information all one had to do was
remove the passwords and copy the files to any media desired.

As you can see there is nothing spectacular about what was done. It was done
a regular basis in homes all across the country. But what I see as a problem
and why I think this information is applicable to RISKS is that it got so
much coverage in the press and served to take a large group of the public 
who are already uncomfortable or afraid of computers and their effect on day
to day life and fed the fires. Here we are 2 years later and this story is
still showing up and what is worse is that it will become more fantastic
in time as the facts become less and less known. There was no mention of
encryption in the TIME article. But as you can see with encryption being
on everyones mind today the story has gone from "boy breaks password" to 
"hacker breaks encryption". 

bill gunshannon

UUCP:      philabs!westpt!bill               PHONE:     (914)446-7747
US SNAIL:  Martin Marietta Data Systems      RADIO:     KB3YV
           USMA, Thayer Hall                 AX.25      KB3YV @WA2RKN-2
           West Point, NY  10996

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
 Taking the "con" out of econometrics and computer modeling: 
</A>
</H3>
<address>
 "John Michael (Mike) Williams" 
&lt;<A HREF="mailto:JWilliams@DOCKMASTER.ARPA">
JWilliams@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Wed, 17 Dec 86 14:55 EST
</i><PRE>
          a correction and a plea
To:  risks@CSL.SRI.COM

In <A HREF="/Risks/4.28.html">RISKS-4.28</A>, Craig Paxton, identified as an economist from Northwestern
University, observes that "E. Leamer" is the correct spelling of the UCLA
economist's name, not the "Edward Learner" I used.  I double-checked the
Science article, and discovered, with the aid of a co-worker, that it is
indeed "Leamer," something my increasing far-sightedness could not
distinguish in the proportionally-spaced Science typefont:  "rn" and "m"
continue to look alike through my (obsolete) prescription.

My apologies to Professor Leamer, Science, and the RISKS readership.
Despite considerable effort on my part (and the moderator's), an error
got through that was caught, finally, by peer review.

Professor Paxton compares this "subtle error" to those in economic
verification.  No one can consider the 91% error rate measured for modeling
of short term oil price changes a subtle error, especially in models sold to
or used by the Government to influence major economic policy.  A stopped
clock is not subtle.  Bob Estell, in <A HREF="/Risks/4.25.html">RISKS-4.25</A>, has it right when he
suggests the ACM, IEEE, et al. should require supporting data be archived
and retrievable, even if not published with the article in question, so that
peer reviewers may at least have some basis for determining reproducability,
much less validity or error rate.

In fact we in computers should help pioneer such archives for scientific
validation and peer review generally, since initial publication itself
is increasingly a computer-based enterprise.  RISKS, but for lack of
referees, is a prototype of the future journal whose articles must be
assessed, reproduced, validated, and archived.

The proprietary arguments do not impress me:  if there are those who
wish to hide their methods in the name of profit, then they needn't
publish in scientific journals, nor expect scientific endorsement.  Let
them make a fortune, but let them be regarded with the same skepticism
that authors of "Get-Rich-Quick" books and newsletters are:  if you're
so smart [about money, economics, etc.], how come you ain't rich?  How
come you're peddling books, or models, instead of profiting from the
contents thereof?

I believe there are many ACM, IEEE and other society officials who are
regular readers and sometime-contributors to RISKS:  may we hear from them?

     o First, have they sampled their own publications, as the Journal of
Money, Credit and and Banking was, to find what percentage of findings, in
computer modeling or otherwise, were reproducible?  Do they have policies
about surrender of data, equations, etc.  on peer request?  Do they find the
falsification of data and experiments plaguing the biomedical community at
the moment to be a problem in computer science publications?

     o What actions will they take against authors/papers/presenters who
refuse to supply information for reproduction, or validation, or who have
falsified, stolen, or otherwise misapplied data and/or findings?  What
policies do they have on authorship of papers, and are its journals free of
the misrepresentations of the number, contribution and even identity of
authors, so serious in other fields that even the Wall Street Journal of
last week had a front-page story on this problem in AIDS research?

     o What is their comment on the challenges for reform in my article
in <A HREF="/Risks/4.21.html">RISKS-4.21</A>, and the additional suggestions by Estell noted above?

Let me ask the readership to forward copies of this discussion to those
officers of societies they may know, who are, or should be, able to set
or adjust policy on these matters.  As a member of ACM since 1964, and
in correspondence with an ACM Committee, I would expect at least a
comment from ACM as a society.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.30.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.32.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-109</DOCNO>
<DOCOLDNO>IA012-000125-B045-343</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.32.html 128.240.150.127 19970217010556 text/html 7432
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:04:29 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 32</TITLE>
<LINK REL="Prev" HREF="/Risks/4.31.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.33.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.31.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.33.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 32</H1>
<H2>Thursday, 17 December 1986</H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
EXTRA! British Telecom payphone Phonecard broken?
</A>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
EXTRA! British Telecom pay phone Phonecard broken?
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Thu 18 Dec 86 11:25:17-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

Britain is currently just at the tip of an iceberg regarding an apparent
vulnerability in its debit cards for British Telecom pay phones.  The debit
cards can be purchased from all sorts of shops, and come in a range of
denominations such as 5, 10, 40, or 100 calling units.  The system has been
in use for a year or two, and card pay phones are both widely accessible and
very popular.  (If you've ever tried to use coins in a London call box, you
know that it is quite an experience.)

My best guess is that it has a holographic stripe, and that a destructive
write is used effectively to burn out a part of the hologram corresponding
to each message unit -- making it difficult to ADD units to the card.

Unfortunately, a relatively simple doctoring of the card has been discovered
that threatens the whole scheme, and makes a card indefinitely reusable [at
least until the system is either modified or withdrawn].

An article appeared as the front-page lead story in The Sunday Post (West
Scotland?), 14 December 1986, with the banner headline "DIAL WORLD WIDE FOR
NOTHING -- TELECOM HIT BY 'PHONE FRAUD'".  The article notes that the trick
was discovered by a British soldier "fed up with paying a fortune to call
his Scottish girlfriend".  The word is now spreading around British troops,
and can be expected to be widely known in a very short time.  (The newspaper
states that they know how it is done, and have proved that it works.  It
cites a variety of calls that they were able to make without any debit to
their card.)  The consequences of the propagation of this trick are awesome
to contemplate.

The system was presumably billed as "foolproof".  But "foolproof" is not
good enough against intelligence -- although it should be pointed out that
the card is not a smart-card in the usual sense.  There is no user
identification number required, and no use of encryption.  The AT&amp;T credit
card number seems somewhat safer, as it is quickly revocable on an
individual basis.  On the other hand, the convenience of the BT phone card
is certainly appealing.

A challenge is presented to RISKS as to how to handle this situation.  My
philosophy is generally to treat the existence of such cases relatively
openly, in the hopes that those who need to be protected will become wiser
fast enough to act accordingly.  If the vulnerability is about to be
replicated elsewhere, then knowledge of it may stave off disasters in
about-to-emerge applications of the technology.  Thus it seems germane at
least to call your attention to the problem at this time.

On the other hand, there is a more sensitive question about whether RISKS
should divulge specific details of the vulnerability.  (Indeed, several
possible approaches immediately come to mind, although I do not know the
technique that was allegedly demonstrated.)  Intelligent discussion on this
topic is welcomed here.  Furthermore, if hard knowledge of the penetration
method is already appearing in the British press, then it would seem to be
suitable for inclusion here.  I hope some of our British correspondents will
keep us informed.

We have previously had some discussions in RISKS on whether to address
operating system and network flaws, where it is vital that vulnerabilities
be quickly known to system personnel -- the flaws may already be widely
known elsewhere.  It might be tempting to think that the holocard situation
is small peanuts -- it is only dealing with 10P at a crack.  But that can
add up in a hurry when people discover they have unlimited free dialing.  It
might alternatively be tempting to think that this situation is more
sensitive than computer system security flaws, e.g., because MONEY is
involved -- namely defrauding British Telecom.  But many computer systems
control very large sums of money, and are vulnerable to much greater frauds
than pay phone ripoffs.  At any rate, stay tuned, and let's see what happens.

It is certainly of concern to RISKS to point out that most such schemes have
vulnerabilities that transcend the set of assumptions made by the designers.
This appears to be a case in point.

There are also risks in smart-cards (widely used in France), although the
frauds are not quite so easy to perpetrate.

   [Thanks to Donn Parker for having brought back with him a copy of the
   Sunday Post whose presence all over a newspaper kiosk caught his eye as
   he was leaving for his flight back from London on Sunday.  It is pure
   coincidence, I guess, that he travels the world hunting down and 
   consulting on computer related crime!]

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.31.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.33.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-110</DOCNO>
<DOCOLDNO>IA012-000125-B045-367</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.33.html 128.240.150.127 19970217010616 text/html 22548
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:04:41 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 33</TITLE>
<LINK REL="Prev" HREF="/Risks/4.32.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.34.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.32.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.34.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 33</H1>
<H2>Sunday, 21 December 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Help British Telecom save a WORM. 
</A>
<DD>
<A HREF="#subj1.1">
Scot E. Wilcoxon
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Security of magnetic-stripe cards 
</A>
<DD>
<A HREF="#subj2.1">
Brian Reid
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Korean Air Lines Flight 007 
</A>
<DD>
<A HREF="#subj3.1">
Dick King
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Car-stress syndrome 
</A>
<DD>
<A HREF="#subj4.1">
Dick King
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Bugs called cockroaches [A True Fable For Our Times] 
</A>
<DD>
<A HREF="#subj5.1">
anonymous
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Re: More on car computers (not Audi) 
</A>
<DD>
<A HREF="#subj6.1">
Miriam Nadel
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Runaway Audi 5000 
</A>
<DD>
<A HREF="#subj7.1">
John O. Rutemiller
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Help British Telecom save a WORM.
</A>
</H3>
<address>
Scot E. Wilcoxon
&lt;<A HREF="mailto:sewilco@mecc.UUCP ">
sewilco@mecc.UUCP 
</A>&gt;
</address>
<i>

</i><PRE>
Summary: Possible solution: test write before accepting card. WORM problem.
Date: 21 Dec 86 04:01:49 GMT
Organization: Minn Ed Comp Corp, St. Paul

  &gt;Unfortunately, a relatively simple doctoring of the card has been discovered
  &gt;that threatens the whole scheme, and makes a card indefinitely reusable [at
  &gt;least until the system is either modified or withdrawn].

A read-after-write test before using the resource (telephone time in this case)
might be the generic solution.  This won't work if the BT reader can't be
positioned to read what has just been written.  Hopefully there aren't many
other major installations with the same flaw (BART &amp; other transport?).

Computer programmers should know of this flaw due to one eagerly-awaited
peripheral which is finally becoming available.  Writeable optical data disks
(ie, WORM drives) promise storage of huge amounts of data.  People who want to
sell large numbers of programs or data will now be able to put hundreds of
programs on one optical disk.  One "demonstration disk" method being used by
some companies is to allow a program to be used a few times or for a few days.
This method may be vulnerable to a write-blocking technique similar to the
British Telecom card doctoring, although different physical tools may be
needed.  The designer of an optical disk collection should be aware of this
technique so he can thwart it.

Scot E. Wilcoxon   Minn Ed Comp Corp  {quest,dayton,meccts}!mecc!sewilco
(612)481-3507           sewilco@MECC.COM       ihnp4!meccts!mecc!sewilco
   
</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
security of magnetic-stripe cards  [This relates to earlier risks.]
</A>
</H3>
<address>
Brian Reid
&lt;<A HREF="mailto:reid@decwrl.DEC.COM ">
reid@decwrl.DEC.COM 
</A>&gt;
</address>
<i>
20 Dec 1986 0109-PST (Saturday)
</i><PRE>

There are three ways that I know of to fraudulently modify magnetic-strip
credit cards. The technology to make mag-stripe credit cards secure against
two of them has existed for almost 15 years. Most credit-card companies do
not use it because it is more expensive than the losses that they are
currently sustaining from fraud. However, the main reasons for its expense
are that it requires new card-reader electronics, and in the fullness of
time one could imagine moving to it.

The three attacks are:
 1) Copying the strip from one card to another
 2) Modifying the contents of a card with read/modify/write (or
    rewriting it completely, if you choose)
 3) Making a checkpoint of a card, using it, and then restoring the
    card to its former state.

This technology can protect against attacks (1) and (2), but not (3). I
first heard about it from a security person at the National Bank of
Washington in 1973.

Here's how it works. When a credit card is molded, it is molded out of
plastic that has had nickel particles stirred in with it. The magnetic
strip is affixed, and the card is run through a machine that senses the
location of the nickel particles on the card and computes a
cryptographic checksum of their positions. The checksum function is
secret. That checksum is used as the decryption key of a 2-way
encryption function, and the remaining information on the magnetic
strip is encrypted in such a way that the nickel-particle checksum of
the plastic card is used as the decrypting key for the data on the
magnetic strip.

This protects against attack 1, copying, because the contents of the
mag strip on one card will not work on a card with a different nickel
checksum. This protects against attack 2, forging, because even if the
forger can determine the position of the nickel particles he does not
know how to compute the checksum from their position. It is easy to
design a system for which attack 3 will not be useful.

I believe that the expense of this system is the expense of the
particle-sensing readers, which are more delicate than mag-strip
readers. I am confident that if electronic fraud with credit cards
starts to cost more than the particle readers, that banks will switch.

Brian Reid
DEC Western Research

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Korean Air Lines Flight 007 (<A HREF="/Risks/4.31.html">RISKS-4.31</A>)
</A>
</H3>
<address>
Dick King
&lt;<A HREF="mailto:king@kestrel.ARPA ">
king@kestrel.ARPA 
</A>&gt;
</address>
<i>
Thu, 18 Dec 86 13:48:36 pst
</i><PRE>
To: RISKS@CSL.SRI.COM

I'm very unimpressed with the straightness of the logic in Shootdown.
There seem to be as many contradictions within that volume as there
are in the record of the shootdown itself.

As one example, on page 24 [hardcover, American edition] he states that "The
full significance of this becomes apparent if one realises that Soviet
ground control was undoubtedly monitoring 007's conversation with Tokyo,
presumably with a slight lag as a translation was obtained.  ...".  The
transcripted conversation, to which the Soviets were "undoubtedly"
listening, clearly identified the airliner as 007.  The thrust of P. 24-27
is that the plane gave out deceptive information that fooled the Soviet air
defence.  On page 187, however, he quotes the Times as quoting US
intelligence analysts as saying "the initial identification of the the
jetliner as a military reconnaissance aircraft became fixed in the mind of
Soviet air defence officials and was strengthened after Soviet interceptors
were unable to locate the plane for two hours".

Mr. Johnson did not explain why the Soviets were, according to him,
listening closely enough to this routine airliner traffic to be fooled, and
why, if they thought the intruder was not 007, they attributed 007's
broadcasts to this intruder.  Remember, they were supposed to be hearing
007; they are just supposed to have thought that this plane wasn't it.

-dick

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Car-stress syndrome (<A HREF="/Risks/4.31.html">RISKS-4.31</A>)
</A>
</H3>
<address>
Dick King
&lt;<A HREF="mailto:king@kestrel.ARPA ">
king@kestrel.ARPA 
</A>&gt;
</address>
<i>
Thu, 18 Dec 86 12:19:22 pst
</i><PRE>
To: RISKS@CSL.SRI.COM

This brings up an interesting RISK imposed by high technology in general --
namely that certain people will take advantage of the public's natural fear
of the unknown.  They can either offer new and different forms of snake oil
or, as this ad seems to do, or they can prey on the public ignorance as to
how things work and what is known or not known about safety and levels of
exposure, to attract a following for whatever reason.

What has this to do with computers?  Two groups I know of are arguably using
this tactic in a computer-related manner.  One group, 9-5 I believe,
attempts to bolster a political base by causing CRT's to be regarded as
*unsafe*.  The second group offers to clear credit problems, doing nothing
you couldn't do for yourself [per CR], but implying in at least some of
their ads that they have an "in" with the computer network.
                                                               -dick

* I will apologize to the first person who can show me that most of the
group's supporters refuse to allow a TV into their homes, or at least that
the group advocates such refusal.  I have never even seen any such
literature claim that monochrome TV's are safer.  This would be obviously
counter-productive because most of the intended audience uses monochrome
monitors, but voltages are lower, images are crisper, flyback noise tends to
be less; this covers most of the claimed problems with CRT's.

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Bugs called cockroaches [A True Fable For Our Times]
</A>
</H3>
<address>
&lt;<A HREF="mailto:anonymous@erehwon">
anonymous@erehwon
</A>&gt;
</address>
<i>

</i><PRE>

[THE FOLLOWING WAS CONTRIBUTED FOR ANONYMOUS INCLUSION ON THE GROUNDS OF SEVERE
AUTHOR EMBARRASSMENT AT EVER ADMITTING TO WRITING SUCH AWFUL DRIVEL (EVEN 
THOUGH THE INCIDENT DESCRIBED IS ABSOLUTELY TRUE) OR TO INCLUDING SOME HORRIBLE
PUNS (MOST OF WHICH HAVE BEEN REMOVED BY THE SOMETIMES IMMODERATE MODERATOR).]

  &gt;  Heisenbugs is already a fairly common term -- it refers to bugs
  &gt;  which go away as soon as you try to run them under a debugger 
  &gt;  (or with the debugging compile- or run-time flags set).

I once had an amusing problem where the most likely cause was that I was
exceeding array bounds.  Naturally I turned on the bounds checking flag,
and got fatal output errors.  So I next put in manual traces, and I still
got fatal output errors.  Highly annoying, no?  A little investigation
revealed that the newly compiled-in format strings were getting trashed.
I'm talking about a genuine cockroach.

What to do, what to do?  I declared a dummy array of dimension 100k--what
the heck, it was on a Cray--so from then on the array overflow was safely
trashing the dummy; I got my trace and I killed the nasty little bugger.

So, what is the moral of this story?  Obviously, 

     "Rough strings do flake the darling bugs of Cray."  


            [Ah, yes, the iambic pentameter is always a giveaway.
            For those of you in search of the original, the first 
            line is exceedingly well known:
                   
                       Shall I compare thee to a summer's day?
                       Thou art more lovely and more temperate:
                       Rough winds do shake the darling buds of May,
                       And summer's lease hath all too short a date:
                       ...

            I hope that any future shaggy bug stories will be more lovely,
            more temperate, and less anonymous.  PGN (LE KOOK or HOTSHOT?)]

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Re: More on car computers (not Audi)
</A>
</H3>
<address>
Controls Wizard
&lt;<A HREF="mailto:dma%euler.Berkeley.EDU@BERKELEY.EDU ">
dma%euler.Berkeley.EDU@BERKELEY.EDU 
</A>&gt;
</address>
<i>
Thu, 18 Dec 86 12:09:35 PST
</i><PRE>

According to the latest issue of Consumer Reports there is a recall of 1982
Toyotas because a problem with the cruise-control computers can result in
uncontrollable acceleration.  Yet another reason for Audi to rethink their
position.
                      Miriam Nadel  [Specify by name in any direct reply]

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
 Runaway Audi 5000
</A>
</H3>
<address>
 "John O. Rutemiller" 
&lt;<A HREF="mailto:Rutemiller@DOCKMASTER.ARPA">
Rutemiller@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Sat, 20 Dec 86 11:03 EST
</i><PRE>
To:  risks@CSL.SRI.COM

The Washington Post Magazine for December 21, 1986 had an article in which
the author supports Audi's position of driver error.  I believe his view
helps show the current trend of people like "60 Minutes" to blame a computer
or machine without looking at operator error.  I'm glad someone is willing
to accept possible operator error.  The full text follows.

      Audi's Runaway Trouble With the 5000,      by Brock Yates
  
  I recently watched in fascination as Ed Bradley reported on the CBS-TV
  show "60 Minutes" that the 1978-'86 Audi 5000 sedans can treacherously
  launch themselves like misfired missiles when their automatic
  transmission levers are placed in drive or reverse.  This phenomenon
  labeled "unintended acceleration," has allegedly been responsible for
  several deaths, including a particularly poignant one - tearily
  documented on the show - in which a pretty young mother crushed her
  young son against the back wall of a garage.  The segment included
  testimony from several victims.  They decried Audi's suggestion that the
  trouble lay not in a mechanical flaw but in driver error.
  
  Audi says the drivers accidentally hit the accelerator, not the brakes,
  after engaging the transmission.  Although Bradley acknowledged Audi's
  explanation and interviewed two of its engineers, he clearly sided with
  the owners.
  
  "60 Minutes" portrayed the Audi 5000 as a flawed automobile, perhaps
  cursed by its "idle stabilizer control," a fuel system component that
  supposedly triggers "transient malfunctions" without warning.
  
  But wait a minute, did Bradly tell us everything?  There is no arguing
  the Audi is in serious trouble with the 5000:  Sales are down 20 percent
  and the Center for Auto Safety has taken the position that the
  Department of Transportation should require Audi to buy back all its
  5000s.  Further, an Audi spokesman agrees that "hundreds" of
  acceleration incidents have occured in the 5000s.  The Center for Auto
  Saftey has received 500 reports and believes more than 750 reports have
  been made altogether.  Audi has ceased to stonewall the issue.  "We take
  the responsibility to resolve the problem," says Audi public relations
  director Ed Triolo.
  
  Furthermore, the phenomenon of "unintended acceleration" is not new.
  The problem has occurred in a variety of autos with automatic
  transmissions.  More than 2,000 complaints have been made about General
  Motors models built between 1973 and 1986.  Owners of Toyotas, Renaults,
  Mercedes-Benzes and Nissans have also reported unintended acceleration
  incidents.  However, the Audi 5000 has the highest percentage of
  acceleration incidents:  about 1 in 400 cars built.
  
  Triolo says that in the 270 accidents that have been examined by Audi
  engineers, only six idle-speed stabilizers were found defective and not
  in a way that would cause rapid, unexpected acceleration.  More
  important, the Audi 5000 - with its 2.2-liter, five cylinder engine
  developing only 110 hp - simply does not have enough power to override
  its brakes.  (Drivers involved in the incidents swear they are standing
  on the brakes.  Audi has found no instances of brake failure in autos it
  has examined.)
  
  Who's right?  Will an Audi 5000 outmuscle its own brakes?  I borrowed a
  1984 Audi 5000, floored the accelerator with my right foot and stepped
  on the brake hard with my left foot.  Then I moved the transmission from
  park to drive.  AND THE ENGINE STALLED!  It lacked sufficient power to
  override the brakes.  According to my brief test, for unintended
  acceleration to occur, two independent systems - fuel supply and brakes
  - must fail simultaneously and somehow return to normal.
  
  Audi says it went even further.  In demonstrations for both CBS and NBC,
  it made full-throttle acceleration runs to speeds between 30 and 50 mph
  and then, with the throttle on the floor, stopped the car with the brakes.
  
  All of which raises some interesting questions "60 Minutes" failed to
  ask about the Audi 5000 incidents:
  
  Why, after millions of starts over an eight-year period, haven't there
  been any runaway 5000s reported at Audi's 410 dealerships?
  
  Why do there seem to be more of these incidents among drivers who have
  relatively little experience driving the Audi 5000?  (There are an
  inordinate number of such incidents within the first 2,000 miles of the
  life of a given car.)
  
  Why are there no reported accidents with the Audi 4000 Quattro, which
  has an identical idle stabilizer mechanism?
  
  Why do independent experts, who have speculated that the trouble is
  centered on throttle linkage, the computer brain in the engine, the
  automatic transmission or the idle stabilizer, still openly admit there
  is no obvious culprit?
  
  Why, in a number of accident investigations, did Audi engineers find the
  accelerator pedal bent, even snapped off, presumably by foot pressure?
  
  While continuing to research the incidents, Audi has so far installed
  32,000 interlock devices that prevent the transmission from being
  engaged without the driver's foot on the brake.  Audi has asked all
  owners of the 5000 model to bring their cars in for free installation of
  the interlock.  Audi is adamant that the device is a solution, although
  Triolo says the company does not expect it to eliminate the problem.
  
  Drivers of three cars equipped with the interlocks have reported runaway
  crashes.  In the first case, an Audi spokesman says, the driver's
  description of the event changed over time, and Audi representatives
  decided it was not a case of brake failure or runaway acceleration.  In
  the second case, Audi says a bushing was installed upside down,
  preventing the interlock from working.  In the third case, Audi says it
  has not been allowed by the owner's attorneys to inspect the vehicle.
  
  Audi contends that the problem of unintended acceleration is a complex
  one involving a number of factors, including the design of the car
  itself, the driver, and external distractions.  Triolo says the problem
  of unintended acceleration is inherent in automatic transmission cars
  throughout the auto industry, not just in Audis.

  There is one potential explanation for the runaway Audis that strikes me as
  obvious:  The brake and accelerator pedals in the Audi 5000 are off-center,
  to the left.  In models of the 5000 built before 1983, it was even possible
  to step on the brake pedal and the accelerator at the same time, a problem
  Audi has since rectified.  Audi maintains that brake and accelerator pedals
  in autos come in a wide range of placements, some farther to the left than
  Audi's.
  
  I maintain the pedals are sufficiently misplaced that inexperienced
  drivers might easily thrust a right foot forward and hit the accelerator
  when intending to hit the brake.  Audi has investigated at least one
  incident in which a 5000 was driven a foot or so into a concrete wall in
  a parking garage, the rear tires spinning in anguish, the driver
  confused as to what was happening until she finally realized her right
  foot was on the accelerator.
  
  Sadly, one of the most troubling aspects of these incidents is that so
  many Audi 5000 drivers fail to avert disaster simply by shoving the
  transmission shifter into neutral or turning off the ignition.  While it
  certainly is understandable that a panicked driver might actually press
  harder on the throttle of a runaway car, thinking he was stepping on the
  brake pedal, such a reaction also exposes the dismal training and
  minimal presence of mind the average American driver has when faced with
  an emergency.
  
  How about a segment on driver training, Mr.  Bradley?
  
</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.32.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.34.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-111</DOCNO>
<DOCOLDNO>IA012-000125-B045-388</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.34.html 128.240.150.127 19970217010629 text/html 21840
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:04:58 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 34</TITLE>
<LINK REL="Prev" HREF="/Risks/4.33.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.35.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.33.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.35.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 34</H1>
<H2>Tuesday, 23 December 1986 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Debit cards that don't 
</A>
<DD>
<A HREF="#subj1.1">
Edward M. Embick
</A><br>
<A HREF="#subj1.2">
 PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Re: security of magnetic-stripe cards 
</A>
<DD>
<A HREF="#subj2.1">
Henry Spencer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Plug-compatible plugs 
</A>
<DD>
<A HREF="#subj3.1">
Henry Spencer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Runaway Audi 5000 
</A>
<DD>
<A HREF="#subj4.1">
Mark Brader
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Ozone layer 
</A>
<DD>
<A HREF="#subj5.1">
Mark Brader
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Another heisenbug 
</A>
<DD>
<A HREF="#subj6.1">
Zhahai Stewart
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  More "bugs" 
</A>
<DD>
<A HREF="#subj7.1">
Tom Parmenter via Richard Lamson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Computer Malpractice 
</A>
<DD>
<A HREF="#subj8.1">
Dave Platt
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj9">
  Financial Servomechanisms 
</A>
<DD>
<A HREF="#subj9.1">
Brian Randell
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Debit cards that don't  (<A HREF="/Risks/4.32.html">RISKS-4.32</A>)
</A>
</H3>
<address>
Edward M. Embick 
&lt;<A HREF="mailto:embick%tetra@nosc.ARPA">
embick%tetra@nosc.ARPA
</A>&gt;
</address>
<i>
Mon, 22 Dec 86 14:05:59 PST
</i><PRE>

I, like others, can only guess at the mechanism used to "debit" the card in
question.  However, it would seem to me that a mechanism so designed would
also reread the card to ascertain the debiting action was taken.  If not,
disconnect!  I suspect that the design of the system was made simple and
cheap, and the design reviewers committed one of the fundamental analysis 
flaws that introduces risks to a system.  They reviewed the basic design,
and assumed that since the device is designed to work that way, that
unless it breaks, which will be apparent, it will only fail by misreading
the card, which will only happen in an acceptably small number of cases
where the call costs more than is on the card.  

This mindset is the same that most peer groups and outside analysts get
after analysing a system for possible fraud or abuse.  They tend to 
profile a community of potential system users and a range of views of
the system, and overlook the obvious vulnerability of a new, but in their
minds, trusted part of the system, because the card has passed the test
and is out of the user's physical control.

Ed Embick    (the more paths I make, the more paths they break! waaaaaaa....)
Computer Sciences Corp.                embick@noscvax.UUCP  or
4045 Hancock St.      {decvax,ihnp4,ucbvax}!sdcsvax!noscvax!embick
San Diego, CA 92110 (619) 225-8401 x516         MILNET:  EMBICK@NOSC

</PRE>
<HR><H3><A NAME="subj1.2">
British Telecom Phone Cards (<A HREF="/Risks/4.32.html">RISKS-4.32</A>)
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Tue 23 Dec 86 11:28:58-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

I had a call from British Telecom about their Phone Card, but I was not
around to receive it.  Despite the newspaper story to the contrary, they
apparently insist that their Phone Card was not compromised, and that the
British Post reporter must have misunderstood what he was told when he
described the free-call scam and when he perpetrated his allegedly free
calls.  Stay tuned, and maybe we'll have more later.

Edward Embick points out an intrinsic security vulnerability that results if
such a system assumes that WRITES always succeed, so that they don't bother
to READ after an attempted (DESTRUCTIVE) WRITE to see if the write worked.
This leaves them open to monster vulnerabilities that sooner or later might
be exploited.  The speculative list of possible attacks is most interesting,
and keeps growing.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Re: security of magnetic-stripe cards
</A>
</H3>
<address>
&lt;<A HREF="mailto:hplabs!pyramid!utzoo!henry@ucbvax.Berkeley.EDU">
hplabs!pyramid!utzoo!henry@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Mon, 22 Dec 86 18:44:47 pst
</i><PRE>

&gt; ... The technology to make mag-stripe credit cards secure against
&gt; two of them has existed for almost 15 years...
&gt; ... The checksum function is secret...

Around this point the alarm bells start ringing.  How long will it *stay*
secret?  Not forever! The safest approach would probably be to burn it into
custom hardware at central sites (*not* in each reader, because it's
impossible to maintain physical security on thousands of readers) so that
programmers don't have routine access to it.  Even then it will probably get
out eventually, unless you shoot the people who lay out the chips after they
finish.

The technique *would* be a major short-term obstacle to magstripe fraud.
But it would not make magstripe cards permanently secure against fraud;
it would stop fraud only for a while, and merely make it harder thereafter.

				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Plug-compatible plugs
</A>
</H3>
<address>
&lt;<A HREF="mailto:hplabs!pyramid!utzoo!henry@ucbvax.Berkeley.EDU">
hplabs!pyramid!utzoo!henry@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Mon, 22 Dec 86 18:45:20 pst
</i><PRE>

&gt; Someone discovered by accident that the IBM monochrome display adapter will
&gt; accept a Token Ring connector cable...
&gt; - Why couldn't they have made the token ring connector a different kind than
&gt; the monochrome display connector? Did (or should) the hardware design process
&gt; include any analysis of its consequences in such conjunctions, given known
&gt; human tendencies?

It does in other areas.  In avionics design, it is normally mandatory that
no two functionally-different plugs be physically identical.  This is
usually achieved by keying systems rather than by a vast inventory of
slightly-different connectors, although there are quite a variety used.

The crucial difference is that avionics systems are, to some degree, designed
around the assumption of imperfect maintenance.  The military in particular
has to contend with complex systems maintained by ill-trained technicians
subject to many distractions (e.g. gas masks, bombs falling nearby, etc.).
Unfortunately, the healthy paranoia that this induces in designers doesn't
seem to be present in the computer business.

Computer systems have been designed around the assumption of perfect
maintenance for quite a while, actually.  The cables used to connect most
disks and tapes to their controllers are physically but not logically
symmetrical, with no keying.  At least a 180-degree rotation from one end
to the other isn't generally destructive, the stuff just doesn't work!
Still worse are symmetrical female connectors which plug onto rows of pins
protruding from boards:  not only is it possible to get the connector on
the wrong way, but it is also possible to get it misaligned with the pins,
so that some pins stick past, rather than into, the connector.  The grid of
pins is regular and symmetrical -- they are normally on the 0.1-inch square
grid that is standard for all manner of electronic components -- and there
often is no housing around them to constrain the plug to fit in only one
place.  Slightly fattening the plug to prevent pins sticking past it would
solve this, but nobody seems to bother.  Even some prefabricated sockets
which *do* have outer plastic shells are roomy enough that a narrow plug
can go in misaligned by one row of pins.  (I speak from experience.)  The
D connectors used since time immemorial for RS232 lines, and increasingly
common for all manner of things on personal computers, at least lack these
flaws.

There is no great mystery about why this stupidity occurs:  it's cheap, and
nobody can be bothered improving it.  The offending connectors are available
from a wide variety of competitive sources, and are available in "mass-
terminated" forms that can simply be clamped onto flat cable without the
expensive and largely manual operation of soldering individual wires into
the connector.  A grid of pins sticking up from the board is cheaper than
a prefabricated connector.  It's cheaper to put the pins on the standard
grid than on a special one that would interfere with improper connections,
and cheaper to buy female connectors that have all holes present rather
than having one blocked off for keying.  And so forth.  Often it's possible
to get at least some degree of protection if one tries -- keyed mass-
terminated connectors do exist, for example -- but all too often suppliers
don't bother.  Even something as simple as making one socket male and the
other female offers at least slight protection against wrong hookups.

				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Runaway Audi 5000
</A>
</H3>
<address>
Mark Brader
&lt;<A HREF="mailto:mnetor!msb@sq.arpa ">
mnetor!msb@sq.arpa 
</A>&gt;
</address>
<i>
Tue, 23 Dec 86 14:19:42 EST
</i><PRE>

The Washington Post article posted by John O. Rutemiller is indeed an
interesting response to the original 60 Minutes story, but it does not cover
two points mentioned -- though not stressed -- in that original story.

1. One of the drivers who was interviewed after the runaway-accident said
   that he had *both* feet on the brake.  From the pedal sizes as seen on
   60 Minutes, it isn't possible to fit both feet on the accelerator.

2. The common description of the accident was that the transmission
   was shifted out of Park and then the engine ran away.  Now, when
   I shift a car out of Park, I normally step on the brake first or
   not at all.  How come the drivers of runaways are shifting out of
   Park and *then* stepping on the pedal?

Mark Brader   utzoo!sq!msb    [* New Address! *]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Ozone layer
</A>
</H3>
<address>
Mark Brader
&lt;<A HREF="mailto:mnetor!msb@sq.arpa ">
mnetor!msb@sq.arpa 
</A>&gt;
</address>
<i>
Tue, 23 Dec 86 18:11:28 EST
</i><PRE>

The delayed discovery of the recent reduction in the atmospheric ozone
layer was discussed earlier in RISKS.  Readers interested in a 1-page
summary of what is now known, and the competing theories, can find this
in the January 1987 Scientific American at pages 67-68.   Mark Brader

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Another heisenbug
</A>
</H3>
<address>
Zhahai Stewart
&lt;<A HREF="mailto:gaia!zhahai%ncar.csnet@RELAY.CS.NET ">
gaia!zhahai%ncar.csnet@RELAY.CS.NET 
</A>&gt;
</address>
<i>
23 Dec 86 08:25:06 GMT
</i><PRE>

If we haven't driven the heisenbugs (bugs that change or disappear
under examination) into the ground yet, I will contribute yet another.
I once had a simple program which ran (or didn't run) under CP/M on
an early microcomputer.  Under the debugger, it ran fine, of course.
I traced the problem to the following.  I had reversed a conditional
jump instruction, causing the program to take an early quick exit.  Under
normal conditions CP/M put the regular return-to-system address on the
stack before calling a program, so one could just return for a shortcut
exit.  Under the debugger, the stack was relocated to just below the
program, with nothing in it.  Thus the program popped the first two
bytes of code as the return address.  This turned out to be exactly the
address after the misdirected conditional jump - continuing the
execution normally and terminating with a more robust method.  I
was more than usually bemused by this coincidence; it also served
as the inspiration for some tricky schemes to thwart disassemblers.

Zhahai Stewart                         {hao | nbires}!gaia!zhahai

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
More "bugs"
</A>
</H3>
<address>
Richard Lamson 
&lt;<A HREF="mailto:rsl@CERRIDWYN.SSF.Symbolics.COM">
rsl@CERRIDWYN.SSF.Symbolics.COM
</A>&gt;
</address>
<i>
Tue, 23 Dec 86 12:23 PST
</i><PRE>
To: Risks@SRI-CSL.ARPA

Date: Tue, 23 Dec 86 10:06 EST
From: Tom Parmenter &lt;parmenter@STONY-BROOK.SCRC.Symbolics.COM&gt;

Here are some alternate attempts.  If we just take the -ug words that
already exist in American, we get

dug - documentation bug
fug - bug that causes you to give up (fug it)
hug - deadly embrace bug
jug - bug that can get you jailed, such as penetrating security or spelling
      Ada lowercase
lug - big, lovable bug (e.g., Unix)
mug - bug that drives you to drink
pug - bug that makes you want to go in the boxing ring with its author
plug - bug that keeps a system going 
rug - bug that knocks the system flat
slug - bug that slows everything down, leaves a trail of slime, and eats up
       your lettuce 
smug - bug you can't find
snug - bug that you put in for job security
tug - bug that you can't forget, no matter how many years ago it was

                    [OK, OK.  I think I have to pull the rug out from 
                    further contributions, unless they are outstanding.  
                    This one gets through because it's Christmas.  PGN]

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
Computer Malpractice
</A>
</H3>
<address>
Dave Platt
&lt;<A HREF="mailto:dplatt@teknowledge-vaxc.ARPA ">
dplatt@teknowledge-vaxc.ARPA 
</A>&gt;
</address>
<i>
Mon, 22 Dec 86 18:05:41 PST
</i><PRE>

The 1/87 issue of High Technology magazine has a one-page article (p.61)
entitled "Safeguarding against computer malpractice".  It doesn't go into
great detail but is probably worth reading.

One point the article's author makes is that the concept of "software
malpractice" has evolved fairly recently, and is tied to the transition
of SE from a "skilled tradesman" discipline to a "professional" one.

</PRE>
<A NAME="subj9"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj9.1">
Financial Servomechanisms
</A>
</H3>
<address>
Brian Randell 
&lt;<A HREF="mailto:brian%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
brian%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Tue, 23 Dec 86 15:43:35 gmt
</i><PRE>

[We have had various fragments on this before.  This one seems to add
a little more, but I have not tried to axe out the duplication...  PGN]

                   SOFTWARE STRIKES IT RICH
         (From The Observer, London, 21 December 1986)

  Computers have produced at least two major crashes on the New York Stock
exchange this year, and are set to repeat the process on exchanges around 
the world, causing wild oscillations in exchange rates.
  Computer programs in the US are set up to look for discrepancies between 
the price of a futures contract on a stock index and the price of the stock
that makes up the index. When the price falls, the stock price tends to fall
more slowly than the futures contract.
  The programs spot the discrepancy, sell the stocks and buy the futures to
make a risk free 'arbitrage' profit. The process is called program trading.
It caused a shudder in the Dow Jones Index in March, when a number of futures
contracts 'unwound' at once, and again in September, when the index fell 86
points one day and 34 the next.
  Software company Data Logic has come up with a program which will spot these 
discrepancies on any index with a futures contract anywhere in the world.
The program will also spot discrepancies between the futures contract on the
currency the stocks are traded in and the spot and forward rates of that 
currency.
  For example, a program on a computer in Chicago - which is the world capital 
of futures and program trading - could spot discrepancies between UK stock
prices and the contract on the Financial Times/Stock Exchange 100 Index.
  It would sell stock and buy the futures contract, amplifying any fall in
the index. This would precipitate a run on sterling, the program would then
spot the discrepancy, sell sterling, buy the futures contract and drag the
sterling rate further down.
  The fortunes freed by US banks to play these markets are phenomenal. Wells
Fargo Investment Advisors, which ISN'T one of the major players, has $3 billion
([pounds]2 billion) available for arbitrage trading. Morgan Stanley in
New York are rumoured to have made more than $1 million on one program during
the first half of this year.
  'On an average day, around 25-33 per cent of the trading on the big board
(at the New York Stock Exchange) is done through programs,' says John Blin,
former chief executive of the NYSE. 'But when there is a severe mispricing 
the volume can exceed 50 per cent, or around 75 million shares.'
  Regulators at the US Securities and Exchange Commission are trying to cut 
down the level of program trading by bringing forward the time futures 
contracts mature to an hour before the exchange closes.
  The next step for Data Logic is to tie in a market predictor program to the
arbirage spotting program. Data Logic's market predictor, ISFX, has been
operating in one London bank for most of the year. It uses a database built
up from various sources - economists, regression analysis, charts - and
weighs these against actuality to predict future movements in the sterling/
dollar spot market.
  The company has a deal with the bank so it gets a percentage of the profits 
made from using the program. In the three months since this arrangement was 
concluded, Data Logic's development costs have been more than covered.
  The plan is to 'sell' to eight or nine banks in The City, but to tailor
it to the individual bank's ethos.
  But, however much Data Logic tries to deny it, eight or nine ISFX programs
built by the same programmers might well simultaneously come to the same
conclusion quite often, so precipitating major movements in the exchange rates.
If all these programs act simultaneously, and enough cash is freed by the banks
for them, then ISFX's prophesies will be self fulfilling, making effective
control of exchange rates impossible.
  The program is now being extended to cover the Deutschmark/dollar markets
then to sterling/Deutschmark forward rates and so on. A similar system 
predicting movements on the gilts and money markets is being developed by 
software house Dealing Systems.
                                                         JASON NISSE

Brian Randell - Computing Laboratory, University of Newcastle upon Tyne
  UUCP  : &lt;UK&gt;!ukc!cheviot!brian
  JANET : brian@uk.ac.newcastle.cheviot

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.33.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.35.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-112</DOCNO>
<DOCOLDNO>IA012-000125-B046-3</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.35.html 128.240.150.127 19970217010646 text/html 27386
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:05:11 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 35</TITLE>
<LINK REL="Prev" HREF="/Risks/4.34.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.36.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.34.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.36.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 35</H1>
<H2>Saturday, 3 January 1987 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Computer Gets Stage Fright 
</A>
<DD>
<A HREF="#subj1.1">
Chuck Youman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Still More on PhoneCards 
</A>
<DD>
<A HREF="#subj2.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Miscarriages Up in Women Exposed In Computer-Chip Process 
</A>
<DD>
<A HREF="#subj3.1">
Martin Minow
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Across the Atlantic with Cast Iron 
</A>
<DD>
<A HREF="#subj4.1">
Earl Boebert
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Heisenbugs -- Two more examples 
</A>
<DD>
<A HREF="#subj5.1">
Maj. Doug Hardie
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Risks Involved in Campus Network-building 
</A>
<DD>
<A HREF="#subj6.1">
Rich Kulawiec
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Update on Swedish Vulnerability Board Report 
</A>
<DD>
<A HREF="#subj7.1">
Martin Minow
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  DES cracked? 
</A>
<DD>
<A HREF="#subj8.1">
Dave Platt
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Computer Gets Stage Fright
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Fri, 02 Jan 87 10:14:36 -0500
From: Chuck Youman &lt;m14817@mitre.ARPA&gt;

The Washington Post reported on December 29 and 30th that the Sunday matinee
and evening performances of Les Miserables at the Kennedy Center Opera House
were cancelled due to a malfunction of a massive rotating stage that is used
in the production.  An estimated 4,600 theatergoers had paid between $22.50
and $40 for their tickets would get a refund or have their tickets exchanged
for another show.  (The show is sold out through the remainder of its run,
however).  Some patrons were reported to be angry because they thought they
would be unable to get a refund for their parking ($4) in the lot in the
center.  It was reported the next day however, that the parking fees would
also be refunded.  It was estimated that each cancelled show could result in
losses of up to $60,000.

The failure was reported to be in a computer that controls the turntable.
The turntable covers most of a 40-foot-wide stage, revolves both clockwise
and counter-clockwise, and at various speeds.  When the components in the
circuitry are not working properly, it can take off at full speed.  It is
used at one point to hold two huge scenery pieces each weighing more than
three tons, not counting the cast members standing on it.  Because they are
computer controlled and so hefty, technicians were unable to arrange a safe
method of manually moving them around the stage.  (I'm not sure I would call
the automated method safe, however.)  The reported problem was a faulty
electronic circuit card that interfaces the computer with the turntable
drive mechanism.  The nearest replacement card was in Chicago.  It arrived
Monday and Monday's performance went on as scheduled.

Charles Youman (youman@mitre)

    [It is apparently not true that To the Victor, Hugo, Go the spoils!  PGN]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
 Still More on PhoneCards
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Wed 24 Dec 86 09:36:03-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

I had a call from Colin Sex at British Telecom at 5PM Christmas Eve GMT.  He
stated that "The card itself is completely secure."  They indeed do a
READ-AFTER-WRITE check (along with some other checking), so that part of it
looks OK.  However, there are problems with physical damage to the laser
reader/writer.  In the case at hand, nail polish had been caked onto the
card, and gummed up the works.  But in such cases the unit is supposed
either to reject the card, or else keep the card if it cannot eject it --
and then shut down.  I think they are still vulnerable to some active-card
attacks, but on the whole they think they protect themselves well against
the man on the street.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Miscarriages Up in Women Exposed In Computer-Chip Process
</A>
</H3>
<address>
Martin Minow, MSD A/D, THUNDR::MINOW
&lt;<A HREF="mailto:minow%bolt.DEC@decwrl.DEC.COM  ">
minow%bolt.DEC@decwrl.DEC.COM  
</A>&gt;
</address>
<i>
27-Dec-1986 2323
</i><PRE>

(For the record, this item does not represent the opinions of my employer.
Martin Minow)

Associated Press Wed 24-DEC-1986

Digital Miscarriages Study: 
Miscarriages Up In Women Exposed In Computer Chip Process

   HUDSON, Mass. (AP) - Significantly more miscarriages have been found
among women production workers at a semiconductor plant than those not
exposed to processes used in making computer chips, a study has found.
   In one principal area of production, the level of miscarriages was twice
that of non-production workers, according to the University of Massachusetts'
School of Public Health study commissioned by Digital Equipment Corp.
   The findings, believed to be the first of its kind in the computer
industry, has broad implications for the computer chip industry, which employs
more than 55,000 U.S. production workers, with most believed to be women.
   The study, which found no evidence of a wide range of other major health
disorders such as birth defects and infertility, surveyed 744 of Digital's
nearly 2,000 workers at the Hudson semiconductor plant. Of those studied,
294 were production-line workers and the rest were non-production workers.
   The study, based on the history of the workers at the plant for five
years, was designed to measure a wide range of possible health problems
among women and men. In all, 471 women were studied and 273 men.
   Among the non-production workers, the study found that 18 percent of the
pregnancies resulted in miscarriages, similar to the general population.
   The incidence of miscarriages among production workers involved in what
is known as photolithography, however, was 29 percent. A variety of solvents
are used in the process, which involves printing circuits on computer chips.
   Among workers in a phase of production that uses acids in an etching
process, researchers found a miscarriage rate of 39 percent, twice that of
the control group.
   Digital said it immediately passed along the findings to its workers.
   ``We've kept our employees informed all along,'' spokesman Jeffrey Gibson
said Tuesday. He said Digital adopted a policy during the study of
encouraging pregnant production workers to seek transfers.
   As a further precaution, Gibson said Digital also is offering to transfer
any female production worker of child-bearing age to non-production work if
they have concerns about future pregnancy.
   Gibson said Digital decided to do a study after employees began noticing
increased cases of miscarriages among their colleagues.
   Digital and the researchers stressed that the link between production-line 
work and increased miscarriages was only a statistical one and that no causal 
relationship between the health and specific chemicals had been established.
   The Semiconductor Industry Association, headquartered south of San
Francisco, said Digital sent it a summary of the findings and that the
information was passed along to 60 of its computer chip manufacturer members.
   ``The reaction (of manufacturers) was that the firms all felt an
obligation to communicate the information about the study to their
employees,'' said Shelia Sandow, association spokeswoman.
   The full study, conducted by Harris Pastides, an associate professor of
public health at the University of Massachusetts in Amherst, and Edward
Calabrese, a professor of toxicology, is still going through review before
publication in a medical journal.
   But Digital officials said they received a copy of the study last month,
and felt, along with its authors, a responsibility to release at least a
summary of the findings because of the health concerns.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
 Across the Atlantic with Cast Iron
</A>
</H3>
<address>
&lt;<A HREF="mailto: Boebert@HI-MULTICS.ARPA">
 Boebert@HI-MULTICS.ARPA
</A>&gt;
</address>
<i>
Wed, 31 Dec 86 09:53 CST
</i><PRE>
To:  risks@CSL.SRI.COM

I am appealing to RISKS readers because this is clearly the polymath's forum
...  I am collecting instances of generic pathologies in engineering project
management, such as cutting the budget for tools (example:  Brunel's ship
the Great Eastern, stranded on the banks of the Thames for months because
the money men would finance the ship but not the launching equipment.  This
was the Victorian equivalent of funding the software but cutting out the
debugger.)  In this vein, I recall seeing a classic case of Victorian
Vaporware, to wit, a proposed cast iron bridge over (I believe) the North
Atlantic.  This was in a book titled "Great Dreams of Victorian Engineers,"
or some such.  Anybody else recall this?  When I get the instances together
I will submit them to this list as an aid to separating risks which are
computer-specific from those which have been around since the dawn of
engineering.

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
 Heisenbugs -- Two more examples
</A>
</H3>
<address>
 "Maj. Doug Hardie" 
&lt;<A HREF="mailto:Hardie@DOCKMASTER.ARPA">
Hardie@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Wed, 24 Dec 86 11:12 EST
</i><PRE>
To:  risks@CSL.SRI.COM

I am reminded by the chain of discussions on Heisenbugs of two interesting
occurrences that I have been involved with.  The first occurred while in
college with an IBM 1620 (the last one IBM maintained).  One day while the
system was running student jobs, the operator was helping me prepare for a
microbiology test (flunkout class), the disk drive stopped functioning.  The
entire system locked up and we investigated.  There was nothing detectably
wrong, the system just wouldn't make the disk work.  Since it was under full
IBM maintenance, we called them.  However, the only person they who had ever
worked on that type of machine was a senior manager and was out of the area
on vacation, they sent the next best.  This tech arrived some time later and
began to try and figure out how it was supposed to work and what was going
on.  Since I was an EE, I "helped" him.  I learned a lot, he learned that
there was nothing wrong - it just didn't work.  After several hours, he
finally gave up and came and sat on a bench by me where I had returned to
microbiology.  All of a sudden, the disk heads jumped, the process picked up
as if nothing had happened, and the system was back in operation.  We tried
everything imaginable to make it fail again.  It continued to work fine for
several hours.  At that point, the tech packed up his tools, tore up his
time card, and left with the statement that he had never been there.

The second occurred a few years later on a military program that used a
militarized processor.  I had a contractor developing software and as usual
they were quite late.  So they took the step of scheduling work around the
clock.  One Monday morning a programmer came in complaining that he had lost
his weekend time.  He was scheduled from 1200 - 1300 on Sat.  Just as he got
on at 1200, the machine started slowing down.  The lights on the front panel
blinked slower and slower until they stopped.  Nothing he did made it start
running again, until 1300 when it started back up as if nothing ever
happened.  Needless to say, his management was not convinced.  However, when
someone else came in the next Monday with the same story, they decided to
investigate.  The next week they reported it to me with the same lack of
appreciation.  However, since the machine was GFE, we were responsible for
its proper operation.  So I got some higher-ups to contact the vendor to fix
it.  The vendor stated that such was absolutely not possible.  It took
several weeks to force them to send a tech out.  Sure enough when they did,
it performed exactly as advertised.  After 1300 when it came back up, the
tech started to leave without saying anything.  We cornered him by the front
door.  All he would say was, we've seen this before - it will go away.  He
was right, it went away after a few more weeks.

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Risks Involved in Campus Network-building
</A>
</H3>
<address>
"Wombat" 
&lt;<A HREF="mailto:rsk@j.cc.purdue.edu">
rsk@j.cc.purdue.edu
</A>&gt;
</address>
<i>
Wed, 24 Dec 86 09:44:10 EST
</i><PRE>

This little scenario popped into my mind after reading Chris Koenigsberg's
comments on plug-compatible plugs in RISKS 4-28.

Imagine a university campus utilizing local area networking in academic
buildings, dormitories, and other locations.  Now picture someone with a
reasonable aptitude for understanding the principles of LANs, and with
motivation to subvert the campus LAN...and whose dorm room contains a wall
socket marked "Southwest Campus Ethernet".

What can this person do, assuming that other people are using this same
physical network, and perhaps that this group of people extends beyond
those whose nodes are actually on the network to those whose nodes are
sending or receiving packets that are being routed over this network
(without their knowledge, assuming that they don't monitor packet routing)?

It seems quite plausible to me that such a person could tap into the
Ethernet and grab interesting packets (the person down the hall's report on
its way to a central printer; private correspondence between two residents;
perhaps a test in preparation being sent from a TA to a prof), and send
interesting packets (same as above, with slight modifications). 
                               [Lots of passwords are flowing as well...  PGN]

Further, it doesn't seem too unlikely that this scenario could be
extended; what could two or more people do in cooperation?  What
goodies could be pulled off the wire if one used a semi-smart program
(say, a keyword searcher) to examine traffic for interesting items?
Could an entire campus network be crippled by a few malicious users
with access to the hardware?  (I think the answer to this is "yes".)

The human consequences could be widespread and difficult to cope with; what
recourse does the student whose term paper disappeared off the network have?
How does one show that a student cheated on a test by gaining a copy the
night before via the network?  What obligation does the university have to
ensure the privacy of electronic mail over a network it designs, builds,
maintains, and supports for student use?  [Side question: could the campus
police monitor electronic mail for suspicious actions without a warrant?
After all, the senders of mail put their letters on a public (withing the
university) network...]

My opinion is that the kind of widespread network-building that's going on
at some colleges and universities is premature; it's a nice idea to build an
electronic village on campus, but peaceful villages have a habit of getting
overrun by barbarian hordes from time to time.  I'm waiting for the day when
the news comes that someone at CMU or Brown or wherever has done something
very antisocial with the campus network.  (Note that I distinguish between
those academic networks where access to the hardware is not provided, or is
at least made difficult to obtain, and those which purposefully provide
hardware access in many places.)

Rich Kulawiec, rsk@j.cc.purdue.edu, j.cc.purdue.edu!rsk
Purdue University Computing Center
</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Update on Swedish Vulnerability Board Report (RISKS 3.85)
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Fri, 02 Jan 87 15:56:10 -0500
From: Chuck Youman &lt;m14817@mitre.ARPA&gt;

In <A HREF="/Risks/3.85.html">RISKS-3:85</A> I referred to an article that appeared in Signal magazine
on "Computers, Vulnerability, and Security in Sweden."  I have since
written to the author of that article, Thomas Osvald, and he sent me an
English summary of a report by the Swedish Vulnerability Board titled
"The Vulnerability of the Computerized Society:  Considerations and
Proposals."  The report was published in December 1979.  The complete
report is only available in Swedish.  If anyone is interested in obtaining
the complete report I now have a mailing address to obtain publications
made by the Vulnerability Board (which no longer exists).  

The vulnerability factors considered by the Board included:
 -Criminal acts
 -Misuse for political purposes
 -Acts of war
 -Registers [i.e., databases] containing information of a confidential nature
 -Functionally sensitive systems
 -Concentration [geographic and functional]
 -Integration and interdependence
 -Processing possibilities in conjunction with the accumulation of large
  quantities of data
 -Deficient education
 -Defective quality of hardware and software
 -Documentation
 -Emergency planning

The original article in Signal magazine mentioned a project by the Board that
addressed the vulnerability problems associated with the complexity of EDP
systems.  This particular study is not mentioned in the summary.  However,
Mr. Osvald also sent me a copy of a position paper he authored on the
subject titled "Systems Design and Data Security Strategy."  Some excerpts
from the paper follow:

 Whether we like it or not our society is rapidly becoming more complicated,
 not the least as a consequence of the extremely rapid development of
 information processing and data communication.  Our times are also 
 characterized by increasingly large scale and long range decisions and 
 effects.  Unfortunately, this development does not correspond to a similar
 progress in our human ability to make wise decisions.  It is therefore
 important that we recognize the limits of the human mind and our ability to 
 to understand and process complicated, long range, decision problems.
 If complexity is not understood and kept within reasonable limits we will not 
 be able to control developments and we will become slaves rather than masters
 of our information systems.

 What are the characteristics of excessively super-complex systems?  One
 important symptom is that even experts find it hard or impossible to 
 understand or comprehend the totality of such a system.  The inability
 to comprehend is not an absolute criterion that does or does not exist
 but rather a vague feeling - mainly of uncertainly.  This basically goes
 back to the well-known fact that the human mind cannot deal with or keep
 track of more than about seven objects, entities or concepts at a time.
 Above that number, errors in the understanding and problem solving process
 increase disproportionately.

 Why are such systems designed?  I can think of three possible reasons.  The
 first is a strategy error of systems development that may be called
 "revolutionary change" or "giant step approach."  During the seventies some
 large, administrative government systems were re-designed in order to take
 advantage of new data processing and communication technology.  At the same
 time, as part of a huge "total" project, organization and administration
 were redesigned - all in one giant revolutionary change.  A better and more
 successful approach would have been - as it always is - to follow a 
 step-by-step master plan where each step is based on previous experience and 
 available resources.

 The second reason is the sometimes uncontrolled, almost cancer-like growth
 of large administrative systems, without a master plan and without clear
 lines of authority and responsibility, in efforts to integrate and to 
 exploit common data.

 The third reason is the inability of systems designers to identify the 
 problems of system complexity and our own inability to handle complex
 systems and to set a limit to growth and integration.
 
Charles Youman (youman@mitre.arpa)

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
DES cracked?
</A>
</H3>
<address>
Dave Platt
&lt;<A HREF="mailto:dplatt@teknowledge-vaxc.ARPA ">
dplatt@teknowledge-vaxc.ARPA 
</A>&gt;
</address>
<i>
Fri, 2 Jan 87 17:51:56 pst
</i><PRE>

There's an interesting article in the 1/87 issue of Radio-Electronics which
states that the Videocypher II television-scrambling system has been
cracked.  As Videocypher depends in some part on the DES cyphering
algorithm, this may have some major implications for computer-system
security (if it's true).

According to the article, "perhaps as many as several dozen persons or
groups have, independent of one another, cracked Videocypher II and we
have seen systems in operation.  Their problem now concerns what they
should do with their knowledge."

As I recall (and I may well be wrong), M/A-Com's Videocypher II system uses
two different scrambling methods: the video signal is passed through a
sync-inverter (or some similar analog-waveform-distorter), while the audio
is digitized and passed through a DES encryption.  Information needed to
decrypt the digital-audio is passed to the subscriber's decoder box in the
one of the "reserved" video lines.  The actual decryption key is not
transmitted; instead, an encyphered key (which itself uses the box's
"subscriber number" as a key) is transmitted, decrypted by the decoder box,
and used to decrypt the audio signal.

I've heard that it's not too difficult (in theory and in practice) to
clean up the video signal, but that un-DES'ing the audio is supposed
to be one of those "unfeasibly-difficult" problems.

I can think of three ways in which the Videocypher II system might be
"cracked".  Two of these ways don't actually involve "breaking" DES,
and thus aren't all that interesting;  the third way does.

Way #1:  someone has found a way of assigning a different "subscriber
number" to an otherwise-legitimate M/A-Com decoder, and has identified
one or more subscriber numbers that are valid for many (most?)
broadcasts.  They might even have found a "reserved" number, or series
of numbers, that are always authorized to receive all broadcasts.

This is a rather minimal "crack"; the satellite companies could defeat it by
performing a recall of all subscriber boxes, and/or by terminating any
reserved subscriber numbers that have "view all" access.

Way #2:  someone has found a way of altering a decoder's subscriber
number, and has implemented a high-speed "search for valid numbers"
circuit.  This could be done (in theory) by stepping through the
complete set of subscriber numbers, and looking for one that would
begin successfully decoding audio within a few seconds.  It should be
pretty easy to distinguish decoded audio from undecoded...

This way would be harder for the satellite companies to defeat;
they'd have to spread the set of assigned subscriber numbers out over
a larger range, so that the search for a usable number would take an
unacceptable amount of time.

Way #3: someone's actually found a way of identifying the key of a DES
transmission, with (or possibly without) the unscrambled "plaintext"
audio as a starting point.

This I find very difficult to believe... it would be difficult enough for
one person or group to do, let alone "perhaps as many as several dozen...
independent" groups.  Naturally, this possibility has the most severe
implications for computer-, organizational- and national security.

I suspect that the reported "cracking" of Videocypher II is a case (or
more) of Method #2, and thus doesn't have immediate implications for
the computer industry (I think).

Has anyone out there heard of any other evidence that DES itself has
been cracked?

Disclaimer: I don't own a TRVO (or even get cable TV), and have no financial
interest in anything related to the TVRO or cable industries.

Second disclaimer: as the Radio-Electronics article points out, it's
horrendously illegal to own or use any piece of equipment that "tampers with
DES or attempts to profit from decoding it" (the article suggests that such
action would be legally equivalent to treason, as DES is/may be under the
protection of the NSA until 4/22/87).  I don't know where such devices might
be purchased.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.34.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.36.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-113</DOCNO>
<DOCOLDNO>IA012-000125-B046-34</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.36.html 128.240.150.127 19970217010702 text/html 30269
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:05:28 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 36</TITLE>
<LINK REL="Prev" HREF="/Risks/4.35.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.37.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.35.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.37.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 36</H1>
<H2>Tuesday, 6 January 1987 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
A Heisenbug Example from the SIFT Computer 
</A>
<DD>
<A HREF="#subj1.1">
Jack Goldberg
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  More Heisen-debugs 
</A>
<DD>
<A HREF="#subj2.1">
Don Lindsay
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  The Conrail train wreck 
</A>
<DD>
<A HREF="#subj3.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Software glitches in high-tech defense systems 
</A>
<DD>
<A HREF="#subj4.1">
from Michael Melliar-Smith
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Computer program zeroes out fifth grader; Computerized gift-wrap 
</A>
<DD>
<A HREF="#subj5.1">
Ed Reid
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Videocypher, DES 
</A>
<DD>
<A HREF="#subj6.1">
Jerry Leichter
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  More on the possible DES crack 
</A>
<DD>
<A HREF="#subj7.1">
David Platt
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Campus LANs 
</A>
<DD>
<A HREF="#subj8.1">
James D. Carlson
</A><br>
<A HREF="#subj8.2">
 Don Wegeng
</A><br>
<A HREF="#subj8.3">
 Henry Spencer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj9">
  Engineering Ethics 
</A>
<DD>
<A HREF="#subj9.1">
Chuck Youman
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
A Heisenbug Example from the SIFT Computer
</A>
</H3>
<address>
Jack Goldberg 
&lt;<A HREF="mailto:JGOLDBERG@CSL.SRI.COM">
JGOLDBERG@CSL.SRI.COM
</A>&gt;
</address>
<i>
Tue 6 Jan 87 11:25:55-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

The following hardware bug was found in the debugging of the SIFT
fault-tolerant computer.  The memory was built of static RAM chips, in which
the memory cells were flip-flops.  Due to a defect in manufacture, the
cross-coupling of the flip-flops in some of the cells was capacitive rather
than conductive.  The effect was that the cells behaved perfectly when
exercised ("observed") frequently, but when information was stored and not
revisited, the charges on the cross-coupling capacitors would leak off and
the flip-flop would become unstable, perhaps switching state.  The quality
of the accidental capacitors was high, so it would take about twenty minutes
of inactivity (non-observation) for the event to occur.  The debugging
problem was compounded by the fact that numerous chips suffered from the
same manufacturing defect.  I won't enumerate all the hypotheses that were
tried before the phenomenon was identified.

A similar phenomenon has been found in logic circuits, associated with charge 
that may accumulate at unused gate inputs that were not properly connected
to a holding potential.  I am aware of some painful debugging experience
that that form caused in another fault-tolerant computer development.

The Heisenberg Risk is evident and easily generalized beyond the chip level
(one can imagine analogs at the program level).  It has substantial
implications for risks to system dependability, because it subverts several
conventional models of testing.  First, a person who is testing a defective
system usually assumes that the defect is due to a fault in the system, that
the fault is static, that there is some test (or test sequence) that will
reveal it, and that when the test is applied, the fault will be revealed
more or less immediately as an observable error.  This phenomenon says that
there may be some latency in the manifestation of a fault, and that the
latency may occur not only after a test sequence has been applied, but after
any element of the sequence has been applied.

A second subversion is to the standard practice of testing during
manufacture.  Chip manufacturers simply cannot afford to let chips stand in
their expensive testers for the time it would take to reveal such phenomena,
and system manufacturers also have practical time limits for their test
exercises.  In practice, such faults, hopefully rare, must be found and
coped with at other points in the system lifecycle.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
More Heisen-debugs
</A>
</H3>
<address>
&lt;<A HREF="mailto:LINDSAY@TL-20B.ARPA">
LINDSAY@TL-20B.ARPA
</A>&gt;
</address>
<i>
Sun 4 Jan 87 22:09:32-EST
</i><PRE>
To: risks@CSL.SRI.COM

I recently encountered a particularly infuriating Heisenbug. A large program,
when given a large input, was just mysteriously dying. Of course, I ran it
under the debugger. The mystery deepened: the program returned quietly
to the debugger. I say "mystery" because the call stack had been
unwound, and yet my breakpoints at the various exits were not reached.

My first reaction was to place a trail of breakpoints, with the idea of
seeing how far it got. Some results were obtained, but each time I tried to
refine the result, with a new set of breakpoints, the problem seemed to have
moved elsewhere.

The clue came when I tried to read some of the debugger's online documentation.
The (VMS 4.1) debugger refused to talk, and instead gave me a message about
a lack of resources. Aha ! The next step was to have an operator increase
my resource allocation ( actually, my maximum number of IO operations ). I
logged out, I logged in, and the problem was gone.

I have harsh words to say about an operating system which will kill a job,
without leaving any evidence that it did so. But, I leave these words to your 
imagination.

I have also had the privilege of a debugging session, done through the
communications software which was being debugged. In this case, I have
advice to novices. &lt;&lt; Keep notes. Good ones. &gt;&gt; Trust me.

Naturally, the hardware world has its share of these things. At one point,
PDP-8 maintainers knew that the fix for a certain kind of crash, was to
wave your hands near the backplane. (I am NOT kidding. Ask very old DEC hands.)

And then there was the hobbyist 8080 board whose clock worked, but only when a 
scope probe was applied to the clock line. Turned out that the capacitance of 
the scope probe overcame the cigar ash under the CPU socket ...

Don Lindsay

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
The Conrail train wreck
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Tue 6 Jan 87 19:16:38-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

It is too early to write the definitive piece on this, but there are various
conflicting reports.  The advance warning signal (back two miles on the main
track) may or may not have indicated GO (an up-bar) instead of CAUTION (a
slant-bar); the crossing locomotive engineer ran his stop signal; the cab
crew of the Conrail train had bypassed the emergency alarm that is supposed
to go off if they run a signal (as suggested by a PBS interview this evening, 
which indicated that three separate safety systems would have had to fail
simultaneously).  Stay tuned for the interpretation of the "event recorder".

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Software glitches in high-tech defense systems
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Tue 6 Jan 87 19:30:06-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

An article by Steve Johnson in the San Jose Mercury News (4 Jan 87)
listed a new bunch of problems.

  * A multimillion-dollar satellite network called "MILSTAR", which is
    supposed to link the president and top generals with tactical field
    units in wartime, is months behind schedule because of software
    troubles... (Lockheed)
  * A computerized system intended to help direct artillery fire for 
    soldiers at Fort Ord in Monterey County and other army bases is beset
    with software delays.
  * Two computer projects intended to make it easier to keep track of
    equipment inventories at the Naval Supply Center in Oakland [CA] and
    similar installations elsewhere have been held up because of software
    development problems.
  * Researchers at SRI International in Menlo Park a few years ago were
    hired to analyze a new "over-the-horizon backscatter" radar system that
    was supposed to detect attacking planes.  They found numerous software
    errors that meant months of delays in the system.

Elsewhere, the Air Force and Navy have had to postpone changes for the F-16C
and F-18 fighter jets because of software hitches...  Similar problems
have hurt... "LANTIRN" ... and "AMRAAM".  [The article also talks about SDI,
software costs escalating, and the shortage of (competent) engineers.]

"If we can't get substantial increase in (software) productivity, there is
just absolutely no way we can produce the amount of software the defense
industry needs in the next few years." (Dorothy McKinney, manager of the
software engineering department at Ford Aerospace (FACC) in Palo Alto.

                 [Thanks to Michael Melliar-Smith for bringing this one in.]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Computer program zeroes out fifth grader; Computerized gift-wrap
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Tue 6 Jan 87 19:46:17-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

Edward Reid dug into his archives for this one, from the Gadsden County
Times (FL), 25 Oct 1984.  One extra blank space between a fifth grader's
first name and his last name resulted in his getting a ZERO score on the
sixth-grade placement test.  Despite protests from his parents, he was
forced to reenter fifth grade.  It was six weeks into the new school year
before the test was finally regraded manually and the error detected.  (The
boy cried and wouldn't eat for days after he got the original score of
ZERO.)

Edward also produced a clipping from the Philadelphia Inquirer, 5 Dec 1986.
Computer printouts of the San Diego Unified School District's payroll
somehow did not make it to the shredder, instead winding up as Christmas
gift-wrapping paper in a local store (Bumper Snickers).  [Perhaps some of
the bumper crop wound up in the NY Mets' victory parade?]

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
 Videocypher, DES
</A>
</H3>
<address>
&lt;<A HREF="mailto:LEICHTER-JERRY@YALE.ARPA">
LEICHTER-JERRY@YALE.ARPA
</A>&gt;
</address>
<i>
5 JAN 1987 12:32:58 EST
</i><PRE>

Dave Platt mentions a Radio Electronics article concerning the breaking of
the Videocypher system, and speculates about the implications.

This whole issue got hashed around in sci.crypt a couple of weeks ago.  The
Radio Electronics article contains a LOT of nonsense, in its claims about the
illegality of breaking DES in particular.  Also, the claims that DES itself
has been broken are not credible.

The Videocypher system has at least two vulnerabilities:  Each box contains a
chip with a fixed key in it (the same in every box) which, if known, would
allow anyone to determine actual working keys and intercept transmissions.
Also, independent of the cryptography, the box itself makes a decision as to
whether to allow you to see a particular channel.

This allows at least to avenues of subversion:  Open a box and read the key
from the embedded chip, or take a box and change its decision procedure so
that it allows you to see channels you are not supposed to be able to see.
(As I understand it, given a valid subscriber key, any box CAN extract the
key for ANY channel - it just refuses to work on channels it is not supposed
to see.)

With enough equipment, it is possible to open up a chip, dissolve off the
epoxy it's embedded in, and read the contents of any PROM with a scanning
electron microscope.  I gather there ARE techniques for protecting chips
against this sort of probing, but they may be too expensive for boxes that
are supposed to sell for a couple of hundred dollars.  (They may also
involve booby traps that would be considered too dangerous in consumer
equipment.)

Meanwhile, "rsk" speculates about the vulnerabilities of campus local area
networks.  This is a REAL concern.  Ethernet, and all other LAN's I'm aware
of, are completely open to anyone who can gain physical access to them.
Listening in to any conversation is easy; spoofing is only a little harder.
Yes, problems will arise.

The solution is the use of well-understood cryptographic techniques.  As far
as I know, while these techniques are understood, there have as yet been few
implementations, mainly because of the expense involved.  (For many years,
billions of dollars a day were transfered "by wire" over telephone lines
with no real protection, cryptographic or otherwise.  It's only in the last
couple of years that concern about security, and technology, have reached
the point that these lines have been protected.)

I expect we will see a re-hash of the OS/360 hacker phenomenon.  (OS/360 had
so many security holes that many people broke into it.  It was never really
fixed, just replaced.)
							-- Jerry

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
More on the possible DES crack
</A>
</H3>
<address>
David Platt
&lt;<A HREF="mailto:dplatt@teknowledge-vaxc.arpa ">
dplatt@teknowledge-vaxc.arpa 
</A>&gt;
</address>
<i>
Tue, 6 Jan 87 09:46:20 PST
</i><PRE>

I just got a copy of the 2/87 issue of Radio-Electronics, which
contains brief descriptions of several of the systems that have
"cracked" the VideoCypher II scrambling system.

The systems described are all "software" approaches that fall into what
I described as "way #1"... they work by cloning copies of an authorized
subscriber number.  At least one has found a way to crack the "tiered
distribution" feature of VideoCypher, thus permitting someone who has
paid for only one service to successfully view several others.

None of the systems described so far actually involve a "cracking" of
DES itself... they're all methods of copying an existing (valid) key
from one decoder to another.  It appears that the MA-Com folks did take
some steps to conceal the subscriber number information (which
generates the actual key dynamically, I believe), but that their steps
were not sufficient.  Apparently, the subscriber-number is stored in
the battery-backed RAM in a small TI microprocessor, and there's no
direct way to query it; during operation, though, it's apparently
possible to trace the signals on some of the micro's pins and "catch"
the subscriber number as it flys by.  Someone has found a way to do
this and to "download" the number into the micro in another decoder...
thus permitting the "cloning" of an authorized number.

So, the vulnerability of the VideoCypher II system appears to boil down
to the fact that its "innards" aren't sufficiently guarded against
probing and/or modification.  If, for example, the box had been
provided with a cover-removal switch that would signal the micro to
erase its subscriber number, it might have been more difficult to
"crack".

A description of several "hardware" approaches is promised for next
month.  I'll summarize once I get my hands on an issue.

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
Campus LANs
</A>
</H3>
<address>
James D. Carlson
&lt;<A HREF="mailto:jc37#@andrew.cmu.edu ">
jc37#@andrew.cmu.edu 
</A>&gt;
</address>
<i>
Sun,  4 Jan 87 17:30:26 est
</i><PRE>

I am a student at Carnegie-Mellon University (Senior, EE) and I therefore
speak only for myself, not the Academic Computing Center.

First of all, in our system there are (basically) two types of files:  local
and network.  Local files, like the password file, are only rarely
transmitted over the network, and network files are maintained on the file
servers.  The password file, when transmitted, is in an encoded form anyway.
You will never see a raw password floating around the packets, at least they
tell me so.  Because of the way the network operates, it would be a lot
easier to get into the file servers themselves (false authentication, and so
forth) than to pick the information up on the net.

To the second part, the University's obligations, I think that they are the
same as with large computers.  If you used the computer to create a paper,
then lost it before the due date, tough! You knew the risks when you
requested the account.  As to the "wrongful" obtaining of information, such
as test questions, anyone who keeps highly sensitive information on a
computer in unencoded form gets what he deserves.  This is not US Mail, and
the same rules cannot apply here.

BTW, the Andrew system here is not quite complete (despite what the wire
services may be saying), and the main convenience of the system is that its
use is free, possibly because of the bugs.  We have many other systems
around that are MANY times faster, more secure, and more often even
*working*, like the IBM 3083 ...

</PRE>
<HR><H3><A NAME="subj8.2">
Re: Risks Involved in Campus Network-building
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
5 Jan 87 17:30:20 EST (Monday)
</i><PRE>
To: RISKS@CSL.SRI.COM
From: Don Wegeng &lt;Wegeng.Henr@Xerox.COM&gt;

I agree with Rich Kulawiec that a campus wide LAN is certainly subject
to a large number of potential security risks, but it seems to me that
such risks are present in any open computing environment. If an
instructor keeps a draft of an exam online, but does not read protect
the file, then any knowledgeable student with access to the system is
capable of making a copy of the exam. There are similar risks associated
with print spools, mail files, etc.

The presence of an LAN may make it difficult to detect some kinds of
security violations, but this isn't a new problem. Any computer
communications link that passes through uncontrolled space is subject to
the same kinds of risks as a campus network. The technology exists to
protect such links. I do not know whether the implementors of campus
networks have made use of this technology, but it's certainly a
reasonable question to ask.
                                             Don

</PRE>
<HR><H3><A NAME="subj8.3">
Risks Involved in Campus Network-building
</A>
</H3>
<address>
&lt;<A HREF="mailto:hplabs!pyramid!utzoo!henry@ucbvax.Berkeley.EDU">
hplabs!pyramid!utzoo!henry@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Tue, 6 Jan 87 16:53:46 pst
</i><PRE>

It can get worse.  Consider someone who is angry at the administration,
perhaps having just flunked out, been expelled, or whatever.  There is
some sophistication involved in doing things like watching the network
for passwords etc.  There is little or no sophistication needed to just
run some copper between the network cable and a 110V wall socket.  Not
only does this disrupt the network, it probably destroys a great deal of
equipment, and creates a serious safety hazard.  Good luck identifying
the culprit, too!  In most networking setups this would probably be
utterly untraceable once the connection was broken.

I see reason for worry about newer, cheaper local-networking schemes that
tend to run the network cable itself onto a board on each computer's
backplane.  Traditional thick-wire Ethernet is costly, but its transceivers
do provide thousands of volts of isolation between network and computer.
A disastrous fault on the network will only destroy transceivers.  Fiber
networks likewise provide inherent isolation.

The same problem exists, on a more modest scale, with existing setups
involving RS232 cables.  There the wiring is (probably) not a shared
resource, but the electronics on the other end are.  If your computer
facility casually runs RS232 cabling all over the building (as we do),
remember that this means your computer is plugged into a net of wire
with exposed pins in all kinds of places.  RS232 interfaces are seldom
opto-isolated, which is what would be needed to defend against electrical
flaws in such setups.

That net of wiring also makes a dandy lightning antenna.  That's one
reason, by the way, why a separate-box modem is almost always a better
idea than one that plugs into a backplane slot -- more isolation between
phone line and computer.
				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

     [Thanks.  Enough on this topic for now?  We seem to have plateued.  PGN]

</PRE>
<A NAME="subj9"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj9.1">
Engineering Ethics
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Fri, 02 Jan 87 11:47:56 -0500
From: Chuck Youman &lt;m14817@mitre.ARPA&gt;

The December 28 op-ed section of the Washington Post included an article
titled "The Slippery Ethics of Engineering" written by Taft H. Broome, Jr.
He is director of the Large Space Structures Institute at Howard University
and chairman of the ethics committee of the American Association of
Engineering Societies.  The article is too long to include in its entirety.
Some excerpts from the article follow:

 Until now, engineers would have been judged wicked or demented if they
 were discovered blantantly ignoring the philosopher Cicero's 2,000-year-old
 imperative:  In whatever you build, "the safety of the public shall be 
 the highest law."

 Today, however, the Ford Pinto, Three-Mile Island, Bhopal, the Challenger,
 Chernobyl and other technological horror stories tell of a cancer growing
 on our values.  These engineering disasters are the results of willful
 actions.  Yet these actions are generally not seen by engineers as being
 morally wrong. . . Some engineers now espouse a morality that explicitly
 rejects the notion that they have as their prime responsibility the
 maintenance of public safety.

 Debate on this issue rages in the open literature, in the courts, at public
 meetings and in private conversations. . . This debate is largely over four
 moral codes--Cicero's placement of the public welfare as of paramount
 importance, and three rival points of view.

 Significantly, the most defensible moral position in opposition to Cicero
 is based on revolutionary ideas about what engineering is.  It assumes that
 engineering is always an experiment involving the public as human subjects.
 This new view suggests that engineering always oversteps the limits of
 science.  Decisions are always made with insufficient scientific information.

 In this view, risks taken by people who depend on engineers are not merely
 the risks over some error of scientific principle.  More important and
 inevitable is the risk that the engineer, confronted with a totally novel
 technological problem, will incorrectly intuit which precedent that worked
 in the past can be successfully applied at this time.

 Most of the codes of ethics adopted by engineering professional societies
 agree with Cicero that "the engineer shall hold paramount the health,
 safety and welfare of the public in the performance of his professional 
 duties."

 But undermining it is the conviction of virtually every engineer that totally
 risk-free engineering can never be achieved.  So the health and welfare of
 the public can never be completely assured.  This gets to be a real problem
 when lawyers start representing victims of technological accidents.  They
 tend to say that if an accident of any kind occurred, then Cicero's code
 demanding that public safety come first was, by definition, defiled, despite
 the fact that such perfection is impossible in engineering.

 A noteworthy exception to engineer's reverence for Cicero's code is that of
 the Institute of Electrical and Electronics Engineers (IEEE)--the largest
 of the engineering professional societies.  Their code includes Cicero's,
 but it adds three other imperitives opposing him--without giving a way to
 resolve conflicts between these four paths.

 The first imperative challenging the public-safety-first approach is called
 the "contractarian" code.  Its advocates point that contracts actually exist
 on paper between engineers and their employers or clients.  They deny that
 any such contract exists--implied or explicit--between them and the public.
 They argue that notions of "social" contracts are abstract, arbitrary and
 absent of authority.

 [The second imperative is called] the "personal-judgment" imperative.  Its
 advocates hold that in a free society such as ours, the interests of business
 and government are always compatible with, or do not conflict with, the 
 interests of the public.  There is only the illusion of such conflicts. . .
 owing to the egoistic efforts of:

 -Self-interest groups (e.g. environmentalists, recreationalists);

 -The few business or government persons who act unlawfully in their own
  interests without the knowledge and consent of business and government; and

 -Reactionaries impassioned by the loss of loved ones or property due to
  business-related accidents.

 The third rival to public-safety-first morality is the one that follows 
 from the new ideas about the fundamental nature of engineering.  And they
 are lethal to Cicero's moral agenda and its two other competitors.

 Science consists of theories for claiming knowledge about the physical world.
 Applied science consists of theories for adapting this knowledge to individual
 practical problems.  Engineering, however, consists of theories for changing
 the physical world before all relevant scientific facts are in.

 Some call it sophisticated guesswork.  Engineers would honor it with a
 capitalization and formally call it "Intuition." . . . It is grounded in
 the practical work of millenia, discovering which bridges continue to stand,
 and which buildings.  They find it so compelling that they rally around its
 complex principles, and totally rely on it to give them confidence about what
 they can achieve.

 This practice of using Intuition leads to the conclusion put forward by
 Mike Martin and Roland Schinzinger in their 1983 book "Ethics in Engineering":
 that engineering is an experiment involving the public as human subjects.

 This is not a metaphor for engineering.  It is a definition for engineering.

 Martin and Schinzinger use it to conclude that moral relationships between
 engineers and the public should be of the informed-consent variety enjoyed
 by some physicians and their patients.  In this moral model, engineers would
 acknowledge to their customers that they do not know everything.  They would
 give the public their best estimate of the benefits of their proposed 
 projects, and the dangers.  And if the public agreed, and the engineers 
 performed honorably and without malpractice, even if they failed, the public
 would not hold them at fault.

 However, most engineers regard the public as insufficiently informed about
 engineering Intuition--and lacking the will to become so informed--to assume
 responsibility for technology in partnership with engineers (or anyone else).
 They are content to let the public continue to delude itself into thinking
 that engineering is an exact science, or loyal to the principles of the
 conventional sciences (i.e., physics, chemistry).

Charles Youman (youman@mitre)

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.35.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.37.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-114</DOCNO>
<DOCOLDNO>IA012-000125-B046-61</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.37.html 128.240.150.127 19970217010718 text/html 19541
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:05:45 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 37</TITLE>
<LINK REL="Prev" HREF="/Risks/4.36.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.38.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.36.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.38.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 37</H1>
<H2> Wednesday, 7 January 1987 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Re: vulnerability of campus LANs 
</A>
<DD>
<A HREF="#subj1.1">
Ted Lee
</A><br>
<A HREF="#subj1.2">
 David Fetrow
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Re: DES cracked? 
</A>
<DD>
<A HREF="#subj2.1">
Henry Spencer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Cellular risks 
</A>
<DD>
<A HREF="#subj3.1">
from Geoff Goodfellow via PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  "Letters From a Deadman" 
</A>
<DD>
<A HREF="#subj4.1">
Rodney Hoffman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Stock Market Volatility 
</A>
<DD>
<A HREF="#subj5.1">
Randall Davis
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Engineering ethics 
</A>
<DD>
<A HREF="#subj6.1">
Dick Karpinski
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Computerized Discrimination 
</A>
<DD>
<A HREF="#subj7.1">
Ken Laws
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
 Re: vulnerability of campus LANs
</A>
</H3>
<address>
&lt;<A HREF="mailto: TMPLee@DOCKMASTER.ARPA">
 TMPLee@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Wed, 7 Jan 87 00:03 EST
</i><PRE>
To:  Risks@CSL.SRI.COM

Unless they're encrypted, of course they'll be busted wide open.  I can
remember in the late 60's the very first thing science or engineering
students did at MIT and Harvard once they found out about the telephone
tie lines was to see how far they could get (legally.)  (you see, from
Harvard you could get to MIT, from MIT to Mitre Bedford, from there to
Washington, ...)  (what got the freshman all excited was strange numbers
that only answered "extension 55" or just "Yes?") (And I'm not talking
about the blue-boxers either, which was big at the same time.)  The
mentality certainly hasn't changed ...

</PRE>
<HR><H3><A NAME="subj1.2">
Risks Involved in Campus Network-building
</A>
</H3>
<address>
David Fetrow
&lt;<A HREF="mailto:fetrow@entropy.ms.washington.edu ">
fetrow@entropy.ms.washington.edu 
</A>&gt;
</address>
<i>
Wed, 7 Jan 87 01:09:58 PST
</i><PRE>

  From: "Wombat" &lt;rsk@j.cc.purdue.edu&gt;
  &gt; Imagine a university campus utilizing local area networking in academic
  &gt; buildings, dormitories, and other locations.  Now picture someone with a
  &gt; reasonable aptitude for understanding the principles of LANs, and with
  &gt; motivation to subvert the campus LAN...and whose dorm room contains a wall
  &gt; socket marked "Southwest Campus Ethernet".

 This particular scenario is partly avoidable by segmentizing the network:
Using Bridges to isolate sections of the cable so that packets that don't
need to be show up on the "dorm" cable, don't. (The Bridges must be secure
of course). This at least removes the temptation of ultra-casual attacks.

 Networking the campus may be "premature", in the sense we are courting a
certain amount of disaster and we know it. We also know we need a lot more
bandwidth than RS-232 can provide. In this case perhaps the right strategy
isn't so much trying to prevent disaster but preparing for it. We've been
here before (the easily cracked operating systems of the mid-70s'). The way
secure (relatively) systems happened was by learning how their non-secure
predecessors were attacked and fixing the holes just a little faster than
90% of the attackers found them.

-Dave "Very Worried" Fetrow-      

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Re: DES cracked?
</A>
</H3>
<address>
&lt;<A HREF="mailto:hplabs!pyramid!utzoo!henry@ucbvax.Berkeley.EDU">
hplabs!pyramid!utzoo!henry@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Tue, 6 Jan 87 16:51:35 pst
</i><PRE>

Rumor hath it that the Videocypher II cracking exploited defects in the
key-management scheme rather than a successful cryptanalysis of full DES.

&gt; Second disclaimer: as the Radio-Electronics article points out, it's
&gt; horrendously illegal to own or use any piece of equipment that "tampers with
&gt; DES or attempts to profit from decoding it" (the article suggests that such
&gt; action would be legally equivalent to treason, as DES is/may be under the
&gt; protection of the NSA until 4/22/87)...

As has been discussed at some length in sci.crypt, this is utter nonsense.
There is nothing illegal about breaking DES in your back yard, although there
are various possible illegalities involved in *using* a DES-breaker for
purposes like watching encrypted TV.  DES is not under NSA's protection, and
never has been.  The R-E article notwithstanding, the US government does not
use DES for its own communications.  And the claim of treason is ludicrous:
treason requires open aid to the US's enemies, including at least one overt
act with multiple eyewitnesses.  Being convicted of treason for anything less
is literally unconstitutional -- the US Constitution itself defines treason
to require these things.  M/A-Com is just trying to scare people.

				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Cellular risks
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
6 Jan 1987 13:37-PST
</i><PRE>
From: Neumann@CSL.SRI.COM
To: RISKS@CSL.SRI.COM

A long time ago Geoff Goodfellow reported on the ease with which one could
spoof the cellular billing.  Here is a more recent comment from him.
(GEOFF@CSL.SRI.COM)

  Fraud and spoofing seem to be on the rise in cellular, 
  with one carrier reportedly suffering at the rate of $180K/mo.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
"Letters From a Deadman"
</A>
</H3>
<address>
&lt;<A HREF="mailto:Hoffman.es@Xerox.COM">
Hoffman.es@Xerox.COM
</A>&gt;
</address>
<i>
7 Jan 87 12:49:05 PST (Wednesday)
</i><PRE>
To: RISKS@CSL.SRI.COM

According to an article by Howard Rosenberg in today's 'Los Angeles
Times', "Letters From a Deadman" is a Soviet-made movie about a nuclear
holocaust triggered by a critical computer error.  Dubbed in English,
the 85-minute film is scheduled to air Feb. 12, on WTBS, Ted Turner's
Atlanta-based cable super-station.                   From the article:

  The movie's central character is a man named Larsen, who is initially
  seen writing to his dead son from an underground bunker.  Larsen is the
  scientist who developed the computers whose error triggered a
  devastating missile exchange that destroyed his family and country.
  Whatever country that is.

  "It's set in Western Europe, " said Martin Killeen, the WTBS producer on
  the movie project.  "It could just as easily be Eastern Europe....
  Having it set in a Western country, I think, allows the film makers more
  freedom.  Obviously, in the Soviet mind, this [making a mistake that
  causes nuclear holocaust] is not something they would do.  I just can't
  see them doing a story about a computer error if it were in the Soviet
  Union."

-- Rodney Hoffman

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Stock Market Volatility
</A>
</H3>
<address>
Randall Davis 
&lt;<A HREF="mailto:DAVIS%OZ.AI.MIT.EDU@XX.LCS.MIT.EDU">
DAVIS%OZ.AI.MIT.EDU@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Wed 7 Jan 87 12:23-EST
</i><PRE>
To: risks@CSL.SRI.COM

Add to the risks of computers the danger of wider and faster dissemination of
misinformation (or at least incomplete information): several postings in the
last few months have considered whether computerized stock trading might be
causing the wild volatility seen in the market recently.  But no one seems to
have asked an important question: was there in fact any markedly higher
volatility.  The answer may in fact be no.

The December 86 issue of Money has an interesting 1-page article with a graph
of stock market volatility, measured as "annualized monthly standard deviation
of the S&amp;P 500", and there's the key issue: how to measure it.  On their
standard, the highest period is a clear peak around 1937, with lesser peaks
around '62, '70, and '74.  Since programmed trading began (in 1982, despite
all the newspaper articles that make it appear to have been invented
yesterday), volatility has in fact DIMINISHED and has only recently begun to
head upward again toward the level of the (smaller) '62 and '70 peaks.

Their interesting claim is that with programmed trading
    "... there is a risk that an innocuous market downturn may be
    greatly magnified.  So far, however, programmed trading has
    proved to have few lingering effects on stocks.  It can
    compress a market movement that would otherwise take a day --
    or even a week -- into a period as short as 10 minutes.  But if
    a market move would not otherwise have occurred, it is likely
    to reverse itself within a few days.... while the market's
    volatility is a bit higher this year than it has been in the
    past three years, it remains quite normal by historical
    standards."

Note in particular the last seven words.

I am neither economist enough nor statistician enough to judge whether their
metric is appropriate, but there are several important overall issues here:

1) The issue requires non-trivial economic and statistical sophistication.
The half-assed analyses widely quoted are appallingly naive in part because
they never even question whether the issue may be deeper than watching the
daily averages and seeing meaningless records set.

2) The media in general want NEWS, something dramatic that has never happened
in the history of the universe and that may in the next 18 seconds lead to the
collapse of civilization.  The story is even better if it involves something
that a large number of people find inherently threatening, and technology --
particularly computer-related -- is a favorite candidate (nuclear energy, gene
splicing and various diseases rank up there pretty high too).  All this, plus
the press of time to get to press lead to two serious faults:

a) not asking the obvious questions: "Has this happened before; is it really
unusual"  Often the answers are yes, and no, respectively.  But what a boring
story that would make.

B) not questioning the premises: the market drop of 86 points on September 11
was the LARGEST IS HISTORY, omigod!  Yes, but it was only the third largest in
terms of percentage.  And what's the right measure anyway?  Absolute points,
percentages?  And why 1 day?  What's sacred about the market's performance
over a 1-day trading cycle?  Why not a week or a month or a year or a business
cycle?  Why doesn't anyone worry about the biggest 1-hour drop on record or
the biggest 10 minute decline?  What is the relevant metric?  Is the alleged
phenomenon even real?

3) Our agenda in RISKS should be to debunk, not contribute to misinformation.
Where our technical skills are relevant, we can do that particularly well.
Where they are not (as in the need here for economic and statistical savvy),
we should tread quite carefully.  We too need to remember to question the
assumptions.

4) There's risk in incorrect and incomplete information; there's
computer-related risk when that information is widely disseminated
electronically: 
	the British telephone billing scam that apparently wasn't; 
	the automated bibliographic retrieval system that required keywords 
	in the article title (only it didn't);
	more recently the illegal cracking of DES that wasn't illegal and
	didn't happen;
	and perhaps the stock market volatility that isn't.  
We should be particularly aware of this misinformation risk since it is 
entirely under our control.

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Engineering ethics
</A>
</H3>
<address>
Dick Karpinski
&lt;<A HREF="mailto:dick@cca.ucsf.edu ">
dick@cca.ucsf.edu 
</A>&gt;
</address>
<i>
Wed, 7 Jan 87 17:43:36 PST
</i><PRE>
To: risks@sri-csl.ARPA
Keywords: wartime risks

Cicero's rule notwithstanding, there are many cases of opposition twixt
risks of doing versus risks of not doing.  I recall, for example, that our
H.J. Kaiser offered to build troop carriers rather quickly using rivets
instead of welded seams.  I'm too young to remember whether his offer was
accepted, but it seems clear that he was not denounced for being prepared to
make less seaworthy ships, which therefor increased the risks of loss of
life during troop transport.  The alternative was increased risks of loss of
life at the front lines of WWII.

I am prepared to accept a dollar value on human life in order to discuss
these decisions in reasonable ways.  Many, even most, people are not so
prepared and would consider me to be a barbarian beast on just those
grounds.  Perhaps it will be necessary to do some heavy duty education (of
which side?) before consensus can be reached.  Incidentally, my guess is
that currently, we should value one human life somewhere between $100k and
$1m.  The risks of failing to do so are in the nature of making the
necessary choices on arbitrary or irrational grounds, or in hiding the
decision entirely from view (and finding scapegoats as needed).

Dick Karpinski    Manager of Unix Services, UCSF Computer Center
UUCP: ...!ucbvax!ucsfcgl!cca.ucsf!dick   (415) 476-4529 (11-7)
BITNET: dick@ucsfcca   Compuserve: 70215,1277  Telemail: RKarpinski
USPS: U-76 UCSF, San Francisco, CA 94143-0704

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Computerized Discrimination
</A>
</H3>
<address>
Ken Laws 
&lt;<A HREF="mailto:LAWS@SRI-IU.ARPA">
LAWS@SRI-IU.ARPA
</A>&gt;
</address>
<i>
Wed 7 Jan 87 15:54:13-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

I just caught up with the Risks discussion and noticed two messages on
computerized discrimination against women and blacks applying to a medical
school.  Randall Davis made the implicit assumption that the discrimination
consisted of a rule subtracting some number of points for sex and race,
and questioned whether the programmer shouldn't have blown the whistle.

I think it much more likely that the decision function was a regression
equation that happened to include coefficients combining sex and
race with other predictor variables.  The programmer -- or statistician,
probably -- would have done this out of carelessness or simply to obtain
the best possible fit to the admissions decisions in the database.  The
school administration would have accepted the formula as valid, probably
without even examining it, if it correctly classified the past applicants
and performed reasonably on the new ones.  I'm not too surprised that
no one paid attention to the sign or magnitude of the coefficients.

So much for the mechanism of this computer (or statistical) risk.  Now
I'd like to put in a few words in defense of the statistical approach.

Suppose you had to screen equal numbers of male and female applicants
and you wanted to admit them equally.  Suppose further that women tended
to have higher verbal scores.  If you used only these scores, too many
women would be admitted.  It would be necessary for you to balance the
high scores, either by subtracting something for being female or by
boosting the coefficient for some male-dominated variable (e.g., math
scores).  This type of twiddling is exactly what a regression program
does.  It selects whichever adjustment (or combination of adjustments)
gives the best fit.  The program could produce exactly the same
results, or discrimination, even if you forced it to use &gt;&gt;positive&lt;&lt;
coefficients for female and black codes.

I'm not suggesting that the school's formula was a good one.  They
should have ignored sex and race unless they intended to set quotas.
By matching a database of past decisions they were undoubtedly
freezing any biases that had existed in the past; perhaps the formula
recorded these biases accurately.

I am suggesting that the individual coefficients in a regression
formula have little meaning unless you consider all of the
intercorrelations and do a proper sensitivity analysis.
The article said that this school had a good admissions record, so
people shouldn't be hasty in putting them down.  Let he who fully
understands his own database cast the first stone.

Also: statistical tools are powerful in the right hands, dangerous
in the wrong ones.  Don't assume that you can do a regression just
because your micro can do one.  If your data is worth being analyzed,
it is probably worth being analyzed by a professional.  And if you
really want good results, work with the professional from the start
instead of collecting the data and mailing it in for an analysis.

					-- Ken Laws

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.36.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.38.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-115</DOCNO>
<DOCOLDNO>IA012-000125-B046-83</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.38.html 128.240.150.127 19970217010741 text/html 21119
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:06:03 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 38</TITLE>
<LINK REL="Prev" HREF="/Risks/4.37.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.39.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.37.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.39.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 38</H1>
<H2> Thursday, 8 January 1987 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
As the year turns ... 
</A>
<DD>
<A HREF="#subj1.1">
Jeffrey Mogul
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Automobile micros 
</A>
<DD>
<A HREF="#subj2.1">
Hal Murray
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Chemicals in semiconductor manufacturing 
</A>
<DD>
<A HREF="#subj3.1">
Michael Scott
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Cellular -- Ref to Geoff 
</A>
<DD>
<A HREF="#subj4.1">
via PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  "Misinformation"?? 
</A>
<DD>
<A HREF="#subj5.1">
Dick Karpinski
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Burnham Book -- A Recommendation 
</A>
<DD>
<A HREF="#subj6.1">
Alan Wexelblat
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Engineering Ethics 
</A>
<DD>
<A HREF="#subj7.1">
Dan Ball
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Re: Stock Market Volatility 
</A>
<DD>
<A HREF="#subj8.1">
Richard A. Cowan
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
As the year turns ...
</A>
</H3>
<address>
Jeffrey Mogul
&lt;<A HREF="mailto:mogul@decwrl.DEC.COM ">
mogul@decwrl.DEC.COM 
</A>&gt;
</address>
<i>
8 Jan 1987 1846-PST (Thursday)
</i><PRE>

A number of sites, including my own, have special receivers for the time
signal transmitted by the GOES satellite; the time information comes from
the National Bureau of Standards, and with a properly adjusted setup you can
set your computer's clock to with a few milliseconds.  Since the clocks are
somewhat expensive, many hosts instead slave their clocks to one of the
hosts endowed with its own satellite receiver.

The data stream from the NBS tells you what time of day it is, and what day
of the year it is, but it does not say what year it is.  The usual practice
is to assume that the local host knows what year it is, and to get the
correct time you combine the satellite clock's time-within-year with your
local knowledge of the year.

Needless to say, this doesn't quite always work.  Mostly, it tends
to not work on New Year's Eve, when many of us would rather not be
fixing our computers.  Dave Mills does a great job keeping a bunch of
clocks running, on which many other hosts on the Internet depend.  This
is his message from early on January 1st:

  From: mills@huey.udel.edu
  Subject: Ask not for whom the chimes tinkle
  To: tcp-ip@sri-nic.arpa, nsfnet@sh.cs.net

  Folks, Every year it's the same - I forget UT midnight comes five hours
  before the ball drops in Times Square. For an hour and sixteen minutes after
  the hoot and holler in Trafalgar Square at least four radiofuzz timetellers
  still squawked yesteryear.  DCN1, UMD1, FORD1 and NCAR springs have now been
  rewound to 1987 and all you guys can forget those whopping disk-usage
  refunds. Thanks to Hans-Werner Braun, who reminded me of my annual first
  duty of the new year and annual first resolution to figure out how to avoid
  paw to keyboard in the absence throughout the world, as far I know, of a
  highly reliable electronic way to find out what year it is.

It turns out that as recently as today, several Internet sites are still
stuck in 1986, apparently as a result of the efficient distribution of
faulty time information.

Meanwhile, I thought I had foreseen all eventualities and fixed the
program used here at DECWRL, so that as the year turned it would avoid
becoming confused.  The important thing is to be sure not to try to
set the time during a period where the satellite clock thinks it is one
year and the local clock thinks it is a different year.  Needless to
say, I got this part wrong, and our clocks promptly jumped ahead exactly
one year.  I found at least two bugs in my code, but I still don't
completely understand what went wrong, and I'm sure something is going
to go wrong again next year.

I guess the lesson is that it is wrong to assume that the least
significant bits are the hardest to get right.  (One reader of
TCP-IP told of how he had to use a similar year-less time format
when designing a missile-tracking system 20 years ago, and had
to put in special logic to be sure that the system could survive
the confusion on New Year's Eve.)  Now, if I could only make it
through January without writing "1986" on any checks.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Automobile micros
</A>
</H3>
<address>
&lt;<A HREF="mailto:Murray.pa@Xerox.COM">
Murray.pa@Xerox.COM
</A>&gt;
</address>
<i>
Thu, 8 Jan 87 12:31:37 PST
</i><PRE>
To: RISKS@CSL.SRI.COM

Our hero, Joe, works for one of the "big 3" automakers near Detroit, that
strange corner of the US where everybody a late model American car.  One day, 
Joe was calmly driving down the highway, accelerating gently, when his car
stuttered a bit. It wasn't a big deal. Most people probably wouldn't have
noticed it. However, Joe's job was programming the small computer that controls
the gas and timing for car engines, so he this behavior caught his attention.

When Joe got to work, he popped the cover off the computer in his car and
took the main chip into the lab. After a bit of work, he managed to
reconstruct the necessary input conditions, and sure enough the glitch was
real. Happy that he had tracked down a minor problem in has car, Joe prowled
around the lab for replacement chip. It didn't take long to find one.

Rather than just installing the new chip in his car, Joe decided to try
it in his test rig. You guessed it, the "good" chip had the same
problem. So did several others - all the ones he found to try.

The story ended there. My guess was a PROM bug, but I didn't get that
from the horses mouth.

Speaking of risks, the first time that GM does a major recall because of
software is going to get a lot of publicity. I'll bet much of it will be
mud for the computer profession rather than teaching the public about
the realities and economics of software.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Chemicals in semiconductor manufacturing
</A>
</H3>
<address>
Michael Scott  
&lt;<A HREF="mailto:scott@rochester.arpa">
scott@rochester.arpa
</A>&gt;
</address>
<i>
Thu, 8 Jan 87 15:12:22 est
</i><PRE>

Several submissions recently have concerned the risks of miscarriages
and other health problems associated with semiconductor manufacturing.
For anyone interested in the subject, I highly recommend the cover
story of the October 1985 issue of the Progressive magazine: "Dead End
in Silicon Valley" by Diana Hembree.  Where recent attention has
focussed on IC fabrication, the Progressive article is mainly about PC
board assembly, where low-paid semi-skilled workers, mostly women, are
reportedly exposed to large numbers of toxic, allergenic, and carcinogenic
chemicals, with a shocking array of side effects.  To obtain background
information, Hembree took a job for four months as an assembler at Q.E.S.
Corp. in Santa Cruz.  Her story makes pretty grim reading.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: re: Cellular -- Ref to Geoff
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Thu 8 Jan 87 11:28:30-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

I had several queries about how could one possibly spoof the cellular phone
system?  Some of you will recall the earlier contribution from Geoff
Goodfellow in <A HREF="/Risks/3.10.html">RISKS-3.10</A> noting that it is indeed utterly trivial to change
your ID.

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
"Misinformation"??
</A>
</H3>
<address>
Dick Karpinski
&lt;<A HREF="mailto:dick@ccb.ucsf.edu ">
dick@ccb.ucsf.edu 
</A>&gt;
</address>
<i>
Thu, 8 Jan 87 17:24:30 PST
</i><PRE>

In RISKS DIGEST 4.37
  Stock Market Volatility (Randall Davis)

&gt;Date: Wed 7 Jan 87 12:23-EST
&gt;From: Randall Davis &lt;DAVIS%OZ.AI.MIT.EDU@XX.LCS.MIT.EDU&gt;
&gt;... 
&gt;4) There's risk in incorrect and incomplete information; there's
&gt;computer-related risk when that information is widely disseminated
&gt;electronically: 
&gt;	the British telephone billing scam that apparently wasn't; 
&gt;	the automated bibliographic retrieval system that required keywords 
&gt;	in the article title (only it didn't);
&gt;	...
&gt;We should be particularly aware of this misinformation risk since it is 
&gt;entirely under our control.

I don't recall being satisfied that there was no British phone scam.
What was it that convinced you?

   [It is altogether possible that BT is covering up.  On the other hand,
   their description of the system (by phone, to me) stated that the
   READ-AFTER-WRITE check is properly implemented and that there are three 
   other checks as well.  They claim that the Sunday Post will print a
   retraction.  (As yet no one has reported seeing it.)  Of course, there
   may be still be other vulnerabilities.  RISKS readers are learning to look 
   the proverbial gift horse in the mouth, as well as the horse you had to
   pay a fortune for.  PGN]

The bibliographic retrieval system is worse that had been alleged in
the 29 Sep 86 Risks 3.70.  It is not a "new policy" according to one
Paul Ryan of the DTIC in 4.23, but a limitation of their software.
But the fact is that only the first five words (including articles 
and prepositions) of titles are involved in automatic searches.  This
strikes me as an unconsciencable restriction to be removed as soon
as practicable.  I would certainly hesitate to count on such a 
system.  For example, Parnas' seminal CACM article on modularity
("On the Criteria to be Used in Decomposing Systems into Modules")
would only show up in searches for "On", "the", "Criteria", "to",
and "be".  What a travesty of search, retrieve, and help!

    ["To be or not to be..." would show up even more dramatically!  PGN]

"We should be particularly aware of this misinformation risk since it is 
entirely under our control."

How can I offer to help these poor souls correct (improve) their
shabby software?  How else but by discussing these problems can we
become informed about the sad existing conditions?

Dick Karpinski  Manager of Unix Services, UCSF Computer Center
UUCP:  ...!ucbvax!ucsfcgl!cca.ucsf!dick        (415) 476-4529 (11-7)
BITNET:  dick@ucsfcca or dick@ucsfvm           Compuserve: 70215,1277  
USPS:  U-76 UCSF, San Francisco, CA 94143-0704   Telemail: RKarpinski   

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Burnham Book -- A Recommendation
</A>
</H3>
<address>
Alan Wexelblat 
&lt;<A HREF="mailto:wex@mcc.com">
wex@mcc.com
</A>&gt;
</address>
<i>
Thu, 8 Jan 87 10:40:59 CST
</i><PRE>

Some time ago, Dave Taylor (on mod.comp-soc) recommended a book called "The
Rise of the Computer State" by David Burnham.  I have purchased this book and
hereby recommend it to RISKS readers.  Burnham is an investigative reporter,
so the book tends to have a bit of a sensationalistic streak, but it is very
interesting and covers many topics of interest to RISKS readers.  The edition
I have is softcover, published in 1984 by Vintage books for $6.95.  It's 
ISBN 0-394-72375-9.

People like PGN who collect RISKS-anecdotes may be interested in some of the
stories he tells (like the part played by punch-cards in the 1942 roundup of
Japanese-Americans).

Alan Wexelblat
ARPA: WEX@MCC.ARPA or WEX@MCC.COM
UUCP: {seismo, harvard, gatech, pyramid, &amp;c.}!ut-sally!im4u!milano!wex

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Engineering Ethics
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Thu, 08 Jan 87 11:29:37 -0500
From: ball@mitre.ARPA &lt;Dan Ball&gt;

The discussions concerning engineering ethics in RISKS 4.36 and 4.37
overlook what I think is a far more critical contributor to modern
engineering disasters than the personal ethics (or lack thereof) of
individual engineers: the organizational environment in which engineers
must function.

Large engineering projects involve many thousands of engineers, and the
time required to complete them has stretched, in many cases, to over
twenty years.  In this environment, it can be difficult for an individual
to feel any personal responsibility for the outcome of the overall project.
Most of the engineers I know are neither "demented" nor "morally wicked."
They are just trying to do their job in the midst of a bureaucracy where
authority is diffused and decisions are made by committee.  It is to be
expected that short-term expediency will usually prevail, particularly
when it is difficult or impossible to assess the long-term consequences
of a decision.

The organizational dynamics involved in the development and operation of
safety-critical systems and their effect on the individuals concerned are
submit, far more important than the contemplation of Cicero's ethics.

Although I don't consider Dick Karpinski a "barbarian beast", I question
whether assigning a monetary value to human life would provide additional
insight into the management of risks.  I am not convinced that we know
how to predict risks, particularly unlikely ones, with any degree of
confidence.  I would hate to see a $500K engineering change traded off
against a loss of 400 lives @ $1M with a 10E-9 expected probability.
I'm afraid reducing the problem to dollars could tend to obsure the
real issues.

Moreover, even if the analyses were performed correctly, the results could
be socially unacceptable.  I suspect that in the case of a spacecraft, or
even a military aircraft, the monetary value of the crew's lives would 
be insignificant in comparison with other program costs, even with a
relatively high hazard probability.  In the case of automobile recalls,
where the sample size is much larger, the manufacturers may already be 
trading off the cost of a recall against the expected cost of resulting
lawsuits, although I hope not.

Clearly, though, those of us concerned with safety need to find some
way of seeing that risks are effectively managed in large projects.
It is not enough to act as a perpetual doomsayer standing in the way
of progress.  To be effective, safety engineers must be perceived as
helpful and participate in the mainstream of the design activity.

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
Re: Stock Market Volatility
</A>
</H3>
<address>
Richard A. Cowan 
&lt;<A HREF="mailto:COWAN@XX.LCS.MIT.EDU">
COWAN@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Thu 8 Jan 87 20:31:27-EST
</i><PRE>
To: risks@csl.sri.com

Randall Davis makes the important point that stock market trading
really isn't any more volatile now than before.  I agree.

Computers have enabled the VOLUME of trading to go up (probably to a
level where much of the speculation serves no useful economic purpose
to non-speculators), but this does not seem to automatically insure
stock market volatility.  Even if computerized trading becomes more
widespread, I would think that any aberrant trades would be quickly
corrected the next day as long there remains some human input into the
trading process.  Yet I still see a potential effect.

A book I once read on the Stock Market crash of 1929 noted that in
times of potential panic, large traders would attempt to shore up the
market by buying, to restore confidence in the market.  It's possible
that the stability of the present market has a lot to do with this
type of activity.  But such "feedback" -- applied by large banks and
brokerage firms -- would not in the foreseeable future be applied
automatically by computer, because the decisions involve analyzing
political events and the psychological mood "on the Street."

If there currently exists a human rudder smoothing the path of the
stock market, I can see why investors might be concerned about
programmed trading.  This practice does not run the risk of a
computerized avalanche of domino trades which will drive the market
1000 points up or down in one day.  But it may interfere with the
ability of large investors to use their resources as a rudder, in the
event that trading does become volatile for economic reasons.

It is easy to see why the programmed trades get media attention.  On
the "triple-witching-hour" days, human beings will have less control
and the market may do unexpected things.  If these events are not
announced and explained before they occur, the movement of the market
may set off an avalanche of HUMAN panic selling.  Of course, this
would only occur if the preconditions existed were met -- if the
market were viewed to be overvalued, relative to economic performance.

It is true that the news media sensationalizes and often fails to put
stories into historical context; they may seem to enjoy blaming
technology.  But consider the motivations of the people from whom the
business press usually get their stories: people in the financial
community.  They may find technology a convenient scapegoat for any
problem with the stock market, especially if they have contributed to
setting up the conditions of an overvalued market.

Perhaps the market hasn't reached the point where it is overvalued.
But consider that the head of the MIT Department of Electrical
Engineering and Computer Science, in an open forum on US
competitiveness with Japan last February, said "I think we're going to
have a depression."

Lester Thurow and several other economists are now making frequent
comparisons to 1929, pointing out when large investors finally lost
confidence in the ability of the market to sustain a continued rally
or plateau, they all raced to pull out.  Anyway, the point is that
computers will not cause a crash, but could set off a crash that is
bound to occur anyway, and be wrongly blamed for it.

Of course, it is possible that there might not be a crash at all!
When US investors sell, foreign investors will buy up all our stock
and we'll be owned by the Japanese!            :)
                                                        -rich

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.37.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.39.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-116</DOCNO>
<DOCOLDNO>IA012-000125-B046-115</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.39.html 128.240.150.127 19970217010801 text/html 14860
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:06:28 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 39</TITLE>
<LINK REL="Prev" HREF="/Risks/4.38.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.40.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.38.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.40.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 39</H1>
<H2> Sunday, 11 January 1987 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Re: As the year turns ... 
</A>
<DD>
<A HREF="#subj1.1">
Jerry Saltzer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  911 computer failure 
</A>
<DD>
<A HREF="#subj2.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Engineering tradeoffs and ethics 
</A>
<DD>
<A HREF="#subj3.1">
Andy Freeman
</A><br>
<A HREF="#subj3.2">
 Ken Laws
</A><br>
<A HREF="#subj3.3">
 George Erhart
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: computerized discrimination 
</A>
<DD>
<A HREF="#subj4.1">
Randall Davis
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Re:  As the year turns ... (Jeffrey Mogul)
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>
Fri, 9 Jan 87 12:40:24 EST
</i><PRE>
From: Jerome H. Saltzer &lt;Saltzer@ATHENA.MIT.EDU&gt;

I believe it was New Year's eve, 1962, when I first found myself
poking around inside a system--M.I.T.'s Compatible Time-Sharing
System for the IBM 709--that was nominally intended for continuous
operation, but that had to be *recompiled* to tell it about the new
year, because whoever designed the hardware calendar clock assumed
that someone (else) could program around the missing year field.

It took only a small amount of contemplation to conclude that any
source that claims to tell you the date has got to mention the year,
and with some browbeating of engineers we got a version of that
design included in the Multics hardware a few years later.

At the time, someone talked me out of writing a paper on the subject on the
basis that the right way to do it is so obvious that noone would ever be so
dumb as to design a date-supplying clock without the year again.  Possible
conclusion for RISKS readers?: nothing, no matter how obvious, is obvious.

						Jerry

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
911 computer failure
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Sat 10 Jan 87 12:04:00-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

From an article by Dave Farrell, San Francisco Chronicle, 9 Jan 1987:

The city's failure to send help to a choking 5-year-old boy was attributed to
equipment failure, not human error, according to Mayor Dianne Feinstein.  When
Gregory Lee began choking, his Cantonese-speaking grandmother dialed 911, but 
gave up when no one understood her.  The automatic call-tracing program somehow
retrieved the wrong address and displayed it on the police controller's
computer screen.  (The rescue crew was directed to the wrong address.)

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Engineering tradeoffs and ethics
</A>
</H3>
<address>
Andy Freeman 
&lt;<A HREF="mailto:ANDY@Sushi.Stanford.EDU">
ANDY@Sushi.Stanford.EDU
</A>&gt;
</address>
<i>
Fri 9 Jan 87 09:58:41-PST
</i><PRE>
To: risks@CSL.SRI.COM

Dan Ball &lt;Ball@mitre.ARPA&gt; wrote:  [He mentions that many engineering
    organizations are so large and projects take so long that individual
    responsibility is suspect and the uncertainty in predicting risks.]
    I'm afraid reducing the problem to dollars could tend to obsure the
    real issues.

What issue is obscured by ignoring information?

    Moreover, even if the [cost-benefit] analyses were performed
    correctly, the results could be socially unacceptable. [...]  In the
    case of automobile recalls, where the sample size is much larger, the
    manufacturers may already be trading off the cost of a recall against
    the expected cost of resulting lawsuits, although I hope not.

Between legal requirements and practical considerations (they can't
pay out more than they take in), manufacturers MUST trade off the cost
of a recall and other legal expenses against costs and probability.

The result of a cost-benefit/risks analysis is information, not a
decision.  This information can be used to make a decison.  I think it
is immoral for a decision maker to ignore, or worse yet, not determine
cost-benefit or other relevant information.  (There is a meta-problem.
How much should gathering the information cost?  People die while
drugs undergo final FDA testing.  Is this acceptable?)  In addition,
gathering the information necessary to determine it often finds
opportunities that the decision maker was unaware of.

Since we'd like to have cars, there will always be some safety feature that
is unavailable because we can't afford a car that includes it.  (Because
autos and "accidents" are so common, auto risks can be predicted fairly
accurately.)  Unfortunately, the current legal system restricts our access
to information about the tradeoffs that have been made for us.  You might
buy a safer car than I would, but you don't have that information.  The
costs are spread over groups that are too diverse.  A legal system that
encourages that is socially unacceptable.
                                                  -andy

</PRE>
<HR><H3><A NAME="subj3.2">
Engineering Ethics
</A>
</H3>
<address>
Ken Laws 
&lt;<A HREF="mailto:LAWS@SRI-IU.ARPA">
LAWS@SRI-IU.ARPA
</A>&gt;
</address>
<i>
Fri 9 Jan 87 10:15:26-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

  Date: Thu, 08 Jan 87 11:29:37 -0500   &lt;<A HREF="/Risks/4.38.html">RISKS-4.38</A>&gt;
  From: ball@mitre.ARPA &lt;Dan Ball&gt;

  ... I am not convinced that we know
  how to predict risks, particularly unlikely ones, with any degree of
  confidence.

True, but that can be treated by a fudge factor on the risk (due to
the risk of incorrectly estimating the risk).  There are difficulties,
of course: we may be off by several orders of magnitude, different
tradeoffs are required for large, unlikely disasters than for small,
likely ones, and certain disasters (e.g., nuclear winter, thalidomide)
may be so unthinkable that a policy of utmost dedication to removing
every conceivable risk makes more sense than one of mathematically
manipulating whatever risk currently exists.

  I would hate to see a $500K engineering change traded off against
  a loss of 400 lives @ $1M with a 10E-9 expected probability.  I'm afraid
  reducing the problem to dollars could tend to obsure the real issues.

How about a $500M tradeoff against a loss of 1 life with a 10E-30
probability?  If so, as the punch line says, "We've already
established what you are, we're just dickering over the price."
The values of a human life that are commonly accepted in different
industries seem to fall in the $1M to $8M range, with something
around $2M being near the "median".

  Moreover, even if the analyses were performed correctly, the results could
  be socially unacceptable.  I suspect that in the case of a spacecraft, or
  even a military aircraft, the monetary value of the crew's lives would 
  be insignificant in comparison with other program costs, even with a
  relatively high hazard probability.

The "value of a human life" is not a constant.  The life of a volunteer or
professional, expended in the line of duty, has always been considered less
costly than the life of innocents.  If we forget this, we end up with a few
$60M fighter aircraft that can be shot down by two or three less-secure $5M
aircraft.  (I predict that the next protracted U.S. war will be won by
expendable men in jeeps with bazookas, not by autonomous vehicles.)

  In the case of automobile recalls, where the sample size is much larger,
  the manufacturers may already be trading off the cost of a recall against
  the expected cost of resulting lawsuits, although I hope not.

Of course they are.  The cost of lawsuits is much more real than any
hypothetical cost of human life.  In fact, the cost of lawsuits &gt;&gt;is&lt;&lt;
the cost of human life under our current system.  The fact that awards
differ depending on manner of death, voluntarily assumed risk,
projected lifetime income, family responsibilities, etc., is the
reason that different industries use different dollar values.

I think we should set a formal value, or set of values, if only to
ease the burden on our courts.  It would give us a firm starting
point, something that could be adjusted according to individual
circumstance.  This is already done by the insurance industry
and their guidelines are also used by the courts in setting reasonable
damage awards ($x for mental suffering, $y for dismemberment, ...).
It would not be a big change to give legal status to such values.
Courts would still be free to award punitive damages sufficient to
inflict genuine influence on rogue corporations.

As for the dangers of incorrectly estimating risks, I think that the
real danger is in not estimating risks.

					-- Ken Laws

</PRE>
<HR><H3><A NAME="subj3.3">
Engineering Ethics
</A>
</H3>
<address>
George Erhart
&lt;<A HREF="mailto:gwe@cbosgd.mis.oh.att.com ">
gwe@cbosgd.mis.oh.att.com 
</A>&gt;
</address>
<i>
Fri, 9 Jan 87 16:05:50 est
</i><PRE>

Whether or not we like to admit it (or even are aware of it), we all
(not just engineers) place a monetary value on human life. For example,
consider the number of people who drive small cars; most of these are less
survivable in a collision than larger, more expensive autos. The purchasers
usually are aware of this, but accept the  risks to save money.

How many of us have rushed out to have airbags installed in our cars ? How
often do we have our brakes checked ? Do we even wear our seatbelts ?

The facts are that :
1)No system can be made 100% safe/infallible.
2)The cost of the system increases geometrically as the 100% mark is approached
3)A compromise *must* be reached between cost and safety.

A good example of the latter would be in the design of ambulances. We could
make them safer via heavier construction, but this would decrease top speed
(which also makes the vehicle safer). The increased response time, however,
would endanger the lives of the patients. Larger engines can be installed to
regain speed, increasing both the purchase cost and operating expense, which
will result in fewer ambulances being available, and increased response time.

We set the value of human life in countless ways. We must; it is an unavoidable
situation.  But that value is rarely set by an engineer; it is fixed by the
consumer (read you and me) who determine how much they are willing to pay for
their own safety.

Bill Thacker   -   AT&amp;T Network Systems, Columbus, Ohio

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: computerized discrimination
</A>
</H3>
<address>
Randall Davis 
&lt;<A HREF="mailto:DAVIS%OZ.AI.MIT.EDU@XX.LCS.MIT.EDU">
DAVIS%OZ.AI.MIT.EDU@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Sun 11 Jan 87 13:54-EST
</i><PRE>
To: risks@CSL.SRI.COM

&gt; Date: Wed 7 Jan 87 15:54:13-PST
&gt; From: Ken Laws &lt;LAWS@SRI-IU.ARPA&gt;
&gt; Subject: Computerized Discrimination
&gt;
&gt; ... Randall Davis made the implicit assumption that the discrimination
&gt;consisted of a rule subtracting some number of points for sex and race,
&gt;and questioned whether the programmer shouldn't have blown the whistle.

Here's the relevant paragraph:

   One can only imagine the reaction of the program authors when they 
   discovered what one last small change to the program's scoring function 
   was necessary to make it match the panel's results.  It raises interesting 
   questions of whistle-blowing.

There's no assumption there at all about the form the scoring function.

One "small change" that would be at the very least worth further investigation
is the need to introduce race as a term.  Whatever its coefficient, the need
to introduce the term in order to match the human result should at least give
one pause.  That's the whistle-blowing part: one ought at least to be wary and
probe deeper.  "Reading the polynomial" to determine the direction of the
effect may not be an easy task, but this is one situation where the
circumstances should quite plausibly inspire the effort.

The point remains that the polynomial, once created, can be examined and
tested objectively.  No such option exists for people's opinions and unstated
decision criteria.

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.38.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.40.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-117</DOCNO>
<DOCOLDNO>IA012-000125-B046-144</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.40.html 128.240.150.127 19970217010821 text/html 20060
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:06:46 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 40</TITLE>
<LINK REL="Prev" HREF="/Risks/4.39.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.41.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.39.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.41.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 40</H1>
<H2> Wednesday, 14 January 1987 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Phone Cards 
</A>
<DD>
<A HREF="#subj1.1">
Brian Randell
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  It's No Joke!! (Microwave oven bakes 3 yrs of PC data) 
</A>
<DD>
<A HREF="#subj2.1">
Lindsay Marshall
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Automation bottoms out 
</A>
<DD>
<A HREF="#subj3.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Amtrak train crash with Conrail freight locomotive -- more 
</A>
<DD>
<A HREF="#subj4.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Re: Cellular risks 
</A>
<DD>
<A HREF="#subj5.1">
Robert Frankston
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Re: Ask not for whom the chimes tinkle 
</A>
<DD>
<A HREF="#subj6.1">
Tom Perrine via Kurt Sauer
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Re: Engineering ethics 
</A>
<DD>
<A HREF="#subj7.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Repetitive Strain Injury and VDTs 
</A>
<DD>
<A HREF="#subj8.1">
Mark Jackson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj9">
  Safety Officers and "Oversight" 
</A>
<DD>
<A HREF="#subj9.1">
Henry Spencer
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Phone Cards
</A>
</H3>
<address>
Brian Randell 
&lt;<A HREF="mailto:brian%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
brian%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Wed, 14 Jan 87 16:12:23 gmt
</i><PRE>

              PHONE CARDS - THE PLOT THICKENS

     At PGN's implied request, I have tracked down, and talked to the Sunday
Post reporter who wrote the original story on the phone card fraud.  These
notes of my telephone conversation with him are being sent to RISKS with his
explicit permission, though he asked that his name not be included.

     The Sunday Post was indeed asked by BT to publish a retraction, but
have refused to do, though they have published a letter from BT expressing
(BT's) full confidence in the phone card system.  Based on previous
experiences - "we often get complaints at our stories" - the reporter
regards the fact that BT did not push for a retraction, but instead merely
settled for publication of their letter, as tantamount to an acceptance of
the truth of the original story.

     He claims to be still sure that the fraud is possible, and to have seen
it being worked, at several different phones, by the soldiers, in the
presence of several other witnesses.  He does admit that he was himself later
unable to demonstrate the fraud successfully to some BT engineers who
travelled to Glasgow to meet him.  He however has since talked to one of the
soldiers, who assures him that the fraud is still working, but will not
reveal to the reporter, leave alone BT, where he (the reporter) went wrong
in trying to duplicate the method of fraud.  (The other soldier - who did not
want the original story published, because it would interfere with "free"
international calls - is now refusing to talk to the reporter.)  Moreover the
reporter claims to have received a phone call from a BT engineer at Watford,
confirming the practicability of the fraud.

Brian Randell - Computing Laboratory, University of Newcastle upon Tyne

  ARPA  : brian%cheviot.newcastle.ac.uk@cs.ucl.ac.uk
  UUCP  : &lt;UK&gt;!ukc!cheviot!brian
  JANET : brian@uk.ac.newcastle.cheviot

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
It's No Joke!! (Microwave oven bakes 3 yrs of PC data)
</A>
</H3>
<address>
"Lindsay F. Marshall" 
&lt;<A HREF="mailto:lindsay%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
lindsay%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Tue, 13 Jan 87 09:58:03 gmt
</i><PRE>

There was a report on the wireless this morning that a well-known comedian
lost 3 years worth of material stored on his home computer when his wife
turned on the microwave oven!! Sadly, I have no more information than this
as the papers have not arrived in Newcastle because of the weather......

     [Continued: Wed, 14 Jan 87 09:09:14 gmt]

The most detailed information about the incident I can find says that the 
comedian's son was playing with the machine in the kitchen when his mother 
turned on the microwave oven. The computer's "memory" was instantly wiped.  
The suggested reason is (of course) leakage from the microwave oven.  The 
wife's comment? "I told him he shouldn't use the computer in the kitchen..."

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Automation bottoms out
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Wed 14 Jan 87 10:27:58-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

``As for the `partially shielded street urinals' of Paris ... they have
been superseded by sexually neutral, fully enclosed, fully automated,
coin-access two-stall elliptical masonry structures.... A few years
ago, a child was killed in one of them by the automated toilet seat.''

(Letter to the editor of the New York Times from Louis Marck, excerpted
[exactly as shown] in the SF Chron, 13 Jan 87, p. 10)

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Amtrak train crash with Conrail freight locomotive -- more
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Wed 14 Jan 87 10:34:53-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

Tests conducted (three times) indicated that the freight locomotive should
have been able to stop in time, and that equipment was all in working order.
Thus human error was the most likely cause of the accident that killed 15
(13 Jan 87, SF Chron, p. 8, from the Washington Post).  (Earlier reports
suggested that three separate safety mechanisms would have had to fail at 
the same time [for it to have been other than human error].)

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
 Re: Cellular risks
</A>
</H3>
<address>
&lt;<A HREF="mailto: Frankston@MIT-MULTICS.ARPA">
 Frankston@MIT-MULTICS.ARPA
</A>&gt;
</address>
<i>
Tue, 13 Jan 87 00:01 EST
</i><PRE>
To:  RISKS@CSL.SRI.COM

I picked up a book entitled "Introducing cellular communications:  The New
Mobile Telephone System" from TAB Books.  The copyright is 1984.  From the
look of it, it seemed to be a lightweight book.  Skimming it, it seems
instead to go into details of message formats, setting up head ends and
other detailed stuff.  I presume it makes it much easier to figure out how
to hack the system.

      [This is an old hack.  As noted here before, the idea(l) is to
       make the system design strong enough that all the documentation
       (except maybe the vulnerability analyses) can be freely handed
       out.  Of course, the reality is far from that ideal.  PGN]

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Re: Ask not for whom the chimes tinkle
</A>
</H3>
<address>
&lt;<A HREF="mailto:ihnp4!nears!ks@ucbvax.Berkeley.EDU">
ihnp4!nears!ks@ucbvax.Berkeley.EDU
</A>&gt;
</address>
<i>
Sat, 10 Jan 87 09:01:37 PST
</i><PRE>
Summary: DCN1 gave the wrong time
Sender: Kurt F. Sauer &lt;ihnp4!svo.decision.com!ks@ucbvax.Berkeley.EDU&gt;
Organization: AT&amp;T Network Systems, Software Systems, Oklahoma City OK USA

In article &lt;8701082340.AA17468@ucbvax.Berkeley.EDU&gt; Perrine@LOGICON.ARPA
(Tom Perrine) wrote:

  WARNING! TIME WARPS AHEAD!
  
  Well the chimes sure tinkled for us!  On Thursday 8 Jan (1987 A.D.) at
  about 1400 PST we queried DCN1 as we booted our PWB UNIX system and
  received a 1986 date stamp!  (Gee Mr. Peabody, set the Wayback machine
  for 1987!)
  
  Further investigation shows that DCN6 and GW.UMICH.EDU are also stuck
  in a time warp.  UMD1 seems to be the only un-nostalgic clock.
  (FORD1 was not reachable.)
  
  For now, everyone better keep one eye on the Timex, and another on the
  packets, and another on the Seiko!
  
  Tom Perrine,   Logicon - OSD

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Re: Engineering ethics 
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Wed 14 Jan 87 19:22:11-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

Sorry.  It is time to blow the whistle on this rather narrowly
focussed discussion.  Sorry to those who thought they had more to say
on the subject.  (I tacked a comment on the Ford Pinto case onto Andy
Freeman's note in <A HREF="/Risks/3.65.html">RISKS-3.65</A> -- some of you will remember -- on how
short-sighted dollar-values on lives can be.)  PGN

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
Repetitive Strain Injury and VDTs
</A>
</H3>
<address>
&lt;<A HREF="mailto:MJackson.Wbst@Xerox.COM">
MJackson.Wbst@Xerox.COM
</A>&gt;
</address>
<i>
14 Jan 87 10:49:09 EST (Wednesday)
</i><PRE>
To: RISKS@CSL.SRI.COM

The January/February issue of the /Columbia Journalism Review/ contains
an article entitled "A Newsroom Hazard Called RSI" about repetitive
strain injury associated with workstation use.  It is much too lengthy
to reproduce, but attached below are some excerpts.
                                                          Mark

"[San Diego /Tribune/ reporter John] Furey is a victim of repetitive
strain injury (RSI), a term that embraces a number of painful and often
disabling afflictions linked to continuous bending, twisting, and
flexing of the hands, arms, or shoulders.  Thousands of these injuries,
which include tendonitis, are found among meat-cutters, garment workers,
and other workers whose jobs require constant, repeated hand movements.
But repetitive strain injuries are also showing up among office workers,
who may strike a computer keyboard up to 45,000 times an hour.  And
automated newspaper offices are no exception:  to the dismay of all
involved, disabling cases of RSI have recently cropped up in newspapers
across the country."

. . . .

"Her doctor, John Adams, a Los Angeles orthopedist, compared her case of
tendonitis to 'four tennis elbows,' [/Los Angeles Times/ reporter
Penelope] McMillan recalls.  'He said he'd never seen anything like it.'
Returning to work after a two-and-a-half-month leave, McMillan found
that anti-inflammatory drugs had no effect on the recurrent 'wild' pain
in her arms."

. . . .

"Steven Sauter, a job-stress specialist with the National Institute for
Occupational Safety and Health, believes that VDT-related injuries are
relatively uncommon.  But, he warns, 'when these problems do occur, they
can be serious and require medical attention.'

"One problem, Sauter notes, is that many VDT jobs 'have little built-in
variety.'  In a job-health manual he wrote while teaching at the
University of Wisconsin, Sauter explained that VDT operators often make
thousands of keystrokes an hour, 'repeating nearly identical motions at
a high rate of speed.'  While typing, each stroke requires muscles to
contract and tendons to move, and the tendons can become irritated as
they slide around bones and against tissues.  In such cases, he warns,
the wear and tear can cause painful inflammation of the tendons, which
will not heal without rest."

. . . .

"Indeed, a question that puzzles many editors is why some employees who
had no problems when they used typewriters are developing hand and arm
injuries now that they are using VDTs.  One answer, say occupational
health specialists, is that, although some typists do develop such
injuries, VDT users may be at greater risk because they can make many
more hand movements per hour.  In addition, using a typewriter calls for
more varied hand movements and breaks in routine, such as inserting
paper.

"Another factor that may contribute to injuries is that some reporters
are simply using their VDTs /more/ than they used typewriters.  'At the
/Times/, we used to do anything to avoid using our clunky old manual
Olympics,' [/Los Angeles Times/ reporter Laurie] Becklund says.  'We'd
take notes by hand--anything.  When we got VDTs, we were thrilled.  They
were so convenient that we began using them for everything.'"

. . . .

"For Becklund, who receives physical therapy for her hands three times a
week, the worst is not knowing when her hands will be healed.  'It's
hard not to feel depressed, especially because the doctors won't tell
you that you're ever going to get over it.  They won't promise to fix
it.  Some articles I've read say that if your hands hurt when you aren't
doing an activity, then you've got it for life.'  She paused.  'I choose
not to believe that.'"

-----

In a sidebar, the following tips to reduce the risk of RSI are
attributed to a fact-sheet published by the Australian Journalists'
Association and a handout distributed by the Australian Council of Trade
Unions:

- Adjust the work station so you can assume a comfortable keying
  position.

- Try to use a soft touch when keying and avoid over-stretching the
  fingers.

- Avoid resting your wrists on the keyboard or edge of the desk when
  typing.

- Don't bend your hands up at the wrists.

- Try to take frequent, short rest breaks, and every half hour or so,
  do some stretching.

- Don't use painkilling drugs in order to keep working.

- Immediately report symptoms of RSI (persistent pain, tenderness,
  tingling, or numbness) and seek medical advice.

</PRE>
<A NAME="subj9"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj9.1">
Safety Officers and "Oversight"
</A>
</H3>
<address>
&lt;<A HREF="mailto:pyramid!utzoo!henry@hplabs.HP.COM">
pyramid!utzoo!henry@hplabs.HP.COM
</A>&gt;
</address>
<i>
Mon, 12 Jan 87 19:38:11 pst
</i><PRE>

In the February Analog (one of the science-fiction magazines), there is
an interesting and partially relevant non-fiction article by Harry Stine.
The relevant part is his discussion of certain shuttle safety issues.
He was one of the people saying all along that NASA had problems, and in
particular he wrote (under his penname "Lee Correy") the SF novel "Shuttle
Down", which exposed how utterly unprepared NASA was for an emergency
landing by a Vandenberg-launched shuttle.  (The only viable landing spot
is Easter Island, where landing would have been difficult and dangerous
and recovery of the orbiter would have been a monumental problem, since
no thought had been given to the issue.)  He notes:

  "There's talk of a 'safety oversight committee' to review each space
  shuttle mission before it's launched.  But isn't that exactly what NASA
  had when the Challenger blew up?

  "Safety committees don't work in the crunch.  One person finally has to
  decide go-no-go and accept the responsibility which cannot and must not
  be spread among a committee, where no single person is accountable if
  something goes wrong..."

He goes on to cite his credentials, including spending some years as Range
Safety Officer at White Sands, and being chairman of the group that wrote
the standard DoD range-safety rules for rocket ranges.

  "There have been some gut-wrenching occurrences.  One night I told a
  well known and politically powerful upper-air scientist [that winds were
  too high and] the unguided Aerobee would impact off the range.  Therefore,
  I told him he should cancel ... He said he was Project Scientist, he
  needed the data, the delay would result in a budget over-run, and
  therefore he was going to launch.  I replied that I would push the destruct
  button the instant the rocket cleared the launch tower.  He launched.  I
  pushed the button.  The commanding officer called me into his office the
  next morning and asked me what happened; I told him.  Nothing more was
  said because the Word of the Safety Officer is as the Word of God.  There
  is no tribunal that can over-rule or second-guess a Safety Officer.
  There can be no retribution against the Safety Officer.  He calls the
  shots.  If he calls too many unsafe ones, the range commander ... transfers
  him to some other position.

  "That decades-old policy works very well.  People can be easily trained
  to use it and be unafraid of invoking it when the need arises.  ...

  "A safety oversight committee cannot prevent another space shuttle
  accident.  It can either delay the program so badly that it won't make
  any difference in the long run, or it will mean that nothing gets launched.
  ... If the automotive industry had a government safety oversight committee
  riding herd on it, we'd all be walking."

The rest of the article discusses other issues, like how to get the space
program in general moving again.  One other point he does raise is that
NASA tends to be asked for its opinion on the viability and reliability
of private launch-vehicle schemes, and as you would expect, its assessments
of potential competitors tend to be rather negative...

				Henry Spencer @ U of Toronto Zoology
				{allegra,ihnp4,decvax,pyramid}!utzoo!henry

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.39.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.41.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-118</DOCNO>
<DOCOLDNO>IA012-000125-B046-164</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.41.html 128.240.150.127 19970217010832 text/html 12922
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:07:03 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 41</TITLE>
<LINK REL="Prev" HREF="/Risks/4.40.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.42.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.40.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.42.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 41</H1>
<H2> Monday, 22 January 1987 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
Audi 5000 recall 
</A>
<DD>
<A HREF="#subj1.1">
Dave Platt
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  UK EFT Risks 
</A>
<DD>
<A HREF="#subj2.1">
Brian Randell
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Another Bank Card Horror Story 
</A>
<DD>
<A HREF="#subj3.1">
Dave Wortman
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Stock Market behavior 
</A>
<DD>
<A HREF="#subj4.1">
Rob Horn
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
Audi 5000 recall
</A>
</H3>
<address>
Dave Platt
&lt;<A HREF="mailto:dplatt@teknowledge-vaxc.ARPA ">
dplatt@teknowledge-vaxc.ARPA 
</A>&gt;
</address>
<i>
Fri, 16 Jan 87 10:09:47 PST
</i><PRE>

Audi has announced a total recall of all pre-'87 Audi 5000s equipped with
automatic transmissions.  The recall is an extension of the earlier,
voluntary callback of these cars to equip them with the shift-lock device,
and to inspect and if necessary correct the idle valve.  Audi is not, at
this time, replacing any microprocessor components, nor have they admitted
or agreed that any such replacement is necessary.  

    [The National Highway Traffic Safety Administration has been informed
     of "5 deaths and 271 injuries related to the problem."  PGN]

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Air Traffic Control Safety -- 1986
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Mon 19 Jan 87 20:06:26-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

SF Chron 15 Jan 87 via Cox News Service:

Reports of near collisions involving commercial aircraft jumped 37.6%
nationwide in 1986...  329 near collisions involving at least one commercial
aircraft...  (239 in 1985) 49 of the 1986 accidents were clasified
"critical", meaning that chance rather than pilot action prevented a
collision...  FAA officials dismiss the notion that the air traffic control
system is not back up to speed.  Agency officials attribute the increase in
part to an improved reporting system, heightened public awareness about air
safety, and increased air traffic.


SF Chron 16 Jan 87, p 25 (UPI):

Air traffic controllers at Southern California's primary radar center
destroyed evidence, falsified reports and lied to investigators to conceal
errors that placed airplanes on collision courses...  [quoting article
from th Orange County Register]  

"On February 16, an 18-passenger Skywest Airlines commuter jet and a six-
passenger private plane were within 3.8 miles of each other and on a
collision course when an air traffic controller reportedly turned off the
computer that was tracking them." ...

"On February 13, a conflict alert signal went off three times, warning a
controller that a 105-passenger DC-9 and a 12-passenger private jet were
within 2.5 miles of each other and on a collision course.   ... the 
controller turned off the alert each time to try to conceal his error."

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
UK EFT Risks
</A>
</H3>
<address>
Brian Randell 
&lt;<A HREF="mailto:brian%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
brian%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Mon, 19 Jan 87 13:00:21 gmt
</i><PRE>
                          EXTENT OF UK EFT RISKS

The Jan 15 issue of Computer News carried an account of a talk by Detective
Inspector John Austen of the Computer Crime Unit at New Scotland Yard which
contained statistics and comments about the use of EFT in the UK, and of the
possible risks due to criminal action. It is contained in a lengthy article
describing a BCS Security Committee Seminar for the National Computer Users'
Forum, held recently in London. I found the following comments, especially
the statistics quoted, particularly interesting/alarming, so thought them
worth reporting to RISKS:

  "EFT now represents 83% of the value of all things paid for - money
  transferred - in Britain. Money, as an invisible export is a major and
  vital part of our GNP. Foreign exchange markets in London transfer $200bn
  daily using EFT via satellite. The transactions take a very short time,
  and once complete there is no calling them back. A lot of people are aware
  of this. And many, both here and abroad, are prepared to steal from EFT
  systems. The rewards are tremendous."
  
  "Companies, and even the economies of smaller countries, could be crippled
  by a sustained hit on EFT systems. Terrorists, such as the Middle East
  factions, the IRA and the Red Army Faction are particularly aware of this
  - and they need money. The Red aRmy Faction has already, unsuccessfully,
  made moves to intercept EFT in Germany. They and others will try again."

Brian Randell - Computing Laboratory, University of Newcastle upon Tyne

  ARPA  : brian%cheviot.newcastle.ac.uk@cs.ucl.ac.uk
  UUCP  : &lt;UK&gt;!ukc!cheviot!brian
  JANET : brian@uk.ac.newcastle.cheviot

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Another Bank Card Horror Story
</A>
</H3>
<address>
Dave Wortman 
&lt;<A HREF="mailto:dw%csri.toronto.edu@RELAY.CS.NET">
dw%csri.toronto.edu@RELAY.CS.NET
</A>&gt;
</address>
<i>
Mon, 19 Jan 87 10:51:12 est
</i><PRE>

The automatic teller machines (ATMs) supported by our local bank are fairly
typical. Each customer has a magic card that references a checking account,
a savings account, a credit card and perhaps some other accounts.  These are
called "checking", "savings", "other#1", etc.  The ATM system never
discloses the real account number.  I recently had a very disconcerting run
in with this system.  My bank card and accounts have been in existence for
several years.  Recently I went to my local bank and opened a new account.

Unbeknownst to me, the act of opening a new account changed the designation
of accounts on my bank card.  What had been my primary checking account was
bumped to other#3 and my new account became my primary checking account.
Apparently the bank card uses indirect references since these changes
happened some night without the bank getting their hands on my card.  I do
not know if the problem was a human error in the setting up of my new
account or a programming error in the ATM system software.

I was lucky, the new account I opened happened to be in a foreign 
currency and so the ATMs started rejecting all transactions against
my "checking" account.  I discovered the explanation given above only
after a couple of frustrating weeks and a couple visits to my bank.

Things could have been a lot worse!  If the new account had not
been rejected immediately by the ATM then I might not have discovered 
the problem until the next round of bank statements a month or so hence.
In the meantime my accounts could have become hopelessly fouled up.

Independent of whether my problem was caused by a processing error or
by a software error, I think my experience demonstrates several inadequacies
in the design of the ATM "system".

1. the carefully negotiated interface between the user and the bank should
   NEVER change without the knowledge of both parties.  Normal procedure
   is for it to change only upon written request by the user.

2. There should be a better mechanism for the user to verify that the
   interface defined by the bank card corresponds exactly to the 
   user's expectations.

3. There should be more immediate feedback in the system in the case of
   errors or changes.  Because of the foreign currency problem described
   above, I happened to get a  fairly immediate indication that something was 
   wrong.  In the worst case, I might have not received any indication
   that something was wrong until the first bank statements for the new
   account arrived (typically 1.5 MONTHS).

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Stock Market behavior
</A>
</H3>
<address>
Rob Horn
&lt;<A HREF="mailto:wanginst!infinet!rhorn@harvard.HARVARD.EDU ">
wanginst!infinet!rhorn@harvard.HARVARD.EDU 
</A>&gt;
</address>
<i>
Thu, 15 Jan 87 22:13:57 est
</i><PRE>

The impact of computer trading on the stock market, and in particular the
``triple witching hour,'' has not gone unattended by the stock market
directors and regulators.  Their response has shown considerably more
insight into market behavior than might be expected.  They have not
considered computers to be the problem.

The problem of the ``triple witching hour'' is that during a few hours on
the third friday of each third month (typically from 3-4PM) there is an
immense burst of market activity as major participants rearrange their
computer selected portfolios.  (This particular time is triggered by the
expiration time of a key financial component in these ``computer based''
trades.)  Before these trading programs, hearing that someone needs to sell
100,000 shares of IBM quick, like in the next 5 minutes, meant that there
was a major problem at IBM.  Many people still react in panic when they hear
such news.  These habits and expectations where being greatly shocked by
massive shifts like this which merely reflected trivial adjustments between
stock prices and interest rates.

For the previous two witching hours, and for the forseeable future, market
makers are now required to publish their required major stock trades several
hours in advance on these Fridays.  This gives all the other participants
time to evaluate the trades and determine what they mean.  It also seems to
be working.  Both Friday's had trading volumes just as huge as other such
Fridays, but did not suffer from the sudden pricing shocks.  Prices were
quite well behaved with no unusual changes.

Based on prior behavior the odds that two in a row would be this orderly is
between 10-20%.  March really tell whether this added information flow is
really all that is necessary for the stock market participants to properly
interpret the meanings of these massive stock trades.  It does look promising.

				Rob  Horn
	UUCP:	...{decvax, seismo!harvard}!wanginst!infinet!rhorn
	Snail:	Infinet,  40 High St., North Andover, MA

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.40.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.42.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-119</DOCNO>
<DOCOLDNO>IA012-000125-B046-185</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.42.html 128.240.150.127 19970217010845 text/html 19764
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:07:14 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 42</TITLE>
<LINK REL="Prev" HREF="/Risks/4.41.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.43.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.41.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.43.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 42</H1>
<H2> Friday, 23 January 1987 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
A scary tale--Sperry avionics module testing bites the dust? 
</A>
<DD>
<A HREF="#subj1.1">
Nancy Leveson
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Computer gotcha 
</A>
<DD>
<A HREF="#subj2.1">
Dave Emery
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: Another Bank Card Horror Story 
</A>
<DD>
<A HREF="#subj3.1">
Robert Frankston
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Stock Market behavior 
</A>
<DD>
<A HREF="#subj4.1">
Howard Israel
</A><br>
<A HREF="#subj4.2">
 Gary Kremen
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  Engineering models applied to systems 
</A>
<DD>
<A HREF="#subj5.1">
Alan Wexelblat
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Re: British EFT note 
</A>
<DD>
<A HREF="#subj6.1">
Alan Wexelblat
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj7">
  Train Wreck Inquiry (Risks 2.9) 
</A>
<DD>
<A HREF="#subj7.1">
Matthew Kruk
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj8">
  Cost-benefit analyses and automobile recalls 
</A>
<DD>
<A HREF="#subj8.1">
John Chambers
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
A scary tale--Sperry avionics module testing bites the dust?
</A>
</H3>
<address>
Nancy Leveson 
&lt;<A HREF="mailto:nancy@ICSD.UCI.EDU">
nancy@ICSD.UCI.EDU
</A>&gt;
</address>
<i>
21 Jan 87 12:53:51 PST (Wed)
</i><PRE>

I just spoke to a man at the FAA who is involved with aircraft certification.  
He told me that Sperry Avionics, who are building the computerized automatic
pilot among other things for a future new aircraft, is trying to convince
them to eliminate module testing for the software.  According to this man,
Sperry argues that programmers find module testing too boring and won't stay
around to do it.  Instead of module testing, Sperry wants to use n-version
programming and perform only functional system test.  As long as the results
from the two channels match, they will assume they are correct.

I am not concerned that they are using n-version programming, but that 
they are arguing that the use of it justifies eliminating something that
is considered reasonable software engineering practice.  The FAA has agreed 
to allow them to try this.  According to my FAA source, the FAA is not 
thoroughly comfortable with this, but the autopilot is only flight-crucial 
on this aircraft during about 45 seconds of the landing.  Also, their tests 
have found that pilots can successfully recover from an autopilot failure
during this period (by performing a go-around) about 80% of the time.  

I am going to talk further about this with some people at the FAA 
who are involved with certification.  If anyone else shares my concern
(or would like to allay my fears), I would appreciate hearing your
opinions and arguments.  I will convey them to the FAA unless you state
that they should remain confidential.
                                        Nancy Leveson (nancy@ics.uci.edu)

(P.S.  Anybody want to join me in writing Congress about saving Amtrak?)

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Computer gotcha
</A>
</H3>
<address>
Dave Emery
&lt;<A HREF="mailto:emery@mitre-bedford.ARPA ">
emery@mitre-bedford.ARPA 
</A>&gt;
</address>
<i>
Tue, 20 Jan 87 15:12:04 est
</i><PRE>
ReSent-To: risks@CSL.SRI.COM

Here's a computer gotcha for you...

Like many other people, I was trying to close on a new house before the end of
the year, for tax reasons.  We had our down payment wired from our bank in New
Jersey to our bank in New Hampshire, supposedly a fail-safe transaction.
Unfortunately, the Bank of New England, which was (one of) the middleman in the
wire transfer failed.  Apparently, their system was overloaded, and crashed.

My mother is a teller in a bank in Pittsburgh.  She says that, at least at her
bank, system crashes are a way of life.  Fortunately, she says, they rarely
lose any money.  
				Dave Emery, MITRE Corp, Bedford MA

P.S.  Bank of New England recovered later that day, and we got our money after 
we signed the papers.  The legal transaction was recorded the following day.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
 Re: Another Bank Card Horror Story
</A>
</H3>
<address>
&lt;<A HREF="mailto: Frankston@MIT-MULTICS.ARPA">
 Frankston@MIT-MULTICS.ARPA
</A>&gt;
</address>
<i>
Tue, 20 Jan 87 23:42 EST
</i><PRE>
To:  risks@CSL.SRI.COM

The issue of ATM accountability reminds me of a problem I am having
untangling my Mastercard transactions here.  In general, the reports
generated on the statements fail to provide the minimal information
necessary for untangling messes.  Information like which card was actually
used is entirely missing.  Only American Express seems to understand that
each instance of a card should be tagged.  This is especially annoying when
the bank doesn't seem to mind that a stolen card is used for 8 months to buy
tickets on the Eastern Shuttle.  Credit transactions don't give a hint as to
what transactions they are being counted against.

While some of this just reflects ineptness and neglect in the bank's DP
department, it also is indicative of what is going to become a real
issue as we attempt to connect our personal computers with existing
services.  (or even banks connect to each other).  Electronic banking
services in general do an inadequate job of export/import of data.  Such
concepts as unique ID's for tagging and tracking transactions don't
really exist.  In the previous mastercard card example, the transaction
id is just some characters associated with the transactions and are
likely to not be unique.  It seems as if my home processing of this
information is more sophisticated than the bank's!

It reminds me of my attempt to setup an equipment tagging system.  I
decided to order two sets of tags -- red for permanent stickers and
black for removeables so that we can tag loaner equipment.  The office
manager followed through on this but both sets were numbered from 1 to
1000.  It was difficult to explain why this was a problem since it was
obvious which was red and which was black.

The problems are manifestations of the issue of fundamental information
processing literacy.  While some of us working with computers have
learned techniques to deal with aspects of this, the knowledge is not
well distributed through society, nor even the DP profession.  But the
use of computers is becoming pervasive.

This conflict is at the heart of a large class of computer-based risks.  In
the short term, the best we can do is point out the issues.  Pointing out
solutions is harder -- especially when they are obvious to us.  The real
question is how we can convey this understanding to the society at large.

Are there any references that exist to try to explain the concepts of
dealing with complicated systems and their interactions?  Maybe even
gather a list of such obvious things as checksums (and limitations on
simple checksums), unique ids (and the low cost of using a lot of
integers), redundancy (and its low cost/benefit) etc.

On the other hand, maybe these difficulties are really blessings since
an efficient EFT system, for example, might be a serious threat to
privacy so that these annoyances and even risks are worth it till we
understand how to deal with the system once it works smoothly.

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
 RISKS 4.41, Stock Market behavior
</A>
</H3>
<address>
 Howard Israel 
&lt;<A HREF="mailto:HIsrael@DOCKMASTER.ARPA">
HIsrael@DOCKMASTER.ARPA
</A>&gt;
</address>
<i>
Tue, 20 Jan 87 16:17 EST
</i><PRE>
To:  risks@CSL.SRI.COM

  &gt;For the previous two witching hours, and for the forseeable future, market
  &gt;makers are now required to publish their required major stock trades several
  &gt;hours in advance on these Fridays.  

Minor correction.  According to the Washington Post business section that
described this new SEC strategy of disclosure, it is not "required", but
recommended.  All major brokerages complied except one (I believe it was
Drexel Burnham Lambert Inc.), which caused a minor fervor as traders acted on
"incomplete information", thus giving Drexel a slight advantage.  Drexel was
criticized for not disclosing their intended trades but countered that it did
not violate any SEC "rules" and thus acted properly.

The intended affect of the disclosure is to give traders advance notice, in
effect, reducing the "shock" factor as well as allowing the "market makers" to
adjust their inventories of stocks to prepare for the expected orders.  

Note: that a trade can be put in by a trader to be executed "at the market
closing price" for a given stock.  Regardless of what the price is, the trade
will be executed.  The deluge of orders on the "triple witching hour" at the
"market closing price" often caused the ticker to be delayed up to a half hour
at the closing.

</PRE>
<HR><H3><A NAME="subj4.2">
Stock Market behavior
</A>
</H3>
<address>
&lt;<A HREF="mailto:">

</A>&gt;
</address>
<i>

</i><PRE>
Date: Wed, 21 Jan 87 13:48:33 -0800
From: kremen@aerospace.ARPA

  &gt;The problem of the ``triple witching hour'' is that during a few hours on
  &gt;the third friday of each third month (typically from 3-4PM) there is an 
  &gt;immense burst of market activity as major participants rearrange their
  &gt;computer selected portfolios.  (This particular time is triggered by the 
  &gt;expiration time of a key financial component in these ``computer based'' 
  &gt;trades.)

Not really true, most of the problem occurs in the last 10 minutes of
trading when the "unwinding" of stock index futures, options
on those futures, and the underlying equities occur. Usually the brunt
of the unwinding occurs in Chicago, where the futures are traded. We
only see a portion of this when one looks at the volume on the New York
Stock Exchange. Also, not all unwinding occurs on "expiration day". If
conditions are favorable, stock positions can be unwound earlier.

  &gt;Before these trading programs, hearing that someone needs to sell 100,000
  &gt;shares of IBM quick, like in the next 5 minutes, meant that there was a
  &gt;major problem at IBM.  Many people still react in panic when they hear 
  &gt;such news.

NO ONE panics. Since 1982, when stock market indexes (such as the Major
Market Index or the S&amp;P 100) started to be traded, the "triple witching
hours" have occurred. Only within the past two years have the underlying
markets been liquid enought to make it really worthwhile. Anyway
institutions frequently sell (or buy) 100,000 shares of IBM for normal
trading purposes.  [...]

For more information see the December 29, 1986 issue of Insight magazine.

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
Engineering models applied to systems
</A>
</H3>
<address>
Alan Wexelblat 
&lt;<A HREF="mailto:wex@mcc.com">
wex@mcc.com
</A>&gt;
</address>
<i>
Tue, 20 Jan 87 10:49:53 CST
</i><PRE>

In Burnham's _The Rise of the Computer State_, MIT Professor Jeffrey A. Meldman
is quoted as follows:
	"In engineering, there is a principle which holds that it is
	frequently best to have a loosely-coupled system.  The problem
	with tightly coupled systems is that should a bad vibration
	start at one end of the machine, it will readiate and may cause
	difficulties in all parts of the system.  Loose coupling is 
	frequently essential to keep a large structure from falling down.
	I think this principle of mechanical engineering may be applicable
	to the way we use computers in the United States."

The context of the quote is a chapter on the aggregations of power that can
accrue in large, centralized computer systems and the risks (and temptations)
of abuse of this power.

RISKS readers have previously dismissed other engineering models as
inapplicable to software systems.  Comments on this one?

Alan Wexelblat
ARPA: WEX@MCC.ARPA or WEX@MCC.COM
UUCP: {seismo, harvard, gatech, pyramid, &amp;c.}!ut-sally!im4u!milano!wex

</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Re: British EFT note
</A>
</H3>
<address>
Alan Wexelblat 
&lt;<A HREF="mailto:wex@mcc.com">
wex@mcc.com
</A>&gt;
</address>
<i>
Tue, 20 Jan 87 10:54:42 CST
</i><PRE>

It is worth reminding RISKS readers that a British "billion" is a million
millions (1,000,000,000,000) rather than the American thousand millions
(1,000,000,000).     --Alan Wexelblat

                  [This was also noted by Howard Israel.  By the way, I
                  observe that the BBC radio broadcasts on PBS now routinely 
                  use "thousand million" and "million million"...  PGN]

</PRE>
<A NAME="subj7"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj7.1">
Train Wreck Inquiry (Risks 2.9)
</A>
</H3>
<address>
&lt;<A HREF="mailto:Matthew_Kruk%UBC.MAILNET@MIT-MULTICS.ARPA">
Matthew_Kruk%UBC.MAILNET@MIT-MULTICS.ARPA
</A>&gt;
</address>
<i>
Fri, 23 Jan 87 08:46:12 PST
</i><PRE>

Just caught bits and pieces on the morning radio news about this item mentioned
in Risks 2.9:

An inquiry into the collision between a VIA passenger train and a Canadian
National freight train near Hinton, Alberta last year, has put the blame on
human error. The freight crew were said to have ignored various safety
procedures. Also, Canadian National was accused of ignoring too many minor
safety infractions and for letting crews work without sufficient rest periods
between shifts.

Computer error was not mentioned as a contributing factor.

</PRE>
<A NAME="subj8"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj8.1">
Cost-benefit analyses and automobile recalls (RISKS DIGEST 4.39)
</A>
</H3>
<address>
&lt;<A HREF="mailto:harvard!cdx39!jc@seismo.CSS.GOV">
harvard!cdx39!jc@seismo.CSS.GOV
</A>&gt;
</address>
<i>
23 Jan 87 19:06:07 GMT
</i><PRE>
From: jc@cdx39.UUCP (John Chambers)

&gt;     Moreover, even if the [cost-benefit] analyses were performed
&gt;     correctly, the results could be socially unacceptable. [...]  In the
&gt;     case of automobile recalls, where the sample size is much larger, the
&gt;     manufacturers may already be trading off the cost of a recall against
&gt;     the expected cost of resulting lawsuits, although I hope not.

Sure they are.  Have you ever heard of "liability insurance"?  

There is also the general observation that, the way most forms
of cost-benefit analysis work, ignoring (i.e., failing to assign
an explicit value to) some factor is mathematically equivalent
to assigning it a cost of zero.  In other words, a cost-benefit
analysis can't generally distiguish an unknown cost from a zero
cost.  Similarly for benefits.

&gt; The "value of a human life" is not a constant.  The life of a volunteer or
&gt; professional, expended in the line of duty, has always been considered less
&gt; costly than the life of innocents.  

Huh?  Most military organizations that I've heard of consider the cost
of training a soldier to be significant; the value of innocents (i.e.,
civilians) is generally ignored, and thus considered to be zero.  This
was painfully obvious during the Vietnamese war, for instance.

&gt; As for the dangers of incorrectly estimating risks, I think that the
&gt; real danger is in not estimating risks.

If you listen to public discussions of risky situations, it soon becomes
quite clear that few people are able to distinguish "We know of no risks"
from "We know there are no risks".  

...

&gt;    One can only imagine the reaction of the program authors when they 
&gt;    discovered what one last small change to the program's scoring function 
&gt;    was necessary to make it match the panel's results.  It raises  
&gt;    interesting questions of whistle-blowing.

Even if the programmers looked at the regression function, it's 
not clear that they would have even seen a racial component.  For 
a nice example of how things can go wrong, consider that you can 
do a rather good job of sex discrimination while totally ignoring 
sex; you just use the person's height instead.  There have been 
published studies that imply that this is widespread practice;  
it's been known for some years that [in the USA] a person's height 
is a better predictor of their income than their sex, and if height 
is included, knowing their sex adds no further information.  It's 
likely that in the UK there are several attributes that are strongly 
correlated with race, so you need not necessarily use race at all.  

	John M Chambers			Phone: 617/364-2000x7304
Email: ...{adelie,bu-cs,harvax,inmet,mcsbos,mit-eddie,mot[bos]}!cdx39!{jc,news,root,usenet,uucp}
Codex Corporation; Mailstop C1-30; 20 Cabot Blvd; Mansfield MA 02048-1193

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.41.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.43.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
<DOC>
<DOCNO>WT11-B28-120</DOCNO>
<DOCOLDNO>IA012-000125-B046-206</DOCOLDNO>
<DOCHDR>
http://catless.ncl.ac.uk:80/Risks/4.43.html 128.240.150.127 19970217010904 text/html 18641
HTTP/1.0 200 OK
MIME-Version: 1.0
Date: Mon, 17 Feb 1997 01:07:28 GMT
Server: phttpd/0.99.72
Content-Type: text/html
</DOCHDR>
<HTML><HEAD><TITLE>The Risks Digest Volume 4: Issue 43</TITLE>
<LINK REL="Prev" HREF="/Risks/4.42.html">
<LINK REL="Up" HREF="/Risks/index.4.html">
<LINK REL="Next" HREF="/Risks/4.44.html">
<LINK REL="Help" HREF="/Risks">
<LINK REV="made" HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">
</HEAD><BODY><HR>
<A HREF="/Risks/4.42.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.44.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
<H1>The Risks Digest Volume 4: Issue 43</H1>
<H2> Monday, 26 January 1987 </H2>
    <H3>Forum on Risks to the Public in Computers and Related Systems</H3>
    <I><A HREF="http://www.acm.org">ACM</A>
Committee on Computers and Public Policy, Peter G. Neumann, moderator</I>
<H2>Contents</H2><DL>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj1">
"Cable `Hackers' Claim Scrambler is History"; other breaches 
</A>
<DD>
<A HREF="#subj1.1">
PGN
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj2">
  Re: VideoCypher II 
</A>
<DD>
<A HREF="#subj2.1">
Michael Grant
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj3">
  Re: DES cracked? 
</A>
<DD>
<A HREF="#subj3.1">
Douglas Humphrey
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj4">
  Re: Billions 
</A>
<DD>
<A HREF="#subj4.1">
Brian Randell
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj5">
  GM On-Board Computers 
</A>
<DD>
<A HREF="#subj5.1">
Wes Williams
</A><br>
<DT><IMG SRC="/Images/redball.gif" ALT= "o"><A HREF="#subj6">
  Active control of skyscrapers 
</A>
<DD>
<A HREF="#subj6.1">
Peter G. Capek
</A><br>
</DL>

<A NAME="subj1"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj1.1">
"Cable `Hackers' Claim Scrambler is History"; other breaches
</A>
</H3>
<address>
Peter G. Neumann 
&lt;<A HREF="mailto:Neumann@CSL.SRI.COM">
Neumann@CSL.SRI.COM
</A>&gt;
</address>
<i>
Mon 26 Jan 87 21:05:14-PST
</i><PRE>
To: RISKS@CSL.SRI.COM

  SF Chron 26 Jan 87, page 3 (from UPI):

  A year-old "unbreakable" scrambler that has kept satellite dish owners
  from receiving pay television channels free has been broken...

The article describes the "Three Musketeers" chip, which you can use to
replace a chip in the $395 decoder if you have any legitimate pay channel.
It then goes on to quote Captain Midnight, who claims that an even more
devastating breach has been discovered that does not even require the "Three
Musketeers" chip! He recommends you not waste your money on the hot chip.

By the way, recently SECURITY@RUTGERS has had quite a few items of interest
to RISKS readers.  Here are two:

  Given an Ethernet board, you can read ALL of the network traffic by
  flipping a single bit.

  A Sun System security breach was described, compromised via unpassworded
  special accounts.

  Some of the experiments with Gould's allegedly secure UNIX.

</PRE>
<A NAME="subj2"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj2.1">
Re: VideoCypher II
</A>
</H3>
<address>
Michael Grant 
&lt;<A HREF="mailto:mgrant@mimsy.umd.edu">
mgrant@mimsy.umd.edu
</A>&gt;
</address>
<i>
Sat, 24 Jan 87 12:24:06 EST
</i><PRE>

&gt;David Platt notes:
&gt;If, for example, the box had been provided with a cover-removal switch that
&gt;would signal the micro to erase it's subscriber number...

Always best to eliminate the problem by redesigning that part in the
next generation of the cypher so that such important numbers as that
never leave the internals of chips.  At that point, it becomes much
more of a pain to probe than it may be worth, but...not entirly imposible.

</PRE>
<A NAME="subj3"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj3.1">
Re: DES cracked?
</A>
</H3>
<address>
Douglas Humphrey 
&lt;<A HREF="mailto:deh@eneevax.umd.edu">
deh@eneevax.umd.edu
</A>&gt;
</address>
<i>
Sun, 25 Jan 87 14:42:09 EST
</i><PRE>

&gt;Way #3: someone's actually found a way of identifying the key of a DES
&gt;transmission, with (or possibly without) the unscrambled "plaintext"
&gt;audio as a starting point.

Note that they can easily have the plaintext, since the best way to
start experimenting on breaking something is to have two devices
there, one subscribed and authorized, and the other not. That way you
have (subject to trivial timing differences which can be ironed out)
two streams of data to play with, and you really are just trying to 
make one look like the other. 

On another note, does anyone know of any good spectrum analysis 
software available for cheap to work with reasonable priced A/D
converters ? There are a number of companies that sell the hardware
required to eat signals, but most of the software that I have seen
for actualy analysing the data is pretty weak. Maybe I'm just not
in touch with the right companies...
                                                  Doug

</PRE>
<A NAME="subj4"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj4.1">
Re: Billions
</A>
</H3>
<address>
Brian Randell 
&lt;<A HREF="mailto:brian%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK">
brian%kelpie.newcastle.ac.uk@Cs.Ucl.AC.UK
</A>&gt;
</address>
<i>
Mon, 26 Jan 87 18:37:17 gmt
</i><PRE>
ReSent-To: RISKS@CSL.SRI.COM

Oops! Sorry - I am usually more careful about transatlantic differences in
the meaning of "billion", though (regretfully) there is a growing tendency
for at least the popular newspapers in the UK to conform to US usage re
"billion", presumably because a "billion" is shorter and sounds more
impressive than "a thousandmillion" and few people know that the proper
English (or, if you insist, British) term for this is "milliard" - a term
which does not seem to exist in American.

In fact my Webster's Dictionary (I smuggled one into the UK with me when I
left IBM) tells me that above one million, all the names differ across the
Atlantic, even "septillion", "quattuordecillion", "novemdecillion", etc.

I wonder whether any actual (computer-based) risks have arisen to the public
from this confusion over billion - to match those that surely must have
arisen over imperial vs metric scales, celsius vs fahrenheit, etc.  For
example, Edsger Dijkstra told me once of a remote manipulator built for the
Anglo-Dutch firm Shell Oil which was usable only by a giant because it was
built in metres instead of feet.  And I recall, from my early days with the
Atomic Power Division of English Electric, that our nuclear reactor codes
had to deal with reactor designs in which the coolant entered a
heat-exchanger (from something designed by physicists) in degrees centigrade
(as it then was) and left (this domain of engineers) in degrees fahrenheit.

Cheers, Brian

  [One such case was the Discovery laser experiment, which aimed upward to a
   point 10,023 MILES above sea level instead of downward to a point 10,023
   FEET above sea level (a mountain top).  Another was the $.5M transaction
   that became $500M because of nonagreement on units.  Both (coincidentally) 
   are described in Software Engineering Notes vol 10 no 3, which appeared
   just before the on-line RISKS Forum began.  PGN]

</PRE>
<A NAME="subj5"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj5.1">
GM On-Board Computers                            [lightly edited]
</A>
</H3>
<address>
"Wes Williams" 
&lt;<A HREF="mailto:GZT.EWW%OZ.AI.MIT.EDU@XX.LCS.MIT.EDU">
GZT.EWW%OZ.AI.MIT.EDU@XX.LCS.MIT.EDU
</A>&gt;
</address>
<i>
Sat 24 Jan 87 11:20:49-EST
</i><PRE>
To: RISKS@CSL.SRI.COM

     As I have spent some time in the automotive repair field, I have come
across an anomaly when General Motors' main computer system repairs are
performed.  I share it with you here.
 
     In two years after 1980 ( the year when GM installed an on-board
computer on the vast majority of its models ) the repair facilities had a
tendency to replace the complete computer assembly rather than troubleshoot
the problem extensively.  This was the transition period.  Repair people
were unfamiliar with the approriate procedures and also had a tendency to
replace (Well I ain't ever done one of these before, boss!) rather than
understand and repair an associated problem.
 
     During these two years, I replaced only two computers.  One was from a
car involved in an electrical fire, the other was in a car that had
collision damage on the right side, close to the computer, and the computer
was damaged (visibly).
 
     In 1985 I was troubleshooting a 1981 Cadillac that had the infamous
8-6-4 engine with a power-on stutter.  I found a broken (cracked) distributor
cap and saw High voltage (30-60,000 volts) shooting from the cap to the lead
that was coming from the computer.  This was the electronic timing advance
control circuit.  I replaced the bad cap, retested the car, and found that
the problem was better but had not disappeared.  All other associated tests
were performed and no other problems were found except that the diagnostics
generated by the on-board computer were all out of whack.  On this model
Caddy, if you press the climate control buttons you will get a diagnostic
check run off by the cpu.  The readout comes out as two-digit numbers on the
temperature control. These numbers were never the same, and some were not
within the diagnostic capability of the cpu.
 
     I was now in the position of the other fellows and said, "Well, gotta
replace the cpu." A logical conclusion, knowing that the readout was not
right, as well as seeing high voltages heading for the cpu.
 
     I pulled the cpu, headed for GM parts and was shocked to learn that I
could not purchase a complete unit (proms included), I had to remove the old
proms and install them in the "rebuilt" computer.  Seemed a little dumb when
the cpu was subjected to high voltages, to keep the old proms.
 
     After the change of cpu's and installation of old proms, there was no
change in the operation of the engine. I quit and gave the car to Cadillac to
repair. They spent untold hours on it, communicated with the Caddy hot line,
had service reps around from the factory and made a large number of updates
to a variety of systems as well as unnecessary other changes.  Total bill?   
=   $0.00.  Even they couldn't fix it. It is running better, the stutter is
still there, the car is on the road and getting slightly lower than average
mileage.   (sigh)
 
     Summary:              To GM --&gt; Why can't one replace the proms to the
CPU. Are they burned in with detailed specific instructions according to each
cars engine performance?
 
             To the public--&gt; when a GM computer is replaced, the "core
charge" or trade-in on the malfunctioning cpu is close to $300.00, so that
drops the price of the cpu from $500.00 to $200.00.  Watch your bills here!!
(These figures are + or - $50.00 for the component only, not the labor.)
 
             To the technical types. --&gt; It would seem feasible to design a
program and attaching hardware to diagnose (at least one type (say GM)) of
an on-board computer with a P.C.  I know that Caddy spent at least 40 hours
on this problem.  At the labor rate of $38.00 per hour and knowing that there
are other similar occurrences, there has to be some money to be made in the
purchase of such a system as well as the sale.
 
   Quote 1: "Not knowing the answer is only being uneducated."

   Quote 2: "Not knowing where to look for the answer is being 'uninformed'."
 
   Quote 3: "When the product is a common one, and none know where to look
             for the answer, nor know it, this is truly ignorance."
 
</PRE>
<A NAME="subj6"> <IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"></A>
<H3><A NAME="subj6.1">
Active control of skyscrapers
</A>
</H3>
<address>
"Peter G. Capek"  
&lt;<A HREF="mailto:CAPEK@ibm.com">
CAPEK@ibm.com
</A>&gt;
</address>
<i>
26 January 1987, 20:37:17 EST
</i><PRE>

Catching up on my reading,  I noticed the recent discussion in RISKS about
active control of skyscrapers.  If this is still of interest, I offer the
following excerpts from an article I happened across some years ago and
clipped.  It appeared in Engineering News Record, August 18, 1977.

         TUNE MASS DAMPERS STEADY SWAY OF SKYSCRAPERS IN WIND

A 50-year-old idea of using the inertia of a heavy floating mass to tame the
sway of a tall building is now getting its first real tryout in New York
City and Boston skyscrapers.  Citicorp Center in New York and Boston's
Hancock Tower are newly fitted out with so-called tuned mass dampers, the
first in tall buildings in the U.S., according to the designers of the
systems, structural consultant LeMessurier Associates/SCI, Cambridge, Mass,
and MTS Systems Corp., the manufacturer, Minneapolis.

A tuned mass damper (TMD) consists of a heavy weight installed near a
building's top in such a way that it tends to remain still while the
building moves beneath it and in away that it can transmit this
inertia to the building's frame, thereby reducing the building's motion.

The mass itself need weigh only 0.25% to 0.75% of the building's total
weight.  When activated, it becomes free-floating (or "levitates" as its
designers like to say) by rising on a nearly frictionless film of oil.
Piston-like connectors, which are pneumatic springs in which pistons react
against compressed nitrogen, are attached both to the mass and the building
frame so that as the building sways away from the mass, the springs pull the
building pack to the center.

"Tuned" simply means the mass can be caused to move in a natural period
equal to the building's natural period so that it will be more effective in
counteracting the building's motion.  During a heavy wind storm, the mass
might appear to move in relation to the building some 2 to 4 ft.  ...

A TMD is a device to minimize the discomfort experienced by occupants when a
building is swaying.  As such, it can be used in place of adding structural
steel to stiffen a building or adding concerete to weigh it down, which
designers say is a much more costly way of reducing uncomfortable levels of
motion.  To the engineers who designed it, the TMD is a positive approach to
relieving wind-induced building motion because it counteracts motion rather
than first receiving it and then deadening it, which is the inefficient and
more costly result of substantially increasing mass or stiffness.  ...

A TMD's advantage becomes academic in a power failure.  It needs electricity
to work and if that's lost in a heavey wind storm, when the TMD would be
most needed, it won't work.  ...

The TMD designed for Citicorp's slender 914-foot tower in midtown Manhattan
has a mass block of concrete 30 x 30 x 10 feet, with cutouts for attachments,
that weighs 400 tons.  It has two spring-damping mechanisms, one to
counteract north-south motion and one for east-west motion.  It also has an
antiyaw device to prevent the mass block from twisting, a failsafe device
consisting of shock absorbers and sunbbers to resist excessive or eccentric
motion, and a control system that collects data on the building's motion
and controls the response of the mass.  It is located in a speciall designed
space in the building's 59th floor, which is supported by trusses below.
It is designed to activate at an acceleration of 3 milli-g's, which could
be caused by about a 40-mph wind, and it is designed to prevent the building
from deflecting more than 12 to 13 inches.

LeMessurier estimates Citicorp's TMD, which cost about $1.5 million, saved
overall a possible $3.5 to $4 million that would have been spent to add some
28,000 tons of structural steel to stiffen the frame and floor concrete to
add weight.

The TMD for the John Hancock Mutual Life Insurance Co.'s glass-clad landmark
in Boston is somewhat different.  First of all ... it was added as an
afterthought when architect I.M. Pei &amp; Partners realized that the building
had insufficient wind bracing to prevent occupant discomfort.  Secondly,
Hancock Tower is rectangular in plan and is a frame building, unlike
Citicorp's essentially bearing wall structure.  For Hancock, then,
LeMessurier placed two TMD's, one at either end of the 58th floor.  Because
of the building's shape and location, it must counteract mainly east-west
winds and a twisting force.  The dampers, then, move only in an east-west
direction and can be induced to work together or in opposition to stablize
the building.  They are located 220 feet apart, and when moving in
opposition act in effect as a 220-ft lever arm to resist twisting.  A
Hancock building official wouldn't reveal what it cost to add the dampers,
which designers say could reduce the building's swaying motion a full 40 to
50% under what it had originally been designed for.  ...

Peter G. Capek, IBM Research -- Yorktown Heights, New York

</PRE>

<IMG SRC="/Images/Lines/hrfadeblue.gif" ALT="---------------------------------------------"><P>
<A HREF="/Risks/4.42.html"><IMG SRC="/Images/Buttons/granite_left.gif" ALT="Previous Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/index.4.html"><IMG SRC="/Images/Buttons/granite_up.gif" ALT="Index" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks/4.44.html"><IMG SRC="/Images/Buttons/granite_right.gif" ALT="Next Issue" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks"><IMG SRC="/Images/Buttons/granite_i.gif" ALT="Info" WIDTH="30" HEIGHT="30"></A>
<A HREF="/Risks.data/search.html"><IMG SRC="/Images/Buttons/granite_qm.gif" ALT="Searching" WIDTH="30" HEIGHT="30"></A>
<A HREF="mailto:risks@csl.sri.com"><IMG SRC="/Images/Buttons/granite_submit.gif" ALT="Submit Article" WIDTH="30" HEIGHT="30"></A>
<HR>
Report problems with the web pages to <A HREF="mailto:Lindsay.Marshall@newcastle.ac.uk">Lindsay.Marshall@newcastle.ac.uk</A>.
</BODY></HTML>
</DOC>
